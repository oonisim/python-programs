{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9566e6c2",
   "metadata": {},
   "source": [
    "# Top2Vec\n",
    "\n",
    "* [Top2Vec Github](https://github.com/ddangelov/Top2Vec)\n",
    "\n",
    "## Tutorial\n",
    "\n",
    "* [The Best Way to do Topic Modeling in Python - Top2Vec Introduction and Tutorial](https://www.youtube.com/watch?v=bEaxKSQ4Av8&list=PL2VXyKi-KpYt4Bb2dDZZoBLG4SQkrAz9g) (MUST WATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01043827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3cef1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Top2Vec\n",
      "\n",
      "    Creates jointly embedded topic, document and word vectors.\n",
      "\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    documents: List of str\n",
      "        Input corpus, should be a list of strings.\n",
      "\n",
      "    min_count: int (Optional, default 50)\n",
      "        Ignores all words with total frequency lower than this. For smaller\n",
      "        corpora a smaller min_count will be necessary.\n",
      "\n",
      "    topic_merge_delta: float (default 0.1)\n",
      "        Merges topic vectors which have a cosine distance smaller than\n",
      "        topic_merge_delta using dbscan. The epsilon parameter of dbscan is\n",
      "        set to the topic_merge_delta.\n",
      "\n",
      "    ngram_vocab: bool (Optional, default False)\n",
      "        Add phrases to topic descriptions.\n",
      "\n",
      "        Uses gensim phrases to find common phrases in the corpus and adds them\n",
      "        to the vocabulary.\n",
      "\n",
      "        For more information visit:\n",
      "        https://radimrehurek.com/gensim/models/phrases.html\n",
      "\n",
      "    ngram_vocab_args: dict (Optional, default None)\n",
      "        Pass custom arguments to gensim phrases.\n",
      "\n",
      "        For more information visit:\n",
      "        https://radimrehurek.com/gensim/models/phrases.html\n",
      "\n",
      "    embedding_model: string or callable\n",
      "        This will determine which model is used to generate the document and\n",
      "        word embeddings. The valid string options are:\n",
      "\n",
      "            * doc2vec\n",
      "            * universal-sentence-encoder\n",
      "            * universal-sentence-encoder-large\n",
      "            * universal-sentence-encoder-multilingual\n",
      "            * universal-sentence-encoder-multilingual-large\n",
      "            * distiluse-base-multilingual-cased\n",
      "            * all-MiniLM-L6-v2\n",
      "            * paraphrase-multilingual-MiniLM-L12-v2\n",
      "\n",
      "        For large data sets and data sets with very unique vocabulary doc2vec\n",
      "        could produce better results. This will train a doc2vec model from\n",
      "        scratch. This method is language agnostic. However multiple languages\n",
      "        will not be aligned.\n",
      "\n",
      "        Using the universal sentence encoder options will be much faster since\n",
      "        those are pre-trained and efficient models. The universal sentence\n",
      "        encoder options are suggested for smaller data sets. They are also\n",
      "        good options for large data sets that are in English or in languages\n",
      "        covered by the multilingual model. It is also suggested for data sets\n",
      "        that are multilingual.\n",
      "\n",
      "        For more information on universal-sentence-encoder options visit:\n",
      "        https://tfhub.dev/google/collections/universal-sentence-encoder/1\n",
      "\n",
      "        The SBERT pre-trained sentence transformer options are\n",
      "        distiluse-base-multilingual-cased,\n",
      "        paraphrase-multilingual-MiniLM-L12-v2, and all-MiniLM-L6-v2.\n",
      "\n",
      "        The distiluse-base-multilingual-cased and\n",
      "        paraphrase-multilingual-MiniLM-L12-v2 are suggested for multilingual\n",
      "        datasets and languages that are not\n",
      "        covered by the multilingual universal sentence encoder. The\n",
      "        transformer is significantly slower than the universal sentence\n",
      "        encoder options(except for the large options).\n",
      "\n",
      "        For more information on SBERT options visit:\n",
      "        https://www.sbert.net/docs/pretrained_models.html\n",
      "\n",
      "        If passing a callable embedding_model note that it will not be saved\n",
      "        when saving a top2vec model. After loading such a saved top2vec model\n",
      "        the set_embedding_model method will need to be called and the same\n",
      "        embedding_model callable used during training must be passed to it.\n",
      "\n",
      "    embedding_model_path: string (Optional)\n",
      "        Pre-trained embedding models will be downloaded automatically by\n",
      "        default. However they can also be uploaded from a file that is in the\n",
      "        location of embedding_model_path.\n",
      "\n",
      "        Warning: the model at embedding_model_path must match the\n",
      "        embedding_model parameter type.\n",
      "\n",
      "    embedding_batch_size: int (default=32)\n",
      "        Batch size for documents being embedded.\n",
      "\n",
      "    split_documents: bool (default False)\n",
      "        If set to True, documents will be split into parts before embedding.\n",
      "        After embedding the multiple document part embeddings will be averaged\n",
      "        to create a single embedding per document. This is useful when documents\n",
      "        are very large or when the embedding model has a token limit.\n",
      "\n",
      "        Document chunking or a senticizer can be used for document splitting.\n",
      "\n",
      "    document_chunker: string or callable (default 'sequential')\n",
      "        This will break the document into chunks. The valid string options are:\n",
      "\n",
      "            * sequential\n",
      "            * random\n",
      "\n",
      "        The sequential chunker will split the document into chunks of specified\n",
      "        length and ratio of overlap. This is the recommended method.\n",
      "\n",
      "        The random chunking option will take random chunks of specified length\n",
      "        from the document. These can overlap and should be thought of as\n",
      "        sampling chunks with replacement from the document.\n",
      "\n",
      "        If a callable is passed it must take as input a list of tokens of\n",
      "        a document and return a list of strings representing the resulting\n",
      "        document chunks.\n",
      "\n",
      "        Only one of document_chunker or sentincizer should be used.\n",
      "\n",
      "    chunk_length: int (default 100)\n",
      "        The number of tokens per document chunk if using the document chunker\n",
      "        string options.\n",
      "\n",
      "    max_num_chunks: int (Optional)\n",
      "        The maximum number of chunks generated per document if using the\n",
      "        document chunker string options.\n",
      "\n",
      "    chunk_overlap_ratio: float (default 0.5)\n",
      "        Only applies to the 'sequential' document chunker.\n",
      "\n",
      "        Fraction of overlapping tokens between sequential chunks. A value of\n",
      "        0 will result i no overlap, where as 0.5 will overlap half of the\n",
      "        previous chunk.\n",
      "\n",
      "    chunk_len_coverage_ratio: float (default 1.0)\n",
      "        Only applies to the 'random' document chunker option.\n",
      "\n",
      "        Proportion of token length that will be covered by chunks. Default\n",
      "        value of 1.0 means chunk lengths will add up to number of tokens of\n",
      "        the document. This does not mean all tokens will be covered since\n",
      "        chunks can be overlapping.\n",
      "\n",
      "    sentencizer: callable (Optional)\n",
      "        A sentincizer callable can be passed. The input should be a string\n",
      "        representing the document and the output should be a list of strings\n",
      "        representing the document sentence chunks.\n",
      "\n",
      "        Only one of document_chunker or sentincizer should be used.\n",
      "\n",
      "    speed: string (Optional, default 'learn')\n",
      "\n",
      "        This parameter is only used when using doc2vec as embedding_model.\n",
      "\n",
      "        It will determine how fast the model takes to train. The\n",
      "        fast-learn option is the fastest and will generate the lowest quality\n",
      "        vectors. The learn option will learn better quality vectors but take\n",
      "        a longer time to train. The deep-learn option will learn the best\n",
      "        quality vectors but will take significant time to train. The valid\n",
      "        string speed options are:\n",
      "        \n",
      "            * fast-learn\n",
      "            * learn\n",
      "            * deep-learn\n",
      "\n",
      "    use_corpus_file: bool (Optional, default False)\n",
      "\n",
      "        This parameter is only used when using doc2vec as embedding_model.\n",
      "\n",
      "        Setting use_corpus_file to True can sometimes provide speedup for\n",
      "        large datasets when multiple worker threads are available. Documents\n",
      "        are still passed to the model as a list of str, the model will create\n",
      "        a temporary corpus file for training.\n",
      "\n",
      "    document_ids: List of str, int (Optional)\n",
      "        A unique value per document that will be used for referring to\n",
      "        documents in search results. If ids are not given to the model, the\n",
      "        index of each document in the original corpus will become the id.\n",
      "\n",
      "    keep_documents: bool (Optional, default True)\n",
      "        If set to False documents will only be used for training and not saved\n",
      "        as part of the model. This will reduce model size. When using search\n",
      "        functions only document ids will be returned, not the actual\n",
      "        documents.\n",
      "\n",
      "    workers: int (Optional)\n",
      "        The amount of worker threads to be used in training the model. Larger\n",
      "        amount will lead to faster training.\n",
      "    \n",
      "    tokenizer: callable (Optional, default None)\n",
      "        Override the default tokenization method. If None then\n",
      "        gensim.utils.simple_preprocess will be used.\n",
      "\n",
      "        Tokenizer must take a document and return a list of tokens.\n",
      "\n",
      "    use_embedding_model_tokenizer: bool (Optional, default False)\n",
      "        If using an embedding model other than doc2vec, use the model's\n",
      "        tokenizer for document embedding. If set to True the tokenizer, either\n",
      "        default or passed callable will be used to tokenize the text to\n",
      "        extract the vocabulary for word embedding.\n",
      "\n",
      "    umap_args: dict (Optional, default None)\n",
      "        Pass custom arguments to UMAP.\n",
      "\n",
      "    hdbscan_args: dict (Optional, default None)\n",
      "        Pass custom arguments to HDBSCAN.\n",
      "    \n",
      "    verbose: bool (Optional, default True)\n",
      "        Whether to print status data during training.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(Top2Vec.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054bf6e8",
   "metadata": {},
   "source": [
    "# Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce3c0d",
   "metadata": {},
   "source": [
    "### topic_num\n",
    "* topic_num: ID of a topic. If there are 12 topics identified, then one in 0 to 11.\n",
    "* topic_nums: List of topic IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa55033",
   "metadata": {},
   "source": [
    "### document_id\n",
    "\n",
    "By default, top2vec assign a sequential number to each document as it appears in the ```documents``` provided. A custom ID (string or in) can be specified.\n",
    "\n",
    "> **document_ids: List of str, int (Optional)**  \n",
    ">     A unique value per document that will be used for referring to documents in search results. If ids are not given to the model, the index of each document in the original corpus will become the id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57daa480",
   "metadata": {},
   "source": [
    "### min_count\n",
    "\n",
    "* [What is the Top2Vec Speed Parameter (fast-learn, learn, and deep-learn) - Top2Vec and Python 01](https://youtu.be/rmWI3xu9SII?list=PL2VXyKi-KpYt4Bb2dDZZoBLG4SQkrAz9g&t=547)\n",
    "\n",
    "A word must appear more than ```min_count``` times in the entire documents to be taken into account in the model. If set to ```1```, every word will be adapted. If the number of documents is small, lower it.\n",
    "\n",
    "### ngram_vocab\n",
    "\n",
    "To capture a bi-gram entity such as **Free Trade**, **Status Quo** that makes a sense as a group of words. Top2Vec only support bi-gram, not N-gram.\n",
    "\n",
    "### speed\n",
    "\n",
    "Only applicable to **doc2vec as embedding_modelmodel**. Fast training does not generate good topic detections with large clusters. The snapshot shows the ```fast``` option gives large cluster of 8667 documents having coarse granularity, whereas ```learn``` option gives a finer gained cluster of 774 documents.\n",
    "\n",
    "Fine grained cluster gives more differntiative power with similar/overwrapping documents only in the cluster. Corase grained cluster can have different topics within that can be differntiated with finer grained clusters.\n",
    "\n",
    "See [What is the Top2Vec Speed Parameter (fast-learn, learn, and deep-learn) - Top2Vec and Python 01](https://youtu.be/rmWI3xu9SII?list=PL2VXyKi-KpYt4Bb2dDZZoBLG4SQkrAz9g&t=1294) for details.\n",
    "\n",
    "\n",
    "<img src=\"./image/top2vec_speed_effect.png\" align=\"left\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d261dce5",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf7b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(linewidth=1024)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "pd.set_option(\"max_seq_items\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a676394e",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "* [Kaggle News Articles Categorization data](https://www.kaggle.com/competitions/learn-ai-bbc/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc5536b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (£5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german business confidence fell in february knocking hopes of a speedy recovery in europe s largest economy.  munich-based research institute ifo said that its confidence index fell to 95.5 in february from 97.5 in january  its first decline in three months. the study found that the outlook in both the manufacturing and retail sectors had worsened. observers had been hoping that a more confident business sector would signal that economic activity was picking up.   we re surprised that the ifo index has taken such a knock   said dz bank economist bernd weidensteiner.  the main reason is probably that the domestic economy is still weak  particularly in the retail trade.  economy and labour minister wolfgang clement called the dip in february s ifo confidence figure  a very mild decline . he said that despite the retreat  the index remained at a relatively high level and that he expected  a modest economic upswing  to continue.  germany s economy grew 1.6% last year after shrinking in 2003. however  the economy contracted by 0.2% during the last three months of 2004  mainly due to the reluctance of consumers to spend. latest indications are that growth is still proving elusive and ifo president hans-werner sinn said any improvement in german domestic demand was sluggish. exports had kept things going during the first half of 2004  but demand for exports was then hit as the value of the euro hit record levels making german products less competitive overseas. on top of that  the unemployment rate has been stuck at close to 10% and manufacturing firms  including daimlerchrysler  siemens and volkswagen  have been negotiating with unions over cost cutting measures. analysts said that the ifo figures and germany s continuing problems may delay an interest rate rise by the european central bank. eurozone interest rates are at 2%  but comments from senior officials have recently focused on the threat of inflation  prompting fears that interest rates may rise.</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in a majority of nations surveyed in a bbc world service poll believe the world economy is worsening.  most respondents also said their national economy was getting worse. but when asked about their own family s financial outlook  a majority in 14 countries said they were positive about the future. almost 23 000 people in 22 countries were questioned for the poll  which was mostly conducted before the asian tsunami disaster. the poll found that a majority or plurality of people in 13 countries believed the economy was going downhill  compared with respondents in nine countries who believed it was improving. those surveyed in three countries were split. in percentage terms  an average of 44% of respondents in each country said the world economy was getting worse  compared to 34% who said it was improving. similarly  48% were pessimistic about their national economy  while 41% were optimistic. and 47% saw their family s economic conditions improving  as against 36% who said they were getting worse.  the poll of 22 953 people was conducted by the international polling firm globescan  together with the program on international policy attitudes (pipa) at the university of maryland.  while the world economy has picked up from difficult times just a few years ago  people seem to not have fully absorbed this development  though they are personally experiencing its effects   said pipa director steven kull.  people around the world are saying:  i m ok  but the world isn t .  there may be a perception that war  terrorism and religious and political divisions are making the world a worse place  even though that has not so far been reflected in global economic performance  says the bbc s elizabeth blunt.  the countries where people were most optimistic  both for the world and for their own families  were two fast-growing developing economies  china and india  followed by indonesia. china has seen two decades of blistering economic growth  which has led to wealth creation on a huge scale  says the bbc s louisa lim in beijing. but the results also may reflect the untrammelled confidence of people who are subject to endless government propaganda about their country s rosy economic future  our correspondent says. south korea was the most pessimistic  while respondents in italy and mexico were also quite gloomy. the bbc s david willey in rome says one reason for that result is the changeover from the lira to the euro in 2001  which is widely viewed as the biggest reason why their wages and salaries are worth less than they used to be. the philippines was among the most upbeat countries on prospects for respondents  families  but one of the most pessimistic about the world economy. pipa conducted the poll from 15 november 2004 to 3 january 2005 across 22 countries in face-to-face or telephone interviews. the interviews took place between 15 november 2004 and 5 january 2005. the margin of error is between 2.5 and 4 points  depending on the country. in eight of the countries  the sample was limited to major metropolitan areas.</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId  \\\n",
       "0       1833   \n",
       "1        154   \n",
       "2       1101   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (£5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  german business confidence slides german business confidence fell in february knocking hopes of a speedy recovery in europe s largest economy.  munich-based research institute ifo said that its confidence index fell to 95.5 in february from 97.5 in january  its first decline in three months. the study found that the outlook in both the manufacturing and retail sectors had worsened. observers had been hoping that a more confident business sector would signal that economic activity was picking up.   we re surprised that the ifo index has taken such a knock   said dz bank economist bernd weidensteiner.  the main reason is probably that the domestic economy is still weak  particularly in the retail trade.  economy and labour minister wolfgang clement called the dip in february s ifo confidence figure  a very mild decline . he said that despite the retreat  the index remained at a relatively high level and that he expected  a modest economic upswing  to continue.  germany s economy grew 1.6% last year after shrinking in 2003. however  the economy contracted by 0.2% during the last three months of 2004  mainly due to the reluctance of consumers to spend. latest indications are that growth is still proving elusive and ifo president hans-werner sinn said any improvement in german domestic demand was sluggish. exports had kept things going during the first half of 2004  but demand for exports was then hit as the value of the euro hit record levels making german products less competitive overseas. on top of that  the unemployment rate has been stuck at close to 10% and manufacturing firms  including daimlerchrysler  siemens and volkswagen  have been negotiating with unions over cost cutting measures. analysts said that the ifo figures and germany s continuing problems may delay an interest rate rise by the european central bank. eurozone interest rates are at 2%  but comments from senior officials have recently focused on the threat of inflation  prompting fears that interest rates may rise.   \n",
       "2  bbc poll indicates economic gloom citizens in a majority of nations surveyed in a bbc world service poll believe the world economy is worsening.  most respondents also said their national economy was getting worse. but when asked about their own family s financial outlook  a majority in 14 countries said they were positive about the future. almost 23 000 people in 22 countries were questioned for the poll  which was mostly conducted before the asian tsunami disaster. the poll found that a majority or plurality of people in 13 countries believed the economy was going downhill  compared with respondents in nine countries who believed it was improving. those surveyed in three countries were split. in percentage terms  an average of 44% of respondents in each country said the world economy was getting worse  compared to 34% who said it was improving. similarly  48% were pessimistic about their national economy  while 41% were optimistic. and 47% saw their family s economic conditions improving  as against 36% who said they were getting worse.  the poll of 22 953 people was conducted by the international polling firm globescan  together with the program on international policy attitudes (pipa) at the university of maryland.  while the world economy has picked up from difficult times just a few years ago  people seem to not have fully absorbed this development  though they are personally experiencing its effects   said pipa director steven kull.  people around the world are saying:  i m ok  but the world isn t .  there may be a perception that war  terrorism and religious and political divisions are making the world a worse place  even though that has not so far been reflected in global economic performance  says the bbc s elizabeth blunt.  the countries where people were most optimistic  both for the world and for their own families  were two fast-growing developing economies  china and india  followed by indonesia. china has seen two decades of blistering economic growth  which has led to wealth creation on a huge scale  says the bbc s louisa lim in beijing. but the results also may reflect the untrammelled confidence of people who are subject to endless government propaganda about their country s rosy economic future  our correspondent says. south korea was the most pessimistic  while respondents in italy and mexico were also quite gloomy. the bbc s david willey in rome says one reason for that result is the changeover from the lira to the euro in 2001  which is widely viewed as the biggest reason why their wages and salaries are worth less than they used to be. the philippines was among the most upbeat countries on prospects for respondents  families  but one of the most pessimistic about the world economy. pipa conducted the poll from 15 november 2004 to 3 january 2005 across 22 countries in face-to-face or telephone interviews. the interviews took place between 15 november 2004 and 5 january 2005. the margin of error is between 2.5 and 4 points  depending on the country. in eight of the countries  the sample was limited to major metropolitan areas.   \n",
       "\n",
       "   Category  \n",
       "0  business  \n",
       "1  business  \n",
       "2  business  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc = pd.read_csv(\"/Volumes/SSD/kaggle/bbc/BBCNewsTrain.csv\")\n",
    "bbc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d6f28",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4921133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 18:19:14,590 - top2vec - INFO - Pre-processing documents for training\n",
      "2023-03-19 18:19:15,602 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-03-19 18:23:15,565 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-03-19 18:23:18,819 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-03-19 18:23:18,842 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "# Does not work\n",
    "# model = Top2Vec(bbc['Text'].tolist(), embedding_model='universal-sentence-encoder', speed=\"learn\", workers=8)\n",
    "model = Top2Vec(\n",
    "    documents=bbc['Text'].tolist(),\n",
    "    document_ids=bbc.index.tolist(),\n",
    "    # embedding_model='distiluse-base-multilingual-cased',\n",
    "    embedding_model='doc2vec',\n",
    "    speed='deep-learn',\n",
    "    ngram_vocab=True,         # Detect bi-gram\n",
    "    workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a8bfdb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of vocabulary: 1465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the', 'to', 'of', 'and', 'in', 'for', 'is', 'that', 'it', 'on']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"number of vocabulary: {len(model.vocab)}\")\n",
    "model.vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "54b7068e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slotnames__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_assign_documents_to_topic',\n",
       " '_calculate_documents_topic',\n",
       " '_calculate_topic_sizes',\n",
       " '_check_document_index_status',\n",
       " '_check_hnswlib_status',\n",
       " '_check_import_status',\n",
       " '_check_model_status',\n",
       " '_check_word_index_status',\n",
       " '_create_topic_vectors',\n",
       " '_deduplicate_topics',\n",
       " '_embed_documents',\n",
       " '_embed_query',\n",
       " '_find_topic_words_and_scores',\n",
       " '_get_combined_vec',\n",
       " '_get_document_ids',\n",
       " '_get_document_indexes',\n",
       " '_l2_normalize',\n",
       " '_less_than_zero',\n",
       " '_reorder_topics',\n",
       " '_search_vectors_by_vector',\n",
       " '_unassign_documents_from_topic',\n",
       " '_validate_doc_ids',\n",
       " '_validate_document_ids_add_doc',\n",
       " '_validate_documents',\n",
       " '_validate_hierarchical_reduction',\n",
       " '_validate_hierarchical_reduction_num_topics',\n",
       " '_validate_keywords',\n",
       " '_validate_num_docs',\n",
       " '_validate_num_topics',\n",
       " '_validate_query',\n",
       " '_validate_topic_num',\n",
       " '_validate_topic_search',\n",
       " '_validate_vector',\n",
       " '_words2word_vectors',\n",
       " 'add_documents',\n",
       " 'change_to_download_embedding_model',\n",
       " 'compute_topics',\n",
       " 'delete_documents',\n",
       " 'doc_dist',\n",
       " 'doc_dist_reduced',\n",
       " 'doc_id2index',\n",
       " 'doc_id2index_id',\n",
       " 'doc_id_type',\n",
       " 'doc_top',\n",
       " 'doc_top_reduced',\n",
       " 'document_ids',\n",
       " 'document_ids_provided',\n",
       " 'document_index',\n",
       " 'document_vectors',\n",
       " 'documents',\n",
       " 'documents_indexed',\n",
       " 'embedding_model',\n",
       " 'embedding_model_path',\n",
       " 'generate_topic_wordcloud',\n",
       " 'get_documents_topics',\n",
       " 'get_num_topics',\n",
       " 'get_topic_hierarchy',\n",
       " 'get_topic_sizes',\n",
       " 'get_topics',\n",
       " 'hierarchical_topic_reduction',\n",
       " 'hierarchy',\n",
       " 'index_document_vectors',\n",
       " 'index_id2doc_id',\n",
       " 'index_word_vectors',\n",
       " 'load',\n",
       " 'model',\n",
       " 'query_documents',\n",
       " 'query_topics',\n",
       " 'save',\n",
       " 'search_documents_by_documents',\n",
       " 'search_documents_by_keywords',\n",
       " 'search_documents_by_topic',\n",
       " 'search_documents_by_vector',\n",
       " 'search_topics',\n",
       " 'search_topics_by_vector',\n",
       " 'search_words_by_vector',\n",
       " 'serialized_document_index',\n",
       " 'serialized_word_index',\n",
       " 'set_embedding_model',\n",
       " 'similar_words',\n",
       " 'topic_sizes',\n",
       " 'topic_sizes_reduced',\n",
       " 'topic_vectors',\n",
       " 'topic_vectors_reduced',\n",
       " 'topic_word_scores',\n",
       " 'topic_word_scores_reduced',\n",
       " 'topic_words',\n",
       " 'topic_words_reduced',\n",
       " 'update_embedding_model_path',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'word_index',\n",
       " 'word_indexes',\n",
       " 'word_vectors',\n",
       " 'words_indexed']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52487fa8",
   "metadata": {},
   "source": [
    "---\n",
    "# Topic\n",
    "\n",
    "**Topic (Topic Vector)** is a mean of a document vector cluster identified by HDBSCAN. It is a **thought vector** that is identified by the context words (topic words) nearby, but NOT a concrete word or sentence. It is desirable to have one specific categorical keyword that represents each topic, e.g. **Sport** for the 2nd topic but such distilation is not available in top2vec.\n",
    "\n",
    "<img src=\"./image/top2vec_topic_vector.png\" align=\"left\" width=600/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41008b5",
   "metadata": {},
   "source": [
    "## Number of topics identified in the documents\n",
    "\n",
    "with  ```embedding_model='distiluse-base-multilingual-cased'```, the number of topic identified is 2, whereas ```embedding_model='doc2vec'``` gives 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55b59ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of topics identified:[10]\n"
     ]
    }
   ],
   "source": [
    "topic_sizes, topic_ids = model.get_topic_sizes()\n",
    "print(f\"number of topics identified:[{len(topic_ids)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a722242",
   "metadata": {},
   "source": [
    "## Topic words \n",
    "\n",
    "List the context words that identify each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "821821a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Topic ID:0\n",
      "--------------------------------------------------------------------------------\n",
      "economic             0.4321683347225189\n",
      "economy              0.4101059138774872\n",
      "government           0.3722527027130127\n",
      "bn                   0.371192067861557\n",
      "oil                  0.3650941848754883\n",
      "exports              0.36492565274238586\n",
      "shares               0.3535774052143097\n",
      "labour               0.34154045581817627\n",
      "inflation            0.3408378064632416\n",
      "minister             0.33825230598449707\n",
      "--------------------------------------------------------------------------------\n",
      "Topic ID:1\n",
      "--------------------------------------------------------------------------------\n",
      "film                 0.5324764251708984\n",
      "comedy               0.5223538875579834\n",
      "oscar                0.5166749358177185\n",
      "nominations          0.5092459321022034\n",
      "stars                0.4950474202632904\n",
      "awards               0.48706600069999695\n",
      "starring             0.47755682468414307\n",
      "hollywood            0.4701189696788788\n",
      "singer               0.4687974452972412\n",
      "nominated            0.465273380279541\n",
      "--------------------------------------------------------------------------------\n",
      "Topic ID:2\n",
      "--------------------------------------------------------------------------------\n",
      "devices              0.5407640933990479\n",
      "mobile               0.529758095741272\n",
      "portable             0.5294241309165955\n",
      "mobiles              0.5123735666275024\n",
      "wireless             0.48670122027397156\n",
      "technology           0.48194894194602966\n",
      "phones               0.4797186553478241\n",
      "digital              0.47395065426826477\n",
      "technologies         0.47238248586654663\n",
      "phone                0.4611557126045227\n",
      "--------------------------------------------------------------------------------\n",
      "Topic ID:3\n",
      "--------------------------------------------------------------------------------\n",
      "arsenal              0.6122004985809326\n",
      "chelsea              0.5805642008781433\n",
      "mourinho             0.5695348978042603\n",
      "league               0.5692119002342224\n",
      "champions            0.5577548742294312\n",
      "wenger               0.5287430882453918\n",
      "premiership          0.5284871459007263\n",
      "goals                0.500278651714325\n",
      "boss                 0.4835274815559387\n",
      "liverpool            0.47825106978416443\n",
      "--------------------------------------------------------------------------------\n",
      "Topic ID:4\n",
      "--------------------------------------------------------------------------------\n",
      "spyware              0.5310124754905701\n",
      "viruses              0.5242020487785339\n",
      "websites             0.5235411524772644\n",
      "spam                 0.5228614211082458\n",
      "web                  0.5221444368362427\n",
      "users                0.5167090892791748\n",
      "programs             0.5148517489433289\n",
      "mail                 0.5033854246139526\n",
      "windows              0.4891601800918579\n",
      "virus                0.4853527247905731\n",
      "--------------------------------------------------------------------------------\n",
      "Topic ID:5\n",
      "--------------------------------------------------------------------------------\n",
      "coach                0.5831032395362854\n",
      "england              0.575279176235199\n",
      "captain              0.5666446685791016\n",
      "rugby                0.561418890953064\n",
      "championship         0.5580590963363647\n",
      "squad                0.5480310320854187\n",
      "robinson             0.5430912971496582\n",
      "wales                0.5039474368095398\n",
      "side                 0.49633777141571045\n",
      "slam                 0.47445493936538696\n",
      "--------------------------------------------------------------------------------\n",
      "Topic ID:6\n",
      "--------------------------------------------------------------------------------\n",
      "seed                 0.6183067560195923\n",
      "tennis               0.6112361550331116\n",
      "roddick              0.5917099118232727\n",
      "tournament           0.5271571278572083\n",
      "hewitt               0.5142931938171387\n",
      "champion             0.5136197805404663\n",
      "slam                 0.5131210684776306\n",
      "australian           0.511099636554718\n",
      "nadal                0.5004326701164246\n",
      "open                 0.46046820282936096\n",
      "--------------------------------------------------------------------------------\n",
      "Topic ID:7\n",
      "--------------------------------------------------------------------------------\n",
      "championships        0.7732793688774109\n",
      "indoor               0.7142432332038879\n",
      "olympic              0.680173397064209\n",
      "champion             0.6566736102104187\n",
      "holmes               0.6019967794418335\n",
      "athens               0.5261711478233337\n",
      "race                 0.48241642117500305\n",
      "gold                 0.42617067694664\n",
      "season               0.40280577540397644\n",
      "seconds              0.3952956795692444\n",
      "--------------------------------------------------------------------------------\n",
      "Topic ID:8\n",
      "--------------------------------------------------------------------------------\n",
      "athens               0.6861785054206848\n",
      "drugs                0.6753924489021301\n",
      "tests                0.5701422691345215\n",
      "test                 0.5051783323287964\n",
      "olympic              0.48647210001945496\n",
      "sport                0.4528025686740875\n",
      "missing              0.4342500567436218\n",
      "charges              0.4112967848777771\n",
      "body                 0.37859103083610535\n",
      "pair                 0.36527085304260254\n",
      "--------------------------------------------------------------------------------\n",
      "Topic ID:9\n",
      "--------------------------------------------------------------------------------\n",
      "ball                 0.7389965653419495\n",
      "penalty              0.7154800891876221\n",
      "kick                 0.6911351680755615\n",
      "minutes              0.6766492128372192\n",
      "goal                 0.6290631890296936\n",
      "scored               0.5255498886108398\n",
      "half                 0.4793589413166046\n",
      "side                 0.4680650234222412\n",
      "minute               0.46197715401649475\n",
      "pass                 0.4459457993507385\n"
     ]
    }
   ],
   "source": [
    "topics_words, topic_scores, topic_ids = model.get_topics()\n",
    "for topic_id, words, scores in zip(topic_ids, topics_words, topic_scores):\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Topic ID:{topic_id}\")\n",
    "    print(\"-\" * 80)\n",
    "    for word, score in zip(words[:10], scores[:10]):\n",
    "        print(f\"{word:20} {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d58a2f5",
   "metadata": {},
   "source": [
    "## Topics of a document\n",
    "\n",
    "Find the **Topic Words** that identify the **Topic** that is closest to the query document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "319afac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'german business confidence slides german business confidence fell in february knocking hopes of a speedy recovery in europe s largest economy.  munich-based research institute ifo said that its confidence index fell to 95.5 in february from 97.5 in january  its first decline in three months. the study found that the outlook in both the manufacturing and retail sectors had worsened. observers had been hoping that a more confident business sector would signal that economic activity was picking up.   we re surprised that the ifo index has taken such a knock   said dz bank economist bernd weidensteiner.  the main reason is probably that the domestic economy is still weak  particularly in the retail trade.  economy and labour minister wolfgang clement called the dip in february s ifo confidence figure  a very mild decline . he said that despite the retreat  the index remained at a relatively high level and that he expected  a modest economic upswing  to continue.  germany s economy grew 1.6% last year after shrinking in 2003. however  the economy contracted by 0.2% during the last three months of 2004  mainly due to the reluctance of consumers to spend. latest indications are that growth is still proving elusive and ifo president hans-werner sinn said any improvement in german domestic demand was sluggish. exports had kept things going during the first half of 2004  but demand for exports was then hit as the value of the euro hit record levels making german products less competitive overseas. on top of that  the unemployment rate has been stuck at close to 10% and manufacturing firms  including daimlerchrysler  siemens and volkswagen  have been negotiating with unions over cost cutting measures. analysts said that the ifo figures and germany s continuing problems may delay an interest rate rise by the european central bank. eurozone interest rates are at 2%  but comments from senior officials have recently focused on the threat of inflation  prompting fears that interest rates may rise.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id = 1\n",
    "query = bbc.iloc[document_id]['Text']\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad4d1fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_nums:[0], topic_score: [0.35290834]\n",
      "economic            : 0.4321683347225189\n",
      "economy             : 0.4101059138774872\n",
      "government          : 0.3722527027130127\n",
      "bn                  : 0.371192067861557\n",
      "oil                 : 0.3650941848754883\n",
      "exports             : 0.36492565274238586\n",
      "shares              : 0.3535774052143097\n",
      "labour              : 0.34154045581817627\n",
      "inflation           : 0.3408378064632416\n",
      "minister            : 0.33825230598449707\n"
     ]
    }
   ],
   "source": [
    "topic_nums, topic_score, topics_words, word_scores = model.get_documents_topics([document_id], reduced=False)\n",
    "print(f\"topic_nums:{topic_nums}, topic_score: {topic_score}\")\n",
    "for word, score in zip(topics_words[0][:10], word_scores[0][:10]):\n",
    "    print(f\"{word:20}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e9a83a",
   "metadata": {},
   "source": [
    "Use text query instead of ```document_id```. ```topic_nums``` is a unique ID of a topic apparently.\n",
    "\n",
    "* [query_topics(query, num_topics, reduced=False, tokenizer=None)](https://top2vec.readthedocs.io/en/latest/api.html?highlight=api#top2vec.Top2Vec.Top2Vec.query_topics)\n",
    "\n",
    "> ```topic_nums``` (array of int, num_topic)) – The **unique number** of every topic will be returned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b87d2b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_nums:[0], topic_score: [0.24809386]\n",
      "economic            : 0.4321683347225189\n",
      "economy             : 0.4101059138774872\n",
      "government          : 0.3722527027130127\n",
      "bn                  : 0.371192067861557\n",
      "oil                 : 0.3650941848754883\n",
      "exports             : 0.36492565274238586\n",
      "shares              : 0.3535774052143097\n",
      "labour              : 0.34154045581817627\n",
      "inflation           : 0.3408378064632416\n",
      "minister            : 0.33825230598449707\n"
     ]
    }
   ],
   "source": [
    "topics_words, word_scores, topic_score, topic_nums = model.query_topics(query=query, num_topics=1)\n",
    "print(f\"topic_nums:{topic_nums}, topic_score: {topic_score}\")\n",
    "for word, score in zip(topics_words[0][:10], word_scores[0][:10]):\n",
    "    print(f\"{word:20}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261adceb",
   "metadata": {},
   "source": [
    "## Documents related to a topic\n",
    "\n",
    "Find documents close to a topic.\n",
    "\n",
    "* [search_documents_by_topic(topic_num, num_docs, return_documents=True, reduced=False)](https://top2vec.readthedocs.io/en/latest/api.html?highlight=Top2Vec#top2vec.Top2Vec.Top2Vec.search_documents_by_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46deb309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "document_id:  210 score:0.36977365612983704\n",
      "stock market eyes japan recovery japanese shares have ended the year at their highest level since 13 july amidst hopes of an economic recovery during 2005. the nikkei index of leading shares gained 7.6% during the year to close at 11 488.76 points. in 2005 it will rise toward 13 000 predicted morgan stanley equity strategist naoki kamiyama. the optimism in the financial markets contrast sharply with pessimism in the japanese business community. earlier this month the quarterly tankan survey of japanese manufacturers found that business confidence had weakened for the first time since march 2003. slower economic growth rising\n",
      "--------------------------------------------------------------------------------\n",
      "document_id: 1225 score:0.36555323004722595\n",
      "us trade gap ballooned in october the us trade deficit widened by more than expected in october hitting record levels after higher oil prices raised import costs figures have shown the trade shortfall was $55.5bn (£29bn) up 9% from september the commerce department said. that pushed the 10 month deficit to $500.5bn. imports rose by 3.4% while exports increased by only 0.6%. a weaker dollar also increased the cost of imports though this should help drive export demand in coming months. things are getting worse but that s to be expected said david wyss of standard & poor s in\n",
      "--------------------------------------------------------------------------------\n",
      "document_id:  326 score:0.3624463975429535\n",
      "ore costs hit global steel firms shares in steel firms have dropped worldwide amid concerns that higher iron ore costs will hit profit growth. shares in germany s thyssenkrupp the uk s corus and france s arcleor fell while japan s nippon steel slid after it agreed to pay 72% more for iron ore. china s baoshan iron and steel co. said it was delaying a share sale because of weak market conditions adding it would raise steel prices to offset ore costs. the threat of higher raw material costs also hit industries such as carmakers. france s peugeot warned\n"
     ]
    }
   ],
   "source": [
    "topic_id = 0\n",
    "documents, scores, ids = model.search_documents_by_topic(\n",
    "    topic_num=topic_id, \n",
    "    num_docs=3, \n",
    "    return_documents=True, \n",
    "    reduced=False\n",
    ")\n",
    "for index, document, score, id in zip(range(len(ids)), documents, scores, ids):\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"document_id:{id:5} score:{score}\")\n",
    "    print(\" \".join(document.split()[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9606acc1",
   "metadata": {},
   "source": [
    "---\n",
    "# Similarity Search\n",
    "\n",
    "## Similar documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41579e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57: german growth goes into reverse germany s economy shrank 0.2% in the last three months\n",
      "360: uk economy facing major risks the uk manufacturing sector will continue to face serious challenges\n",
      "1078: south african car demand surges car manufacturers with plants in south africa including bmw general\n",
      "109: ecb holds rates amid growth fears the european central bank has left its key interest\n"
     ]
    }
   ],
   "source": [
    "query = bbc.iloc[1]['Text']\n",
    "documents, scores, ids  = model.query_documents(query=query, num_docs=5)\n",
    "for index, doc in [\n",
    "    (_id, \" \".join(documents[_i].split()[:15]))    # Top 15 words only\n",
    "    for _i, _id in enumerate(ids) \n",
    "    if _id != document_id                          # Remove the query tself\n",
    "]:\n",
    "    print(f\"{index}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "010e1a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57: german growth goes into reverse germany s economy shrank 0.2% in the last three months\n",
      "360: uk economy facing major risks the uk manufacturing sector will continue to face serious challenges\n",
      "1078: south african car demand surges car manufacturers with plants in south africa including bmw general\n",
      "109: ecb holds rates amid growth fears the european central bank has left its key interest\n",
      "449: industrial revival hope for japan japanese industry is growing faster than expected boosting hopes that\n"
     ]
    }
   ],
   "source": [
    "documents, scores, ids = model.search_documents_by_documents(\n",
    "    doc_ids=[document_id],\n",
    "    num_docs=5\n",
    ")\n",
    "for index, doc in [\n",
    "    (_id, \" \".join(documents[_i].split()[:15]))    # Top 15 words only\n",
    "    for _i, _id in enumerate(ids) \n",
    "    if _id != document_id                          # Remove the query tself\n",
    "]:\n",
    "    print(f\"{index}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c23b173",
   "metadata": {},
   "source": [
    "## Similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9299d39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['germany', 'deutsche', 'exchange', 'lse', 'domestic'], dtype='<U8'),\n",
       " array([0.47056147, 0.39058539, 0.36001014, 0.31677641, 0.31554154]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_words(keywords=[\"german\"], num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3e82f4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Custom document_id\n",
    "\n",
    "Use cutom document ID (string or int) to identify the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "699f9a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc4c7d",
   "metadata": {},
   "source": [
    "## 20 news gruop dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dc8917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22c00f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'filenames', 'target', 'target_names']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(newsgroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add87c0",
   "metadata": {},
   "source": [
    "### News Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3583a3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My brother is in the market for a high-performance video card that supports VESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:    - Diamond Stealth Pro Local Bus    - Orchid Farenheit 1280    - ATI Graphics Ultra Pro    - Any other high-performance VLB card   Please post or email.  Thank you!    - Matt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(newsgroups.data[1].split('\\n')).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc5e558",
   "metadata": {},
   "source": [
    "### Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26e69c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'60215'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(newsgroups.filenames[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb1a6d",
   "metadata": {},
   "source": [
    "### Target Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e27312fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b61e6",
   "metadata": {},
   "source": [
    "## UUID as custom document_id\n",
    "\n",
    "Use UUID as document id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e58cf08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids = [\n",
    "    str(uuid.uuid4()) for _ in range(len(newsgroups.data))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8d1296b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 18:31:11,232 - top2vec - INFO - Pre-processing documents for training\n",
      "/Users/oonisim/venv/tf/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "2023-03-19 18:31:18,713 - top2vec - INFO - Downloading distiluse-base-multilingual-cased model\n",
      "2023-03-19 18:31:21,000 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-03-19 18:41:27,865 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-03-19 18:41:44,876 - top2vec - INFO - Finding dense areas of documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 18:41:46,549 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "news = Top2Vec(\n",
    "    documents=newsgroups.data,\n",
    "    document_ids=document_ids,\n",
    "    # embedding_model='doc2vec',\n",
    "    # speed='deep-learn',\n",
    "    embedding_model='distiluse-base-multilingual-cased',\n",
    "    workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c796192",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, scores, ids  = news.query_documents(query=\"ATI Graphics Ultra Pro\", num_docs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0faab3e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f60c4045-f39f-4b66-ada5-8a05d024607f:\n",
      "I received my Graphite VL on Thursday, and I've had a little bit of experience with it now. In general, it feels *FAST*, although this is the first VLB card that I have tried. Still, the results are impressive. With my 486DX2/66 w/16 MB, running at 1024x768/256, I've had the\n",
      "\n",
      "d6e84c2f-fe40-4e9b-a102-1be040ed3947:\n",
      "I ran into this about six months ago. My system is a GW2000 486DX/66V, 8 megs RAM, 1Meg ATI GUP VLB. It seems the problem is that the ATI Graphics Ultra Pro card consumes the COM4 port for some reason, so only COM1-3 are available. I believe this is documented\n",
      "\n",
      "d0730de8-8bdf-43ac-8687-7e34b745c027:\n",
      "Could someone tell me if the ATI graphic ultra pro is supported in a version of vpic now. If so where is it located. thanks Robert email replies would be appreciated :-)\n",
      "\n",
      "75ac47e5-e80c-47f9-8115-6edad8abd3ff:\n",
      "subscribe comp.graphics quit\n",
      "\n",
      "48701578-4ea1-4b90-86bd-d54c5ab2093b:\n",
      "Are there any graphics cards for the SE/30 that also have, say, an 040 accelerator? There seem to be plenty of accelerator/graphics cards for the _SE_, but none (that I've seen) for the SE/30. Thanks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, doc in [\n",
    "    (_id, \" \".join(documents[_i].split()[:50]))    # Top 50 words only\n",
    "    for _i, _id in enumerate(ids) \n",
    "    if _id != document_id                          # Remove the query tself\n",
    "]:\n",
    "    print(f\"{index}:\\n{doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc56e175",
   "metadata": {},
   "source": [
    "---\n",
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ac4f6f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p model\n",
    "news.save(\"./model/scikit_learn_20_news_groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f37aca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
