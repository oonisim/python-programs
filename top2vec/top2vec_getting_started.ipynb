{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9566e6c2",
   "metadata": {},
   "source": [
    "# Top2Vec\n",
    "\n",
    "* [Top2Vec Github](https://github.com/ddangelov/Top2Vec)\n",
    "\n",
    "## Tutorial\n",
    "\n",
    "* [The Best Way to do Topic Modeling in Python - Top2Vec Introduction and Tutorial](https://www.youtube.com/watch?v=bEaxKSQ4Av8&list=PL2VXyKi-KpYt4Bb2dDZZoBLG4SQkrAz9g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01043827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3cef1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Top2Vec\n",
      "\n",
      "    Creates jointly embedded topic, document and word vectors.\n",
      "\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    documents: List of str\n",
      "        Input corpus, should be a list of strings.\n",
      "\n",
      "    min_count: int (Optional, default 50)\n",
      "        Ignores all words with total frequency lower than this. For smaller\n",
      "        corpora a smaller min_count will be necessary.\n",
      "\n",
      "    topic_merge_delta: float (default 0.1)\n",
      "        Merges topic vectors which have a cosine distance smaller than\n",
      "        topic_merge_delta using dbscan. The epsilon parameter of dbscan is\n",
      "        set to the topic_merge_delta.\n",
      "\n",
      "    ngram_vocab: bool (Optional, default False)\n",
      "        Add phrases to topic descriptions.\n",
      "\n",
      "        Uses gensim phrases to find common phrases in the corpus and adds them\n",
      "        to the vocabulary.\n",
      "\n",
      "        For more information visit:\n",
      "        https://radimrehurek.com/gensim/models/phrases.html\n",
      "\n",
      "    ngram_vocab_args: dict (Optional, default None)\n",
      "        Pass custom arguments to gensim phrases.\n",
      "\n",
      "        For more information visit:\n",
      "        https://radimrehurek.com/gensim/models/phrases.html\n",
      "\n",
      "    embedding_model: string or callable\n",
      "        This will determine which model is used to generate the document and\n",
      "        word embeddings. The valid string options are:\n",
      "\n",
      "            * doc2vec\n",
      "            * universal-sentence-encoder\n",
      "            * universal-sentence-encoder-large\n",
      "            * universal-sentence-encoder-multilingual\n",
      "            * universal-sentence-encoder-multilingual-large\n",
      "            * distiluse-base-multilingual-cased\n",
      "            * all-MiniLM-L6-v2\n",
      "            * paraphrase-multilingual-MiniLM-L12-v2\n",
      "\n",
      "        For large data sets and data sets with very unique vocabulary doc2vec\n",
      "        could produce better results. This will train a doc2vec model from\n",
      "        scratch. This method is language agnostic. However multiple languages\n",
      "        will not be aligned.\n",
      "\n",
      "        Using the universal sentence encoder options will be much faster since\n",
      "        those are pre-trained and efficient models. The universal sentence\n",
      "        encoder options are suggested for smaller data sets. They are also\n",
      "        good options for large data sets that are in English or in languages\n",
      "        covered by the multilingual model. It is also suggested for data sets\n",
      "        that are multilingual.\n",
      "\n",
      "        For more information on universal-sentence-encoder options visit:\n",
      "        https://tfhub.dev/google/collections/universal-sentence-encoder/1\n",
      "\n",
      "        The SBERT pre-trained sentence transformer options are\n",
      "        distiluse-base-multilingual-cased,\n",
      "        paraphrase-multilingual-MiniLM-L12-v2, and all-MiniLM-L6-v2.\n",
      "\n",
      "        The distiluse-base-multilingual-cased and\n",
      "        paraphrase-multilingual-MiniLM-L12-v2 are suggested for multilingual\n",
      "        datasets and languages that are not\n",
      "        covered by the multilingual universal sentence encoder. The\n",
      "        transformer is significantly slower than the universal sentence\n",
      "        encoder options(except for the large options).\n",
      "\n",
      "        For more information on SBERT options visit:\n",
      "        https://www.sbert.net/docs/pretrained_models.html\n",
      "\n",
      "        If passing a callable embedding_model note that it will not be saved\n",
      "        when saving a top2vec model. After loading such a saved top2vec model\n",
      "        the set_embedding_model method will need to be called and the same\n",
      "        embedding_model callable used during training must be passed to it.\n",
      "\n",
      "    embedding_model_path: string (Optional)\n",
      "        Pre-trained embedding models will be downloaded automatically by\n",
      "        default. However they can also be uploaded from a file that is in the\n",
      "        location of embedding_model_path.\n",
      "\n",
      "        Warning: the model at embedding_model_path must match the\n",
      "        embedding_model parameter type.\n",
      "\n",
      "    embedding_batch_size: int (default=32)\n",
      "        Batch size for documents being embedded.\n",
      "\n",
      "    split_documents: bool (default False)\n",
      "        If set to True, documents will be split into parts before embedding.\n",
      "        After embedding the multiple document part embeddings will be averaged\n",
      "        to create a single embedding per document. This is useful when documents\n",
      "        are very large or when the embedding model has a token limit.\n",
      "\n",
      "        Document chunking or a senticizer can be used for document splitting.\n",
      "\n",
      "    document_chunker: string or callable (default 'sequential')\n",
      "        This will break the document into chunks. The valid string options are:\n",
      "\n",
      "            * sequential\n",
      "            * random\n",
      "\n",
      "        The sequential chunker will split the document into chunks of specified\n",
      "        length and ratio of overlap. This is the recommended method.\n",
      "\n",
      "        The random chunking option will take random chunks of specified length\n",
      "        from the document. These can overlap and should be thought of as\n",
      "        sampling chunks with replacement from the document.\n",
      "\n",
      "        If a callable is passed it must take as input a list of tokens of\n",
      "        a document and return a list of strings representing the resulting\n",
      "        document chunks.\n",
      "\n",
      "        Only one of document_chunker or sentincizer should be used.\n",
      "\n",
      "    chunk_length: int (default 100)\n",
      "        The number of tokens per document chunk if using the document chunker\n",
      "        string options.\n",
      "\n",
      "    max_num_chunks: int (Optional)\n",
      "        The maximum number of chunks generated per document if using the\n",
      "        document chunker string options.\n",
      "\n",
      "    chunk_overlap_ratio: float (default 0.5)\n",
      "        Only applies to the 'sequential' document chunker.\n",
      "\n",
      "        Fraction of overlapping tokens between sequential chunks. A value of\n",
      "        0 will result i no overlap, where as 0.5 will overlap half of the\n",
      "        previous chunk.\n",
      "\n",
      "    chunk_len_coverage_ratio: float (default 1.0)\n",
      "        Only applies to the 'random' document chunker option.\n",
      "\n",
      "        Proportion of token length that will be covered by chunks. Default\n",
      "        value of 1.0 means chunk lengths will add up to number of tokens of\n",
      "        the document. This does not mean all tokens will be covered since\n",
      "        chunks can be overlapping.\n",
      "\n",
      "    sentencizer: callable (Optional)\n",
      "        A sentincizer callable can be passed. The input should be a string\n",
      "        representing the document and the output should be a list of strings\n",
      "        representing the document sentence chunks.\n",
      "\n",
      "        Only one of document_chunker or sentincizer should be used.\n",
      "\n",
      "    speed: string (Optional, default 'learn')\n",
      "\n",
      "        This parameter is only used when using doc2vec as embedding_model.\n",
      "\n",
      "        It will determine how fast the model takes to train. The\n",
      "        fast-learn option is the fastest and will generate the lowest quality\n",
      "        vectors. The learn option will learn better quality vectors but take\n",
      "        a longer time to train. The deep-learn option will learn the best\n",
      "        quality vectors but will take significant time to train. The valid\n",
      "        string speed options are:\n",
      "        \n",
      "            * fast-learn\n",
      "            * learn\n",
      "            * deep-learn\n",
      "\n",
      "    use_corpus_file: bool (Optional, default False)\n",
      "\n",
      "        This parameter is only used when using doc2vec as embedding_model.\n",
      "\n",
      "        Setting use_corpus_file to True can sometimes provide speedup for\n",
      "        large datasets when multiple worker threads are available. Documents\n",
      "        are still passed to the model as a list of str, the model will create\n",
      "        a temporary corpus file for training.\n",
      "\n",
      "    document_ids: List of str, int (Optional)\n",
      "        A unique value per document that will be used for referring to\n",
      "        documents in search results. If ids are not given to the model, the\n",
      "        index of each document in the original corpus will become the id.\n",
      "\n",
      "    keep_documents: bool (Optional, default True)\n",
      "        If set to False documents will only be used for training and not saved\n",
      "        as part of the model. This will reduce model size. When using search\n",
      "        functions only document ids will be returned, not the actual\n",
      "        documents.\n",
      "\n",
      "    workers: int (Optional)\n",
      "        The amount of worker threads to be used in training the model. Larger\n",
      "        amount will lead to faster training.\n",
      "    \n",
      "    tokenizer: callable (Optional, default None)\n",
      "        Override the default tokenization method. If None then\n",
      "        gensim.utils.simple_preprocess will be used.\n",
      "\n",
      "        Tokenizer must take a document and return a list of tokens.\n",
      "\n",
      "    use_embedding_model_tokenizer: bool (Optional, default False)\n",
      "        If using an embedding model other than doc2vec, use the model's\n",
      "        tokenizer for document embedding. If set to True the tokenizer, either\n",
      "        default or passed callable will be used to tokenize the text to\n",
      "        extract the vocabulary for word embedding.\n",
      "\n",
      "    umap_args: dict (Optional, default None)\n",
      "        Pass custom arguments to UMAP.\n",
      "\n",
      "    hdbscan_args: dict (Optional, default None)\n",
      "        Pass custom arguments to HDBSCAN.\n",
      "    \n",
      "    verbose: bool (Optional, default True)\n",
      "        Whether to print status data during training.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(Top2Vec.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf7b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(linewidth=1024)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "pd.set_option(\"max_seq_items\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054bf6e8",
   "metadata": {},
   "source": [
    "# Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce3c0d",
   "metadata": {},
   "source": [
    "* topic_num: ID of a topic. If there are 12 topics identified, then one in 0 to 11.\n",
    "* topic_nums: List of topic IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a676394e",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "* [Kaggle News Articles Categorization data](https://www.kaggle.com/competitions/learn-ai-bbc/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc5536b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (£5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german business confidence fell in february knocking hopes of a speedy recovery in europe s largest economy.  munich-based research institute ifo said that its confidence index fell to 95.5 in february from 97.5 in january  its first decline in three months. the study found that the outlook in both the manufacturing and retail sectors had worsened. observers had been hoping that a more confident business sector would signal that economic activity was picking up.   we re surprised that the ifo index has taken such a knock   said dz bank economist bernd weidensteiner.  the main reason is probably that the domestic economy is still weak  particularly in the retail trade.  economy and labour minister wolfgang clement called the dip in february s ifo confidence figure  a very mild decline . he said that despite the retreat  the index remained at a relatively high level and that he expected  a modest economic upswing  to continue.  germany s economy grew 1.6% last year after shrinking in 2003. however  the economy contracted by 0.2% during the last three months of 2004  mainly due to the reluctance of consumers to spend. latest indications are that growth is still proving elusive and ifo president hans-werner sinn said any improvement in german domestic demand was sluggish. exports had kept things going during the first half of 2004  but demand for exports was then hit as the value of the euro hit record levels making german products less competitive overseas. on top of that  the unemployment rate has been stuck at close to 10% and manufacturing firms  including daimlerchrysler  siemens and volkswagen  have been negotiating with unions over cost cutting measures. analysts said that the ifo figures and germany s continuing problems may delay an interest rate rise by the european central bank. eurozone interest rates are at 2%  but comments from senior officials have recently focused on the threat of inflation  prompting fears that interest rates may rise.</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in a majority of nations surveyed in a bbc world service poll believe the world economy is worsening.  most respondents also said their national economy was getting worse. but when asked about their own family s financial outlook  a majority in 14 countries said they were positive about the future. almost 23 000 people in 22 countries were questioned for the poll  which was mostly conducted before the asian tsunami disaster. the poll found that a majority or plurality of people in 13 countries believed the economy was going downhill  compared with respondents in nine countries who believed it was improving. those surveyed in three countries were split. in percentage terms  an average of 44% of respondents in each country said the world economy was getting worse  compared to 34% who said it was improving. similarly  48% were pessimistic about their national economy  while 41% were optimistic. and 47% saw their family s economic conditions improving  as against 36% who said they were getting worse.  the poll of 22 953 people was conducted by the international polling firm globescan  together with the program on international policy attitudes (pipa) at the university of maryland.  while the world economy has picked up from difficult times just a few years ago  people seem to not have fully absorbed this development  though they are personally experiencing its effects   said pipa director steven kull.  people around the world are saying:  i m ok  but the world isn t .  there may be a perception that war  terrorism and religious and political divisions are making the world a worse place  even though that has not so far been reflected in global economic performance  says the bbc s elizabeth blunt.  the countries where people were most optimistic  both for the world and for their own families  were two fast-growing developing economies  china and india  followed by indonesia. china has seen two decades of blistering economic growth  which has led to wealth creation on a huge scale  says the bbc s louisa lim in beijing. but the results also may reflect the untrammelled confidence of people who are subject to endless government propaganda about their country s rosy economic future  our correspondent says. south korea was the most pessimistic  while respondents in italy and mexico were also quite gloomy. the bbc s david willey in rome says one reason for that result is the changeover from the lira to the euro in 2001  which is widely viewed as the biggest reason why their wages and salaries are worth less than they used to be. the philippines was among the most upbeat countries on prospects for respondents  families  but one of the most pessimistic about the world economy. pipa conducted the poll from 15 november 2004 to 3 january 2005 across 22 countries in face-to-face or telephone interviews. the interviews took place between 15 november 2004 and 5 january 2005. the margin of error is between 2.5 and 4 points  depending on the country. in eight of the countries  the sample was limited to major metropolitan areas.</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId  \\\n",
       "0       1833   \n",
       "1        154   \n",
       "2       1101   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (£5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  german business confidence slides german business confidence fell in february knocking hopes of a speedy recovery in europe s largest economy.  munich-based research institute ifo said that its confidence index fell to 95.5 in february from 97.5 in january  its first decline in three months. the study found that the outlook in both the manufacturing and retail sectors had worsened. observers had been hoping that a more confident business sector would signal that economic activity was picking up.   we re surprised that the ifo index has taken such a knock   said dz bank economist bernd weidensteiner.  the main reason is probably that the domestic economy is still weak  particularly in the retail trade.  economy and labour minister wolfgang clement called the dip in february s ifo confidence figure  a very mild decline . he said that despite the retreat  the index remained at a relatively high level and that he expected  a modest economic upswing  to continue.  germany s economy grew 1.6% last year after shrinking in 2003. however  the economy contracted by 0.2% during the last three months of 2004  mainly due to the reluctance of consumers to spend. latest indications are that growth is still proving elusive and ifo president hans-werner sinn said any improvement in german domestic demand was sluggish. exports had kept things going during the first half of 2004  but demand for exports was then hit as the value of the euro hit record levels making german products less competitive overseas. on top of that  the unemployment rate has been stuck at close to 10% and manufacturing firms  including daimlerchrysler  siemens and volkswagen  have been negotiating with unions over cost cutting measures. analysts said that the ifo figures and germany s continuing problems may delay an interest rate rise by the european central bank. eurozone interest rates are at 2%  but comments from senior officials have recently focused on the threat of inflation  prompting fears that interest rates may rise.   \n",
       "2  bbc poll indicates economic gloom citizens in a majority of nations surveyed in a bbc world service poll believe the world economy is worsening.  most respondents also said their national economy was getting worse. but when asked about their own family s financial outlook  a majority in 14 countries said they were positive about the future. almost 23 000 people in 22 countries were questioned for the poll  which was mostly conducted before the asian tsunami disaster. the poll found that a majority or plurality of people in 13 countries believed the economy was going downhill  compared with respondents in nine countries who believed it was improving. those surveyed in three countries were split. in percentage terms  an average of 44% of respondents in each country said the world economy was getting worse  compared to 34% who said it was improving. similarly  48% were pessimistic about their national economy  while 41% were optimistic. and 47% saw their family s economic conditions improving  as against 36% who said they were getting worse.  the poll of 22 953 people was conducted by the international polling firm globescan  together with the program on international policy attitudes (pipa) at the university of maryland.  while the world economy has picked up from difficult times just a few years ago  people seem to not have fully absorbed this development  though they are personally experiencing its effects   said pipa director steven kull.  people around the world are saying:  i m ok  but the world isn t .  there may be a perception that war  terrorism and religious and political divisions are making the world a worse place  even though that has not so far been reflected in global economic performance  says the bbc s elizabeth blunt.  the countries where people were most optimistic  both for the world and for their own families  were two fast-growing developing economies  china and india  followed by indonesia. china has seen two decades of blistering economic growth  which has led to wealth creation on a huge scale  says the bbc s louisa lim in beijing. but the results also may reflect the untrammelled confidence of people who are subject to endless government propaganda about their country s rosy economic future  our correspondent says. south korea was the most pessimistic  while respondents in italy and mexico were also quite gloomy. the bbc s david willey in rome says one reason for that result is the changeover from the lira to the euro in 2001  which is widely viewed as the biggest reason why their wages and salaries are worth less than they used to be. the philippines was among the most upbeat countries on prospects for respondents  families  but one of the most pessimistic about the world economy. pipa conducted the poll from 15 november 2004 to 3 january 2005 across 22 countries in face-to-face or telephone interviews. the interviews took place between 15 november 2004 and 5 january 2005. the margin of error is between 2.5 and 4 points  depending on the country. in eight of the countries  the sample was limited to major metropolitan areas.   \n",
       "\n",
       "   Category  \n",
       "0  business  \n",
       "1  business  \n",
       "2  business  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc = pd.read_csv(\"/Volumes/SSD/kaggle/bbc/BBCNewsTrain.csv\")\n",
    "bbc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d6f28",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4921133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 15:40:58,574 - top2vec - INFO - Pre-processing documents for training\n",
      "/Users/oonisim/venv/tf/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "2023-03-19 15:40:59,268 - top2vec - INFO - Downloading distiluse-base-multilingual-cased model\n",
      "2023-03-19 15:41:00,858 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-03-19 15:41:35,498 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-03-19 15:41:39,627 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-03-19 15:41:39,640 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "# Does not work\n",
    "# model = Top2Vec(bbc['Text'].tolist(), embedding_model='universal-sentence-encoder', speed=\"learn\", workers=8)\n",
    "model = Top2Vec(\n",
    "    documents=bbc['Text'].tolist(),\n",
    "    document_ids=bbc.index.tolist(),\n",
    "    embedding_model='distiluse-base-multilingual-cased'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52487fa8",
   "metadata": {},
   "source": [
    "---\n",
    "# Topic\n",
    "\n",
    "**Topic (Topic Vector)** is a mean of a document vector cluster identified by HDBSCAN. It is a **thought vector** that is identified by the context words (topic words) nearby, but NOT a concrete word or sentence. It is desirable to have one specific categorical keyword that represents each topic, e.g. **Sport** for the 2nd topic but such distilation is not available in top2vec.\n",
    "\n",
    "<img src=\"./image/top2vec_topic_vector.png\" align=\"left\" width=600/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41008b5",
   "metadata": {},
   "source": [
    "## Number of topics identified in the documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55b59ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of topics identified:[4]\n"
     ]
    }
   ],
   "source": [
    "topic_sizes, topic_ids = model.get_topic_sizes()\n",
    "print(f\"number of topics identified:[{len(topic_ids)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a722242",
   "metadata": {},
   "source": [
    "## Topic words \n",
    "\n",
    "List the context words that identify each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "821821a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Topic ID:0\n",
      "--------------------------------------------------------------------------------\n",
      "parliament           0.10377583652734756\n",
      "politicians          0.10281675308942795\n",
      "britain              0.10191775858402252\n",
      "election             0.09515437483787537\n",
      "elections            0.0923602283000946\n",
      "no                   0.08872390538454056\n",
      "non                  0.0843275785446167\n",
      "voters               0.08393856137990952\n",
      "british              0.08337553590536118\n",
      "bbc                  0.08136938512325287\n",
      "--------------------------------------------------------------------------------\n",
      "Topic ID:1\n",
      "--------------------------------------------------------------------------------\n",
      "rugby                0.22808963060379028\n",
      "mourinho             0.2248014509677887\n",
      "football             0.21560746431350708\n",
      "britain              0.17336198687553406\n",
      "coach                0.15696346759796143\n",
      "england              0.1561509370803833\n",
      "tournament           0.14941182732582092\n",
      "referee              0.14490145444869995\n",
      "championship         0.14313378930091858\n",
      "liverpool            0.14182285964488983\n",
      "--------------------------------------------------------------------------------\n",
      "Topic ID:2\n",
      "--------------------------------------------------------------------------------\n",
      "olympic              0.29441696405410767\n",
      "tournament           0.2752123475074768\n",
      "championship         0.25938212871551514\n",
      "won                  0.23704145848751068\n",
      "championships        0.2365473508834839\n",
      "winning              0.21371442079544067\n",
      "competition          0.21168753504753113\n",
      "winners              0.20301266014575958\n",
      "champion             0.20144090056419373\n",
      "winner               0.19605916738510132\n",
      "--------------------------------------------------------------------------------\n",
      "Topic ID:3\n",
      "--------------------------------------------------------------------------------\n",
      "olympic              0.25685709714889526\n",
      "referee              0.16343265771865845\n",
      "sports               0.15681388974189758\n",
      "sport                0.1539703905582428\n",
      "trial                0.15350961685180664\n",
      "tournament           0.15267398953437805\n",
      "penalty              0.13404521346092224\n",
      "tennis               0.12623366713523865\n",
      "championships        0.12596672773361206\n",
      "drugs                0.12240470945835114\n"
     ]
    }
   ],
   "source": [
    "topics_words, topic_scores, topic_ids = model.get_topics()\n",
    "for topic_id, words, scores in zip(topic_ids, topics_words, topic_scores):\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Topic ID:{topic_id}\")\n",
    "    print(\"-\" * 80)\n",
    "    for word, score in zip(words[:10], scores[:10]):\n",
    "        print(f\"{word:20} {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d58a2f5",
   "metadata": {},
   "source": [
    "## Topics of a document\n",
    "\n",
    "Find the **Topic Words** that identify the **Topic** that is closest to the query document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "319afac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'german business confidence slides german business confidence fell in february knocking hopes of a speedy recovery in europe s largest economy.  munich-based research institute ifo said that its confidence index fell to 95.5 in february from 97.5 in january  its first decline in three months. the study found that the outlook in both the manufacturing and retail sectors had worsened. observers had been hoping that a more confident business sector would signal that economic activity was picking up.   we re surprised that the ifo index has taken such a knock   said dz bank economist bernd weidensteiner.  the main reason is probably that the domestic economy is still weak  particularly in the retail trade.  economy and labour minister wolfgang clement called the dip in february s ifo confidence figure  a very mild decline . he said that despite the retreat  the index remained at a relatively high level and that he expected  a modest economic upswing  to continue.  germany s economy grew 1.6% last year after shrinking in 2003. however  the economy contracted by 0.2% during the last three months of 2004  mainly due to the reluctance of consumers to spend. latest indications are that growth is still proving elusive and ifo president hans-werner sinn said any improvement in german domestic demand was sluggish. exports had kept things going during the first half of 2004  but demand for exports was then hit as the value of the euro hit record levels making german products less competitive overseas. on top of that  the unemployment rate has been stuck at close to 10% and manufacturing firms  including daimlerchrysler  siemens and volkswagen  have been negotiating with unions over cost cutting measures. analysts said that the ifo figures and germany s continuing problems may delay an interest rate rise by the european central bank. eurozone interest rates are at 2%  but comments from senior officials have recently focused on the threat of inflation  prompting fears that interest rates may rise.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id = 1\n",
    "query = bbc.iloc[document_id]['Text']\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad4d1fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_nums:[0], topic_score: [0.3969033]\n",
      "parliament          : 0.10377583652734756\n",
      "politicians         : 0.10281675308942795\n",
      "britain             : 0.10191775858402252\n",
      "election            : 0.09515437483787537\n",
      "elections           : 0.0923602283000946\n",
      "no                  : 0.08872390538454056\n",
      "non                 : 0.0843275785446167\n",
      "voters              : 0.08393856137990952\n",
      "british             : 0.08337553590536118\n",
      "bbc                 : 0.08136938512325287\n"
     ]
    }
   ],
   "source": [
    "topic_nums, topic_score, topics_words, word_scores = model.get_documents_topics([document_id], reduced=False)\n",
    "print(f\"topic_nums:{topic_nums}, topic_score: {topic_score}\")\n",
    "for word, score in zip(topics_words[0][:10], word_scores[0][:10]):\n",
    "    print(f\"{word:20}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e9a83a",
   "metadata": {},
   "source": [
    "Use text query instead of ```document_id```. ```topic_nums``` is a unique ID of a topic apparently.\n",
    "\n",
    "* [query_topics(query, num_topics, reduced=False, tokenizer=None)](https://top2vec.readthedocs.io/en/latest/api.html?highlight=api#top2vec.Top2Vec.Top2Vec.query_topics)\n",
    "\n",
    "> ```topic_nums``` (array of int, num_topic)) – The **unique number** of every topic will be returned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b87d2b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_nums:[0], topic_score: [0.36120173]\n",
      "parliament          : 0.10377583652734756\n",
      "politicians         : 0.10281675308942795\n",
      "britain             : 0.10191775858402252\n",
      "election            : 0.09515437483787537\n",
      "elections           : 0.0923602283000946\n",
      "no                  : 0.08872390538454056\n",
      "non                 : 0.0843275785446167\n",
      "voters              : 0.08393856137990952\n",
      "british             : 0.08337553590536118\n",
      "bbc                 : 0.08136938512325287\n"
     ]
    }
   ],
   "source": [
    "topics_words, word_scores, topic_score, topic_nums = model.query_topics(query=query, num_topics=1)\n",
    "print(f\"topic_nums:{topic_nums}, topic_score: {topic_score}\")\n",
    "for word, score in zip(topics_words[0][:10], word_scores[0][:10]):\n",
    "    print(f\"{word:20}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261adceb",
   "metadata": {},
   "source": [
    "## Documents related to a topic\n",
    "\n",
    "Find documents close to a topic.\n",
    "\n",
    "* [search_documents_by_topic(topic_num, num_docs, return_documents=True, reduced=False)](https://top2vec.readthedocs.io/en/latest/api.html?highlight=Top2Vec#top2vec.Top2Vec.Top2Vec.search_documents_by_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46deb309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "document_id:  658 score:0.6199405193328857\n",
      "pre-poll clash on tax and spend labour and the tories have clashed over tax and spending plans as the row over gordon brown s budget turned into a full scale pre-election battle. tony blair claimed a tory government would cut £35bn from public services hitting schools hospitals and police. tory chairman liam fox accused labour of at best misrepresentation at worst a downright lie and said the smear tactics were a sign of desperation. the lib dems accused mr brown of ducking the issue of council tax rises. appearing together at a labour poster launch the prime minister hailed his\n",
      "--------------------------------------------------------------------------------\n",
      "document_id:  128 score:0.6122792959213257\n",
      "howard and blair tax pledge clash tony blair has said voters will have to wait for labour s manifesto to see if the party has plans to increase tax. the premier was responding to a challenge from tory leader michael howard who said labour would raise taxes in its post-election budget. mr blair derided tory claims they could cut £35bn in wasteful spending saying the party had got its sums wrong. the two political leaders clashed just days after the opening salvoes of the pre-election period. mr howard told mps that every independent expert from the international monetary fund to\n",
      "--------------------------------------------------------------------------------\n",
      "document_id: 1170 score:0.6036664843559265\n",
      "howard attacks pay later budget tory leader michael howard has dismissed gordon brown s budget as vote now pay later spending plans. the simple fact was that under a new labour government taxes would go up after the election to plug a financial black hole mr howard said. everyone could see the chancellor s sweeteners but these hid tax rises for hard working families he said. labour s faltering election campaign would not be helped by the package of measures mr howard added. mr brown s budget was not about what was good for the country but all about the\n"
     ]
    }
   ],
   "source": [
    "topic_id = 0\n",
    "documents, scores, ids = model.search_documents_by_topic(\n",
    "    topic_num=topic_id, \n",
    "    num_docs=3, \n",
    "    return_documents=True, \n",
    "    reduced=False\n",
    ")\n",
    "for index, document, score, id in zip(range(len(ids)), documents, scores, ids):\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"document_id:{id:5} score:{score}\")\n",
    "    print(\" \".join(document.split()[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9606acc1",
   "metadata": {},
   "source": [
    "---\n",
    "# Similarity Search\n",
    "\n",
    "## Similar documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41579e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57: german growth goes into reverse germany s economy shrank 0.2% in the last three months\n",
      "1155: imf cuts german growth estimate the international monetary fund is to cut its 2005 growth\n",
      "422: economy strong in election year uk businesses are set to prosper during the next few\n",
      "1458: economy strong in election year uk businesses are set to prosper during the next few\n"
     ]
    }
   ],
   "source": [
    "query = bbc.iloc[1]['Text']\n",
    "documents, scores, ids  = model.query_documents(query=query, num_docs=5)\n",
    "for index, doc in [\n",
    "    (_id, \" \".join(documents[_i].split()[:15]))    # Top 15 words only\n",
    "    for _i, _id in enumerate(ids) \n",
    "    if _id != document_id                          # Remove the query tself\n",
    "]:\n",
    "    print(f\"{index}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "010e1a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57: german growth goes into reverse germany s economy shrank 0.2% in the last three months\n",
      "570: slowdown hits us factory growth us industrial production increased for the 21st month in a\n",
      "1155: imf cuts german growth estimate the international monetary fund is to cut its 2005 growth\n",
      "422: economy strong in election year uk businesses are set to prosper during the next few\n",
      "1458: economy strong in election year uk businesses are set to prosper during the next few\n"
     ]
    }
   ],
   "source": [
    "documents, scores, ids = model.search_documents_by_documents(\n",
    "    doc_ids=[document_id],\n",
    "    num_docs=5\n",
    ")\n",
    "for index, doc in [\n",
    "    (_id, \" \".join(documents[_i].split()[:15]))    # Top 15 words only\n",
    "    for _i, _id in enumerate(ids) \n",
    "    if _id != document_id                          # Remove the query tself\n",
    "]:\n",
    "    print(f\"{index}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c23b173",
   "metadata": {},
   "source": [
    "## Similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9299d39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['deutsche', 'germany', 'england', 'english', 'japan'], dtype='<U8'),\n",
       " array([0.97359509, 0.95568295, 0.73594704, 0.6911235 , 0.65781959]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_words(keywords=[\"german\"], num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3e82f4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Custom document_id\n",
    "\n",
    "By default, top2vec assign sequential ID from 0. Use cutom document ID (string or int) to identify the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "699f9a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc4c7d",
   "metadata": {},
   "source": [
    "## 20 news gruop dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dc8917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22c00f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'filenames', 'target', 'target_names']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(newsgroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add87c0",
   "metadata": {},
   "source": [
    "### News Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3583a3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My brother is in the market for a high-performance video card that supports VESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:    - Diamond Stealth Pro Local Bus    - Orchid Farenheit 1280    - ATI Graphics Ultra Pro    - Any other high-performance VLB card   Please post or email.  Thank you!    - Matt'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(newsgroups.data[1].split('\\n')).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc5e558",
   "metadata": {},
   "source": [
    "### Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26e69c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'60215'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(newsgroups.filenames[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb1a6d",
   "metadata": {},
   "source": [
    "### Target Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e27312fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b61e6",
   "metadata": {},
   "source": [
    "## UUID as custom document_id\n",
    "\n",
    "Use UUID as document id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e58cf08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids = [\n",
    "    str(uuid.uuid4()) for _ in range(len(newsgroups.data))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8d1296b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 15:42:37,998 - top2vec - INFO - Pre-processing documents for training\n",
      "/Users/oonisim/venv/tf/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "2023-03-19 15:42:42,349 - top2vec - INFO - Downloading distiluse-base-multilingual-cased model\n",
      "2023-03-19 15:42:43,661 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-03-19 15:47:57,956 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-03-19 15:48:06,927 - top2vec - INFO - Finding dense areas of documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 15:48:07,882 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "news = Top2Vec(\n",
    "    documents=newsgroups.data,\n",
    "    document_ids=document_ids,\n",
    "    embedding_model='distiluse-base-multilingual-cased'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e3003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
