{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84727e84-032b-475a-a545-b84c161c8b88",
   "metadata": {},
   "source": [
    "# Importance of weight initialization and data normalization\n",
    "\n",
    "**Weight Initialization is the critical balancing adjustments** without which training will not work. Network architecture, optimizer, hyper parameter **do NOT matter without proper initializations**.\n",
    "\n",
    "* [Building makemore Part 3: Activations & Gradients, BatchNorm](https://www.youtube.com/watch?v=P6sfmUTpUmc) - Andrej Karpathy (MUST)\n",
    "* [Stanford CS232 Lecture 6 | Training Neural Networks I](https://www.youtube.com/watch?v=wEoyxE0GP2M)\n",
    "* [Deep Learning AI - The importance of effective initialization](https://www.deeplearning.ai/ai-notes/initialization/index.html) - MUST\n",
    "* [He Initialization](https://arxiv.org/pdf/1502.01852.pdf)\n",
    "* [Variance of product of multiple independent random variables](https://stats.stackexchange.com/questions/52646/)\n",
    "\n",
    "> To prevent the gradients of the network’s activations from vanishing or exploding, we will stick to the following rules of thumb:\n",
    "> \n",
    "> 1. The mean of the activations should be zero.\n",
    "> 2. The variance of the activations should stay the same across every layer.\n",
    ">\n",
    "> Under these two assumptions, the backpropagated gradient signal should not be multiplied by values too small or too large in any layer. It should travel to the input layer without exploding or vanishing.\n",
    "> n other words, all the **weights of layer ```l``` are random samples from a normal distribution** with mean ```μ=0``` and variance ```v=1/N(l-1)``` where ```N(l-1)``` is the dimensions of the input (number of outputs or number of neurons of the previous layer).\n",
    "\n",
    "Initialize W with the same value is **variance == 0** either the value is 0 or other values.\n",
    "\n",
    "* [NN - 18 - Weight Initialization 1 - What not to do?](https://youtu.be/eoNVmZDnn9w?t=250)\n",
    "> \n",
    "> Back Propagation will move the same direction in all $W$ if variance of W is 0 (set the same value in W, e.g. set all 0 in W).\n",
    "> <img src=\"image/initiailzation_sigmoid_backprop_impoact.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f339ed9-df4b-4a75-9fff-c71da7dd289d",
   "metadata": {},
   "source": [
    "* [NN - 18 - Weight Initialization 1 - What not to do?](https://youtu.be/eoNVmZDnn9w?t=482)\n",
    "> \n",
    "> <img src=\"image/initialization_what_to_do.png\" aligh=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f88b65f-3803-4ee8-8825-45b60652f54b",
   "metadata": {},
   "source": [
    "## Transformer Weight Initialization\n",
    "\n",
    "* [T-Fixup Improving Transformer Optimization Through Better Initialization](https://www.cs.toronto.edu/~mvolkovs/ICML2020_tfixup.pdf)\n",
    "* [Effective Theory of Transformers at Initialization](https://arxiv.org/pdf/2304.02034.pdf)\n",
    "* [Improving Deep Transformer\n",
    "with Depth-Scaled Initialization and Merged Attention](https://aclanthology.org/D19-1083.pdf)\n",
    "* [FIXUP INITIALIZATION:\n",
    "RESIDUAL LEARNING WITHOUT NORMALIZATION](https://arxiv.org/pdf/1901.09321.pdf)\n",
    "* [MS Research - DeepNet 1000 layer transformer](https://arxiv.org/pdf/2203.00555.pdf)\n",
    "* [Meta AI - Norm Former](https://arxiv.org/pdf/2110.09456.pdf) - Address larger gradient at lower layer by Pre-LN.\n",
    "* [Learning Deep Transformer Models for Machine Translation](https://arxiv.org/pdf/1906.01787.pdf)\n",
    "* [ReZero is All You Need: Fast Convergence at Large Depth](https://arxiv.org/pdf/2003.04887.pdf)\n",
    "* [Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247.pdf)\n",
    "* [PyTorch - Transformer Initialization #72253](https://github.com/pytorch/pytorch/issues/72253)\n",
    "\n",
    "### Initialization Code\n",
    "* [BertConfig][4]\n",
    "\n",
    "> initializer_range (float, optional, **defaults to 0.02**) — The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "\n",
    "* [HuggingFace Bert Weight Initialization code](https://github.com/huggingface/transformers/blob/a9aa7456ac/src/transformers/modeling_bert.py#L520-L530)\n",
    "\n",
    "```\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range) # <--- initializer_range = 0.02\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "```\n",
    "\n",
    "* [PyTorch Tutorial - LANGUAGE TRANSLATION WITH NN.TRANSFORMER AND TORCHTEXT](https://pytorch.org/tutorials/beginner/translation_transformer.html#seq2seq-network-using-transformer)\n",
    "\n",
    "```\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "```\n",
    "\n",
    "* [Nano GPT](https://github.com/karpathy/nanoGPT/blob/master/model.py#L141)\n",
    "\n",
    "```\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826aba0-f683-4300-a3a4-84c35a845c6f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Problems due to Inproper Initializations\n",
    "\n",
    "## Waste of training cycles\n",
    "\n",
    "* [Building makemore Part 3: Activations & Gradients, BatchNorm](https://youtu.be/P6sfmUTpUmc?t=259)\n",
    "\n",
    "If the weights are not properly, initial training cycles will be spent to mitigate it -> Manifest as **initial large loss** being squashed down quickly (hockey stick like learning curve).\n",
    "\n",
    "<img src=\"image/nn_weight_initialization_too_large.png\" align=\"left\" width=400/>\n",
    "\n",
    "  [4]: https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaf9556-5551-4fd8-ae44-4f6528d5999b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Vanishing Gradients\n",
    "\n",
    "If W is initialized to small values, layer output ```y=Wn@W(n-1)@...@W1X``` will be diminished to zero for ReLU, Tanh activations where activation is 0 for 0 input (**result depends on the activation functions**). Then  input X(i) to the next layer i+1 will be zero. Then the gradient update to W(i+1) by X(i) will be zero, hence there is no gradient update. The neuron is dead with no update forever.\n",
    "\n",
    "The hisgram is the activations of neurons (output of neurons) at each layer squashed to zero, hence no activations/signals from neurons.\n",
    "\n",
    "<img src=\"image/vanishing_gradient.png\" align=\"left\" width=650/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d223139-4d53-4155-94a7-4c1afa24edc0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"image/vanishing_gradient_example.png\" align=\"left\" width=650/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688de08f-815c-4397-9fed-9f9ee8a61473",
   "metadata": {},
   "source": [
    "[Building makemore Part 3: Activations & Gradients, BatchNorm](https://youtu.be/P6sfmUTpUmc?t=5455)\n",
    "\n",
    "With smaler weight value initialzations, activations (output of neurons) and gradients get shrunk to around zero. Neurons become dead without signaling nor learning.\n",
    "\n",
    "<img src=\"image/small_weight_initialization_impact_on_activation_gradient.png\" align=\"left\" width=650/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07208e9c-e603-4cbd-ab75-a434de1a1cf5",
   "metadata": {},
   "source": [
    "\n",
    "## Exploding Gradients\n",
    "\n",
    "<img src=\"image/explording_gradients.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9f872e-22ee-484b-941c-be982fdab778",
   "metadata": {},
   "source": [
    "With large weight initializations, activations are saturated (using tanh here) and gradients of the saturated tanh area gets to zero too.\n",
    "\n",
    "<img src=\"image/large_weight_initialization_impact_on_activation_gradient.png\" align=\"left\" width=650/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e19c435-e5fa-4428-9fef-294a29ae7820",
   "metadata": {},
   "source": [
    "## Sub Optimal Gradient Update by non zero mean normalized data\n",
    "\n",
    "Zero mean data initialization for better gradient update on W. W is updated with X. Hence if X is all positive, W update will be always negative.\n",
    "\n",
    "<img src=\"image/data_normalize_zero_center.png\" align=\"left\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e3393f-df8f-4032-a68d-733cdbb56c6f",
   "metadata": {},
   "source": [
    "###  Rate of weight update at Gradient Desent\n",
    "\n",
    "The rate by which the weight gets updated should be approx ```1e-3``` or 1/1000 ```log10(1e-3) == -3```.\n",
    "\n",
    "* [Building makemore Part 3: Activations & Gradients, BatchNorm](https://youtu.be/P6sfmUTpUmc?t=5998)\n",
    "\n",
    "> If the rate of wegit update is below 1e-3, the learning is too slow. If 1e-1, it is too high and too muuch chage.  \n",
    "> ``` update_ration_log = (lr*W.grad).std() / W.data.std()).log10() ```  # Should be approx -3\n",
    "\n",
    "<img src=\"image/proper_weight_update_ratio.png\" align=\"left\" width=750/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995f67d7-c434-4e80-b77e-2222876c6975",
   "metadata": {},
   "source": [
    "# Solutions\n",
    "\n",
    "1. Verify the weights during training that they are normally distributed with 0 mean and 1/D variance where D is input dimensions.\n",
    "2. Verify the graidients are not 0 (vanished) or too large (how much is too large?) (exploding).\n",
    "3. Use fit-for-purpose initialization e.g. Xavier, He depneing on the activation to use.\n",
    "4. Use Batch or Layer Normalization.\n",
    "5. Normalize input data.\n",
    "\n",
    "Note: Weight Initialization depends on the activation function. Xavier initialization does not work with ReLU. Needs He.\n",
    "\n",
    "### Demonstration of Xavier initialization does not work with ReLU\n",
    "<img src=\"image/xavier_break_with_reul.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc38493-2749-4238-aa1e-bba48e61b602",
   "metadata": {},
   "source": [
    "### Pytorch He Initialization\n",
    "\n",
    "<img src=\"image/torch_he_initialization.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e26efb97-1e9b-4884-96cf-4abe9d25c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229ef672-124e-4a6d-a67d-cb5ec27ce8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(t, p):\n",
    "    return np.sum(-t * np.log(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5891cccf-2798-4b33-998d-ced6c2cd7399",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "The network output logits ```y``` should be close to 0 because the model has no confidence of which class is true (for multi label classification). \n",
    "\n",
    "## Initial Large Loss\n",
    "\n",
    "If the weights are not initialized to produce small (close to 0), the logits can be large resulting in a large loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ea94117-49b9-4987-9440-481a658cdd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: [4.53978687e-05 1.18501106e-27 3.13899028e-17 9.99954602e-01]\n",
      "loss  : 62.00004539889922\n"
     ]
    }
   ],
   "source": [
    "t = np.array([0, 1, 0, 0])\n",
    "y = np.array([67., 15., 39., 77.])\n",
    "p = softmax(y, axis=-1)\n",
    "p\n",
    "\n",
    "print(f\"output: {p}\")\n",
    "print(f\"loss  : {log_loss(t=t, p=p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0412b3f-06b4-4b88-b1df-2775cd3af54a",
   "metadata": {},
   "source": [
    "### Expected Loss\n",
    "\n",
    "Ideal expected logits are ```y=[0,0,0,0]``` from which the loss value is 1.386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "538d6f85-b5d6-44e6-9f4d-3e716c84ad51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: [0.25 0.25 0.25 0.25]\n",
      "loss  : 1.3862943611198906\n"
     ]
    }
   ],
   "source": [
    "y = np.zeros(shape=4)\n",
    "p = softmax(y, axis=-1)\n",
    "\n",
    "print(f\"output: {p}\")\n",
    "print(f\"loss  : {log_loss(t=t, p=p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af05941-2725-4e5e-a967-1e3ebda8252f",
   "metadata": {},
   "source": [
    "### Mitigation\n",
    "\n",
    "For matmul ```y=x@W.T```, initialize W with normal distribution and divide by square root of the input dimension. As in the image, the standard deviation or scale of the normal distribution on the left is ```sqrt(10)``` wider after the product ```x@w``` on the right where x and w has dimension D=10. Hence, make the standard deviation of W to ```1/sqrt(D)``` so that the variance of ```x@w``` will be 1.0.\n",
    "\n",
    "* [Building makemore Part 3: Activations & Gradients, BatchNorm](https://youtu.be/P6sfmUTpUmc?t=1800)\n",
    "\n",
    "<img src=\"image/product_of_two_normal_distributions.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dc59921-6eaa-4efd-abfd-dc85646965d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: [0.15056858 0.03632626 0.69038642 0.12271874]\n",
      "loss  : 3.315214342930334\n"
     ]
    }
   ],
   "source": [
    "t = np.array([0, 1, 0, 0])\n",
    "M = len(t)  # number of labels\n",
    "D = 8\n",
    "\n",
    "x = np.random.normal(size=(D,))\n",
    "W = np.random.normal(size=(M, D)) / np.sqrt(D)\n",
    "\n",
    "y = x @ W.T\n",
    "p = softmax(y, axis=-1)\n",
    "\n",
    "print(f\"output: {p}\")\n",
    "print(f\"loss  : {log_loss(t=t, p=p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a014eb6-8b55-4cbf-b2b2-4a583f6709b4",
   "metadata": {},
   "source": [
    "### Xavier Initialization\n",
    "\n",
    "This is almost same with Xavier initialization.\n",
    "\n",
    "* [Understanding Xavier Initialization In Deep Neural Networks](https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/)\n",
    "* [Stanford CS230 Xavier Initialization](https://cs230.stanford.edu/section/4/)\n",
    "\n",
    "<img src=\"image/xavier_initialization.png\" align=\"left\" width=600/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5142b489-2310-466a-93c4-a7bcb2e8b411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: [0.76176628 0.10239613 0.05155247 0.08428512]\n",
      "loss  : 2.2789063413401904\n"
     ]
    }
   ],
   "source": [
    "# Originally Xavier initialization is using the dimensions of input and output, but using input only is common.\n",
    "W2 = np.random.normal(loc=0, scale=2/np.sqrt(D+M), size=(M,D))\n",
    "y = x @ W2.T\n",
    "p = softmax(y, axis=-1)\n",
    "\n",
    "print(f\"output: {p}\")\n",
    "print(f\"loss  : {log_loss(t=t, p=p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3dd7ba-eece-4cdb-9bc6-ed082d4d644f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
