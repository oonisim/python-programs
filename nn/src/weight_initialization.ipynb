{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84727e84-032b-475a-a545-b84c161c8b88",
   "metadata": {},
   "source": [
    "# Importance of weight initialization\n",
    "\n",
    "* [Deep Learning AI - The importance of effective initialization](https://www.deeplearning.ai/ai-notes/initialization/index.html) - MUST\n",
    "\n",
    "\n",
    "> To prevent the gradients of the network’s activations from vanishing or exploding, we will stick to the following rules of thumb:\n",
    "> \n",
    "> 1. The mean of the activations should be zero.\n",
    "> 2. The variance of the activations should stay the same across every layer.\n",
    ">\n",
    "> Under these two assumptions, the backpropagated gradient signal should not be multiplied by values too small or too large in any layer. It should travel to the input layer without exploding or vanishing.\n",
    "> n other words, all the **weights of layer ```l``` are random samples from a normal distribution** with mean ```μ=0``` and variance ```v=1/N(l-1)``` where ```N(l-1)``` is the dimensions of the input (number of outputs or number of neurons of the previous layer).\n",
    " \n",
    "## Problems\n",
    "\n",
    "### Exploding Gradients\n",
    "\n",
    "### Vanishing Gradients\n",
    "\n",
    "\n",
    "### Waste of training cycles\n",
    "\n",
    "* [Building makemore Part 3: Activations & Gradients, BatchNorm](https://youtu.be/P6sfmUTpUmc?t=259)\n",
    "\n",
    "If the weights are not properly, initial training cycles will be spent to mitigate it -> Manifest as **initial large loss** being squashed down quickly (hockey stick like learning curve).\n",
    "\n",
    "<img src=\"image/nn_weight_initialization_too_large.png\" align=\"left\" width=400/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995f67d7-c434-4e80-b77e-2222876c6975",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "1. Verify the weights during training that they are normally distributed with 0 mean and 1/D variance where D is input dimensions.\n",
    "2. Verify the graidients are not 0 (vanished) or too large (how much is too large?) (exploding).\n",
    "3. Use fit-for-purpose initialization e.g. Xavier, He depneing on the activation to use.\n",
    "4. Use Batch or Layer Normalization.\n",
    "5. Normalize input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e26efb97-1e9b-4884-96cf-4abe9d25c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229ef672-124e-4a6d-a67d-cb5ec27ce8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(t, p):\n",
    "    return np.sum(-t * np.log(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5891cccf-2798-4b33-998d-ced6c2cd7399",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "The network output logits ```y``` should be close to 0 because the model has no confidence of which class is true (for multi label classification). \n",
    "\n",
    "## Initial Large Loss\n",
    "\n",
    "If the weights are not initialized to produce small (close to 0), the logits can be large resulting in a large loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ea94117-49b9-4987-9440-481a658cdd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: [4.53978687e-05 1.18501106e-27 3.13899028e-17 9.99954602e-01]\n",
      "loss  : 62.00004539889922\n"
     ]
    }
   ],
   "source": [
    "t = np.array([0, 1, 0, 0])\n",
    "y = np.array([67., 15., 39., 77.])\n",
    "p = softmax(y, axis=-1)\n",
    "p\n",
    "\n",
    "print(f\"output: {p}\")\n",
    "print(f\"loss  : {log_loss(t=t, p=p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0412b3f-06b4-4b88-b1df-2775cd3af54a",
   "metadata": {},
   "source": [
    "### Expected Loss\n",
    "\n",
    "Ideal expected logits are ```y=[0,0,0,0]``` from which the loss value is 1.386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "538d6f85-b5d6-44e6-9f4d-3e716c84ad51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: [0.25 0.25 0.25 0.25]\n",
      "loss  : 1.3862943611198906\n"
     ]
    }
   ],
   "source": [
    "y = np.zeros(shape=4)\n",
    "p = softmax(y, axis=-1)\n",
    "\n",
    "print(f\"output: {p}\")\n",
    "print(f\"loss  : {log_loss(t=t, p=p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af05941-2725-4e5e-a967-1e3ebda8252f",
   "metadata": {},
   "source": [
    "### Mitigation\n",
    "\n",
    "For matmul ```y=x@W.T```, initialize W with normal distribution and divide by square root of the input dimension. As in the image, the standard deviation or scale of the normal distribution on the left is ```sqrt(10)``` wider after the product ```x@w``` on the right where x and w has dimension D=10. Hence, make the standard deviation of W to ```1/sqrt(D)``` so that the variance of ```x@w``` will be 1.0.\n",
    "\n",
    "* [Building makemore Part 3: Activations & Gradients, BatchNorm](https://youtu.be/P6sfmUTpUmc?t=1800)\n",
    "\n",
    "<img src=\"image/product_of_two_normal_distributions.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dc59921-6eaa-4efd-abfd-dc85646965d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: [0.15056858 0.03632626 0.69038642 0.12271874]\n",
      "loss  : 3.315214342930334\n"
     ]
    }
   ],
   "source": [
    "t = np.array([0, 1, 0, 0])\n",
    "M = len(t)  # number of labels\n",
    "D = 8\n",
    "\n",
    "x = np.random.normal(size=(D,))\n",
    "W = np.random.normal(size=(M, D)) / np.sqrt(D)\n",
    "\n",
    "y = x @ W.T\n",
    "p = softmax(y, axis=-1)\n",
    "\n",
    "print(f\"output: {p}\")\n",
    "print(f\"loss  : {log_loss(t=t, p=p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a014eb6-8b55-4cbf-b2b2-4a583f6709b4",
   "metadata": {},
   "source": [
    "### Xavier Initialization\n",
    "\n",
    "This is almost same with Xavier initialization.\n",
    "\n",
    "* [Understanding Xavier Initialization In Deep Neural Networks](https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/)\n",
    "* [Stanford CS230 Xavier Initialization](https://cs230.stanford.edu/section/4/)\n",
    "\n",
    "<img src=\"image/xavier_initialization.png\" align=\"left\" width=600/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5142b489-2310-466a-93c4-a7bcb2e8b411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: [0.76176628 0.10239613 0.05155247 0.08428512]\n",
      "loss  : 2.2789063413401904\n"
     ]
    }
   ],
   "source": [
    "# Originally Xavier initialization is using the dimensions of input and output, but using input only is common.\n",
    "W2 = np.random.normal(loc=0, scale=2/np.sqrt(D+M), size=(M,D))\n",
    "y = x @ W2.T\n",
    "p = softmax(y, axis=-1)\n",
    "\n",
    "print(f\"output: {p}\")\n",
    "print(f\"loss  : {log_loss(t=t, p=p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3dd7ba-eece-4cdb-9bc6-ed082d4d644f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
