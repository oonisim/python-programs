{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e745fa-7af1-4267-aa98-6914fda4cd54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "882df75e-ecdc-46a6-852a-89b269c5eb36",
   "metadata": {},
   "source": [
    "# Misconception - Dropout kills/rmoves an entire Neuraon\n",
    "\n",
    "It is NOT true. Every neuron is active, just temporarily/randomly disabled at each forward and back-propagation training step. \n",
    "\n",
    "* [If dropout is going to remove neurons, why are those neurons built?](https://stats.stackexchange.com/a/590808/105137)\n",
    "\n",
    "> The neurons are only dropped **temporarily during training**. They are not dropped from the network altogether. It is just that it turns out that we get better weights if we randomly set them to zero, temporarily, so the other neurons \"think\" they cannot \"rely\" on the other neurons and have to \"perform well themselves\". The neural network that you get out **at the end contains all the neurons**.\n",
    "\n",
    ">the neurons that are dropped out are **randomly selected each time the weights are updated**. So while on each iteration only some of the neurons are used and updated, **over the entire training cycle all the neurons are trained**. According to Jason Brownlee's A Gentle Introduction to Dropout for Regularizing Deep Neural Networks, dropout can be thought of as training an ensemble of models in parallel.\n",
    "\n",
    "As in the PyTorch documentation, it is **NOT the entire neuron** that is zeroed out, but the **random sampled elements in each channel** (D features e.g. **single Token Embedding vector** in Transformer).\n",
    "\n",
    "* [PyTorch Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)\n",
    "\n",
    "> The zeroed elements are chosen independently for **each forward call and are sampled** from a Bernoulli distribution.\n",
    "> Each **CHANNEL** will be zeroed out independently on every forward call.\n",
    "\n",
    "## Evidence from the implementaton\n",
    "\n",
    "As below, given ```X.shape: (N, D)``` and ```W.shape: (M, D)```, then ```H:shape = (N, M)```. The elements to be zeroed out are randomply sampled from ```(N, M)``` matrix. Therefore, a entire neuron of shape ```(M,)``` in ```H``` will NOT be entirely zeroed-out (removed). \n",
    "\n",
    "For instance, ```M``` is the dimension (num features) of a token embedding vector in Transformer. Only some of ```M``` features of a token embedding vector will be zeroed out. Hence the entire token will NOT be killed (zeroed out). Exception is when ```M==1``` such as a pixel in a MNIST digit image.\n",
    "\n",
    "<img src=\"../image/cs231n_dropout_summary.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8377d3-26b5-468b-9528-db45f8540798",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The diagram below is **misleading or entirely incorrect depending on architectures** by giving the impression that neurons get removed from the network.\n",
    "\n",
    "<img src=\"../image/incorrect_dropout_concept.png\" align=\"left\" width=250/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a6855-20ce-4b86-ae8c-0bcdc6b3d4fe",
   "metadata": {},
   "source": [
    "\n",
    "* [Why Transformer applies Dropout after Positional Encoding?](https://datascience.stackexchange.com/a/128330/68313)\n",
    "\n",
    "> Normal dropout does not remove whole tokens, but individual values within the vectors. Therefore, dropout does not remove 10% of the tokens in a sequence, but 10% of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5935a88d-62a1-4895-9710-3177de15f3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
