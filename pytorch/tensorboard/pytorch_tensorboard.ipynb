{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d766612c-cf85-4fd3-be67-3fa2f8417e00",
   "metadata": {},
   "source": [
    "* [how-to-use-tensorboard-with-pytorch](https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-tensorboard-with-pytorch.md)\n",
    "* [Pytorch TensorBoard Tutorial](https://www.youtube.com/watch?v=RLqsxWaQdHE)\n",
    "* [Using Tensorboard in Pytorch](https://krishansubudhi.github.io/deeplearning/2020/03/24/tensorboard-pytorch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34059c8d-7d09-4e51-b819-d2889c285cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from itertools import (\n",
    "    product\n",
    ")\n",
    "from typing import (\n",
    "    List,\n",
    "    Union,\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix\n",
    ")\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import (\n",
    "    SummaryWriter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89039171-97ab-425f-9121-b2730c52feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c311e96-df11-47fd-9465-61bb7f9426d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from main import (\n",
    "    tensorboard_write_histogram,\n",
    "    tensorboard_write_graph,\n",
    "    tensorboard_write_image,\n",
    "    tensorboard_write_scalar,\n",
    "    plot_confusion_matrix,\n",
    "    tensorboard_write_confusion_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e78926a-c19a-49a5-8dd1-fcd96a186603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data_mean_std(\n",
    "        loader: DataLoader\n",
    "):\n",
    "    \"\"\"Compute the mean and standard deviation of all pixels in the dataset.\n",
    "    https://saturncloud.io/blog/how-to-normalize-image-dataset-using-pytorch/\n",
    "\n",
    "    Args:\n",
    "        loader: data loader\n",
    "\n",
    "    Returns: (mean, std) where mean.shape=(3,) and std.shape=(3,)\n",
    "    \"\"\"\n",
    "    count: int = 0\n",
    "    mean: float = 0.0\n",
    "    std: float = 0.0\n",
    "\n",
    "    for images, _ in loader:\n",
    "        assert images.ndim == 4\n",
    "        batch_size, num_channels, height, width = images.shape\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Compute the mean and standard deviation for each channel separately\n",
    "        # (e.g., one value for each of the RGB channels) by specifying axis=(0, 2, 3),\n",
    "        # as the mean and standard deviation are computed across the batch, height,\n",
    "        # and width dimensions, but not across the channel dimension.\n",
    "        # --------------------------------------------------------------------------------\n",
    "        count += 1\n",
    "        mean += images.mean(axis=(0, 2, 3))\n",
    "        std += images.std(axis=(0, 2, 3))\n",
    "\n",
    "    mean /= count\n",
    "    std /= count\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db02dacd-23e9-4d0c-b263-0bbbb9ba163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predictions: torch.Tensor, truth: torch.Tensor) -> float:\n",
    "    \"\"\"Calculate prediction accuracy\n",
    "    Returns: accuracy as float\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(predictions, torch.Tensor) and isinstance(truth, torch.Tensor)\n",
    "    assert predictions.shape == truth.shape\n",
    "    return float((predictions == truth).sum().numpy().item()) / float(predictions.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6b66c4-7478-4ce3-8aed-adabe4d10424",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d24155-9d18-45b9-a2f5-c0aaf01173a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FashionMNIST(\n",
    "    os.getcwd(), \n",
    "    download=True, \n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88332e0d-4251-49ed-9c0d-12e950e34d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = test_size = (len(dataset) - train_size) // 2\n",
    "training_data, val_data, test_data = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=16, shuffle=True, num_workers=1)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=val_size, shuffle=True, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=test_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55e63a25-5cca-4f8e-a02a-e8c7a6d06c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-Shirt',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle Boot']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_label = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "list(id_to_label.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e52059bf-52bc-4fa3-b9fe-e0d49cbb06cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = next(iter(train_loader))\n",
    "x = X[0]\n",
    "\n",
    "channels: int = x.shape[0]\n",
    "width: int = x.shape[1]\n",
    "height: int = x.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac727ff9-f81b-48d0-a8b2-259083a15505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 9, 0, 8, 2, 0, 6, 5, 6, 3, 5, 5, 8, 0, 4, 5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "048f13a9-6fe9-4dc7-9b15-0e564c58b753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a401dc90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f14595-b937-46b0-a81f-bd9355c23126",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5525d8ed-1e35-42ad-9a7e-f2e36b3ffead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "  '''\n",
    "    Simple Convolutional Neural Network\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Conv2d(channels, 8, kernel_size=3, padding=\"same\"),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(8, 16, kernel_size=3, padding=\"same\"),\n",
    "      nn.BatchNorm2d(16),\n",
    "      nn.MaxPool2d(kernel_size=2),\n",
    "      nn.ReLU(),\n",
    "      nn.Flatten(),\n",
    "      nn.Dropout(p=0.2, inplace=False),\n",
    "      nn.Linear(width * height * 16 // 4, 64),\n",
    "      nn.Linear(64, 32),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(32, 10)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd57da0-b866-4bf1-b17a-196dcaac6d55",
   "metadata": {},
   "source": [
    "# Tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6ac6f08-c246-477f-897b-bda98afb4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf ./logs\n",
    "# writer = SummaryWriter(\"./logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c988ef-a029-4d44-9df5-d48b22114042",
   "metadata": {},
   "source": [
    "## Model Grapph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fba9735-7593-4714-88a3-c299e7a497b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard_write_graph(writer=writer, model=model, x=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb3952-1d51-4b51-bc61-cf853a8ce781",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19c38cd7-4033-4837-90c6-bc250a849ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes: List[int] = [\n",
    "    8, 16, 32, 64\n",
    "]\n",
    "learning_rates: List[float] = [\n",
    "    1e-3, 1e-4, 1e-5\n",
    "]\n",
    "beta1s: List[float] = [\n",
    "    0.85, 0.9, 0.95\n",
    "]\n",
    "beta2s: List[float] = [\n",
    "    0.9985, 0.999, 0.9995\n",
    "]\n",
    "decays: List[float] = [\n",
    "    0.005, 0.01, 0.015\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d473882a-fefb-4c2b-97a7-68b5ef33faec",
   "metadata": {},
   "source": [
    "learning_rates: List[float] = [\n",
    "    1e-4\n",
    "]\n",
    "beta1s: List[float] = [\n",
    "    0.9, 0.95\n",
    "]\n",
    "beta2s: List[float] = [\n",
    "    0.9985, 0.999\n",
    "]\n",
    "decays: List[float] = [\n",
    "    0.01\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389290c5-bc33-416c-8038-f1f297485e0e",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7dd062b-008c-444d-8a82-8e49d67430c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "NUM_EPOCHS: int = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c5b2482-38b1-40a8-adae-03742348e036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.85 beta2:0.9985 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch 1\n",
      "training loss/batch     1: 15.267\n",
      "training loss/batch  1001: 14.383\n",
      "training loss/batch  2001: 14.369\n",
      "val_acc 0.097 val_loss  44.729\n",
      "Starting epoch 2\n",
      "training loss/batch     1: 13.846\n",
      "training loss/batch  1001: 14.170\n",
      "training loss/batch  2001: 14.363\n",
      "val_acc 0.098 val_loss  44.445\n",
      "Starting epoch 3\n",
      "training loss/batch     1: 15.908\n",
      "training loss/batch  1001: 14.143\n",
      "training loss/batch  2001: 14.340\n",
      "val_acc 0.099 val_loss  44.533\n",
      "Starting epoch 4\n",
      "training loss/batch     1: 10.525\n",
      "training loss/batch  1001: 14.328\n",
      "training loss/batch  2001: 14.395\n",
      "val_acc 0.098 val_loss  44.867\n",
      "Starting epoch 5\n",
      "training loss/batch     1: 15.287\n",
      "training loss/batch  1001: 14.217\n",
      "training loss/batch  2001: 14.470\n",
      "val_acc 0.097 val_loss  44.983\n",
      "early stop\n",
      "test_acc 0.102 test_loss  44.729\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.85 beta2:0.9985 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch 1\n",
      "training loss/batch     1: 22.214\n",
      "training loss/batch  1001: 21.048\n",
      "training loss/batch  2001: 21.076\n",
      "val_acc 0.090 val_loss  48.208\n",
      "Starting epoch 2\n",
      "training loss/batch     1: 25.349\n",
      "training loss/batch  1001: 21.373\n",
      "training loss/batch  2001: 21.199\n",
      "val_acc 0.088 val_loss  48.278\n",
      "Starting epoch 3\n",
      "training loss/batch     1: 24.925\n",
      "training loss/batch  1001: 21.073\n",
      "training loss/batch  2001: 21.176\n",
      "val_acc 0.086 val_loss  48.176\n",
      "Starting epoch 4\n",
      "training loss/batch     1: 22.224\n",
      "training loss/batch  1001: 21.023\n",
      "training loss/batch  2001: 21.066\n",
      "val_acc 0.092 val_loss  48.185\n",
      "Starting epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/oonisim/.pyenv/versions/3.10.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/oonisim/.pyenv/versions/3.10.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \"/Users/oonisim/venv/ml/lib/python3.10/site-packages/torch/__init__.py\", line 1504, in <module>\n",
      "    from . import masked\n",
      "  File \"/Users/oonisim/venv/ml/lib/python3.10/site-packages/torch/masked/__init__.py\", line 3, in <module>\n",
      "    from ._ops import (\n",
      "  File \"/Users/oonisim/venv/ml/lib/python3.10/site-packages/torch/masked/_ops.py\", line 11, in <module>\n",
      "    from torch._prims_common import corresponding_real_dtype\n",
      "  File \"/Users/oonisim/venv/ml/lib/python3.10/site-packages/torch/_prims_common/__init__.py\", line 23, in <module>\n",
      "    import sympy\n",
      "  File \"/Users/oonisim/venv/ml/lib/python3.10/site-packages/sympy/__init__.py\", line 51, in <module>\n",
      "    from .core import (sympify, SympifyError, cacheit, Basic, Atom,\n",
      "  File \"/Users/oonisim/venv/ml/lib/python3.10/site-packages/sympy/core/__init__.py\", line 4, in <module>\n",
      "    from .sympify import sympify, SympifyError\n",
      "  File \"/Users/oonisim/venv/ml/lib/python3.10/site-packages/sympy/core/sympify.py\", line 635, in <module>\n",
      "    from .basic import Basic\n",
      "  File \"/Users/oonisim/venv/ml/lib/python3.10/site-packages/sympy/core/basic.py\", line 8, in <module>\n",
      "    from .assumptions import ManagedProperties\n",
      "  File \"/Users/oonisim/venv/ml/lib/python3.10/site-packages/sympy/core/assumptions.py\", line 220, in <module>\n",
      "    _assume_rules = FactRules([\n",
      "  File \"/Users/oonisim/venv/ml/lib/python3.10/site-packages/sympy/core/facts.py\", line 436, in __init__\n",
      "    impl_a = deduce_alpha_implications(P.rules_alpha)\n",
      "  File \"/Users/oonisim/venv/ml/lib/python3.10/site-packages/sympy/core/facts.py\", line 116, in deduce_alpha_implications\n",
      "    full_implications = transitive_closure(implications)\n",
      "  File \"/Users/oonisim/venv/ml/lib/python3.10/site-packages/sympy/core/facts.py\", line 81, in transitive_closure\n",
      "    full_implications = set(implications)\n",
      "  File \"/Users/oonisim/venv/ml/lib/python3.10/site-packages/sympy/core/logic.py\", line 236, in __hash__\n",
      "    def __hash__(self):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m _num_records: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Iterate over the DataLoader for training data\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     35\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     36\u001b[0m     _batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n",
      "File \u001b[0;32m~/venv/ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/venv/ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/venv/ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for lr, beta1, beta2, decay in product(learning_rates, beta1s, beta2s, decays):\n",
    "    print()\n",
    "    print('-' * 80)\n",
    "    print(f\"lr:{lr} beat1:{beta1} beta2:{beta2} decay:{decay}\")\n",
    "    print('-' * 80)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Initialize for each parameter combination.\n",
    "    # --------------------------------------------------------------------------------\n",
    "    prev_loss: float = float(sys.maxsize)\n",
    "    early_stop_tolerance: int = 3\n",
    "\n",
    "    run_name: str = f\"./logs/lr{lr}_beat1{beta1}_beta2{beta2}_decay{decay}\"\n",
    "    writer = SummaryWriter(run_name)\n",
    "\n",
    "    model: nn.Module = ConvNet()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(beta1, beta2), weight_decay=decay)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Train for each parameter combination.\n",
    "    # --------------------------------------------------------------------------------\n",
    "    for epoch in range(0, NUM_EPOCHS):   # epochs at maximum\n",
    "        tensorboard_write_histogram(writer=writer, model=model, step=epoch)\n",
    "    \n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "    \n",
    "        # Reset current loss value at eacch epoch\n",
    "        _current_loss: float = 0.0\n",
    "        _num_records: int = 0\n",
    "    \n",
    "        # Iterate over the DataLoader for training data\n",
    "        for index, data in enumerate(train_loader, 0):\n",
    "            inputs, targets = data\n",
    "            _batch_size: int = len(inputs)\n",
    "            _num_records += _batch_size\n",
    "    \n",
    "            # Write an image at every batch 0\n",
    "            if index == 0:\n",
    "                tensorboard_write_image(\n",
    "                    writer=writer, tag=\"image\", image=inputs[0], step=epoch, dataformats=\"CHW\"\n",
    "                )\n",
    "    \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Perform forward pass\n",
    "            # outputs = torch.argmax(model(inputs), axis=-1)\n",
    "            outputs = model(model.inputs)\n",
    "    \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs.to(torch.float), targets.to(torch.float))\n",
    "            # To avoid RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
    "            # loss.requires_grad = True\n",
    "    \n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "    \n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Print statistics\n",
    "            _current_loss += loss.item()\n",
    "    \n",
    "            if index % 1000 == 1:\n",
    "                print('training loss/batch %5d: %.3f' % (index, _current_loss / _num_records))\n",
    "\n",
    "            # if index > 209: break\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Reports per epoch\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Loss\n",
    "        tensorboard_write_scalar(\n",
    "            writer=writer, tag=\"training loss/epoch\", value=_current_loss / _num_records, step=epoch\n",
    "        )\n",
    "    \n",
    "        # Validation accuracy & confusion matrix\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = next(iter(val_loader))\n",
    "            predictions = torch.argmax(model(inputs), axis=-1)\n",
    "\n",
    "        # val_loss = loss_fn(predictions.to(torch.float), labels.to(torch.float)) / len(labels)\n",
    "        val_loss = loss_fn(predictions.to(torch.float), labels.to(torch.float)) / len(labels)\n",
    "        val_acc = get_accuracy(predictions=predictions, truth=labels)\n",
    "    \n",
    "        tensorboard_write_scalar(\n",
    "            writer=writer, tag=\"validation loss/epoch\", value=val_loss, step=epoch\n",
    "        )\n",
    "        tensorboard_write_scalar(\n",
    "            writer=writer, tag=\"validation accuracy/epoch\", value=val_acc, step=epoch\n",
    "        )\n",
    "        tensorboard_write_confusion_matrix(\n",
    "            writer=writer,\n",
    "            tag=\"validation confusion matrix\",\n",
    "            predictions=predictions.numpy(),\n",
    "            truth=labels.numpy(),\n",
    "            class_names=list(id_to_label.values()),\n",
    "            step=epoch\n",
    "        )\n",
    "        writer.add_hparams(\n",
    "            hparam_dict={\"lr\": lr, \"beta1\": beta1, \"beta2\": beta2, \"decay\": decay},\n",
    "            metric_dict={\n",
    "                \"val_accuracy\": val_acc,\n",
    "                \"val_loss\": val_loss,\n",
    "            },\n",
    "            run_name=run_name,\n",
    "        )\n",
    "        print(f'val_acc {val_acc:.03f} val_loss {val_loss: .03f}')\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Early stop\n",
    "        # --------------------------------------------------------------------------------\n",
    "        if val_loss > prev_loss:\n",
    "            early_stop_tolerance -= 1\n",
    "        else:\n",
    "            prev_loss = val_loss\n",
    "    \n",
    "        if early_stop_tolerance <= 0:\n",
    "            print(\"early stop\")\n",
    "            break\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Test\n",
    "    # --------------------------------------------------------------------------------\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = next(iter(test_loader))\n",
    "        predictions = torch.argmax(model(inputs), axis=-1)\n",
    "\n",
    "    test_loss = loss_fn(predictions.to(torch.float), labels.to(torch.float)) / len(labels)\n",
    "    test_acc = get_accuracy(predictions=predictions, truth=labels)\n",
    "    print(f'test_acc {test_acc:.03f} test_loss {test_loss: .03f}')\n",
    "\n",
    "    tensorboard_write_scalar(\n",
    "        writer=writer, tag=\"test loss/epoch\", value=test_loss, step=epoch\n",
    "    )\n",
    "    tensorboard_write_scalar(\n",
    "        writer=writer, tag=\"test accuracy/epoch\", value=test_acc, step=epoch\n",
    "    )\n",
    "    tensorboard_write_confusion_matrix(\n",
    "        writer=writer,\n",
    "        tag=\"test confusion matrix\",\n",
    "        predictions=predictions,\n",
    "        truth=labels,\n",
    "        class_names=list(id_to_label.values()),\n",
    "        step=epoch\n",
    "    )\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b6784-644f-4104-8fb4-0638f5d9765c",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b6e5e9-ee9a-4aeb-9a89-d4068cd223d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inputs, labels = next(iter(test_loader))\n",
    "    predictions = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f287e3d-2c11-4af3-b981-378398c91d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.numpy()\n",
    "predictions = np.argmax(predictions.numpy(), axis=-1).squeeze()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb111cbe-1d52-41ea-8f07-8248a99c6a73",
   "metadata": {},
   "source": [
    "matrix = confusion_matrix(y_true=labels, y_pred=predictions)\n",
    "fig = plot_confusion_matrix(matrix=matrix, class_names=list(id_to_label.values()))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d288c-5574-4ab5-822b-c23e531202ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_write_confusion_matrix(\n",
    "    writer=writer,\n",
    "    tag=\"confusion matrix\",\n",
    "    predictions=predictions,\n",
    "    truth=labels,\n",
    "    class_names=list(id_to_label.values()),\n",
    "    step=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f31978-e40b-4857-8adb-a2c986b4fca3",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae47d8ff-8122-40ca-a34f-f6353141c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d88095-f3ac-4f27-b11b-5bec21dea481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
