{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d766612c-cf85-4fd3-be67-3fa2f8417e00",
   "metadata": {},
   "source": [
    "# Pytorch Tensorboard\n",
    "\n",
    "* [PyTorch - Visualizing Models, Data, and Training with TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html)\n",
    "* [how-to-use-tensorboard-with-pytorch](https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-tensorboard-with-pytorch.md)\n",
    "* [Pytorch TensorBoard Tutorial](https://www.youtube.com/watch?v=RLqsxWaQdHE)\n",
    "* [Using Tensorboard in Pytorch](https://krishansubudhi.github.io/deeplearning/2020/03/24/tensorboard-pytorch.html)\n",
    "* [PyTorch Profiler With TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)\n",
    "\n",
    "> In this tutorial, we will use a simple Resnet model to demonstrate how to use TensorBoard plugin to analyze model performance. Profiler API is capable of recording the CPU side operations as well as the CUDA kernel launches on the GPU side. The profiler can visualize this information in TensorBoard Plugin and provide analysis of the performance bottlenecks.\n",
    "\n",
    "## Training examples\n",
    "\n",
    "[Training with PyTorch](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)\n",
    "\n",
    "> For this tutorial, weâ€™ll be using the Fashion-MNIST dataset provided by TorchVision. We use torchvision.transforms.Normalize() to zero-center and normalize the distribution of the image tile content, and download both training and validation data splits.\n",
    "\n",
    "* [Kaggle - Fashion MNIST with Pytorch](https://www.kaggle.com/code/pankajj/fashion-mnist-with-pytorch-93-accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34059c8d-7d09-4e51-b819-d2889c285cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from itertools import (\n",
    "    product\n",
    ")\n",
    "from typing import (\n",
    "    List,\n",
    "    Union,\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix\n",
    ")\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import (\n",
    "    SummaryWriter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89039171-97ab-425f-9121-b2730c52feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c311e96-df11-47fd-9465-61bb7f9426d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from main import (\n",
    "    tensorboard_write_histogram,\n",
    "    tensorboard_write_graph,\n",
    "    tensorboard_write_image,\n",
    "    tensorboard_write_scalar,\n",
    "    plot_confusion_matrix,\n",
    "    tensorboard_write_confusion_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e78926a-c19a-49a5-8dd1-fcd96a186603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data_mean_std(\n",
    "        loader: DataLoader\n",
    "):\n",
    "    \"\"\"Compute the mean and standard deviation of all pixels in the dataset.\n",
    "    Args:\n",
    "        loader: data loader\n",
    "\n",
    "    Returns: (mean, std) where mean and std has shape:(num_channels,).\n",
    "    \"\"\"\n",
    "    count: int = 0\n",
    "    mean: float = 0.0\n",
    "    std: float = 0.0\n",
    "\n",
    "    for images, _ in loader:\n",
    "        assert images.ndim == 4\n",
    "        batch_size, num_channels, height, width = images.shape\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Compute the mean and standard deviation for each channel separately\n",
    "        # (e.g., one value for each of the RGB channels) by specifying axis=(0, 2, 3),\n",
    "        # as the mean and standard deviation are computed across the batch, height,\n",
    "        # and width dimensions, but not across the channel dimension.\n",
    "        # --------------------------------------------------------------------------------\n",
    "        count += 1\n",
    "        mean += images.mean(axis=(0, 2, 3))\n",
    "        std += images.std(axis=(0, 2, 3))\n",
    "\n",
    "    mean /= count\n",
    "    std /= count\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db02dacd-23e9-4d0c-b263-0bbbb9ba163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predictions: torch.Tensor, truth: torch.Tensor) -> float:\n",
    "    \"\"\"Calculate prediction accuracy\n",
    "    Returns: accuracy as float\n",
    "    \"\"\"\n",
    "    assert isinstance(predictions, torch.Tensor) and isinstance(truth, torch.Tensor)\n",
    "    predictions = torch.argmax(predictions, axis=-1)\n",
    "    assert predictions.shape == truth.shape\n",
    "    return float((predictions == truth).sum().numpy().item()) / float(predictions.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6b66c4-7478-4ce3-8aed-adabe4d10424",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b8639-5210-41cb-b410-d1cac00857ba",
   "metadata": {},
   "source": [
    "## Train & Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ce2c7-864b-4376-b83c-d9383373c6c0",
   "metadata": {},
   "source": [
    "### Calculate the mean and std of the pixels (features) per each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d24155-9d18-45b9-a2f5-c0aaf01173a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = FashionMNIST(\n",
    "    os.getcwd(), \n",
    "    train=True,\n",
    "    download=True, \n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "mean, std = get_image_data_mean_std(\n",
    "    loader=torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True, num_workers=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b7f144-70dd-48b8-800f-f15a666af25e",
   "metadata": {},
   "source": [
    "### Normalize the image pixels (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "238ebb87-ac1a-4cbc-8058-ad59d1dd4995",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = FashionMNIST(\n",
    "    os.getcwd(), \n",
    "    train=True,\n",
    "    download=True, \n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a311e8c-7357-416c-9db3-20041ae0d7cf",
   "metadata": {},
   "source": [
    "### Verify the std/mean of normalized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac6a62dd-626a-4b47-85c7-ba57f1e339ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(mean, std)=(tensor([4.1406e-07]), tensor([1.0001]))\n"
     ]
    }
   ],
   "source": [
    "mean, std = get_image_data_mean_std(\n",
    "    loader=torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True, num_workers=1)\n",
    ")\n",
    "print(f\"(mean, std)={mean, std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a8ed9c-520a-4fe1-b2b1-0a24cb0bd00b",
   "metadata": {},
   "source": [
    "### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88332e0d-4251-49ed-9c0d-12e950e34d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset_train))\n",
    "val_size = len(dataset_train) - train_size\n",
    "training_data, val_data = torch.utils.data.random_split(dataset_train, [train_size, val_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=16, shuffle=True, num_workers=1)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=val_size,  shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd6f467-92be-4066-9c24-26d3cbb6c297",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da970e-84de-4c64-81e4-ed31a7de45ed",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8deaa01e-8e7c-4d29-942a-6024ff0f659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = FashionMNIST(\n",
    "    os.getcwd(), \n",
    "    train=False,\n",
    "    download=True, \n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    ")\n",
    "test_size: int = len(dataset_test)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=test_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa04316f-0c34-42ac-bc18-683170ac35be",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55e63a25-5cca-4f8e-a02a-e8c7a6d06c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-Shirt',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle Boot']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_label = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "list(id_to_label.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e029c38-4e64-40be-a73f-ebbf4f20d16c",
   "metadata": {},
   "source": [
    "## Sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e52059bf-52bc-4fa3-b9fe-e0d49cbb06cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = next(iter(train_loader))\n",
    "x = X[0]\n",
    "\n",
    "channels: int = x.shape[0]\n",
    "width: int = x.shape[1]\n",
    "height: int = x.shape[2]\n",
    "(channels, width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "048f13a9-6fe9-4dc7-9b15-0e564c58b753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29a7a5db0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac727ff9-f81b-48d0-a8b2-259083a15505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 0, 2, 9, 8, 0, 8, 1, 1, 3, 2, 8, 0, 2, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f14595-b937-46b0-a81f-bd9355c23126",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5525d8ed-1e35-42ad-9a7e-f2e36b3ffead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "  '''\n",
    "    Simple Convolutional Neural Network\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Conv2d(channels, 16, kernel_size=3, padding=\"same\"),\n",
    "      nn.BatchNorm2d(16),\n",
    "      nn.MaxPool2d(kernel_size=2),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(16, 32, kernel_size=3, padding=\"same\"),\n",
    "      nn.BatchNorm2d(32),\n",
    "      nn.MaxPool2d(kernel_size=2),\n",
    "      nn.ReLU(),\n",
    "      nn.Flatten(),\n",
    "      nn.Dropout(p=0.2, inplace=False),\n",
    "      nn.Linear(width * height * 32 // 16, 64),\n",
    "      nn.Linear(64, 32),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(32, 10)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd57da0-b866-4bf1-b17a-196dcaac6d55",
   "metadata": {},
   "source": [
    "# Tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6ac6f08-c246-477f-897b-bda98afb4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf ./logs\n",
    "# writer = SummaryWriter(\"./logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c988ef-a029-4d44-9df5-d48b22114042",
   "metadata": {},
   "source": [
    "## Model Grapph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fba9735-7593-4714-88a3-c299e7a497b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard_write_graph(writer=writer, model=ConvNet(), x=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb3952-1d51-4b51-bc61-cf853a8ce781",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce240eae-1c9e-41f2-9c8c-003b67dd56a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes: List[int] = [\n",
    "    8, 16, 32, 64\n",
    "]\n",
    "learning_rates: List[float] = [\n",
    "    1e-3, 1e-4, 1e-5\n",
    "]\n",
    "beta1s: List[float] = [\n",
    "    0.85, 0.9, 0.95\n",
    "]\n",
    "beta2s: List[float] = [\n",
    "    0.9985, 0.999, 0.9995\n",
    "]\n",
    "decays: List[float] = [\n",
    "    0.005, 0.01, 0.015\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b0a09a2-3511-4b8e-b908-8c2d9f666f66",
   "metadata": {},
   "source": [
    "learning_rates: List[float] = [\n",
    "    1e-4\n",
    "]\n",
    "beta1s: List[float] = [\n",
    "    0.9, 0.95\n",
    "]\n",
    "beta2s: List[float] = [\n",
    "    0.9985, 0.999\n",
    "]\n",
    "decays: List[float] = [\n",
    "    0.01\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389290c5-bc33-416c-8038-f1f297485e0e",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7dd062b-008c-444d-8a82-8e49d67430c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "NUM_EPOCHS: int = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c5b2482-38b1-40a8-adae-03742348e036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.85 beta2:0.9985 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.261\n",
      "training loss at batch:[01000]: 0.537\n",
      "training loss at batch:[02000]: 0.462\n",
      "val_acc 0.886 val_loss  0.315\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.206\n",
      "training loss at batch:[01000]: 0.319\n",
      "training loss at batch:[02000]: 0.316\n",
      "val_acc 0.903 val_loss  0.273\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.210\n",
      "training loss at batch:[01000]: 0.283\n",
      "training loss at batch:[02000]: 0.280\n",
      "val_acc 0.900 val_loss  0.274\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.209\n",
      "training loss at batch:[01000]: 0.258\n",
      "training loss at batch:[02000]: 0.260\n",
      "val_acc 0.905 val_loss  0.260\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.212\n",
      "training loss at batch:[01000]: 0.236\n",
      "training loss at batch:[02000]: 0.239\n",
      "val_acc 0.916 val_loss  0.230\n",
      "test_acc 0.389 test_loss  2.335\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.85 beta2:0.9985 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.316\n",
      "training loss at batch:[01000]: 0.549\n",
      "training loss at batch:[02000]: 0.466\n",
      "val_acc 0.887 val_loss  0.315\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.270\n",
      "training loss at batch:[01000]: 0.319\n",
      "training loss at batch:[02000]: 0.318\n",
      "val_acc 0.903 val_loss  0.278\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.105\n",
      "training loss at batch:[01000]: 0.275\n",
      "training loss at batch:[02000]: 0.277\n",
      "val_acc 0.904 val_loss  0.259\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.315\n",
      "training loss at batch:[01000]: 0.254\n",
      "training loss at batch:[02000]: 0.256\n",
      "val_acc 0.898 val_loss  0.274\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.058\n",
      "training loss at batch:[01000]: 0.236\n",
      "training loss at batch:[02000]: 0.240\n",
      "val_acc 0.914 val_loss  0.242\n",
      "test_acc 0.409 test_loss  1.825\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.85 beta2:0.9985 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.338\n",
      "training loss at batch:[01000]: 0.540\n",
      "training loss at batch:[02000]: 0.466\n",
      "val_acc 0.894 val_loss  0.304\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.627\n",
      "training loss at batch:[01000]: 0.319\n",
      "training loss at batch:[02000]: 0.311\n",
      "val_acc 0.899 val_loss  0.278\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.280\n",
      "training loss at batch:[01000]: 0.273\n",
      "training loss at batch:[02000]: 0.274\n",
      "val_acc 0.899 val_loss  0.273\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.290\n",
      "training loss at batch:[01000]: 0.247\n",
      "training loss at batch:[02000]: 0.250\n",
      "val_acc 0.914 val_loss  0.245\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.068\n",
      "training loss at batch:[01000]: 0.228\n",
      "training loss at batch:[02000]: 0.235\n",
      "val_acc 0.916 val_loss  0.233\n",
      "test_acc 0.394 test_loss  2.060\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.85 beta2:0.999 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.293\n",
      "training loss at batch:[01000]: 0.532\n",
      "training loss at batch:[02000]: 0.468\n",
      "val_acc 0.894 val_loss  0.303\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.566\n",
      "training loss at batch:[01000]: 0.320\n",
      "training loss at batch:[02000]: 0.314\n",
      "val_acc 0.900 val_loss  0.279\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.503\n",
      "training loss at batch:[01000]: 0.279\n",
      "training loss at batch:[02000]: 0.280\n",
      "val_acc 0.906 val_loss  0.265\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.206\n",
      "training loss at batch:[01000]: 0.253\n",
      "training loss at batch:[02000]: 0.255\n",
      "val_acc 0.904 val_loss  0.269\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.238\n",
      "training loss at batch:[01000]: 0.236\n",
      "training loss at batch:[02000]: 0.241\n",
      "val_acc 0.904 val_loss  0.271\n",
      "test_acc 0.603 test_loss  1.283\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.85 beta2:0.999 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.288\n",
      "training loss at batch:[01000]: 0.548\n",
      "training loss at batch:[02000]: 0.467\n",
      "val_acc 0.890 val_loss  0.305\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.100\n",
      "training loss at batch:[01000]: 0.324\n",
      "training loss at batch:[02000]: 0.316\n",
      "val_acc 0.905 val_loss  0.266\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.041\n",
      "training loss at batch:[01000]: 0.284\n",
      "training loss at batch:[02000]: 0.279\n",
      "val_acc 0.902 val_loss  0.264\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.436\n",
      "training loss at batch:[01000]: 0.259\n",
      "training loss at batch:[02000]: 0.255\n",
      "val_acc 0.903 val_loss  0.265\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.032\n",
      "training loss at batch:[01000]: 0.236\n",
      "training loss at batch:[02000]: 0.244\n",
      "val_acc 0.911 val_loss  0.243\n",
      "test_acc 0.407 test_loss  1.851\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.85 beta2:0.999 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.330\n",
      "training loss at batch:[01000]: 0.545\n",
      "training loss at batch:[02000]: 0.465\n",
      "val_acc 0.893 val_loss  0.298\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.274\n",
      "training loss at batch:[01000]: 0.322\n",
      "training loss at batch:[02000]: 0.317\n",
      "val_acc 0.894 val_loss  0.296\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.619\n",
      "training loss at batch:[01000]: 0.283\n",
      "training loss at batch:[02000]: 0.284\n",
      "val_acc 0.909 val_loss  0.254\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.444\n",
      "training loss at batch:[01000]: 0.261\n",
      "training loss at batch:[02000]: 0.260\n",
      "val_acc 0.908 val_loss  0.253\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.081\n",
      "training loss at batch:[01000]: 0.234\n",
      "training loss at batch:[02000]: 0.239\n",
      "val_acc 0.910 val_loss  0.249\n",
      "test_acc 0.518 test_loss  1.590\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.85 beta2:0.9995 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.338\n",
      "training loss at batch:[01000]: 0.540\n",
      "training loss at batch:[02000]: 0.461\n",
      "val_acc 0.886 val_loss  0.312\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.420\n",
      "training loss at batch:[01000]: 0.322\n",
      "training loss at batch:[02000]: 0.315\n",
      "val_acc 0.898 val_loss  0.278\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.113\n",
      "training loss at batch:[01000]: 0.277\n",
      "training loss at batch:[02000]: 0.280\n",
      "val_acc 0.904 val_loss  0.268\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.079\n",
      "training loss at batch:[01000]: 0.253\n",
      "training loss at batch:[02000]: 0.258\n",
      "val_acc 0.906 val_loss  0.262\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.358\n",
      "training loss at batch:[01000]: 0.240\n",
      "training loss at batch:[02000]: 0.240\n",
      "val_acc 0.907 val_loss  0.252\n",
      "test_acc 0.203 test_loss  3.168\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.85 beta2:0.9995 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.251\n",
      "training loss at batch:[01000]: 0.549\n",
      "training loss at batch:[02000]: 0.469\n",
      "val_acc 0.893 val_loss  0.317\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.662\n",
      "training loss at batch:[01000]: 0.333\n",
      "training loss at batch:[02000]: 0.324\n",
      "val_acc 0.901 val_loss  0.282\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.378\n",
      "training loss at batch:[01000]: 0.294\n",
      "training loss at batch:[02000]: 0.288\n",
      "val_acc 0.897 val_loss  0.281\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.522\n",
      "training loss at batch:[01000]: 0.251\n",
      "training loss at batch:[02000]: 0.259\n",
      "val_acc 0.908 val_loss  0.260\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.214\n",
      "training loss at batch:[01000]: 0.245\n",
      "training loss at batch:[02000]: 0.247\n",
      "val_acc 0.914 val_loss  0.236\n",
      "test_acc 0.358 test_loss  2.658\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.85 beta2:0.9995 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.225\n",
      "training loss at batch:[01000]: 0.540\n",
      "training loss at batch:[02000]: 0.462\n",
      "val_acc 0.896 val_loss  0.297\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.344\n",
      "training loss at batch:[01000]: 0.311\n",
      "training loss at batch:[02000]: 0.309\n",
      "val_acc 0.891 val_loss  0.298\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.217\n",
      "training loss at batch:[01000]: 0.280\n",
      "training loss at batch:[02000]: 0.273\n",
      "val_acc 0.913 val_loss  0.254\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.288\n",
      "training loss at batch:[01000]: 0.257\n",
      "training loss at batch:[02000]: 0.257\n",
      "val_acc 0.902 val_loss  0.273\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.303\n",
      "training loss at batch:[01000]: 0.242\n",
      "training loss at batch:[02000]: 0.241\n",
      "val_acc 0.914 val_loss  0.238\n",
      "test_acc 0.399 test_loss  1.674\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.9 beta2:0.9985 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.472\n",
      "training loss at batch:[01000]: 0.548\n",
      "training loss at batch:[02000]: 0.468\n",
      "val_acc 0.894 val_loss  0.298\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.410\n",
      "training loss at batch:[01000]: 0.313\n",
      "training loss at batch:[02000]: 0.311\n",
      "val_acc 0.905 val_loss  0.263\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.177\n",
      "training loss at batch:[01000]: 0.278\n",
      "training loss at batch:[02000]: 0.280\n",
      "val_acc 0.903 val_loss  0.273\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.110\n",
      "training loss at batch:[01000]: 0.253\n",
      "training loss at batch:[02000]: 0.255\n",
      "val_acc 0.890 val_loss  0.283\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.056\n",
      "training loss at batch:[01000]: 0.238\n",
      "training loss at batch:[02000]: 0.238\n",
      "val_acc 0.911 val_loss  0.250\n",
      "test_acc 0.474 test_loss  1.714\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.9 beta2:0.9985 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.311\n",
      "training loss at batch:[01000]: 0.547\n",
      "training loss at batch:[02000]: 0.473\n",
      "val_acc 0.874 val_loss  0.333\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.230\n",
      "training loss at batch:[01000]: 0.328\n",
      "training loss at batch:[02000]: 0.318\n",
      "val_acc 0.889 val_loss  0.308\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.152\n",
      "training loss at batch:[01000]: 0.279\n",
      "training loss at batch:[02000]: 0.278\n",
      "val_acc 0.907 val_loss  0.254\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.338\n",
      "training loss at batch:[01000]: 0.256\n",
      "training loss at batch:[02000]: 0.256\n",
      "val_acc 0.908 val_loss  0.249\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.567\n",
      "training loss at batch:[01000]: 0.238\n",
      "training loss at batch:[02000]: 0.237\n",
      "val_acc 0.914 val_loss  0.237\n",
      "test_acc 0.531 test_loss  1.158\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.9 beta2:0.9985 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.298\n",
      "training loss at batch:[01000]: 0.561\n",
      "training loss at batch:[02000]: 0.472\n",
      "val_acc 0.881 val_loss  0.326\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.127\n",
      "training loss at batch:[01000]: 0.328\n",
      "training loss at batch:[02000]: 0.321\n",
      "val_acc 0.905 val_loss  0.265\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.242\n",
      "training loss at batch:[01000]: 0.280\n",
      "training loss at batch:[02000]: 0.279\n",
      "val_acc 0.904 val_loss  0.263\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.152\n",
      "training loss at batch:[01000]: 0.252\n",
      "training loss at batch:[02000]: 0.255\n",
      "val_acc 0.909 val_loss  0.250\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.261\n",
      "training loss at batch:[01000]: 0.235\n",
      "training loss at batch:[02000]: 0.237\n",
      "val_acc 0.910 val_loss  0.243\n",
      "test_acc 0.250 test_loss  2.774\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.9 beta2:0.999 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.364\n",
      "training loss at batch:[01000]: 0.556\n",
      "training loss at batch:[02000]: 0.481\n",
      "val_acc 0.891 val_loss  0.308\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.164\n",
      "training loss at batch:[01000]: 0.327\n",
      "training loss at batch:[02000]: 0.324\n",
      "val_acc 0.876 val_loss  0.330\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.096\n",
      "training loss at batch:[01000]: 0.283\n",
      "training loss at batch:[02000]: 0.285\n",
      "val_acc 0.906 val_loss  0.265\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.202\n",
      "training loss at batch:[01000]: 0.264\n",
      "training loss at batch:[02000]: 0.256\n",
      "val_acc 0.916 val_loss  0.233\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.315\n",
      "training loss at batch:[01000]: 0.240\n",
      "training loss at batch:[02000]: 0.239\n",
      "val_acc 0.900 val_loss  0.278\n",
      "test_acc 0.420 test_loss  1.826\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.9 beta2:0.999 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.298\n",
      "training loss at batch:[01000]: 0.526\n",
      "training loss at batch:[02000]: 0.459\n",
      "val_acc 0.890 val_loss  0.313\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.049\n",
      "training loss at batch:[01000]: 0.320\n",
      "training loss at batch:[02000]: 0.324\n",
      "val_acc 0.902 val_loss  0.271\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.464\n",
      "training loss at batch:[01000]: 0.281\n",
      "training loss at batch:[02000]: 0.283\n",
      "val_acc 0.905 val_loss  0.261\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.253\n",
      "training loss at batch:[01000]: 0.258\n",
      "training loss at batch:[02000]: 0.259\n",
      "val_acc 0.906 val_loss  0.259\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.126\n",
      "training loss at batch:[01000]: 0.244\n",
      "training loss at batch:[02000]: 0.245\n",
      "val_acc 0.914 val_loss  0.241\n",
      "test_acc 0.330 test_loss  1.951\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.9 beta2:0.999 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.327\n",
      "training loss at batch:[01000]: 0.542\n",
      "training loss at batch:[02000]: 0.472\n",
      "val_acc 0.881 val_loss  0.331\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.200\n",
      "training loss at batch:[01000]: 0.326\n",
      "training loss at batch:[02000]: 0.315\n",
      "val_acc 0.887 val_loss  0.311\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.337\n",
      "training loss at batch:[01000]: 0.287\n",
      "training loss at batch:[02000]: 0.283\n",
      "val_acc 0.893 val_loss  0.286\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.179\n",
      "training loss at batch:[01000]: 0.259\n",
      "training loss at batch:[02000]: 0.260\n",
      "val_acc 0.906 val_loss  0.255\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.119\n",
      "training loss at batch:[01000]: 0.236\n",
      "training loss at batch:[02000]: 0.243\n",
      "val_acc 0.913 val_loss  0.241\n",
      "test_acc 0.322 test_loss  3.085\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.9 beta2:0.9995 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.303\n",
      "training loss at batch:[01000]: 0.533\n",
      "training loss at batch:[02000]: 0.462\n",
      "val_acc 0.867 val_loss  0.361\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.152\n",
      "training loss at batch:[01000]: 0.315\n",
      "training loss at batch:[02000]: 0.308\n",
      "val_acc 0.903 val_loss  0.270\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.756\n",
      "training loss at batch:[01000]: 0.272\n",
      "training loss at batch:[02000]: 0.271\n",
      "val_acc 0.910 val_loss  0.254\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.053\n",
      "training loss at batch:[01000]: 0.245\n",
      "training loss at batch:[02000]: 0.251\n",
      "val_acc 0.908 val_loss  0.252\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.143\n",
      "training loss at batch:[01000]: 0.231\n",
      "training loss at batch:[02000]: 0.234\n",
      "val_acc 0.910 val_loss  0.249\n",
      "test_acc 0.564 test_loss  1.403\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.9 beta2:0.9995 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.300\n",
      "training loss at batch:[01000]: 0.551\n",
      "training loss at batch:[02000]: 0.475\n",
      "val_acc 0.881 val_loss  0.325\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.088\n",
      "training loss at batch:[01000]: 0.319\n",
      "training loss at batch:[02000]: 0.314\n",
      "val_acc 0.898 val_loss  0.282\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.811\n",
      "training loss at batch:[01000]: 0.290\n",
      "training loss at batch:[02000]: 0.286\n",
      "val_acc 0.900 val_loss  0.273\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.117\n",
      "training loss at batch:[01000]: 0.255\n",
      "training loss at batch:[02000]: 0.258\n",
      "val_acc 0.907 val_loss  0.255\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.034\n",
      "training loss at batch:[01000]: 0.244\n",
      "training loss at batch:[02000]: 0.244\n",
      "val_acc 0.910 val_loss  0.247\n",
      "test_acc 0.587 test_loss  1.231\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.9 beta2:0.9995 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.261\n",
      "training loss at batch:[01000]: 0.559\n",
      "training loss at batch:[02000]: 0.478\n",
      "val_acc 0.885 val_loss  0.316\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.405\n",
      "training loss at batch:[01000]: 0.311\n",
      "training loss at batch:[02000]: 0.314\n",
      "val_acc 0.903 val_loss  0.276\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.115\n",
      "training loss at batch:[01000]: 0.279\n",
      "training loss at batch:[02000]: 0.278\n",
      "val_acc 0.905 val_loss  0.259\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.258\n",
      "training loss at batch:[01000]: 0.263\n",
      "training loss at batch:[02000]: 0.259\n",
      "val_acc 0.907 val_loss  0.256\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.226\n",
      "training loss at batch:[01000]: 0.250\n",
      "training loss at batch:[02000]: 0.245\n",
      "val_acc 0.916 val_loss  0.237\n",
      "test_acc 0.502 test_loss  1.579\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.95 beta2:0.9985 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.406\n",
      "training loss at batch:[01000]: 0.556\n",
      "training loss at batch:[02000]: 0.477\n",
      "val_acc 0.872 val_loss  0.345\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.261\n",
      "training loss at batch:[01000]: 0.318\n",
      "training loss at batch:[02000]: 0.315\n",
      "val_acc 0.891 val_loss  0.295\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.289\n",
      "training loss at batch:[01000]: 0.284\n",
      "training loss at batch:[02000]: 0.286\n",
      "val_acc 0.910 val_loss  0.255\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.478\n",
      "training loss at batch:[01000]: 0.255\n",
      "training loss at batch:[02000]: 0.253\n",
      "val_acc 0.914 val_loss  0.243\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.211\n",
      "training loss at batch:[01000]: 0.241\n",
      "training loss at batch:[02000]: 0.243\n",
      "val_acc 0.916 val_loss  0.228\n",
      "test_acc 0.319 test_loss  2.137\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.95 beta2:0.9985 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.311\n",
      "training loss at batch:[01000]: 0.532\n",
      "training loss at batch:[02000]: 0.467\n",
      "val_acc 0.880 val_loss  0.343\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.149\n",
      "training loss at batch:[01000]: 0.332\n",
      "training loss at batch:[02000]: 0.328\n",
      "val_acc 0.898 val_loss  0.280\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.339\n",
      "training loss at batch:[01000]: 0.289\n",
      "training loss at batch:[02000]: 0.286\n",
      "val_acc 0.886 val_loss  0.311\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.384\n",
      "training loss at batch:[01000]: 0.251\n",
      "training loss at batch:[02000]: 0.259\n",
      "val_acc 0.908 val_loss  0.253\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.230\n",
      "training loss at batch:[01000]: 0.245\n",
      "training loss at batch:[02000]: 0.242\n",
      "val_acc 0.918 val_loss  0.236\n",
      "test_acc 0.102 test_loss  4.430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.95 beta2:0.9985 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.353\n",
      "training loss at batch:[01000]: 0.533\n",
      "training loss at batch:[02000]: 0.467\n",
      "val_acc 0.889 val_loss  0.314\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.618\n",
      "training loss at batch:[01000]: 0.317\n",
      "training loss at batch:[02000]: 0.322\n",
      "val_acc 0.895 val_loss  0.286\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.049\n",
      "training loss at batch:[01000]: 0.269\n",
      "training loss at batch:[02000]: 0.275\n",
      "val_acc 0.909 val_loss  0.250\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.259\n",
      "training loss at batch:[01000]: 0.258\n",
      "training loss at batch:[02000]: 0.260\n",
      "val_acc 0.901 val_loss  0.269\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.133\n",
      "training loss at batch:[01000]: 0.241\n",
      "training loss at batch:[02000]: 0.246\n",
      "val_acc 0.910 val_loss  0.253\n",
      "test_acc 0.159 test_loss  4.094\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.95 beta2:0.999 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.338\n",
      "training loss at batch:[01000]: 0.558\n",
      "training loss at batch:[02000]: 0.476\n",
      "val_acc 0.888 val_loss  0.309\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.253\n",
      "training loss at batch:[01000]: 0.315\n",
      "training loss at batch:[02000]: 0.314\n",
      "val_acc 0.895 val_loss  0.294\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.364\n",
      "training loss at batch:[01000]: 0.277\n",
      "training loss at batch:[02000]: 0.279\n",
      "val_acc 0.891 val_loss  0.303\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.108\n",
      "training loss at batch:[01000]: 0.259\n",
      "training loss at batch:[02000]: 0.260\n",
      "val_acc 0.906 val_loss  0.273\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.578\n",
      "training loss at batch:[01000]: 0.240\n",
      "training loss at batch:[02000]: 0.241\n",
      "val_acc 0.916 val_loss  0.235\n",
      "test_acc 0.408 test_loss  2.313\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.95 beta2:0.999 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.300\n",
      "training loss at batch:[01000]: 0.563\n",
      "training loss at batch:[02000]: 0.480\n",
      "val_acc 0.874 val_loss  0.360\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.349\n",
      "training loss at batch:[01000]: 0.327\n",
      "training loss at batch:[02000]: 0.322\n",
      "val_acc 0.889 val_loss  0.306\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.289\n",
      "training loss at batch:[01000]: 0.287\n",
      "training loss at batch:[02000]: 0.283\n",
      "val_acc 0.904 val_loss  0.268\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.177\n",
      "training loss at batch:[01000]: 0.257\n",
      "training loss at batch:[02000]: 0.259\n",
      "val_acc 0.909 val_loss  0.250\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.216\n",
      "training loss at batch:[01000]: 0.238\n",
      "training loss at batch:[02000]: 0.241\n",
      "val_acc 0.912 val_loss  0.247\n",
      "test_acc 0.383 test_loss  1.968\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.95 beta2:0.999 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.334\n",
      "training loss at batch:[01000]: 0.540\n",
      "training loss at batch:[02000]: 0.467\n",
      "val_acc 0.889 val_loss  0.314\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.246\n",
      "training loss at batch:[01000]: 0.326\n",
      "training loss at batch:[02000]: 0.320\n",
      "val_acc 0.902 val_loss  0.273\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.152\n",
      "training loss at batch:[01000]: 0.276\n",
      "training loss at batch:[02000]: 0.277\n",
      "val_acc 0.900 val_loss  0.277\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.572\n",
      "training loss at batch:[01000]: 0.257\n",
      "training loss at batch:[02000]: 0.258\n",
      "val_acc 0.908 val_loss  0.248\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.287\n",
      "training loss at batch:[01000]: 0.245\n",
      "training loss at batch:[02000]: 0.242\n",
      "val_acc 0.912 val_loss  0.241\n",
      "test_acc 0.411 test_loss  2.004\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.95 beta2:0.9995 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.282\n",
      "training loss at batch:[01000]: 0.559\n",
      "training loss at batch:[02000]: 0.476\n",
      "val_acc 0.867 val_loss  0.353\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.268\n",
      "training loss at batch:[01000]: 0.320\n",
      "training loss at batch:[02000]: 0.320\n",
      "val_acc 0.892 val_loss  0.296\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.186\n",
      "training loss at batch:[01000]: 0.277\n",
      "training loss at batch:[02000]: 0.285\n",
      "val_acc 0.902 val_loss  0.269\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.251\n",
      "training loss at batch:[01000]: 0.261\n",
      "training loss at batch:[02000]: 0.259\n",
      "val_acc 0.909 val_loss  0.249\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.216\n",
      "training loss at batch:[01000]: 0.238\n",
      "training loss at batch:[02000]: 0.241\n",
      "val_acc 0.912 val_loss  0.241\n",
      "test_acc 0.335 test_loss  2.324\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.95 beta2:0.9995 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.255\n",
      "training loss at batch:[01000]: 0.536\n",
      "training loss at batch:[02000]: 0.461\n",
      "val_acc 0.890 val_loss  0.311\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.367\n",
      "training loss at batch:[01000]: 0.326\n",
      "training loss at batch:[02000]: 0.315\n",
      "val_acc 0.904 val_loss  0.265\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.376\n",
      "training loss at batch:[01000]: 0.282\n",
      "training loss at batch:[02000]: 0.285\n",
      "val_acc 0.909 val_loss  0.255\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.297\n",
      "training loss at batch:[01000]: 0.260\n",
      "training loss at batch:[02000]: 0.264\n",
      "val_acc 0.892 val_loss  0.295\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.506\n",
      "training loss at batch:[01000]: 0.240\n",
      "training loss at batch:[02000]: 0.243\n",
      "val_acc 0.916 val_loss  0.234\n",
      "test_acc 0.306 test_loss  2.062\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.001 beat1:0.95 beta2:0.9995 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.382\n",
      "training loss at batch:[01000]: 0.534\n",
      "training loss at batch:[02000]: 0.464\n",
      "val_acc 0.887 val_loss  0.319\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.195\n",
      "training loss at batch:[01000]: 0.320\n",
      "training loss at batch:[02000]: 0.316\n",
      "val_acc 0.902 val_loss  0.279\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.458\n",
      "training loss at batch:[01000]: 0.272\n",
      "training loss at batch:[02000]: 0.279\n",
      "val_acc 0.899 val_loss  0.272\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.446\n",
      "training loss at batch:[01000]: 0.262\n",
      "training loss at batch:[02000]: 0.260\n",
      "val_acc 0.913 val_loss  0.243\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.424\n",
      "training loss at batch:[01000]: 0.235\n",
      "training loss at batch:[02000]: 0.238\n",
      "val_acc 0.901 val_loss  0.271\n",
      "test_acc 0.390 test_loss  1.915\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.85 beta2:0.9985 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.348\n",
      "training loss at batch:[01000]: 0.777\n",
      "training loss at batch:[02000]: 0.614\n",
      "val_acc 0.871 val_loss  0.366\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.477\n",
      "training loss at batch:[01000]: 0.359\n",
      "training loss at batch:[02000]: 0.355\n",
      "val_acc 0.886 val_loss  0.319\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.107\n",
      "training loss at batch:[01000]: 0.314\n",
      "training loss at batch:[02000]: 0.316\n",
      "val_acc 0.899 val_loss  0.292\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.284\n",
      "training loss at batch:[01000]: 0.307\n",
      "training loss at batch:[02000]: 0.304\n",
      "val_acc 0.906 val_loss  0.273\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.242\n",
      "training loss at batch:[01000]: 0.288\n",
      "training loss at batch:[02000]: 0.286\n",
      "val_acc 0.909 val_loss  0.261\n",
      "test_acc 0.321 test_loss  1.954\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.85 beta2:0.9985 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.383\n",
      "training loss at batch:[01000]: 0.821\n",
      "training loss at batch:[02000]: 0.646\n",
      "val_acc 0.875 val_loss  0.367\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.285\n",
      "training loss at batch:[01000]: 0.374\n",
      "training loss at batch:[02000]: 0.363\n",
      "val_acc 0.893 val_loss  0.315\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.279\n",
      "training loss at batch:[01000]: 0.326\n",
      "training loss at batch:[02000]: 0.323\n",
      "val_acc 0.892 val_loss  0.307\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.564\n",
      "training loss at batch:[01000]: 0.308\n",
      "training loss at batch:[02000]: 0.301\n",
      "val_acc 0.900 val_loss  0.286\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.286\n",
      "training loss at batch:[01000]: 0.283\n",
      "training loss at batch:[02000]: 0.286\n",
      "val_acc 0.904 val_loss  0.272\n",
      "test_acc 0.552 test_loss  1.423\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.85 beta2:0.9985 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.332\n",
      "training loss at batch:[01000]: 0.764\n",
      "training loss at batch:[02000]: 0.611\n",
      "val_acc 0.872 val_loss  0.368\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.174\n",
      "training loss at batch:[01000]: 0.372\n",
      "training loss at batch:[02000]: 0.366\n",
      "val_acc 0.889 val_loss  0.318\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.274\n",
      "training loss at batch:[01000]: 0.330\n",
      "training loss at batch:[02000]: 0.324\n",
      "val_acc 0.896 val_loss  0.293\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.451\n",
      "training loss at batch:[01000]: 0.309\n",
      "training loss at batch:[02000]: 0.307\n",
      "val_acc 0.900 val_loss  0.282\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.374\n",
      "training loss at batch:[01000]: 0.290\n",
      "training loss at batch:[02000]: 0.285\n",
      "val_acc 0.902 val_loss  0.280\n",
      "test_acc 0.316 test_loss  1.879\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.85 beta2:0.999 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.288\n",
      "training loss at batch:[01000]: 0.775\n",
      "training loss at batch:[02000]: 0.617\n",
      "val_acc 0.880 val_loss  0.357\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.187\n",
      "training loss at batch:[01000]: 0.362\n",
      "training loss at batch:[02000]: 0.356\n",
      "val_acc 0.894 val_loss  0.311\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.325\n",
      "training loss at batch:[01000]: 0.327\n",
      "training loss at batch:[02000]: 0.323\n",
      "val_acc 0.897 val_loss  0.292\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.343\n",
      "training loss at batch:[01000]: 0.294\n",
      "training loss at batch:[02000]: 0.295\n",
      "val_acc 0.897 val_loss  0.293\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.147\n",
      "training loss at batch:[01000]: 0.285\n",
      "training loss at batch:[02000]: 0.281\n",
      "val_acc 0.907 val_loss  0.264\n",
      "test_acc 0.548 test_loss  1.385\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.85 beta2:0.999 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.326\n",
      "training loss at batch:[01000]: 0.806\n",
      "training loss at batch:[02000]: 0.626\n",
      "val_acc 0.883 val_loss  0.349\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.274\n",
      "training loss at batch:[01000]: 0.361\n",
      "training loss at batch:[02000]: 0.354\n",
      "val_acc 0.895 val_loss  0.309\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.855\n",
      "training loss at batch:[01000]: 0.311\n",
      "training loss at batch:[02000]: 0.314\n",
      "val_acc 0.903 val_loss  0.283\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.392\n",
      "training loss at batch:[01000]: 0.298\n",
      "training loss at batch:[02000]: 0.293\n",
      "val_acc 0.900 val_loss  0.286\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.327\n",
      "training loss at batch:[01000]: 0.282\n",
      "training loss at batch:[02000]: 0.281\n",
      "val_acc 0.910 val_loss  0.261\n",
      "test_acc 0.408 test_loss  1.547\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.85 beta2:0.999 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.248\n",
      "training loss at batch:[01000]: 0.776\n",
      "training loss at batch:[02000]: 0.618\n",
      "val_acc 0.874 val_loss  0.362\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.292\n",
      "training loss at batch:[01000]: 0.370\n",
      "training loss at batch:[02000]: 0.366\n",
      "val_acc 0.885 val_loss  0.322\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.058\n",
      "training loss at batch:[01000]: 0.331\n",
      "training loss at batch:[02000]: 0.329\n",
      "val_acc 0.892 val_loss  0.307\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.416\n",
      "training loss at batch:[01000]: 0.312\n",
      "training loss at batch:[02000]: 0.307\n",
      "val_acc 0.903 val_loss  0.275\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.089\n",
      "training loss at batch:[01000]: 0.290\n",
      "training loss at batch:[02000]: 0.293\n",
      "val_acc 0.902 val_loss  0.272\n",
      "test_acc 0.390 test_loss  1.754\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.85 beta2:0.9995 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.295\n",
      "training loss at batch:[01000]: 0.792\n",
      "training loss at batch:[02000]: 0.623\n",
      "val_acc 0.869 val_loss  0.373\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.382\n",
      "training loss at batch:[01000]: 0.363\n",
      "training loss at batch:[02000]: 0.358\n",
      "val_acc 0.893 val_loss  0.311\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.415\n",
      "training loss at batch:[01000]: 0.316\n",
      "training loss at batch:[02000]: 0.317\n",
      "val_acc 0.899 val_loss  0.288\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.269\n",
      "training loss at batch:[01000]: 0.299\n",
      "training loss at batch:[02000]: 0.295\n",
      "val_acc 0.900 val_loss  0.279\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.356\n",
      "training loss at batch:[01000]: 0.272\n",
      "training loss at batch:[02000]: 0.277\n",
      "val_acc 0.907 val_loss  0.267\n",
      "test_acc 0.627 test_loss  1.303\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.85 beta2:0.9995 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.271\n",
      "training loss at batch:[01000]: 0.756\n",
      "training loss at batch:[02000]: 0.602\n",
      "val_acc 0.879 val_loss  0.352\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.257\n",
      "training loss at batch:[01000]: 0.363\n",
      "training loss at batch:[02000]: 0.363\n",
      "val_acc 0.890 val_loss  0.320\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.192\n",
      "training loss at batch:[01000]: 0.321\n",
      "training loss at batch:[02000]: 0.322\n",
      "val_acc 0.895 val_loss  0.295\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.138\n",
      "training loss at batch:[01000]: 0.301\n",
      "training loss at batch:[02000]: 0.300\n",
      "val_acc 0.903 val_loss  0.280\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.247\n",
      "training loss at batch:[01000]: 0.287\n",
      "training loss at batch:[02000]: 0.286\n",
      "val_acc 0.907 val_loss  0.267\n",
      "test_acc 0.595 test_loss  1.306\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.85 beta2:0.9995 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.243\n",
      "training loss at batch:[01000]: 0.796\n",
      "training loss at batch:[02000]: 0.630\n",
      "val_acc 0.864 val_loss  0.379\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.215\n",
      "training loss at batch:[01000]: 0.381\n",
      "training loss at batch:[02000]: 0.372\n",
      "val_acc 0.892 val_loss  0.319\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.270\n",
      "training loss at batch:[01000]: 0.328\n",
      "training loss at batch:[02000]: 0.329\n",
      "val_acc 0.899 val_loss  0.296\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.608\n",
      "training loss at batch:[01000]: 0.308\n",
      "training loss at batch:[02000]: 0.307\n",
      "val_acc 0.906 val_loss  0.277\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.299\n",
      "training loss at batch:[01000]: 0.293\n",
      "training loss at batch:[02000]: 0.292\n",
      "val_acc 0.907 val_loss  0.270\n",
      "test_acc 0.541 test_loss  1.606\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.9 beta2:0.9985 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.291\n",
      "training loss at batch:[01000]: 0.788\n",
      "training loss at batch:[02000]: 0.623\n",
      "val_acc 0.877 val_loss  0.359\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.666\n",
      "training loss at batch:[01000]: 0.372\n",
      "training loss at batch:[02000]: 0.366\n",
      "val_acc 0.890 val_loss  0.318\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.375\n",
      "training loss at batch:[01000]: 0.335\n",
      "training loss at batch:[02000]: 0.324\n",
      "val_acc 0.888 val_loss  0.316\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.182\n",
      "training loss at batch:[01000]: 0.306\n",
      "training loss at batch:[02000]: 0.302\n",
      "val_acc 0.898 val_loss  0.293\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.143\n",
      "training loss at batch:[01000]: 0.291\n",
      "training loss at batch:[02000]: 0.289\n",
      "val_acc 0.901 val_loss  0.276\n",
      "test_acc 0.260 test_loss  2.103\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.9 beta2:0.9985 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.334\n",
      "training loss at batch:[01000]: 0.783\n",
      "training loss at batch:[02000]: 0.625\n",
      "val_acc 0.869 val_loss  0.369\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.964\n",
      "training loss at batch:[01000]: 0.373\n",
      "training loss at batch:[02000]: 0.366\n",
      "val_acc 0.887 val_loss  0.315\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.196\n",
      "training loss at batch:[01000]: 0.322\n",
      "training loss at batch:[02000]: 0.316\n",
      "val_acc 0.900 val_loss  0.282\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.276\n",
      "training loss at batch:[01000]: 0.296\n",
      "training loss at batch:[02000]: 0.298\n",
      "val_acc 0.901 val_loss  0.280\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.428\n",
      "training loss at batch:[01000]: 0.282\n",
      "training loss at batch:[02000]: 0.283\n",
      "val_acc 0.906 val_loss  0.265\n",
      "test_acc 0.583 test_loss  1.395\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.9 beta2:0.9985 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.308\n",
      "training loss at batch:[01000]: 0.805\n",
      "training loss at batch:[02000]: 0.635\n",
      "val_acc 0.866 val_loss  0.378\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.353\n",
      "training loss at batch:[01000]: 0.373\n",
      "training loss at batch:[02000]: 0.371\n",
      "val_acc 0.890 val_loss  0.319\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.181\n",
      "training loss at batch:[01000]: 0.327\n",
      "training loss at batch:[02000]: 0.328\n",
      "val_acc 0.888 val_loss  0.310\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.199\n",
      "training loss at batch:[01000]: 0.309\n",
      "training loss at batch:[02000]: 0.311\n",
      "val_acc 0.900 val_loss  0.285\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.224\n",
      "training loss at batch:[01000]: 0.297\n",
      "training loss at batch:[02000]: 0.292\n",
      "val_acc 0.904 val_loss  0.273\n",
      "test_acc 0.702 test_loss  1.146\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.9 beta2:0.999 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.368\n",
      "training loss at batch:[01000]: 0.785\n",
      "training loss at batch:[02000]: 0.614\n",
      "val_acc 0.883 val_loss  0.345\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.391\n",
      "training loss at batch:[01000]: 0.364\n",
      "training loss at batch:[02000]: 0.356\n",
      "val_acc 0.892 val_loss  0.310\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.252\n",
      "training loss at batch:[01000]: 0.315\n",
      "training loss at batch:[02000]: 0.322\n",
      "val_acc 0.898 val_loss  0.288\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.349\n",
      "training loss at batch:[01000]: 0.299\n",
      "training loss at batch:[02000]: 0.295\n",
      "val_acc 0.898 val_loss  0.288\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.865\n",
      "training loss at batch:[01000]: 0.274\n",
      "training loss at batch:[02000]: 0.281\n",
      "val_acc 0.905 val_loss  0.267\n",
      "test_acc 0.428 test_loss  1.749\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.9 beta2:0.999 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.306\n",
      "training loss at batch:[01000]: 0.777\n",
      "training loss at batch:[02000]: 0.614\n",
      "val_acc 0.870 val_loss  0.371\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.824\n",
      "training loss at batch:[01000]: 0.355\n",
      "training loss at batch:[02000]: 0.354\n",
      "val_acc 0.884 val_loss  0.325\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.179\n",
      "training loss at batch:[01000]: 0.315\n",
      "training loss at batch:[02000]: 0.315\n",
      "val_acc 0.902 val_loss  0.284\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.087\n",
      "training loss at batch:[01000]: 0.288\n",
      "training loss at batch:[02000]: 0.292\n",
      "val_acc 0.898 val_loss  0.285\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.145\n",
      "training loss at batch:[01000]: 0.275\n",
      "training loss at batch:[02000]: 0.276\n",
      "val_acc 0.909 val_loss  0.269\n",
      "test_acc 0.568 test_loss  1.361\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.9 beta2:0.999 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.295\n",
      "training loss at batch:[01000]: 0.783\n",
      "training loss at batch:[02000]: 0.620\n",
      "val_acc 0.876 val_loss  0.359\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.195\n",
      "training loss at batch:[01000]: 0.363\n",
      "training loss at batch:[02000]: 0.355\n",
      "val_acc 0.894 val_loss  0.310\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.239\n",
      "training loss at batch:[01000]: 0.316\n",
      "training loss at batch:[02000]: 0.317\n",
      "val_acc 0.895 val_loss  0.300\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.127\n",
      "training loss at batch:[01000]: 0.308\n",
      "training loss at batch:[02000]: 0.301\n",
      "val_acc 0.897 val_loss  0.294\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.144\n",
      "training loss at batch:[01000]: 0.286\n",
      "training loss at batch:[02000]: 0.286\n",
      "val_acc 0.908 val_loss  0.264\n",
      "test_acc 0.306 test_loss  1.914\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.9 beta2:0.9995 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.357\n",
      "training loss at batch:[01000]: 0.767\n",
      "training loss at batch:[02000]: 0.607\n",
      "val_acc 0.862 val_loss  0.385\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.565\n",
      "training loss at batch:[01000]: 0.358\n",
      "training loss at batch:[02000]: 0.352\n",
      "val_acc 0.888 val_loss  0.327\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 1.085\n",
      "training loss at batch:[01000]: 0.316\n",
      "training loss at batch:[02000]: 0.316\n",
      "val_acc 0.898 val_loss  0.292\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.138\n",
      "training loss at batch:[01000]: 0.291\n",
      "training loss at batch:[02000]: 0.294\n",
      "val_acc 0.897 val_loss  0.289\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.091\n",
      "training loss at batch:[01000]: 0.276\n",
      "training loss at batch:[02000]: 0.279\n",
      "val_acc 0.904 val_loss  0.269\n",
      "test_acc 0.473 test_loss  1.646\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.9 beta2:0.9995 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.292\n",
      "training loss at batch:[01000]: 0.795\n",
      "training loss at batch:[02000]: 0.623\n",
      "val_acc 0.872 val_loss  0.369\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.459\n",
      "training loss at batch:[01000]: 0.359\n",
      "training loss at batch:[02000]: 0.361\n",
      "val_acc 0.891 val_loss  0.311\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.187\n",
      "training loss at batch:[01000]: 0.318\n",
      "training loss at batch:[02000]: 0.319\n",
      "val_acc 0.901 val_loss  0.286\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.638\n",
      "training loss at batch:[01000]: 0.300\n",
      "training loss at batch:[02000]: 0.296\n",
      "val_acc 0.906 val_loss  0.271\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.160\n",
      "training loss at batch:[01000]: 0.279\n",
      "training loss at batch:[02000]: 0.278\n",
      "val_acc 0.905 val_loss  0.273\n",
      "test_acc 0.583 test_loss  1.302\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.9 beta2:0.9995 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.319\n",
      "training loss at batch:[01000]: 0.788\n",
      "training loss at batch:[02000]: 0.632\n",
      "val_acc 0.876 val_loss  0.364\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.335\n",
      "training loss at batch:[01000]: 0.366\n",
      "training loss at batch:[02000]: 0.364\n",
      "val_acc 0.890 val_loss  0.317\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.471\n",
      "training loss at batch:[01000]: 0.323\n",
      "training loss at batch:[02000]: 0.322\n",
      "val_acc 0.899 val_loss  0.292\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.441\n",
      "training loss at batch:[01000]: 0.296\n",
      "training loss at batch:[02000]: 0.302\n",
      "val_acc 0.903 val_loss  0.278\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.266\n",
      "training loss at batch:[01000]: 0.279\n",
      "training loss at batch:[02000]: 0.279\n",
      "val_acc 0.900 val_loss  0.288\n",
      "test_acc 0.651 test_loss  1.042\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.95 beta2:0.9985 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.411\n",
      "training loss at batch:[01000]: 0.774\n",
      "training loss at batch:[02000]: 0.614\n",
      "val_acc 0.884 val_loss  0.349\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.367\n",
      "training loss at batch:[01000]: 0.369\n",
      "training loss at batch:[02000]: 0.356\n",
      "val_acc 0.895 val_loss  0.307\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.563\n",
      "training loss at batch:[01000]: 0.309\n",
      "training loss at batch:[02000]: 0.314\n",
      "val_acc 0.898 val_loss  0.292\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.168\n",
      "training loss at batch:[01000]: 0.291\n",
      "training loss at batch:[02000]: 0.293\n",
      "val_acc 0.901 val_loss  0.282\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.356\n",
      "training loss at batch:[01000]: 0.279\n",
      "training loss at batch:[02000]: 0.278\n",
      "val_acc 0.908 val_loss  0.264\n",
      "test_acc 0.396 test_loss  1.770\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.95 beta2:0.9985 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.297\n",
      "training loss at batch:[01000]: 0.791\n",
      "training loss at batch:[02000]: 0.620\n",
      "val_acc 0.876 val_loss  0.359\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.133\n",
      "training loss at batch:[01000]: 0.363\n",
      "training loss at batch:[02000]: 0.358\n",
      "val_acc 0.890 val_loss  0.315\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.353\n",
      "training loss at batch:[01000]: 0.325\n",
      "training loss at batch:[02000]: 0.322\n",
      "val_acc 0.900 val_loss  0.287\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.065\n",
      "training loss at batch:[01000]: 0.301\n",
      "training loss at batch:[02000]: 0.300\n",
      "val_acc 0.903 val_loss  0.277\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.411\n",
      "training loss at batch:[01000]: 0.284\n",
      "training loss at batch:[02000]: 0.282\n",
      "val_acc 0.899 val_loss  0.283\n",
      "test_acc 0.362 test_loss  1.685\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.95 beta2:0.9985 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.325\n",
      "training loss at batch:[01000]: 0.827\n",
      "training loss at batch:[02000]: 0.645\n",
      "val_acc 0.874 val_loss  0.365\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.300\n",
      "training loss at batch:[01000]: 0.376\n",
      "training loss at batch:[02000]: 0.372\n",
      "val_acc 0.889 val_loss  0.321\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.252\n",
      "training loss at batch:[01000]: 0.328\n",
      "training loss at batch:[02000]: 0.326\n",
      "val_acc 0.895 val_loss  0.301\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.199\n",
      "training loss at batch:[01000]: 0.304\n",
      "training loss at batch:[02000]: 0.308\n",
      "val_acc 0.898 val_loss  0.289\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.123\n",
      "training loss at batch:[01000]: 0.287\n",
      "training loss at batch:[02000]: 0.293\n",
      "val_acc 0.901 val_loss  0.279\n",
      "test_acc 0.467 test_loss  1.738\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.95 beta2:0.999 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.337\n",
      "training loss at batch:[01000]: 0.827\n",
      "training loss at batch:[02000]: 0.648\n",
      "val_acc 0.873 val_loss  0.366\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.316\n",
      "training loss at batch:[01000]: 0.375\n",
      "training loss at batch:[02000]: 0.369\n",
      "val_acc 0.889 val_loss  0.320\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.323\n",
      "training loss at batch:[01000]: 0.339\n",
      "training loss at batch:[02000]: 0.328\n",
      "val_acc 0.898 val_loss  0.290\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.439\n",
      "training loss at batch:[01000]: 0.306\n",
      "training loss at batch:[02000]: 0.303\n",
      "val_acc 0.890 val_loss  0.309\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.385\n",
      "training loss at batch:[01000]: 0.292\n",
      "training loss at batch:[02000]: 0.288\n",
      "val_acc 0.895 val_loss  0.302\n",
      "test_acc 0.238 test_loss  1.900\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.95 beta2:0.999 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.278\n",
      "training loss at batch:[01000]: 0.817\n",
      "training loss at batch:[02000]: 0.635\n",
      "val_acc 0.873 val_loss  0.367\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.549\n",
      "training loss at batch:[01000]: 0.363\n",
      "training loss at batch:[02000]: 0.365\n",
      "val_acc 0.880 val_loss  0.341\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.282\n",
      "training loss at batch:[01000]: 0.331\n",
      "training loss at batch:[02000]: 0.326\n",
      "val_acc 0.897 val_loss  0.290\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.174\n",
      "training loss at batch:[01000]: 0.294\n",
      "training loss at batch:[02000]: 0.305\n",
      "val_acc 0.904 val_loss  0.277\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.529\n",
      "training loss at batch:[01000]: 0.283\n",
      "training loss at batch:[02000]: 0.285\n",
      "val_acc 0.900 val_loss  0.282\n",
      "test_acc 0.370 test_loss  1.779\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.95 beta2:0.999 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.275\n",
      "training loss at batch:[01000]: 0.765\n",
      "training loss at batch:[02000]: 0.607\n",
      "val_acc 0.880 val_loss  0.353\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.426\n",
      "training loss at batch:[01000]: 0.369\n",
      "training loss at batch:[02000]: 0.362\n",
      "val_acc 0.884 val_loss  0.328\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.383\n",
      "training loss at batch:[01000]: 0.314\n",
      "training loss at batch:[02000]: 0.319\n",
      "val_acc 0.900 val_loss  0.287\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.300\n",
      "training loss at batch:[01000]: 0.307\n",
      "training loss at batch:[02000]: 0.302\n",
      "val_acc 0.901 val_loss  0.278\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.470\n",
      "training loss at batch:[01000]: 0.285\n",
      "training loss at batch:[02000]: 0.284\n",
      "val_acc 0.907 val_loss  0.266\n",
      "test_acc 0.663 test_loss  1.076\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.95 beta2:0.9995 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.317\n",
      "training loss at batch:[01000]: 0.780\n",
      "training loss at batch:[02000]: 0.619\n",
      "val_acc 0.866 val_loss  0.377\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.459\n",
      "training loss at batch:[01000]: 0.376\n",
      "training loss at batch:[02000]: 0.367\n",
      "val_acc 0.890 val_loss  0.312\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.354\n",
      "training loss at batch:[01000]: 0.321\n",
      "training loss at batch:[02000]: 0.320\n",
      "val_acc 0.899 val_loss  0.289\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.149\n",
      "training loss at batch:[01000]: 0.306\n",
      "training loss at batch:[02000]: 0.301\n",
      "val_acc 0.899 val_loss  0.282\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.556\n",
      "training loss at batch:[01000]: 0.285\n",
      "training loss at batch:[02000]: 0.287\n",
      "val_acc 0.908 val_loss  0.263\n",
      "test_acc 0.390 test_loss  1.734\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.95 beta2:0.9995 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.294\n",
      "training loss at batch:[01000]: 0.800\n",
      "training loss at batch:[02000]: 0.620\n",
      "val_acc 0.876 val_loss  0.368\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.290\n",
      "training loss at batch:[01000]: 0.363\n",
      "training loss at batch:[02000]: 0.356\n",
      "val_acc 0.897 val_loss  0.309\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.490\n",
      "training loss at batch:[01000]: 0.331\n",
      "training loss at batch:[02000]: 0.325\n",
      "val_acc 0.899 val_loss  0.294\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.241\n",
      "training loss at batch:[01000]: 0.304\n",
      "training loss at batch:[02000]: 0.304\n",
      "val_acc 0.905 val_loss  0.278\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.489\n",
      "training loss at batch:[01000]: 0.290\n",
      "training loss at batch:[02000]: 0.285\n",
      "val_acc 0.907 val_loss  0.270\n",
      "test_acc 0.412 test_loss  1.752\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.95 beta2:0.9995 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.295\n",
      "training loss at batch:[01000]: 0.784\n",
      "training loss at batch:[02000]: 0.623\n",
      "val_acc 0.860 val_loss  0.392\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.900\n",
      "training loss at batch:[01000]: 0.365\n",
      "training loss at batch:[02000]: 0.364\n",
      "val_acc 0.892 val_loss  0.314\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.202\n",
      "training loss at batch:[01000]: 0.325\n",
      "training loss at batch:[02000]: 0.322\n",
      "val_acc 0.900 val_loss  0.295\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.338\n",
      "training loss at batch:[01000]: 0.308\n",
      "training loss at batch:[02000]: 0.300\n",
      "val_acc 0.903 val_loss  0.280\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.570\n",
      "training loss at batch:[01000]: 0.282\n",
      "training loss at batch:[02000]: 0.282\n",
      "val_acc 0.899 val_loss  0.287\n",
      "test_acc 0.356 test_loss  1.462\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.85 beta2:0.9985 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.304\n",
      "training loss at batch:[01000]: 1.847\n",
      "training loss at batch:[02000]: 1.495\n",
      "val_acc 0.774 val_loss  0.726\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.518\n",
      "training loss at batch:[01000]: 0.691\n",
      "training loss at batch:[02000]: 0.652\n",
      "val_acc 0.813 val_loss  0.544\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.325\n",
      "training loss at batch:[01000]: 0.539\n",
      "training loss at batch:[02000]: 0.531\n",
      "val_acc 0.835 val_loss  0.473\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.743\n",
      "training loss at batch:[01000]: 0.484\n",
      "training loss at batch:[02000]: 0.475\n",
      "val_acc 0.849 val_loss  0.436\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.479\n",
      "training loss at batch:[01000]: 0.448\n",
      "training loss at batch:[02000]: 0.439\n",
      "val_acc 0.864 val_loss  0.402\n",
      "test_acc 0.633 test_loss  1.332\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.85 beta2:0.9985 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.270\n",
      "training loss at batch:[01000]: 1.757\n",
      "training loss at batch:[02000]: 1.366\n",
      "val_acc 0.794 val_loss  0.670\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.889\n",
      "training loss at batch:[01000]: 0.646\n",
      "training loss at batch:[02000]: 0.615\n",
      "val_acc 0.825 val_loss  0.516\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.608\n",
      "training loss at batch:[01000]: 0.521\n",
      "training loss at batch:[02000]: 0.511\n",
      "val_acc 0.846 val_loss  0.458\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.877\n",
      "training loss at batch:[01000]: 0.462\n",
      "training loss at batch:[02000]: 0.460\n",
      "val_acc 0.858 val_loss  0.422\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.438\n",
      "training loss at batch:[01000]: 0.436\n",
      "training loss at batch:[02000]: 0.434\n",
      "val_acc 0.864 val_loss  0.401\n",
      "test_acc 0.645 test_loss  1.415\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.85 beta2:0.9985 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.357\n",
      "training loss at batch:[01000]: 1.912\n",
      "training loss at batch:[02000]: 1.540\n",
      "val_acc 0.779 val_loss  0.733\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.637\n",
      "training loss at batch:[01000]: 0.693\n",
      "training loss at batch:[02000]: 0.663\n",
      "val_acc 0.811 val_loss  0.549\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.450\n",
      "training loss at batch:[01000]: 0.550\n",
      "training loss at batch:[02000]: 0.542\n",
      "val_acc 0.833 val_loss  0.484\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.356\n",
      "training loss at batch:[01000]: 0.487\n",
      "training loss at batch:[02000]: 0.481\n",
      "val_acc 0.846 val_loss  0.445\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.359\n",
      "training loss at batch:[01000]: 0.451\n",
      "training loss at batch:[02000]: 0.449\n",
      "val_acc 0.856 val_loss  0.416\n",
      "test_acc 0.264 test_loss  1.889\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.85 beta2:0.999 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.356\n",
      "training loss at batch:[01000]: 1.842\n",
      "training loss at batch:[02000]: 1.484\n",
      "val_acc 0.780 val_loss  0.716\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.583\n",
      "training loss at batch:[01000]: 0.675\n",
      "training loss at batch:[02000]: 0.646\n",
      "val_acc 0.814 val_loss  0.540\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.546\n",
      "training loss at batch:[01000]: 0.536\n",
      "training loss at batch:[02000]: 0.532\n",
      "val_acc 0.830 val_loss  0.480\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.603\n",
      "training loss at batch:[01000]: 0.488\n",
      "training loss at batch:[02000]: 0.481\n",
      "val_acc 0.846 val_loss  0.443\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.706\n",
      "training loss at batch:[01000]: 0.448\n",
      "training loss at batch:[02000]: 0.449\n",
      "val_acc 0.857 val_loss  0.415\n",
      "test_acc 0.429 test_loss  1.731\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.85 beta2:0.999 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.337\n",
      "training loss at batch:[01000]: 1.780\n",
      "training loss at batch:[02000]: 1.433\n",
      "val_acc 0.784 val_loss  0.709\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.500\n",
      "training loss at batch:[01000]: 0.692\n",
      "training loss at batch:[02000]: 0.653\n",
      "val_acc 0.819 val_loss  0.538\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.790\n",
      "training loss at batch:[01000]: 0.537\n",
      "training loss at batch:[02000]: 0.529\n",
      "val_acc 0.838 val_loss  0.474\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.351\n",
      "training loss at batch:[01000]: 0.486\n",
      "training loss at batch:[02000]: 0.480\n",
      "val_acc 0.854 val_loss  0.434\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.512\n",
      "training loss at batch:[01000]: 0.443\n",
      "training loss at batch:[02000]: 0.441\n",
      "val_acc 0.861 val_loss  0.411\n",
      "test_acc 0.634 test_loss  1.388\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.85 beta2:0.999 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.298\n",
      "training loss at batch:[01000]: 1.828\n",
      "training loss at batch:[02000]: 1.474\n",
      "val_acc 0.769 val_loss  0.738\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.832\n",
      "training loss at batch:[01000]: 0.702\n",
      "training loss at batch:[02000]: 0.671\n",
      "val_acc 0.806 val_loss  0.567\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.594\n",
      "training loss at batch:[01000]: 0.565\n",
      "training loss at batch:[02000]: 0.554\n",
      "val_acc 0.830 val_loss  0.498\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.605\n",
      "training loss at batch:[01000]: 0.507\n",
      "training loss at batch:[02000]: 0.499\n",
      "val_acc 0.845 val_loss  0.461\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.298\n",
      "training loss at batch:[01000]: 0.468\n",
      "training loss at batch:[02000]: 0.463\n",
      "val_acc 0.857 val_loss  0.428\n",
      "test_acc 0.730 test_loss  1.240\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.85 beta2:0.9995 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.293\n",
      "training loss at batch:[01000]: 1.808\n",
      "training loss at batch:[02000]: 1.432\n",
      "val_acc 0.781 val_loss  0.694\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.845\n",
      "training loss at batch:[01000]: 0.662\n",
      "training loss at batch:[02000]: 0.635\n",
      "val_acc 0.816 val_loss  0.529\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.671\n",
      "training loss at batch:[01000]: 0.529\n",
      "training loss at batch:[02000]: 0.521\n",
      "val_acc 0.838 val_loss  0.468\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.454\n",
      "training loss at batch:[01000]: 0.470\n",
      "training loss at batch:[02000]: 0.470\n",
      "val_acc 0.855 val_loss  0.429\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.273\n",
      "training loss at batch:[01000]: 0.436\n",
      "training loss at batch:[02000]: 0.436\n",
      "val_acc 0.864 val_loss  0.403\n",
      "test_acc 0.410 test_loss  1.942\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.85 beta2:0.9995 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.179\n",
      "training loss at batch:[01000]: 1.848\n",
      "training loss at batch:[02000]: 1.466\n",
      "val_acc 0.782 val_loss  0.692\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 1.054\n",
      "training loss at batch:[01000]: 0.661\n",
      "training loss at batch:[02000]: 0.638\n",
      "val_acc 0.820 val_loss  0.534\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.464\n",
      "training loss at batch:[01000]: 0.535\n",
      "training loss at batch:[02000]: 0.526\n",
      "val_acc 0.836 val_loss  0.474\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.510\n",
      "training loss at batch:[01000]: 0.487\n",
      "training loss at batch:[02000]: 0.478\n",
      "val_acc 0.851 val_loss  0.438\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.497\n",
      "training loss at batch:[01000]: 0.449\n",
      "training loss at batch:[02000]: 0.440\n",
      "val_acc 0.860 val_loss  0.411\n",
      "test_acc 0.653 test_loss  1.345\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.85 beta2:0.9995 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.247\n",
      "training loss at batch:[01000]: 1.837\n",
      "training loss at batch:[02000]: 1.528\n",
      "val_acc 0.774 val_loss  0.790\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.645\n",
      "training loss at batch:[01000]: 0.749\n",
      "training loss at batch:[02000]: 0.699\n",
      "val_acc 0.823 val_loss  0.549\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.691\n",
      "training loss at batch:[01000]: 0.552\n",
      "training loss at batch:[02000]: 0.533\n",
      "val_acc 0.844 val_loss  0.474\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.416\n",
      "training loss at batch:[01000]: 0.486\n",
      "training loss at batch:[02000]: 0.479\n",
      "val_acc 0.859 val_loss  0.427\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.294\n",
      "training loss at batch:[01000]: 0.444\n",
      "training loss at batch:[02000]: 0.438\n",
      "val_acc 0.869 val_loss  0.403\n",
      "test_acc 0.700 test_loss  1.385\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.9 beta2:0.9985 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.346\n",
      "training loss at batch:[01000]: 1.722\n",
      "training loss at batch:[02000]: 1.363\n",
      "val_acc 0.774 val_loss  0.693\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.487\n",
      "training loss at batch:[01000]: 0.669\n",
      "training loss at batch:[02000]: 0.639\n",
      "val_acc 0.812 val_loss  0.542\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.746\n",
      "training loss at batch:[01000]: 0.533\n",
      "training loss at batch:[02000]: 0.526\n",
      "val_acc 0.835 val_loss  0.477\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.176\n",
      "training loss at batch:[01000]: 0.484\n",
      "training loss at batch:[02000]: 0.475\n",
      "val_acc 0.850 val_loss  0.436\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.498\n",
      "training loss at batch:[01000]: 0.452\n",
      "training loss at batch:[02000]: 0.443\n",
      "val_acc 0.859 val_loss  0.411\n",
      "test_acc 0.343 test_loss  2.175\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.9 beta2:0.9985 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.347\n",
      "training loss at batch:[01000]: 1.752\n",
      "training loss at batch:[02000]: 1.380\n",
      "val_acc 0.784 val_loss  0.680\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.675\n",
      "training loss at batch:[01000]: 0.651\n",
      "training loss at batch:[02000]: 0.623\n",
      "val_acc 0.816 val_loss  0.531\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.515\n",
      "training loss at batch:[01000]: 0.529\n",
      "training loss at batch:[02000]: 0.517\n",
      "val_acc 0.840 val_loss  0.465\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.305\n",
      "training loss at batch:[01000]: 0.473\n",
      "training loss at batch:[02000]: 0.464\n",
      "val_acc 0.857 val_loss  0.432\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.471\n",
      "training loss at batch:[01000]: 0.437\n",
      "training loss at batch:[02000]: 0.434\n",
      "val_acc 0.862 val_loss  0.406\n",
      "test_acc 0.562 test_loss  1.511\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.9 beta2:0.9985 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.281\n",
      "training loss at batch:[01000]: 1.779\n",
      "training loss at batch:[02000]: 1.467\n",
      "val_acc 0.780 val_loss  0.754\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.692\n",
      "training loss at batch:[01000]: 0.715\n",
      "training loss at batch:[02000]: 0.676\n",
      "val_acc 0.814 val_loss  0.554\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.459\n",
      "training loss at batch:[01000]: 0.557\n",
      "training loss at batch:[02000]: 0.548\n",
      "val_acc 0.837 val_loss  0.486\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.309\n",
      "training loss at batch:[01000]: 0.500\n",
      "training loss at batch:[02000]: 0.488\n",
      "val_acc 0.848 val_loss  0.447\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.851\n",
      "training loss at batch:[01000]: 0.463\n",
      "training loss at batch:[02000]: 0.455\n",
      "val_acc 0.857 val_loss  0.419\n",
      "test_acc 0.451 test_loss  1.657\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.9 beta2:0.999 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.257\n",
      "training loss at batch:[01000]: 1.660\n",
      "training loss at batch:[02000]: 1.306\n",
      "val_acc 0.783 val_loss  0.667\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 1.051\n",
      "training loss at batch:[01000]: 0.645\n",
      "training loss at batch:[02000]: 0.619\n",
      "val_acc 0.816 val_loss  0.524\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.412\n",
      "training loss at batch:[01000]: 0.537\n",
      "training loss at batch:[02000]: 0.518\n",
      "val_acc 0.839 val_loss  0.469\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.282\n",
      "training loss at batch:[01000]: 0.476\n",
      "training loss at batch:[02000]: 0.467\n",
      "val_acc 0.852 val_loss  0.430\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.316\n",
      "training loss at batch:[01000]: 0.439\n",
      "training loss at batch:[02000]: 0.434\n",
      "val_acc 0.863 val_loss  0.407\n",
      "test_acc 0.463 test_loss  1.734\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.9 beta2:0.999 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.210\n",
      "training loss at batch:[01000]: 1.806\n",
      "training loss at batch:[02000]: 1.463\n",
      "val_acc 0.778 val_loss  0.719\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.709\n",
      "training loss at batch:[01000]: 0.690\n",
      "training loss at batch:[02000]: 0.652\n",
      "val_acc 0.816 val_loss  0.545\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.331\n",
      "training loss at batch:[01000]: 0.545\n",
      "training loss at batch:[02000]: 0.531\n",
      "val_acc 0.838 val_loss  0.481\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.428\n",
      "training loss at batch:[01000]: 0.479\n",
      "training loss at batch:[02000]: 0.473\n",
      "val_acc 0.852 val_loss  0.436\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.255\n",
      "training loss at batch:[01000]: 0.452\n",
      "training loss at batch:[02000]: 0.440\n",
      "val_acc 0.863 val_loss  0.411\n",
      "test_acc 0.573 test_loss  1.484\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.9 beta2:0.999 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.255\n",
      "training loss at batch:[01000]: 1.754\n",
      "training loss at batch:[02000]: 1.416\n",
      "val_acc 0.775 val_loss  0.705\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.354\n",
      "training loss at batch:[01000]: 0.669\n",
      "training loss at batch:[02000]: 0.635\n",
      "val_acc 0.815 val_loss  0.526\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.665\n",
      "training loss at batch:[01000]: 0.524\n",
      "training loss at batch:[02000]: 0.514\n",
      "val_acc 0.841 val_loss  0.458\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.488\n",
      "training loss at batch:[01000]: 0.481\n",
      "training loss at batch:[02000]: 0.461\n",
      "val_acc 0.858 val_loss  0.417\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.263\n",
      "training loss at batch:[01000]: 0.433\n",
      "training loss at batch:[02000]: 0.428\n",
      "val_acc 0.869 val_loss  0.392\n",
      "test_acc 0.408 test_loss  1.758\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.9 beta2:0.9995 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.365\n",
      "training loss at batch:[01000]: 1.846\n",
      "training loss at batch:[02000]: 1.469\n",
      "val_acc 0.781 val_loss  0.696\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.669\n",
      "training loss at batch:[01000]: 0.673\n",
      "training loss at batch:[02000]: 0.638\n",
      "val_acc 0.818 val_loss  0.538\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.289\n",
      "training loss at batch:[01000]: 0.539\n",
      "training loss at batch:[02000]: 0.529\n",
      "val_acc 0.837 val_loss  0.473\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.623\n",
      "training loss at batch:[01000]: 0.482\n",
      "training loss at batch:[02000]: 0.479\n",
      "val_acc 0.849 val_loss  0.439\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.388\n",
      "training loss at batch:[01000]: 0.452\n",
      "training loss at batch:[02000]: 0.448\n",
      "val_acc 0.858 val_loss  0.416\n",
      "test_acc 0.447 test_loss  1.875\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.9 beta2:0.9995 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.312\n",
      "training loss at batch:[01000]: 1.762\n",
      "training loss at batch:[02000]: 1.390\n",
      "val_acc 0.782 val_loss  0.690\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.892\n",
      "training loss at batch:[01000]: 0.673\n",
      "training loss at batch:[02000]: 0.637\n",
      "val_acc 0.824 val_loss  0.537\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.768\n",
      "training loss at batch:[01000]: 0.541\n",
      "training loss at batch:[02000]: 0.524\n",
      "val_acc 0.845 val_loss  0.471\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.711\n",
      "training loss at batch:[01000]: 0.481\n",
      "training loss at batch:[02000]: 0.474\n",
      "val_acc 0.857 val_loss  0.433\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.227\n",
      "training loss at batch:[01000]: 0.444\n",
      "training loss at batch:[02000]: 0.438\n",
      "val_acc 0.864 val_loss  0.406\n",
      "test_acc 0.451 test_loss  1.552\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.9 beta2:0.9995 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.251\n",
      "training loss at batch:[01000]: 1.737\n",
      "training loss at batch:[02000]: 1.373\n",
      "val_acc 0.782 val_loss  0.671\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 1.010\n",
      "training loss at batch:[01000]: 0.653\n",
      "training loss at batch:[02000]: 0.624\n",
      "val_acc 0.818 val_loss  0.526\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.347\n",
      "training loss at batch:[01000]: 0.524\n",
      "training loss at batch:[02000]: 0.519\n",
      "val_acc 0.841 val_loss  0.458\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.253\n",
      "training loss at batch:[01000]: 0.467\n",
      "training loss at batch:[02000]: 0.460\n",
      "val_acc 0.858 val_loss  0.424\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.604\n",
      "training loss at batch:[01000]: 0.442\n",
      "training loss at batch:[02000]: 0.438\n",
      "val_acc 0.864 val_loss  0.401\n",
      "test_acc 0.329 test_loss  1.831\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.95 beta2:0.9985 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.267\n",
      "training loss at batch:[01000]: 1.693\n",
      "training loss at batch:[02000]: 1.360\n",
      "val_acc 0.789 val_loss  0.682\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.539\n",
      "training loss at batch:[01000]: 0.658\n",
      "training loss at batch:[02000]: 0.626\n",
      "val_acc 0.821 val_loss  0.528\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.936\n",
      "training loss at batch:[01000]: 0.527\n",
      "training loss at batch:[02000]: 0.522\n",
      "val_acc 0.840 val_loss  0.469\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.649\n",
      "training loss at batch:[01000]: 0.482\n",
      "training loss at batch:[02000]: 0.474\n",
      "val_acc 0.854 val_loss  0.432\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.378\n",
      "training loss at batch:[01000]: 0.446\n",
      "training loss at batch:[02000]: 0.442\n",
      "val_acc 0.861 val_loss  0.408\n",
      "test_acc 0.535 test_loss  1.494\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.95 beta2:0.9985 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.249\n",
      "training loss at batch:[01000]: 1.857\n",
      "training loss at batch:[02000]: 1.523\n",
      "val_acc 0.786 val_loss  0.745\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.753\n",
      "training loss at batch:[01000]: 0.697\n",
      "training loss at batch:[02000]: 0.659\n",
      "val_acc 0.823 val_loss  0.540\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.814\n",
      "training loss at batch:[01000]: 0.540\n",
      "training loss at batch:[02000]: 0.528\n",
      "val_acc 0.844 val_loss  0.474\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.660\n",
      "training loss at batch:[01000]: 0.473\n",
      "training loss at batch:[02000]: 0.468\n",
      "val_acc 0.859 val_loss  0.429\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.613\n",
      "training loss at batch:[01000]: 0.439\n",
      "training loss at batch:[02000]: 0.436\n",
      "val_acc 0.866 val_loss  0.403\n",
      "test_acc 0.400 test_loss  1.760\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.95 beta2:0.9985 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.335\n",
      "training loss at batch:[01000]: 1.804\n",
      "training loss at batch:[02000]: 1.451\n",
      "val_acc 0.779 val_loss  0.715\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.419\n",
      "training loss at batch:[01000]: 0.681\n",
      "training loss at batch:[02000]: 0.650\n",
      "val_acc 0.817 val_loss  0.541\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.552\n",
      "training loss at batch:[01000]: 0.545\n",
      "training loss at batch:[02000]: 0.526\n",
      "val_acc 0.840 val_loss  0.467\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.499\n",
      "training loss at batch:[01000]: 0.477\n",
      "training loss at batch:[02000]: 0.466\n",
      "val_acc 0.857 val_loss  0.426\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.644\n",
      "training loss at batch:[01000]: 0.439\n",
      "training loss at batch:[02000]: 0.435\n",
      "val_acc 0.866 val_loss  0.400\n",
      "test_acc 0.507 test_loss  1.660\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.95 beta2:0.999 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.297\n",
      "training loss at batch:[01000]: 1.812\n",
      "training loss at batch:[02000]: 1.436\n",
      "val_acc 0.778 val_loss  0.710\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.487\n",
      "training loss at batch:[01000]: 0.680\n",
      "training loss at batch:[02000]: 0.653\n",
      "val_acc 0.807 val_loss  0.553\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.441\n",
      "training loss at batch:[01000]: 0.555\n",
      "training loss at batch:[02000]: 0.541\n",
      "val_acc 0.829 val_loss  0.493\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.461\n",
      "training loss at batch:[01000]: 0.493\n",
      "training loss at batch:[02000]: 0.485\n",
      "val_acc 0.844 val_loss  0.447\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.338\n",
      "training loss at batch:[01000]: 0.450\n",
      "training loss at batch:[02000]: 0.452\n",
      "val_acc 0.855 val_loss  0.422\n",
      "test_acc 0.435 test_loss  1.682\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.95 beta2:0.999 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.300\n",
      "training loss at batch:[01000]: 1.838\n",
      "training loss at batch:[02000]: 1.526\n",
      "val_acc 0.782 val_loss  0.743\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.782\n",
      "training loss at batch:[01000]: 0.703\n",
      "training loss at batch:[02000]: 0.655\n",
      "val_acc 0.819 val_loss  0.530\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.447\n",
      "training loss at batch:[01000]: 0.531\n",
      "training loss at batch:[02000]: 0.518\n",
      "val_acc 0.844 val_loss  0.457\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.705\n",
      "training loss at batch:[01000]: 0.474\n",
      "training loss at batch:[02000]: 0.460\n",
      "val_acc 0.857 val_loss  0.421\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.325\n",
      "training loss at batch:[01000]: 0.438\n",
      "training loss at batch:[02000]: 0.428\n",
      "val_acc 0.867 val_loss  0.393\n",
      "test_acc 0.396 test_loss  1.787\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.95 beta2:0.999 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.240\n",
      "training loss at batch:[01000]: 1.777\n",
      "training loss at batch:[02000]: 1.417\n",
      "val_acc 0.777 val_loss  0.705\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.990\n",
      "training loss at batch:[01000]: 0.684\n",
      "training loss at batch:[02000]: 0.648\n",
      "val_acc 0.814 val_loss  0.545\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.430\n",
      "training loss at batch:[01000]: 0.552\n",
      "training loss at batch:[02000]: 0.536\n",
      "val_acc 0.834 val_loss  0.477\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.384\n",
      "training loss at batch:[01000]: 0.485\n",
      "training loss at batch:[02000]: 0.482\n",
      "val_acc 0.848 val_loss  0.441\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.474\n",
      "training loss at batch:[01000]: 0.454\n",
      "training loss at batch:[02000]: 0.442\n",
      "val_acc 0.857 val_loss  0.416\n",
      "test_acc 0.757 test_loss  1.372\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.95 beta2:0.9995 decay:0.005\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.295\n",
      "training loss at batch:[01000]: 1.825\n",
      "training loss at batch:[02000]: 1.458\n",
      "val_acc 0.781 val_loss  0.697\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 0.548\n",
      "training loss at batch:[01000]: 0.664\n",
      "training loss at batch:[02000]: 0.640\n",
      "val_acc 0.814 val_loss  0.541\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.259\n",
      "training loss at batch:[01000]: 0.543\n",
      "training loss at batch:[02000]: 0.534\n",
      "val_acc 0.832 val_loss  0.481\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.536\n",
      "training loss at batch:[01000]: 0.487\n",
      "training loss at batch:[02000]: 0.477\n",
      "val_acc 0.849 val_loss  0.442\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.414\n",
      "training loss at batch:[01000]: 0.445\n",
      "training loss at batch:[02000]: 0.445\n",
      "val_acc 0.859 val_loss  0.412\n",
      "test_acc 0.271 test_loss  1.820\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.95 beta2:0.9995 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.320\n",
      "training loss at batch:[01000]: 1.696\n",
      "training loss at batch:[02000]: 1.351\n",
      "val_acc 0.781 val_loss  0.690\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 1.110\n",
      "training loss at batch:[01000]: 0.669\n",
      "training loss at batch:[02000]: 0.639\n",
      "val_acc 0.820 val_loss  0.539\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.620\n",
      "training loss at batch:[01000]: 0.535\n",
      "training loss at batch:[02000]: 0.525\n",
      "val_acc 0.838 val_loss  0.473\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.789\n",
      "training loss at batch:[01000]: 0.485\n",
      "training loss at batch:[02000]: 0.476\n",
      "val_acc 0.854 val_loss  0.438\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.467\n",
      "training loss at batch:[01000]: 0.448\n",
      "training loss at batch:[02000]: 0.446\n",
      "val_acc 0.862 val_loss  0.410\n",
      "test_acc 0.329 test_loss  1.841\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:1e-05 beat1:0.95 beta2:0.9995 decay:0.015\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch:[0].\n",
      "training loss at batch:[00000]: 2.330\n",
      "training loss at batch:[01000]: 1.779\n",
      "training loss at batch:[02000]: 1.443\n",
      "val_acc 0.773 val_loss  0.711\n",
      "Starting epoch:[1].\n",
      "training loss at batch:[00000]: 1.063\n",
      "training loss at batch:[01000]: 0.687\n",
      "training loss at batch:[02000]: 0.651\n",
      "val_acc 0.816 val_loss  0.539\n",
      "Starting epoch:[2].\n",
      "training loss at batch:[00000]: 0.576\n",
      "training loss at batch:[01000]: 0.543\n",
      "training loss at batch:[02000]: 0.532\n",
      "val_acc 0.837 val_loss  0.478\n",
      "Starting epoch:[3].\n",
      "training loss at batch:[00000]: 0.517\n",
      "training loss at batch:[01000]: 0.486\n",
      "training loss at batch:[02000]: 0.477\n",
      "val_acc 0.853 val_loss  0.435\n",
      "Starting epoch:[4].\n",
      "training loss at batch:[00000]: 0.281\n",
      "training loss at batch:[01000]: 0.443\n",
      "training loss at batch:[02000]: 0.439\n",
      "val_acc 0.860 val_loss  0.409\n",
      "test_acc 0.348 test_loss  1.666\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "max_test_acc: float = 0.0\n",
    "\n",
    "for lr, beta1, beta2, decay in product(learning_rates, beta1s, beta2s, decays):\n",
    "    print()\n",
    "    print('-' * 80)\n",
    "    print(f\"lr:{lr} beta1:{beta1} beta2:{beta2} decay:{decay}\")\n",
    "    print('-' * 80)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Initialize for each parameter combination.\n",
    "    # --------------------------------------------------------------------------------\n",
    "    prev_loss: float = float(sys.maxsize)\n",
    "    early_stop_tolerance: int = 3\n",
    "    if isinstance(writer, SummaryWriter): \n",
    "        writer.close()\n",
    "\n",
    "    run_name: str = f\"./logs/lr{lr}_beta1{beta1}_beta2{beta2}_decay{decay}\"\n",
    "    writer = SummaryWriter(run_name)\n",
    "\n",
    "    model: nn.Module = ConvNet()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(beta1, beta2), weight_decay=decay)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='mean')   # normalized by batch size\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Train for each parameter combination.\n",
    "    # --------------------------------------------------------------------------------\n",
    "    for epoch in range(0, NUM_EPOCHS):   # epochs at maximum\n",
    "        tensorboard_write_histogram(writer=writer, model=model, step=epoch)\n",
    "    \n",
    "        # Print epoch\n",
    "        print(f'Starting epoch:[{epoch}].')\n",
    "    \n",
    "        # Reset current loss value at eacch epoch\n",
    "        _current_loss: float = 0.0\n",
    "        _num_records: int = 0\n",
    "    \n",
    "        # Iterate over the DataLoader for training data\n",
    "        model.train(True)\n",
    "        for index, data in enumerate(train_loader, 0):\n",
    "            inputs, targets = data\n",
    "            _batch_size: int = len(inputs)\n",
    "    \n",
    "            # Write an image at every batch 0\n",
    "            if index == 0:\n",
    "                tensorboard_write_image(\n",
    "                    writer=writer, tag=\"image\", image=inputs[0], step=epoch, dataformats=\"CHW\"\n",
    "                )\n",
    "    \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Perform forward pass\n",
    "            outputs = model(inputs)\n",
    "    \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "    \n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "    \n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Print statistics\n",
    "            _current_loss += loss.item()\n",
    "    \n",
    "            if index % 1000 == 0:\n",
    "                print(f'training loss at batch:[{index:05d}]: {_current_loss / (index + 1):.03f}')\n",
    "\n",
    "            # if index > 209: break\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Reports per epoch\n",
    "        # --------------------------------------------------------------------------------\n",
    "        model.eval()\n",
    "\n",
    "        # Loss\n",
    "        tensorboard_write_scalar(\n",
    "            writer=writer, tag=\"training loss/epoch\", value=_current_loss / (index + 1), step=epoch\n",
    "        )\n",
    "    \n",
    "        # Validation accuracy & confusion matrix\n",
    "        inputs = labels = predictions = None\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = next(iter(val_loader))\n",
    "            # predictions = torch.argmax(model(inputs), axis=-1) # No need to argmax\n",
    "            predictions = model(inputs)\n",
    "            # val_loss = loss_fn(predictions.to(torch.float), labels.to(torch.float)) / len(labels)\n",
    "            val_loss = loss_fn(predictions, labels)\n",
    "\n",
    "        val_acc = get_accuracy(predictions=predictions, truth=labels)\n",
    "    \n",
    "        tensorboard_write_scalar(\n",
    "            writer=writer, tag=\"validation loss/epoch\", value=val_loss, step=epoch\n",
    "        )\n",
    "        tensorboard_write_scalar(\n",
    "            writer=writer, tag=\"validation accuracy/epoch\", value=val_acc, step=epoch\n",
    "        )\n",
    "        tensorboard_write_confusion_matrix(\n",
    "            writer=writer,\n",
    "            tag=\"validation confusion matrix\",\n",
    "            predictions=torch.argmax(predictions, axis=-1),\n",
    "            truth=labels,\n",
    "            class_names=list(id_to_label.values()),\n",
    "            step=epoch\n",
    "        )\n",
    "        print(f'val_acc {val_acc:.03f} val_loss {val_loss: .03f}')\n",
    "        writer.flush()\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Early stop\n",
    "        # --------------------------------------------------------------------------------\n",
    "        if val_loss > prev_loss:\n",
    "            early_stop_tolerance -= 1\n",
    "        else:\n",
    "            prev_loss = val_loss\n",
    "    \n",
    "        if early_stop_tolerance <= 0:\n",
    "            print(\"early stop\")\n",
    "            break\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Test\n",
    "    # --------------------------------------------------------------------------------\n",
    "    inputs = labels = predictions = None\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = next(iter(test_loader))\n",
    "        predictions = model(inputs)\n",
    "\n",
    "    test_loss = loss_fn(predictions, labels)\n",
    "    test_acc = get_accuracy(predictions=predictions, truth=labels)\n",
    "    print(f'test_acc {test_acc:.03f} test_loss {test_loss: .03f}')\n",
    "\n",
    "    # Report test results\n",
    "    tensorboard_write_scalar(\n",
    "        writer=writer, tag=\"test loss/epoch\", value=test_loss, step=epoch\n",
    "    )\n",
    "    tensorboard_write_scalar(\n",
    "        writer=writer, tag=\"test accuracy/epoch\", value=test_acc, step=epoch\n",
    "    )\n",
    "    tensorboard_write_confusion_matrix(\n",
    "        writer=writer,\n",
    "        tag=\"test confusion matrix\",\n",
    "        predictions=torch.argmax(predictions, axis=-1),\n",
    "        truth=labels,\n",
    "        class_names=list(id_to_label.values()),\n",
    "        step=epoch\n",
    "    )\n",
    "    writer.add_hparams(\n",
    "        hparam_dict={\"lr\": lr, \"beta1\": beta1, \"beta2\": beta2, \"decay\": decay},\n",
    "        metric_dict={\n",
    "            \"test_accuracy\": test_acc,\n",
    "            \"test_loss\": test_loss,\n",
    "        },\n",
    "        run_name=run_name,\n",
    "    )\n",
    "    writer.flush()\n",
    "\n",
    "    # Save model if the accuracy is better\n",
    "    if max_test_acc < test_acc:\n",
    "        print(f\"max test accuracy:[{test_acc}] with lr:{lr} beta1:{beta1} beta2:{beta2} decay:{decay}\")\n",
    "        max_test_acc = test_acc\n",
    "        \n",
    "        print(f\"saving the model as model.pth...\")\n",
    "        torch.save(model, 'model.pth')\n",
    "\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b6784-644f-4104-8fb4-0638f5d9765c",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83b6e5e9-ee9a-4aeb-9a89-d4068cd223d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inputs, labels = next(iter(test_loader))\n",
    "    predictions = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f287e3d-2c11-4af3-b981-378398c91d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.numpy()\n",
    "predictions = np.argmax(predictions.numpy(), axis=-1).squeeze()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb111cbe-1d52-41ea-8f07-8248a99c6a73",
   "metadata": {},
   "source": [
    "matrix = confusion_matrix(y_true=labels, y_pred=predictions)\n",
    "fig = plot_confusion_matrix(matrix=matrix, class_names=list(id_to_label.values()))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a0d288c-5574-4ab5-822b-c23e531202ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_write_confusion_matrix(\n",
    "    writer=writer,\n",
    "    tag=\"confusion matrix\",\n",
    "    predictions=predictions,\n",
    "    truth=labels,\n",
    "    class_names=list(id_to_label.values()),\n",
    "    step=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f31978-e40b-4857-8adb-a2c986b4fca3",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae47d8ff-8122-40ca-a34f-f6353141c639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7aaf3248975c789a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7aaf3248975c789a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ./logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cb4ce9-8e89-4333-9682-dab7e138ce7a",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "668cd9b7-d1e8-4054-92e2-09151246b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6172bda5-e104-4432-a334-2d01593e275d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
