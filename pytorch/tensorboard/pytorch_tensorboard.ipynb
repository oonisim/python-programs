{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d766612c-cf85-4fd3-be67-3fa2f8417e00",
   "metadata": {},
   "source": [
    "* [how-to-use-tensorboard-with-pytorch](https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-tensorboard-with-pytorch.md)\n",
    "* [Pytorch TensorBoard Tutorial](https://www.youtube.com/watch?v=RLqsxWaQdHE)\n",
    "* [Using Tensorboard in Pytorch](https://krishansubudhi.github.io/deeplearning/2020/03/24/tensorboard-pytorch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34059c8d-7d09-4e51-b819-d2889c285cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from itertools import (\n",
    "    product\n",
    ")\n",
    "from typing import (\n",
    "    List,\n",
    "    Union,\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix\n",
    ")\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import (\n",
    "    SummaryWriter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89039171-97ab-425f-9121-b2730c52feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c311e96-df11-47fd-9465-61bb7f9426d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from main import (\n",
    "    tensorboard_write_histogram,\n",
    "    tensorboard_write_graph,\n",
    "    tensorboard_write_image,\n",
    "    tensorboard_write_scalar,\n",
    "    plot_confusion_matrix,\n",
    "    tensorboard_write_confusion_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e78926a-c19a-49a5-8dd1-fcd96a186603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data_mean_std(\n",
    "        loader: DataLoader\n",
    "):\n",
    "    \"\"\"Compute the mean and standard deviation of all pixels in the dataset.\n",
    "    https://saturncloud.io/blog/how-to-normalize-image-dataset-using-pytorch/\n",
    "\n",
    "    Args:\n",
    "        loader: data loader\n",
    "\n",
    "    Returns: (mean, std) where mean.shape=(3,) and std.shape=(3,)\n",
    "    \"\"\"\n",
    "    count: int = 0\n",
    "    mean: float = 0.0\n",
    "    std: float = 0.0\n",
    "\n",
    "    for images, _ in loader:\n",
    "        assert images.ndim == 4\n",
    "        batch_size, num_channels, height, width = images.shape\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Compute the mean and standard deviation for each channel separately\n",
    "        # (e.g., one value for each of the RGB channels) by specifying axis=(0, 2, 3),\n",
    "        # as the mean and standard deviation are computed across the batch, height,\n",
    "        # and width dimensions, but not across the channel dimension.\n",
    "        # --------------------------------------------------------------------------------\n",
    "        count += 1\n",
    "        mean += images.mean(axis=(0, 2, 3))\n",
    "        std += images.std(axis=(0, 2, 3))\n",
    "\n",
    "    mean /= count\n",
    "    std /= count\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db02dacd-23e9-4d0c-b263-0bbbb9ba163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predictions: torch.Tensor, truth: torch.Tensor) -> float:\n",
    "    \"\"\"Calculate prediction accuracy\n",
    "    Returns: accuracy as float\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(predictions, torch.Tensor) and isinstance(truth, torch.Tensor)\n",
    "    assert predictions.shape == truth.shape\n",
    "    return float((predictions == truth).sum().numpy().item()) / float(predictions.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6b66c4-7478-4ce3-8aed-adabe4d10424",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d24155-9d18-45b9-a2f5-c0aaf01173a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FashionMNIST(\n",
    "    os.getcwd(), \n",
    "    download=True, \n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88332e0d-4251-49ed-9c0d-12e950e34d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = test_size = (len(dataset) - train_size) // 2\n",
    "training_data, val_data, test_data = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=8, shuffle=True, num_workers=1)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=val_size, shuffle=False, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=test_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55e63a25-5cca-4f8e-a02a-e8c7a6d06c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-Shirt',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle Boot']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_label = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "list(id_to_label.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e52059bf-52bc-4fa3-b9fe-e0d49cbb06cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = next(iter(train_loader))\n",
    "x = X[0]\n",
    "\n",
    "channels: int = x.shape[0]\n",
    "width: int = x.shape[1]\n",
    "height: int = x.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "048f13a9-6fe9-4dc7-9b15-0e564c58b753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a3591d80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/SrBM8AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAjk0lEQVR4nO3de3SV9Z3v8c/ObUMg7BhCbhJoABEViFOENF4QJQOkUwcK0+Ots8DlwGiDU2SsDj1VtO1MptjluGwpTKcV6hzxwjkCo6vFgWDC2Ca0IhzKqCmJUcBcuNjcyYXs5/zBMTNREL+PO/nl8n6ttdciez+fPD8envDZT/bONwHP8zwBANDHolwvAAAwNFFAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyIcb2AjwuHw6qurlZCQoICgYDr5QAAjDzPU1NTkzIyMhQVdeHrnH5XQNXV1crMzHS9DADA53Ts2DGNHTv2go/3uwJKSEiQJF2vLytGsY5XMzQEYvruNPDOnu2T/ZxckeMr1zzrjDkTe2S4OROONkfkTWoxZy79hb9/25iSg/ZQlJ+/VNie8YOJY33qrDr1un7Z/f/5hfTa/zzr16/X448/rtraWmVnZ+tHP/qRZs2addHcR992i1GsYgIUUF8IBPqwgPro26rRccN85aLi7f9RRQft+/JzyMPxXeZMjM8nF76+9gI+Ckh9VECigPrU/z/cF3sZpVfehPDCCy9o9erVWrt2rd58801lZ2dr/vz5OnHiRG/sDgAwAPVKAT3xxBNavny57rrrLl155ZXauHGj4uPj9fTTT/fG7gAAA1DEC6ijo0P79+9XXl7ef+0kKkp5eXkqLS39xPbt7e1qbGzscQMADH4RL6BTp06pq6tLqampPe5PTU1VbW3tJ7YvLCxUKBTqvvEOOAAYGpz/IOqaNWvU0NDQfTt27JjrJQEA+kDE3/6UnJys6Oho1dXV9bi/rq5OaWlpn9g+GAwqGAxGehkAgH4u4ldAcXFxmjFjhoqKirrvC4fDKioqUm5ubqR3BwAYoHrlB0BWr16tpUuX6pprrtGsWbP05JNPqqWlRXfddVdv7A4AMAD1SgHdeuutOnnypB555BHV1tbq6quv1s6dOz/xxgQAwNAV8Lz+NaOisbFRoVBIc7SQSQg++Bmr01fjcfrSq9UHfeW6fIyG2d9hn1AQ9uzf/f7SMPukgbc7Ws0ZSVr1hWt95fqrQGycr5zX2RHhlQwNZ71OFWuHGhoaNGrUqAtu5/xdcACAoYkCAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATvTKNGy405eDRVsX55gzJ6+2P+eZcuO75kxlZ7M5I/l7Rvab1qnmTGzAPsD0irg/mDOjo33OGi4aa46c3JppzqSVnDJnut4+Ys74HioaZR8Aq7D933ao4goIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATjANe5DpuumL5sxXfrzH176Sov+POXNp7B/NmX+o+jNzpqh1sjkjSdcNrzRnVl3ynq99WRWfGWHOxAb8TUcfHtNpzpQ+/JQ589P7JpkzT76ab85MXvuf5owkhZuafOXw2XAFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOMIy0P4uKNkf+Yv2r5swT//bn5owkJR32zJmydRvNmfcu/a0509AVb85IUpcC5syzTaPNmaauYeZMho9Brk9X32DOSNKPs+yDZq987T5zZvKKcnMm8YUPzZlZr9szklSWHesrh8+GKyAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIJhpP1Y66JrzJm7Qz8xZ/Zc/wdzRpJa/nmkOTPxhXvMmd9/7Slz5u1Oc0SSVB+2Dwmt9zH4NC2mwZxpCg83Z1KHN5ozkvR3x79izkz6+gFzpvqb15oz2cmHzZllifvMGUnadesD5kzCC2W+9jUUcQUEAHCCAgIAOBHxAnr00UcVCAR63KZMmRLp3QAABrheeQ3oqquu0u7du/9rJzG81AQA6KlXmiEmJkZpaWm98akBAINEr7wGdOTIEWVkZGjChAm68847dfTo0Qtu297ersbGxh43AMDgF/ECysnJ0ebNm7Vz505t2LBBVVVVuuGGG9TU1HTe7QsLCxUKhbpvmZmZkV4SAKAfingB5efn62tf+5qmT5+u+fPn65e//KXq6+v14osvnnf7NWvWqKGhoft27NixSC8JANAP9fq7AxITEzV58mRVVFSc9/FgMKhgMNjbywAA9DO9/nNAzc3NqqysVHp6em/vCgAwgES8gB544AGVlJTovffe029+8xt99atfVXR0tG6//fZI7woAMIBF/Ftwx48f1+23367Tp09rzJgxuv7661VWVqYxY8ZEelcAgAEs4gX0/PPPR/pTDln1k6LNmVavw5y5I9XfoMb7H7zDnBn3y7A5s+SRuebMO49fYc5I0u+/Yh98+u+NieZMw1n7ANNr4t81Z3739NXmjCSN2Vhqztx46Iw587+2miM6UDvWnEkYG7DvSFLN3C77vl7wtashiVlwAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOBEr/9COvh35urWPtlPl+fveUj+NYfMmSXzfmfOrNrw1+bMld97z5yRpBmXrDBn/uFPtvval9W9ZV83Zyb5GCoqSXX3XWvOPP/uH82ZxUv+w5zp9OxDev3Kver8v0jz05zuhXUMVlwBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAmmYfdj12a9a860hrvMmcviTpgzklQxLNWc+evty80ZL/uMOXPqxHhzRpLSnzlrzpyclmDOXBtfac5M/mG7ORM2J86J6vTMmZiXE82Z7X823Zx5acZPzZn3z8aaM5J0Z6p9mvhTmuJrX0MRV0AAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4ATDSPuxmy95x5xptc+Q9O2nRXPNmVBFwJxZcPN+c+a+G39jzkjSsnHXmzNHvz/anFk4stycCXxw0px59we55owk/dWXd5szz2/8U3Nm/vi3zZlo2U/y010jzBlJmje8xZx5yteehiaugAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACYaR9mNfGVFlzpzssj+naPOizRlJenGhfezi/9j2N+bMrJHvmjPXvWbfjyRdnn3GnEmO3WfOHOsKmjPN12aZMyPftw9/laQ7QwfMmX/Jsg+n/Y/aiebMN5L3mjOtnv14S1JsoNNXDp8NV0AAACcoIACAE+YC2rt3r2655RZlZGQoEAho+/btPR73PE+PPPKI0tPTNXz4cOXl5enIkSORWi8AYJAwF1BLS4uys7O1fv368z6+bt06PfXUU9q4caP27dunESNGaP78+Wpra/vciwUADB7mNyHk5+crPz//vI95nqcnn3xS3/nOd7Rw4UJJ0jPPPKPU1FRt375dt9122+dbLQBg0Ijoa0BVVVWqra1VXl5e932hUEg5OTkqLS09b6a9vV2NjY09bgCAwS+iBVRbWytJSk1N7XF/ampq92MfV1hYqFAo1H3LzMyM5JIAAP2U83fBrVmzRg0NDd23Y8eOuV4SAKAPRLSA0tLSJEl1dXU97q+rq+t+7OOCwaBGjRrV4wYAGPwiWkBZWVlKS0tTUVFR932NjY3at2+fcnNzI7krAMAAZ34XXHNzsyoqKro/rqqq0sGDB5WUlKRx48Zp1apV+v73v6/LLrtMWVlZevjhh5WRkaFFixZFct0AgAHOXEBvvPGGbrrppu6PV69eLUlaunSpNm/erAcffFAtLS1asWKF6uvrdf3112vnzp0aNmxY5FYNABjwAp7nea4X8d81NjYqFAppjhYqJhDrejlOvVp90JzZ395hzkTL3ykwOdY+6HLq9vvMmVHl9mGpzeP8/Z1i7LNI9dQdPzNnogNhc+aviu8yZ0a+E2fOSFLCUfv6PrzS/h393Xets+8nbJ+h/F5nkjkjSX8+otWcWZCVY8547e3mTH921utUsXaooaHhU1/Xd/4uOADA0EQBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIAT9rGy8CWqj34dRYtnn348KuBvEm98lH1f3oiz5kzHKPs07LHZNeaMJFXvTzdnVh281Zy5efwRc8bPZOuWcV3mjCQ1X2afhn3JAft09H1tGebMzGHV5oyf6eO+TZ1kz+z/z8ivYwDgCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnGAYaR/xpl3mI1UW8XWcz+lwvK/cqpo/MWduvqLcnHkrLdWcaemwD+6UpOBp+0DNX9250Zx5rjHbnGmeaB/kqijPnpG0eMZ+c2Z78yxz5u+23WnOHPnLDfZMZ6c541fThJHmzEj74R4UuAICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcYRtpHTlxjH1DYVz7ovMRX7t+32YdPjrv5fXPm9MEUcyZ6UrM5I0njixrMmf9911Rz5isjf2/OvPC7eeZM1zD7cFVJ+rf6HHNmRK19Xylvtpkz+kt7JDbQZQ9JqjlrP4+ax0abM/33f4fexRUQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADjBMNI+0preN/uJk33o4vGO0b72FdNqzyQPazFnTlXY9zNp9gf2kKQP48eZM++eGWPOXHFJvDkzsvqsORPbbM9IUsOVcebMpEXvmjPN+y41Zw512AeYRsv+95GkVs+eaR4f9rWvoYgrIACAExQQAMAJcwHt3btXt9xyizIyMhQIBLR9+/Yejy9btkyBQKDHbcGCBZFaLwBgkDAXUEtLi7Kzs7V+/foLbrNgwQLV1NR035577rnPtUgAwOBjfhNCfn6+8vPzP3WbYDCotLQ034sCAAx+vfIaUHFxsVJSUnT55Zfr3nvv1enTpy+4bXt7uxobG3vcAACDX8QLaMGCBXrmmWdUVFSkH/zgByopKVF+fr66us7/9uDCwkKFQqHuW2ZmZqSXBADohyL+c0C33XZb95+nTZum6dOna+LEiSouLtbcuXM/sf2aNWu0evXq7o8bGxspIQAYAnr9bdgTJkxQcnKyKirO/9OEwWBQo0aN6nEDAAx+vV5Ax48f1+nTp5We3kejAAAAA4L5W3DNzc09rmaqqqp08OBBJSUlKSkpSY899piWLFmitLQ0VVZW6sEHH9SkSZM0f/78iC4cADCwmQvojTfe0E033dT98Uev3yxdulQbNmzQoUOH9Itf/EL19fXKyMjQvHnz9L3vfU/BYDByqwYADHjmApozZ44878IT+l599dXPtaDBqmvSGXPGz9DFeB/fVK1uT7SHJGU89Vtz5t6V9kziw78yZ+7+n/ebM5LUcq39AH4j9LY5U9nZbM6cvirWnMn44e/MGUn6l82/N2cSo+zn+Ld/Pcuceacj1ZyZGHvSnJGkprD9mMek+5jSO0QxCw4A4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABORPxXcuP8Zo5/35xpCseZM+Nj7JN4934wwZyRpIyro82Zv38/xZxJH95ozoQq/U0kDny9xZzJjPnQnNn0x1xz5utLd5kze344wpyRpOWvLDdnxl1Ra87Ej7b/2+5rsj9vvjq52pyRpHofX4OXpdonb3eaE4MDV0AAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4ATDSPtIYtwZc6bLx/ODsTEjzZmz+y8xZyTpTNpZcyY7odycKb/GPqrx6KP24yBJJVM3mDM//eMMX/uyunxYjTnzsy1/4Wtfl91RZs5Uf+tacyZe9mGku45dbs48lPIf5owk1XbFmjOXxtebM++ZE4MDV0AAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4ATDSPvIiOh2c6bTi/axpy5zYsRxz8d+pOg2+75Kf3KNOXNqU4c587PZ/2zOSNIvW7LMmfgo+/r8/Nt+0GkfGjtj/FFzRpImHAibM63fs58PXac/NGea6iaaMx2ev3O8LWwfRjrSx9f6UMUVEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4wTDSPhIbsA9qjJa/AYpWl7zT2if7kaQxf1ljzvxq0lZz5gcnrzdnJGnmyHfNmZaooDlztPlScyY9tt6ceb/RPsBUki4dZt/Xn393tzmz+6UEc2ZMqf2/rbG3jDRnJOkt+5xZDY/u9LEnP4OHBz6ugAAATlBAAAAnTAVUWFiomTNnKiEhQSkpKVq0aJHKy8t7bNPW1qaCggKNHj1aI0eO1JIlS1RXVxfRRQMABj5TAZWUlKigoEBlZWXatWuXOjs7NW/ePLW0tHRvc//99+vll1/W1q1bVVJSourqai1evDjiCwcADGymV/N27tzZ4+PNmzcrJSVF+/fv1+zZs9XQ0KCf//zn2rJli26++WZJ0qZNm3TFFVeorKxMX/rSlyK3cgDAgPa5XgNqaGiQJCUlJUmS9u/fr87OTuXl5XVvM2XKFI0bN06lpaXn/Rzt7e1qbGzscQMADH6+CygcDmvVqlW67rrrNHXqVElSbW2t4uLilJiY2GPb1NRU1dbWnvfzFBYWKhQKdd8yMzP9LgkAMID4LqCCggIdPnxYzz///OdawJo1a9TQ0NB9O3bs2Of6fACAgcHXD6KuXLlSr7zyivbu3auxY8d235+WlqaOjg7V19f3uAqqq6tTWlraeT9XMBhUMGj/QT4AwMBmugLyPE8rV67Utm3btGfPHmVlZfV4fMaMGYqNjVVRUVH3feXl5Tp69Khyc3Mjs2IAwKBgugIqKCjQli1btGPHDiUkJHS/rhMKhTR8+HCFQiHdfffdWr16tZKSkjRq1Cjdd999ys3N5R1wAIAeTAW0YcMGSdKcOXN63L9p0yYtW7ZMkvRP//RPioqK0pIlS9Te3q758+frJz/5SUQWCwAYPEwF5HkXH445bNgwrV+/XuvXr/e9qMGo/my8ORMbOOtjT/b3lUT/3yM+9iO9v/pqc+bp8U+bM9/6YIE588d2+/GW/A0jjVbYnBkRbZ9yOSzKPuQya9SH5owkvVIx1ZzZ+SX7E82Xbn/AnBm99ZA5o3+wRyQpOmD/t61pC/nYU7OPzMDHLDgAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA44es3osLuRNtIcyY20OVjT/bnFOHWVh/7kVJmV5szbV6sOVP85hXmzIj3/Z3aicv3mDOtUX3zG32/EHvKnDnw7/ZjJ0kx9mHdKsmeYM7UzrNP+B71XIs549foKPvXxqFTGeZMkv5gzgwGXAEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMMI+0jp87Yh5GOiWo3Z2rO2jN+pY1oNGf+9eS15kxUu/15UtuYsDkjSZ2KNmeiZN/XiBj7v9MHZy8xZ9rSz5ozkhR1xn7Mf/jWn5ozfzLxqDnjZxRpu2cfeipJSdH23If1I+z7MScGB66AAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJhpH2kT+2Djdn/IzTfKFpqo+UP6vSd5kzm05db85c9cX3zJlRsW3mjCS91XapORMf1WHOJETb17e3YbI5c930P5gzktR6NtacGR1sNWdWpe42Z1Yr15x5pWW0OSNJOcOqzZlguf1rfajiCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnGAYaR9JGNZuzkyMHWnOLE44bM78SvYBoZKU4GMIZ0HKa+bMvzVebc5EyTNn/GoNx5kznV60OdMRtn+5XjbihDnj1zUj3jVnrorrm8GdfoaKStLYGPvXYPvkM772NRRxBQQAcIICAgA4YSqgwsJCzZw5UwkJCUpJSdGiRYtUXl7eY5s5c+YoEAj0uN1zzz0RXTQAYOAzFVBJSYkKCgpUVlamXbt2qbOzU/PmzVNLS0uP7ZYvX66ampru27p16yK6aADAwGd6VXPnzp09Pt68ebNSUlK0f/9+zZ49u/v++Ph4paWlRWaFAIBB6XO9BtTQ0CBJSkpK6nH/s88+q+TkZE2dOlVr1qxRa+uFf1Vve3u7Ghsbe9wAAIOf77dhh8NhrVq1Stddd52mTp3aff8dd9yh8ePHKyMjQ4cOHdJDDz2k8vJyvfTSS+f9PIWFhXrsscf8LgMAMED5LqCCggIdPnxYr7/+eo/7V6xY0f3nadOmKT09XXPnzlVlZaUmTpz4ic+zZs0arV69uvvjxsZGZWZm+l0WAGCA8FVAK1eu1CuvvKK9e/dq7Nixn7ptTk6OJKmiouK8BRQMBhUMBv0sAwAwgJkKyPM83Xfffdq2bZuKi4uVlZV10czBgwclSenp6b4WCAAYnEwFVFBQoC1btmjHjh1KSEhQbW2tJCkUCmn48OGqrKzUli1b9OUvf1mjR4/WoUOHdP/992v27NmaPn16r/wFAAADk6mANmzYIOncD5v+d5s2bdKyZcsUFxen3bt368knn1RLS4syMzO1ZMkSfec734nYggEAg4P5W3CfJjMzUyUlJZ9rQQCAoYFp2H0k+sfJ5szkO5eaM7G/H2HOjNVvzBlJ+pu7V5ozRf/6c3MmdtSb5syIqLA5I0kJAfuPxsX6yIyMGmbOnOpqufhGH9N5kSeNF9LkBcyZybH2c+/a1fYxXQkqM2fm/uJb5owkXXVjhTkz/heM2PysOFIAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4ETAu9iI6z7W2NioUCikOVqomECs6+WgH4i+crI5c2rmaF/7+nCa/cuha6R98Gmg3f7cL6bFPiB0+El7RpIyXqs3Z8IH3/K1Lww+Z71OFWuHGhoaNGrUqAtuxxUQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwIsb1Aj7uo9F0Z9Up9aspdXDF62o3Z7o62nztK9xmP+nC0X0zCy7cZp/r1tXubxbcWR/HPOx1+toXBp+zOncuXGzUaL8bRnr8+HFlZma6XgYA4HM6duyYxo4de8HH+10BhcNhVVdXKyEhQYFAz2dvjY2NyszM1LFjxz51wupgx3E4h+NwDsfhHI7DOf3hOHiep6amJmVkZCgq6sJX+/3uW3BRUVGf2piSNGrUqCF9gn2E43AOx+EcjsM5HIdzXB+HUCh00W14EwIAwAkKCADgxIAqoGAwqLVr1yoYDLpeilMch3M4DudwHM7hOJwzkI5Dv3sTAgBgaBhQV0AAgMGDAgIAOEEBAQCcoIAAAE4MmAJav369vvCFL2jYsGHKycnRb3/7W9dL6nOPPvqoAoFAj9uUKVNcL6vX7d27V7fccosyMjIUCAS0ffv2Ho97nqdHHnlE6enpGj58uPLy8nTkyBE3i+1FFzsOy5Yt+8T5sWDBAjeL7SWFhYWaOXOmEhISlJKSokWLFqm8vLzHNm1tbSooKNDo0aM1cuRILVmyRHV1dY5W3Ds+y3GYM2fOJ86He+65x9GKz29AFNALL7yg1atXa+3atXrzzTeVnZ2t+fPn68SJE66X1ueuuuoq1dTUdN9ef/1110vqdS0tLcrOztb69evP+/i6dev01FNPaePGjdq3b59GjBih+fPnq63N30DS/upix0GSFixY0OP8eO655/pwhb2vpKREBQUFKisr065du9TZ2al58+appaWle5v7779fL7/8srZu3aqSkhJVV1dr8eLFDlcdeZ/lOEjS8uXLe5wP69atc7TiC/AGgFmzZnkFBQXdH3d1dXkZGRleYWGhw1X1vbVr13rZ2dmul+GUJG/btm3dH4fDYS8tLc17/PHHu++rr6/3gsGg99xzzzlYYd/4+HHwPM9bunSpt3DhQifrceXEiROeJK+kpMTzvHP/9rGxsd7WrVu7t3n77bc9SV5paamrZfa6jx8Hz/O8G2+80fvmN7/pblGfQb+/Auro6ND+/fuVl5fXfV9UVJTy8vJUWlrqcGVuHDlyRBkZGZowYYLuvPNOHT161PWSnKqqqlJtbW2P8yMUCiknJ2dInh/FxcVKSUnR5ZdfrnvvvVenT592vaRe1dDQIElKSkqSJO3fv1+dnZ09zocpU6Zo3Lhxg/p8+Phx+Mizzz6r5ORkTZ06VWvWrFFra6uL5V1QvxtG+nGnTp1SV1eXUlNTe9yfmpqqd955x9Gq3MjJydHmzZt1+eWXq6amRo899phuuOEGHT58WAkJCa6X50Rtba0knff8+OixoWLBggVavHixsrKyVFlZqW9/+9vKz89XaWmpoqOjXS8v4sLhsFatWqXrrrtOU6dOlXTufIiLi1NiYmKPbQfz+XC+4yBJd9xxh8aPH6+MjAwdOnRIDz30kMrLy/XSSy85XG1P/b6A8F/y8/O7/zx9+nTl5ORo/PjxevHFF3X33Xc7XBn6g9tuu637z9OmTdP06dM1ceJEFRcXa+7cuQ5X1jsKCgp0+PDhIfE66Ke50HFYsWJF95+nTZum9PR0zZ07V5WVlZo4cWJfL/O8+v234JKTkxUdHf2Jd7HU1dUpLS3N0ar6h8TERE2ePFkVFRWul+LMR+cA58cnTZgwQcnJyYPy/Fi5cqVeeeUVvfbaaz1+fUtaWpo6OjpUX1/fY/vBej5c6DicT05OjiT1q/Oh3xdQXFycZsyYoaKiou77wuGwioqKlJub63Bl7jU3N6uyslLp6emul+JMVlaW0tLSepwfjY2N2rdv35A/P44fP67Tp08PqvPD8zytXLlS27Zt0549e5SVldXj8RkzZig2NrbH+VBeXq6jR48OqvPhYsfhfA4ePChJ/et8cP0uiM/i+eef94LBoLd582bvrbfe8lasWOElJiZ6tbW1rpfWp/72b//WKy4u9qqqqrxf//rXXl5enpecnOydOHHC9dJ6VVNTk3fgwAHvwIEDniTviSee8A4cOOC9//77nud53j/+4z96iYmJ3o4dO7xDhw55Cxcu9LKysrwzZ844XnlkfdpxaGpq8h544AGvtLTUq6qq8nbv3u198Ytf9C677DKvra3N9dIj5t577/VCoZBXXFzs1dTUdN9aW1u7t7nnnnu8cePGeXv27PHeeOMNLzc318vNzXW46si72HGoqKjwvvvd73pvvPGGV1VV5e3YscObMGGCN3v2bMcr72lAFJDned6PfvQjb9y4cV5cXJw3a9Ysr6yszPWS+tytt97qpaene3Fxcd6ll17q3XrrrV5FRYXrZfW61157zZP0idvSpUs9zzv3VuyHH37YS01N9YLBoDd37lyvvLzc7aJ7wacdh9bWVm/evHnemDFjvNjYWG/8+PHe8uXLB92TtPP9/SV5mzZt6t7mzJkz3je+8Q3vkksu8eLj472vfvWrXk1NjbtF94KLHYejR496s2fP9pKSkrxgMOhNmjTJ+9a3vuU1NDS4XfjH8OsYAABO9PvXgAAAgxMFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnPh/AAS7NBpSUN0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f14595-b937-46b0-a81f-bd9355c23126",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5525d8ed-1e35-42ad-9a7e-f2e36b3ffead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "  '''\n",
    "    Simple Convolutional Neural Network\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Conv2d(channels, 32, kernel_size=3, padding=\"same\"),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(32, 64, kernel_size=3, padding=\"same\"),\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.ReLU(),\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(width * height * 64, 128),     \n",
    "#      nn.BatchNorm1d(128),\n",
    "#      nn.ReLU(),\n",
    "#      nn.Linear(128, 128),     \n",
    "#      nn.ReLU(),\n",
    "      nn.Linear(128, 64),\n",
    "      nn.Linear(64, 32),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(32, 10)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21035f64-a16e-47b3-8305-0df0e57ec916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: nn.Module = ConvNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd57da0-b866-4bf1-b17a-196dcaac6d55",
   "metadata": {},
   "source": [
    "# Tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6ac6f08-c246-477f-897b-bda98afb4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf ./logs\n",
    "# writer = SummaryWriter(\"./logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c988ef-a029-4d44-9df5-d48b22114042",
   "metadata": {},
   "source": [
    "## Model Grapph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fba9735-7593-4714-88a3-c299e7a497b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard_write_graph(writer=writer, model=model, x=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb3952-1d51-4b51-bc61-cf853a8ce781",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7eba20d-ba50-4b58-a9ce-2b65c64b76c9",
   "metadata": {},
   "source": [
    "batch_sizes: List[int] = [\n",
    "    8, 16, 32, 64\n",
    "]\n",
    "learning_rates: List[float] = [\n",
    "    1e-5, 1e-4, 1e-3\n",
    "]\n",
    "beta1s: List[float] = [\n",
    "    0.85, 0.9, 0.95\n",
    "]\n",
    "beta2s: List[float] = [\n",
    "    0.9985, 0.999, 0.9995\n",
    "]\n",
    "decays: List[float] = [\n",
    "    0.005, 0.01, 0.015\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d536e2f-0dcf-4752-b491-ebcf9055b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates: List[float] = [\n",
    "    1e-4\n",
    "]\n",
    "beta1s: List[float] = [\n",
    "    0.9, 0.95\n",
    "]\n",
    "beta2s: List[float] = [\n",
    "    0.9985, 0.999\n",
    "]\n",
    "decays: List[float] = [\n",
    "    0.01\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389290c5-bc33-416c-8038-f1f297485e0e",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7dd062b-008c-444d-8a82-8e49d67430c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b2482-38b1-40a8-adae-03742348e036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.9 beta2:0.9985 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch 1\n",
      "loss after batch     1: 0.270\n",
      "val_acc 0.794 val_loss  43.182842\n",
      "test_acc 0.799 test_loss  43.310238\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.9 beta2:0.999 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch 1\n",
      "loss after batch     1: 0.299\n",
      "val_acc 0.766 val_loss  43.933022\n",
      "test_acc 0.770 test_loss  44.091457\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "lr:0.0001 beat1:0.95 beta2:0.9985 decay:0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Starting epoch 1\n",
      "loss after batch     1: 0.291\n",
      "val_acc 0.792 val_loss  44.554764\n"
     ]
    }
   ],
   "source": [
    "prev_loss: float = float(sys.maxsize)\n",
    "early_stop_tolerance: int = 3\n",
    "\n",
    "for lr, beta1, beta2, decay in product(learning_rates, beta1s, beta2s, decays):\n",
    "    print()\n",
    "    print('-' * 80)\n",
    "    print(f\"lr:{lr} beat1:{beta1} beta2:{beta2} decay:{decay}\")\n",
    "    print('-' * 80)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Initialize for each parameter combination.\n",
    "    # --------------------------------------------------------------------------------\n",
    "    run_name: str = f\"./logs/lr{lr}_beat1{beta1}_beta2{beta2}_decay{decay}\"\n",
    "    writer = SummaryWriter(run_name)\n",
    "\n",
    "    model: nn.Module = ConvNet()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(beta1, beta2), weight_decay=decay)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Train for each parameter combination.\n",
    "    # --------------------------------------------------------------------------------\n",
    "    for epoch in range(0, 1):   # epochs at maximum\n",
    "        tensorboard_write_histogram(writer=writer, model=model, step=epoch)\n",
    "    \n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "    \n",
    "        # Reset current loss value at eacch epoch\n",
    "        _current_loss: float = 0.0\n",
    "        _num_records: int = 0\n",
    "    \n",
    "        # Iterate over the DataLoader for training data\n",
    "        for index, data in enumerate(train_loader, 0):\n",
    "            inputs, targets = data\n",
    "            _batch_size: int = len(inputs)\n",
    "            _num_records += _batch_size\n",
    "    \n",
    "            # Write an image at every batch 0\n",
    "            if index == 0:\n",
    "                tensorboard_write_image(writer=writer, tag=\"image\", image=inputs[0], step=epoch, dataformats=\"CHW\")\n",
    "    \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Perform forward pass\n",
    "            outputs = model(inputs)\n",
    "    \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "    \n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "    \n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Print statistics\n",
    "            _current_loss += loss.item()\n",
    "    \n",
    "            if index % 1000 == 1:\n",
    "                print('loss after batch %5d: %.3f' % (index, _current_loss / _num_records))\n",
    "\n",
    "            # if index > 209: break\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Reports per epoch\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Loss\n",
    "        tensorboard_write_scalar(\n",
    "            writer=writer, tag=\"training loss/epoch\", value=_current_loss / _num_records, step=epoch\n",
    "        )\n",
    "    \n",
    "        # Validation accuracy & confusion matrix\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = next(iter(val_loader))\n",
    "            predictions = torch.argmax(model(inputs), axis=-1)\n",
    "\n",
    "        val_loss = loss_fn(predictions.to(torch.float), labels.to(torch.float)) / len(inputs)\n",
    "        val_acc = get_accuracy(predictions=predictions, truth=labels)\n",
    "    \n",
    "        tensorboard_write_scalar(\n",
    "            writer=writer, tag=\"validation loss/epoch\", value=val_loss, step=epoch\n",
    "        )\n",
    "        tensorboard_write_scalar(\n",
    "            writer=writer, tag=\"validation accuracy/epoch\", value=val_acc, step=epoch\n",
    "        )\n",
    "        tensorboard_write_confusion_matrix(\n",
    "            writer=writer,\n",
    "            tag=\"validation confusion matrix\",\n",
    "            predictions=predictions.numpy(),\n",
    "            truth=labels.numpy(),\n",
    "            class_names=list(id_to_label.values()),\n",
    "            step=epoch\n",
    "        )\n",
    "        writer.add_hparams(\n",
    "            hparam_dict={\"lr\": lr, \"beta1\": beta1, \"beta2\": beta2, \"decay\": decay},\n",
    "            metric_dict={\n",
    "                \"val_accuracy\": val_acc,\n",
    "                \"val_loss\": val_loss,\n",
    "            },\n",
    "            run_name=run_name,\n",
    "        )\n",
    "        print(f'val_acc {val_acc:.03f} val_loss {val_loss: 03f}')\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Early stop\n",
    "        # --------------------------------------------------------------------------------\n",
    "        if val_loss > prev_loss:\n",
    "            early_stop_tolerance -= 1\n",
    "        else:\n",
    "            prev_loss = val_loss\n",
    "    \n",
    "        if early_stop_tolerance <= 0:\n",
    "            print(\"early stop\")\n",
    "            break\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Test\n",
    "    # --------------------------------------------------------------------------------\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = next(iter(test_loader))\n",
    "        predictions = torch.argmax(model(inputs), axis=-1)\n",
    "\n",
    "    test_loss = loss_fn(predictions.to(torch.float), labels.to(torch.float)) / len(inputs)\n",
    "    test_acc = get_accuracy(predictions=predictions, truth=labels)\n",
    "    print(f'test_acc {test_acc:.03f} test_loss {test_loss: 03f}')\n",
    "\n",
    "    tensorboard_write_scalar(\n",
    "        writer=writer, tag=\"test loss/epoch\", value=test_loss, step=epoch\n",
    "    )\n",
    "    tensorboard_write_scalar(\n",
    "        writer=writer, tag=\"test accuracy/epoch\", value=test_acc, step=epoch\n",
    "    )\n",
    "    tensorboard_write_confusion_matrix(\n",
    "        writer=writer,\n",
    "        tag=\"test confusion matrix\",\n",
    "        predictions=predictions,\n",
    "        truth=labels,\n",
    "        class_names=list(id_to_label.values()),\n",
    "        step=epoch\n",
    "    )\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b6784-644f-4104-8fb4-0638f5d9765c",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b6e5e9-ee9a-4aeb-9a89-d4068cd223d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inputs, labels = next(iter(test_loader))\n",
    "    predictions = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f287e3d-2c11-4af3-b981-378398c91d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.numpy()\n",
    "predictions = np.argmax(predictions.numpy(), axis=-1).squeeze()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb111cbe-1d52-41ea-8f07-8248a99c6a73",
   "metadata": {},
   "source": [
    "matrix = confusion_matrix(y_true=labels, y_pred=predictions)\n",
    "fig = plot_confusion_matrix(matrix=matrix, class_names=list(id_to_label.values()))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d288c-5574-4ab5-822b-c23e531202ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_write_confusion_matrix(\n",
    "    writer=writer,\n",
    "    tag=\"confusion matrix\",\n",
    "    predictions=predictions,\n",
    "    truth=labels,\n",
    "    class_names=list(id_to_label.values()),\n",
    "    step=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f31978-e40b-4857-8adb-a2c986b4fca3",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae47d8ff-8122-40ca-a34f-f6353141c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d88095-f3ac-4f27-b11b-5bec21dea481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
