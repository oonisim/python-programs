{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2d77ce1-1ca9-4662-b91a-193b34cfe04e",
   "metadata": {},
   "source": [
    "# Best Practices\n",
    "\n",
    "* [PYTORCH COMMON MISTAKES - How To Save Time](https://www.youtube.com/watch?v=O2wJ3tkc-TU)\n",
    "\n",
    "1. Train 1000 epochs only with the first batch with batch_size=1 and 32. This is to verify the **train loss gets close to 0**.\n",
    "2. Check the loss of the first batch is around ```-log(1/num_classes)```.\n",
    "3. Double check ```optimizer.zero_grad()```.\n",
    "4. Double check ```model.train(True)``` ... ```model.eval()``` block.\n",
    "5. Double check ```model.eval()``` before any evaluations (validation, test). Evaluation with ```model.train(True)``` cause inaccuracy e.g. **Dropout** is not disabled.\n",
    "6. Double check if the loss function uses **logits** or **probability (softmax output)**.\n",
    "7. Double check if the loss function for multi label classification uses **sparse index to label** or **one hot encoding**\n",
    "8. Monitor weight distribution mean and variance.\n",
    "9. Monitor activation distribution mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f763a7be-9227-42a2-952c-41e91ebb8cea",
   "metadata": {},
   "source": [
    "# Bias=False before Normalization Layer\n",
    "\n",
    "```bias=False``` before **Normalization Layer** as it will zero-center the data.\n",
    "\n",
    "* [When should you not use the bias in a layer?](https://ai.stackexchange.com/a/27742/45763)\n",
    "\n",
    "> The BatchNorm layer will re-center the data, removing the bias and making it a useless trainable parameter.\n",
    "\n",
    "* [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "> Note that, since we normalize ```𝑊𝑢+𝑏```, the **bias ```𝑏``` can be ignored** since its effect will be canceled by the subsequent mean subtraction.\n",
    "\n",
    "* [Why are biases (typically) not used in attention mechanism?](https://ai.stackexchange.com/a/40256/45763)\n",
    "\n",
    "> The reason for this is that these layers are typically followed by a normalization layer, such as Batch Normalization or Layer Normalization. These normalization layers center the data at mean=0 (and std=1), effectively removing any bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f0cccb-24cd-4a9c-9885-e193692d492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Conv2d(channels, 16, kernel_size=3, padding=\"same\", bias=False),   # <--- Bias=False before Batch Norm\n",
    "      nn.BatchNorm2d(16),\n",
    "      nn.MaxPool2d(kernel_size=2),\n",
    "      nn.ReLU(),\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(width * height * 32 // 16, 64, bias=False),                 # <--- Bias=False before Batch Norm\n",
    "      nn.BatchNorm1d(64),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
