{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aea49b73-b43b-4245-9cd4-eed9fe8c6d29",
   "metadata": {},
   "source": [
    "# Spacy Matcher\n",
    "\n",
    "* [Rule-based matching](https://spacy.io/usage/rule-based-matching)\n",
    "* [Matcher](https://spacy.io/api/matcher)\n",
    "* [Rule-based Matcher Explorer](https://demos.explosion.ai/matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57220627-74a8-4f9b-b4ef-b46c4271458a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea865b1e-e456-4b6b-a635-cd2c55bde280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    List, \n",
    "    Dict,\n",
    "    Tuple\n",
    ")\n",
    "import json\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e5c689-f7a3-4f91-92ac-65339cd7a655",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11ce345d-e9e5-4920-b125-e42e9497832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "vocabulrary: spacy.vocab.Vocab = nlp.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e2fd2f-1f90-4483-ae8c-bc87ddfbc12e",
   "metadata": {},
   "source": [
    "# Matcher\n",
    "\n",
    "The matcher must always share the same ```vocab``` of the documents it will operate on. Use the vocabulrary of the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd967ee5-bdf1-426f-9b28-68829c18de34",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Mathcher(vocabulrary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b8220-d1d6-41ba-8177-f6e324db7624",
   "metadata": {},
   "source": [
    "## Pattern (Token Sequence)\n",
    "\n",
    "Pattern is to find a sequence of token**s** whose [attributes](https://spacy.io/api/attributes) match the rules defined. \n",
    "\n",
    "* each **pattern** is a list of rules sequenced as ```AND``` logic.\n",
    "* each **rule** is a dictionary listing one or more expression ```{expression+}```.\n",
    "* each **expression** can have ```token-attribute : value [: operator]``` where ```operator``` is a regular expression repetition operator.\n",
    "\n",
    "```\n",
    "pattern=[\n",
    "    {expression},\n",
    "    {expression},\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "### Operator\n",
    "\n",
    "To match exactly once, omit the OP.\n",
    "\n",
    "\n",
    "| OP     |                                                                     |\n",
    "|:-------|:--------------------------------------------------------------------|\n",
    "| !      | Negate the pattern, by requiring it to match exactly 0 times.       |\n",
    "| ?      | Make the pattern optional, by allowing it to match 0 or 1 times.    |\n",
    "| +      | Require the pattern to match 1 or more times.                       |\n",
    "| *      | Allow the pattern to match zero or more times.                      |\n",
    "| {n}    | Require the pattern to match exactly n times.                       |\n",
    "| {n,m}  | Require the pattern to match at least n but not more than m times.  |\n",
    "| {n,}   | Require the pattern to match at least n times.                      |\n",
    "| {,m}   | Require the pattern to match at most m times.                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14efe28-890f-49d1-96bf-e2d6ac866b53",
   "metadata": {},
   "source": [
    "### Single Token Match \n",
    "Example to find one token whose ```POS``` is **noun**, ```lemma``` is **match**, in ```LOWER``` case as **matches**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "138a007d-7fe1-45af-827f-19c26532ce6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"POS\": \"NOUN\",\n",
      "        \"LEMMA\": \"match\",\n",
      "        \"LOWER\": \"matches\",\n",
      "        \"OP\": \"?\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "rule = {\n",
    "    'POS': 'NOUN', \n",
    "    'LEMMA': 'match',\n",
    "    'LOWER': 'matches',\n",
    "    'OP': '?',\n",
    "}\n",
    "\n",
    "pattern: List[Dict[str, str]] = [\n",
    "    rule    \n",
    "]\n",
    "print(json.dumps(pattern, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5f01b3c-7caf-4dc2-9855-6db16498d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\n",
    "    \"find_noun_matches\", \n",
    "    [\n",
    "        pattern\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfc426e-df66-40b5-998e-a7bdf2592fe7",
   "metadata": {},
   "source": [
    "Find the matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce76f605-1b4c-43ae-ae28-00136045b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text: str = \"\"\"\n",
    "A match starts a fire. Modern matches are small wooden sticks.\n",
    "Regex \\w+es matches plurals.\n",
    "Little Girl Selling Matches is about a girl selling matches dying.\n",
    "\"\"\"\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "26fbc683-e807-4a62-b7e7-43b01b95886b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 8 end: 9 match: matches\n",
      "start: 30 end: 31 match: matches\n",
      "token: '\\n'\n",
      "token: A           \n",
      "token: match       \n",
      "token: starts      \n",
      "token: a           \n",
      "token: fire        \n",
      "token: .           \n",
      "token: Modern      \n",
      "token: matches      <----- found\n",
      "token: are         \n",
      "token: small       \n",
      "token: wooden      \n",
      "token: sticks      \n",
      "token: .           \n",
      "token: '\\n'\n",
      "token: Regex       \n",
      "token: \\w+es       \n",
      "token: matches     \n",
      "token: plurals     \n",
      "token: .           \n",
      "token: '\\n'\n",
      "token: Little      \n",
      "token: Girl        \n",
      "token: Selling     \n",
      "token: Matches     \n",
      "token: is          \n",
      "token: about       \n",
      "token: a           \n",
      "token: girl        \n",
      "token: selling     \n",
      "token: matches      <----- found\n",
      "token: dying       \n",
      "token: .           \n",
      "token: '\\n'\n"
     ]
    }
   ],
   "source": [
    "matched_token_span_locations: List[Tuple] = []\n",
    "for match_id, start, end in matches:\n",
    "    matched_token_span_locations.append((start, end))\n",
    "\n",
    "for start, end in matched_token_span_locations:\n",
    "    print(f\"start: {start} end: {end} match: {doc[start:end]}\")\n",
    "\n",
    "mached: int = 0\n",
    "for token in doc:\n",
    "    if matched_token_span_locations[mached][0] <= token.i < matched_token_span_locations[mached][1]:\n",
    "        print(f\"token: {token.text:12} <----- found\")        \n",
    "    elif token.is_space:\n",
    "        print(f\"token: {repr(token.text)}\") \n",
    "    else:\n",
    "        print(f\"token: {token.text:12}\")        \n",
    "\n",
    "    if token.i >= matched_token_span_locations[mached][1]:\n",
    "        if mached < len(matched_token_span_locations) -1:\n",
    "            mached += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06802670-8345-42cc-8a73-da564f18d8a9",
   "metadata": {},
   "source": [
    "### Multi Token Match\n",
    "\n",
    "Listing multiple rules defines ```AND``` pattern to match a specific token sequence. Example to find a token sequence ```hello, world``` or ```hello! world```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "625c03b1-cfc0-4de5-a24b-8d7e72a414d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"LOWER\": \"hello\"},    # AND\n",
    "    {\"IS_PUNCT\": True},    # AND\n",
    "    {\"LOWER\": \"world\"}\n",
    "]\n",
    "matcher.add(\n",
    "    \"find_hello_punctuation_world\", \n",
    "    [\n",
    "        pattern\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c93dbdc6-3f88-4d75-9af2-346638ecbc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text: str = \"Start learning with hello! world is from The C Programming Language.\"\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e84d1dd4-cad5-407f-8d5f-a234fdc9a2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: Start       \n",
      "token: learning    \n",
      "token: with        \n",
      "token: hello        <----- found\n",
      "token: !            <----- found\n",
      "token: world        <----- found\n",
      "token: is          \n",
      "token: from        \n",
      "token: The         \n",
      "token: C           \n",
      "token: Programming \n",
      "token: Language    \n",
      "token: .           \n"
     ]
    }
   ],
   "source": [
    "matched_token_span_locations: List[Tuple] = []\n",
    "for match_id, start, end in matches:\n",
    "    matched_token_span_locations.append((start, end))\n",
    "\n",
    "mached: int = 0\n",
    "for token in doc:\n",
    "    if matched_token_span_locations[mached][0] <= token.i < matched_token_span_locations[mached][1]:\n",
    "        print(f\"token: {token.text:12} <----- found\")        \n",
    "    elif token.is_space:\n",
    "        print(f\"token: {repr(token.text)}\") \n",
    "    else:\n",
    "        print(f\"token: {token.text:12}\")        \n",
    "\n",
    "    if token.i >= matched_token_span_locations[mached][1]:\n",
    "        if mached < len(matched_token_span_locations) -1:\n",
    "            mached += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
