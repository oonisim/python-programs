{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91acc77c089ba7fa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Using GPU with Tensorslow\n",
    "\n",
    "# TensorFlow CUDA package\n",
    "\n",
    "* [Install TensorFlow with pip](https://www.tensorflow.org/install/gpu)\n",
    "\n",
    "> Software requirements\n",
    "> * Python 3.9–3.11\n",
    "> *  pip version 19.0 or higher for Linux (requires manylinux2014 support) and Windows. pip version 20.3 or higher for macOS.\n",
    "> *Windows Native Requires Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019\n",
    "> \n",
    "> The following NVIDIA® software are only required for GPU support.\n",
    "> \n",
    "> NVIDIA® GPU drivers version 450.80.02 or higher.\n",
    "> * CUDA® Toolkit 11.8.    <-----\n",
    "> * cuDNN SDK 8.6.0.       <-----\n",
    "> ```\n",
    "> python3 -m pip install tensorflow[and-cuda]\n",
    "> # Verify the installation:\n",
    "> python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n",
    "> ```\n",
    "\n",
    "## CUDA driver\n",
    "\n",
    "Make sure CUDA driver for the current GPU is installed.\n",
    "\n",
    "## Cuda Toolkit\n",
    "\n",
    "Install the CUDA toolkit version supported by the Tensorflow\n",
    "* [CUDA Toolkit Archive](https://developer.nvidia.com/cuda-toolkit-archive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22292535772d13a4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_HOME']\n",
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ddc7562",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-18T10:10:54.238198704Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 21:10:54.494256: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-18 21:10:54.553926: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-18 21:10:54.553972: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-18 21:10:54.554002: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-18 21:10:54.566119: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-18 21:10:56.779920: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad1c2a9",
   "metadata": {},
   "source": [
    "# GPU Devices\n",
    "\n",
    "* [GPU support](https://www.tensorflow.org/install/gpu)\n",
    "\n",
    "> TensorFlow GPU support requires an assortment of drivers and libraries. To simplify installation and avoid library conflicts, we recommend using a TensorFlow Docker image with GPU support (Linux only). This setup only requires the NVIDIA® GPU drivers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8bc8b3",
   "metadata": {},
   "source": [
    "* [Use a GPU](https://www.tensorflow.org/guide/gpu)\n",
    "\n",
    "> * \"/device:CPU:0\": The CPU of your machine.<br>\n",
    "> * \"/GPU:0\": Short-hand notation for the first GPU of your machine that is visible to TensorFlow.\n",
    "> * \"/job:localhost/replica:0/task:0/device:GPU:1\": Fully qualified name of the second GPU of your machine that is visible to TensorFlow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ecff42",
   "metadata": {},
   "source": [
    "## Confirm GPU\n",
    "\n",
    "On a GPU instance (using Google colab GPU runtime)\n",
    "\n",
    "```\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')\n",
    "---\n",
    "Num GPUs Available:  1\n",
    "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "744f1513",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T10:10:59.920186982Z",
     "start_time": "2023-11-18T10:10:59.912623749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a GPU available: \n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 21:10:59.714001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-18 21:10:59.910942: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "print(\"Is there a GPU available: \"),\n",
    "print(tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b997b",
   "metadata": {},
   "source": [
    "### Number of available GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6af688a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T10:11:09.488736846Z",
     "start_time": "2023-11-18T10:11:09.277272049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e9078",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c416be",
   "metadata": {},
   "source": [
    "# TF usage of GPU\n",
    "\n",
    "Using CPU or GPU is transparent to TensorFlow code.\n",
    "\n",
    "* [Use a GPU](https://www.tensorflow.org/guide/gpu)\n",
    "\n",
    "> TensorFlow code, and tf.keras models will transparently run on a single GPU with no code changes required.\n",
    "\n",
    ">tf.matmul has both CPU and GPU kernels. On a system with devices CPU:0 and GPU:0, the GPU:0 device will be selected to run tf.matmul unless you explicitly request running it on another device."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05825821",
   "metadata": {},
   "source": [
    "## Confirm if TF is using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4b2fdb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T10:56:24.734597842Z",
     "start_time": "2023-11-18T10:56:24.691342995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d616c4",
   "metadata": {},
   "source": [
    "## Confirm a tensor is allocated on a GPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "916bce71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T10:11:16.781784547Z",
     "start_time": "2023-11-18T10:11:16.690140993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the Tensor on GPU #0:  \n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.uniform([3, 3])\n",
    "\n",
    "print(\"Is the Tensor on GPU #0:  \"),\n",
    "print(x.device.endswith('GPU:0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe180ea",
   "metadata": {},
   "source": [
    "## Explicit device assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "253de7e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T10:11:18.296283730Z",
     "start_time": "2023-11-18T10:11:17.964866784Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Place tensors on the CPU\n",
    "with tf.device('/CPU:0'):\n",
    "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "    c = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e13684",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "with tf.device('/GPU:0'):\n",
    "    # Place tensors on the GPU\n",
    "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "    \n",
    "    # Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
    "    c = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbda17a7-5be9-492a-82f2-feafc86d5479",
   "metadata": {},
   "source": [
    "# Control TF GPU Usage\n",
    "\n",
    "* [Limiting GPU memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth)\n",
    "\n",
    "> to only allocate a subset of the available memory, or to **only grow the memory usage** as is needed by the process. TensorFlow provides two methods to control this.\n",
    ">   \n",
    "> The first option is to turn on memory growth by calling ```tf.config.experimental.set_memory_growth```, which **attempts to allocate only as much GPU memory as needed** for the runtime allocations: it starts out allocating very little memory, and as the program gets run and more GPU memory is needed, the GPU memory region is extended for the TensorFlow process. Memory is not released since it can lead to memory fragmentation. To turn on memory growth for a specific GPU, use the following code prior to allocating any tensors or executing any ops. Another way to enable this option is to set the environmental variable TF_FORCE_GPU_ALLOW_GROWTH to true. This configuration is platform specific.\n",
    "> ```\n",
    "> gpus = tf.config.list_physical_devices('GPU')\n",
    "> if gpus:\n",
    ">   try:\n",
    ">     # Currently, memory growth needs to be the same across GPUs\n",
    ">     for gpu in gpus:\n",
    ">       tf.config.experimental.set_memory_growth(gpu, True)\n",
    ">     logical_gpus = tf.config.list_logical_devices('GPU')\n",
    ">     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    ">   except RuntimeError as e:\n",
    ">     # Memory growth must be set before GPUs have been initialized\n",
    ">     print(e)\n",
    "> ```\n",
    ">\n",
    "> The second method is to configure a virtual GPU device with tf.config.set_logical_device_configuration and set a hard limit on the total memory to allocate on the GPU. This is useful if you want to truly bound the amount of GPU memory available to the TensorFlow process. This is common practice for local development when the GPU is shared with other applications such as a workstation GUI.\n",
    "> ```\n",
    "> gpus = tf.config.list_physical_devices('GPU')\n",
    "> if gpus:\n",
    ">   # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    ">   try:\n",
    ">     tf.config.set_logical_device_configuration(\n",
    ">         gpus[0],\n",
    ">         [tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n",
    ">     logical_gpus = tf.config.list_logical_devices('GPU')\n",
    ">     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    ">   except RuntimeError as e:\n",
    ">     # Virtual devices must be set before GPUs have been initialized\n",
    ">     print(e)\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f7b760e87d80cd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Confirm GPU usage\n",
    "\n",
    "Run ```nvidia-smi``` while training/inference is on-going to make sure GPU is being used.\n",
    "\n",
    "```\n",
    "$ nvidia-smi\n",
    "Sat Nov 18 22:05:29 2023       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 4050 ...    Off | 00000000:01:00.0 Off |                  N/A |\n",
    "| N/A   45C    P8               3W /  35W |   4398MiB /  6141MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A    119310      C   /home/eml/venv/ml/bin/python3              4392MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ba4f56de0095111",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T11:06:22.369283011Z",
     "start_time": "2023-11-18T11:06:22.168187041Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 18 22:06:22 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 4050 ...    Off | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   44C    P8               3W /  35W |   4398MiB /  6141MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A    119310      C   /home/eml/venv/ml/bin/python3              4392MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6aba6a7e974ea4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# OutOfMemory\n",
    "\n",
    "Make sure the batch can be allocated to GPU memory to avoid OOM:\n",
    "\n",
    "```\n",
    "OOM when allocating tensor with shape[32,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
    "\t [[{{node pascal_voc_cnn/conv01/Conv2D}}]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9cab40-e164-48ec-a3b9-e7374fde3d0d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "# Using Multiple GPU\n",
    "\n",
    "* [Using multiple GPUs](https://www.tensorflow.org/guide/gpu#using_multiple_gpus)\n",
    "\n",
    "> The best practice for using multiple GPUs is to use tf.distribute.Strategy. This program will run a copy of your model on each GPU, splitting the input data between them, also known as \"data parallelism\".\n",
    "> ```\n",
    "> tf.debugging.set_log_device_placement(True)\n",
    "> gpus = tf.config.list_logical_devices('GPU')\n",
    "> strategy = tf.distribute.MirroredStrategy(gpus)\n",
    "> with strategy.scope():\n",
    ">   inputs = tf.keras.layers.Input(shape=(1,))\n",
    ">   predictions = tf.keras.layers.Dense(1)(inputs)\n",
    ">   model = tf.keras.models.Model(inputs=inputs, outputs=predictions)\n",
    ">   model.compile(loss='mse',\n",
    ">                 optimizer=tf.keras.optimizers.SGD(learning_rate=0.2))\n",
    "> ```\n",
    "\n",
    "* [Distributed training with TensorFlow](https://www.tensorflow.org/guide/distributed_training)\n",
    "\n",
    "> tf.distribute.Strategy is a TensorFlow API to distribute training across multiple GPUs, multiple machines, or TPUs. Using this API, you can distribute your existing models and training code with minimal code changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Disable TF GPU Usage\n",
    "\n",
    "* [CUDA Pro Tip: Control GPU Visibility with CUDA_VISIBLE_DEVICES](https://developer.nvidia.com/blog/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/)\n",
    "\n",
    "> easy solution for testing is to use the environment variable CUDA_VISIBLE_DEVICES to restrict the devices that your CUDA application sees.  \n",
    ">   \n",
    "> \n",
    "> To use it, set CUDA_VISIBLE_DEVICES to a comma-separated list of device IDs to make only those devices visible to the application.  Note that you can use this technique both to mask out devices or to change the visibility order of devices so that the CUDA runtime enumerates them in a specific order.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7f9504b61617f62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Disable GPU\n",
    "!export CUDA_VISIBLE_DEVICES='-1'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6b6a5fc8106e100"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# From Python. Run this at the start before any TF operation.\n",
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != 'GPU'\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T01:23:44.314482658Z",
     "start_time": "2023-11-19T01:23:44.314001250Z"
    }
   },
   "id": "9cd2a829bcbc7905"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "392e97449236af72"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
