{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ffa6365",
   "metadata": {},
   "source": [
    "# Custom Data x Generator for model.fit(x)\n",
    "\n",
    "## Motivation\n",
    "\n",
    "You want to have a Data Genrator x that continuously provides a batch for model.fit(). There are two approaches.\n",
    "\n",
    "1. Use [from_generator](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator).\n",
    "2. Use [tf.keras.utils.Sequence](https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence).\n",
    "\n",
    "\n",
    "## Sequence\n",
    "\n",
    "* [tf.keras.utils.Sequence](https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence)\n",
    "\n",
    "> Base object for fitting to a sequence of data, such as a dataset. Every Sequence must implement the __getitem__ and the __len__ methods. If you want to modify your dataset between epochs you may implement on_epoch_end. The method __getitem__ should return a complete batch.\n",
    "\n",
    "\n",
    "```\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Here, `x_set` is list of path to the images\n",
    "# and `y_set` are the associated classes.\n",
    "\n",
    "class CIFAR10Sequence(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]\n",
    "\n",
    "        return np.array([\n",
    "            resize(imread(file_name), (200, 200))\n",
    "               for file_name in batch_x]), np.array(batch_y)\n",
    "```\n",
    "\n",
    "## Examples\n",
    "\n",
    "* [A detailed example of how to use data generators with Keras](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly)\n",
    "\n",
    "> In this blog post, we are going to show you how to generate your dataset on multiple cores in real time and feed it right away to your deep learning model.\n",
    "\n",
    "* [Training on Large Datasets That Donâ€™t Fit In Memory in Keras - Creation of Custom Generator](https://medium.com/@mrgarg.rajat/training-on-large-datasets-that-dont-fit-in-memory-in-keras-60a974785d71)\n",
    "\n",
    "> Note: As our dataset is too large to fit in memory, we have to load the dataset from the hard disk in batches to our memory. To do so, we are going to create a custom generator. Our Custom Generator is going to load the dataset from the hard disk in batches to memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd5a179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
