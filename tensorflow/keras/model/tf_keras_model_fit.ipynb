{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94926c28bb723e26",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Model.fit() input as ```tf.data.Dataset```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e4b02da9bf4b4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "* [tf.keras.Model.fit()](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)\n",
    "\n",
    "> A tf.data dataset. Should return a tuple of either **(inputs, targets)** or (inputs, targets, sample_weights).\n",
    "\n",
    "* [How do I train my keras model using Tf Datasets #720](https://github.com/tensorflow/datasets/issues/720)\n",
    "\n",
    "> You can use ```tfds.load(as_supervised=True)``` kwargs to return an **```(image, label)``` tuple expected by keras**. \n",
    "For images, you would have in addition to cast/normalize the image to tf.float32, for this, you can use ```tf.data.Dataset.map```.\n",
    "> \n",
    "> ```\n",
    "> def _normalize_img(img, label):\n",
    ">   img = tf.cast(img, tf.float32) / 255.\n",
    ">   return (img, label)\n",
    "> \n",
    "> ds = tfds.load('mnist', split='train', as_supervised=True)\n",
    "> ds = ds.batch(32)\n",
    "> ds = ds.map(_normalize_img)\n",
    ">\n",
    "> model.fit(ds_train, epochs=5)\n",
    "> ```\n",
    " \n",
    "* [How does tf.keras.Model tell between features and label(s) in tf.data.Dataset and in TFRecords?](https://stackoverflow.com/a/59838140/4281353) \n",
    "\n",
    "> As such, the dataset that is given to model.fit is actually **a dataset of tuples**, and to the best of my knowledge, this is exactly what the model will assume if you provide a tf.data.Dataset as input to the fit function -- **a dataset of tuples (inputs, labels)**. So the first will be taken as input to the model, the second as target for the loss function.\n",
    "\n",
    "* [Support model.fit using targets in a dictionary](https://github.com/tensorflow/tensorflow/issues/24962#issuecomment-475709720)\n",
    "\n",
    "> ```\n",
    "> def make_dataset(images, labels, batch_size=64, buffer_size=1024, shuffle=True):\n",
    ">     inputs = dict(images=images)\n",
    ">     outputs = dict(labels=labels)\n",
    ">     dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    ">     if shuffle:\n",
    ">         dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    ">     dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ">     dataset = dataset.map(map_func=normalize_fn, num_parallel_calls=8)\n",
    ">     dataset = dataset.batch(batch_size)\n",
    ">     return dataset\n",
    "> \n",
    "> model.add(tf.keras.Input(shape=(28, 28, 1), name='images'))\n",
    "> model.add(tf.keras.layers.Dense(10, activation='softmax', name='labels'))\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d085b8-cb2b-4811-8de7-6bf27ab13662",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T00:16:31.140340504Z",
     "start_time": "2023-11-19T00:16:31.134568327Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Baics\n",
    "\n",
    "1. Generate a ```tf.data.Dataset``` that returns ```(data, label)```.\n",
    "2. Use tf.data.Dataset.from_tensor_slices((inputs, outputs)) where inputs and outputs are separate sequences.\n",
    "\n",
    "## Batch Shape\n",
    "\n",
    "```\n",
    "model.fit(dataset)\n",
    "---\n",
    "ValueError: Input 0 of layer \"model_6\" is incompatible with the layer:\n",
    "expected shape=(None, 448, 448, 3), found shape=(448, 448, 3)\n",
    "```\n",
    "\n",
    "```tf.keras.Model.fit()``` expects batches. DO not forget ```tf.data.Dataset.batch(batch_size)``` to have the batched shape.\n",
    "```\n",
    "model.fit(dataset.batch(batch_size))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c9e64fa150b7e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Don'ts\n",
    "\n",
    "Do not attempt to manipulate the shape. The ```tf.data.Dataset``` should be already in the state that the ```tf.keras.model.fit()``` can accept. \n",
    "\n",
    "### ValueError: Creating variables on a non-first call to a function decorated with tf.function.\n",
    "\n",
    "If try to manipulate, it can cause issues. For instance, the code creates new Tensors in the ```tf.data.Dataset.map()``` function which is invoked during ```tf.keras.model.fit()``` which runs in Graph mode.  DO NOT use ```tf.config.run_functions_eagerly(True)``` to get around.\n",
    "\n",
    "```\n",
    "# tf.config.run_functions_eagerly(False)\n",
    "def mapper(image, label):\n",
    "    return (\n",
    "        tf.expand_dims(image, axis=0), # <--- creating a new Tensor\n",
    "        tf.expand_dims(label, axis=0   # <--- creating a new Tensor\n",
    "    )\n",
    "\n",
    "model.fit(train_dataset.map(mapper))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "See [Running the Tensorflow 2.0 code gives 'ValueError: tf.function-decorated function tried to create variables on non-first call'. What am I doing wrong?](https://stackoverflow.com/a/59209937/4281353) for other errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b327a9357d2acf95",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---\n",
    "# Using generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5a0cf250042a9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## steps_per_epoch\n",
    "\n",
    "```tf.keras.Model.fit()``` does not know the number of records that the generator can provide. Need to tell ```fit()``` that it can consume ```num_batches_per_epoch * batch_size``` records per epoch. This ```num_batches_per_epoch``` is passed via ```steps_per_epoch``` argument.\n",
    "\n",
    "```tf.keras.Model.fit()``` keeps consuming records from the generator during the training. In total, ```fit``` consumes ```batch_size * num_batches_per_epoch * num_epochs``` records. The generator needs to be able to provide the amount of records.\n",
    "\n",
    "### Calculation\n",
    "\n",
    "```steps_per_epoch = total_availble_records / batch_size / num_epochs```\n",
    "\n",
    "\n",
    "## Prevent exhausting generator\n",
    "\n",
    "1. Set ```steps_per_epoch``` and ```validation_steps``` arguments, or\n",
    "2. Implement loop inside the generator to keep producing records. \n",
    "\n",
    "\n",
    "<img src=\"./image/tf_keras_model_fit_steps_per_epoch.png\" align=\"left\" width=700/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273cbbef1806e18c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1552a8b888c7248",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T04:49:26.064942054Z",
     "start_time": "2023-11-21T04:49:24.364878280Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 18:32:16.667353: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-21 18:32:16.693939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-21 18:32:16.693960: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-21 18:32:16.693979: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-21 18:32:16.699778: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f59ab916cd313db6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T04:50:08.913076985Z",
     "start_time": "2023-11-21T04:50:01.776283054Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 18:32:18.011344: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-21 18:32:18.019980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-21 18:32:18.020203: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-21 18:32:18.022090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-21 18:32:18.022262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-21 18:32:18.022370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-21 18:32:18.083609: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-21 18:32:18.083733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-21 18:32:18.083825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-21 18:32:18.083905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4285 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "(train, test), info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc892177-5c17-4154-a00d-456152e430ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(image, label):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eff71f24-3ecb-45bd-ae45-c88eebf9d757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "num_total_train_records = len(list(\n",
    "    train.map(f)\n",
    "))\n",
    "num_total_test_records = len(list(\n",
    "    test.map(f)\n",
    "))\n",
    "print(num_total_train_records, num_total_test_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5672032f-bca2-4001-82e6-8f2887c0142f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eef839e-4d98-479c-9c5b-df848d66cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f71f07-b5d6-406b-a4d6-08dff0a3e05b",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd49eb4c-cb42-418a-8d66-fd8009c78cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750 624\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "batch_size = 8\n",
    "\n",
    "num_x_batches_per_epoch = int(np.floor(num_total_train_records / batch_size / num_epochs))\n",
    "num_v_batches_per_epoch = int(np.floor(num_total_test_records / batch_size / num_epochs)) -1  # Cuase ran out of data without -1\n",
    "\n",
    "print(num_x_batches_per_epoch, num_v_batches_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e056d2-037c-4326-bdae-973e1c031447",
   "metadata": {},
   "source": [
    "## Without steps_per_epoch\n",
    "\n",
    "```model.fit``` will exhaust the genreator and cause the error:\n",
    "\n",
    "> Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1250 batches). You may need to use the repeat() function when building your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fd0ec42-f4f3-4237-8851-60274f098f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 18:32:20.475119: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f3890641fd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-21 18:32:20.475135: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9\n",
      "2023-11-21 18:32:20.478134: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-21 18:32:20.487187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2023-11-21 18:32:20.551998: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 24s 3ms/step - loss: 1.6823 - sparse_categorical_accuracy: 0.8409\n",
      "Epoch 2/2\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 15000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 18:32:43.649131: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2933709128551705068\n",
      "2023-11-21 18:32:43.649195: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 897582143205307554\n",
      "2023-11-21 18:32:43.675769: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2933709128551705068\n",
      "2023-11-21 18:32:43.675837: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 897582143205307554\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 15000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 2us/step - loss: 0.0000e+00 - sparse_categorical_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f39b015ab30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_generator = train.batch(batch_size).as_numpy_iterator()\n",
    "v_generator = test.batch(batch_size).as_numpy_iterator()\n",
    "\n",
    "model.fit(\n",
    "    x=x_generator ,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    #steps_per_epoch=num_x_batches_per_epoch,\n",
    "    #validation_data=v_generator,\n",
    "    #validation_steps=num_v_batches_per_epoch,\n",
    "    #validation_batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098e3134-9d1c-4c41-b494-d424924d00b9",
   "metadata": {},
   "source": [
    "## With steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5472bf8f-e3c5-4187-baa2-d415cb3ad6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3750/3750 [==============================] - 11s 3ms/step - loss: 0.4219 - sparse_categorical_accuracy: 0.9019\n",
      "Epoch 2/2\n",
      "3750/3750 [==============================] - 9s 2ms/step - loss: 0.4273 - sparse_categorical_accuracy: 0.9034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f39b00b7e20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_generator = train.batch(batch_size).as_numpy_iterator()\n",
    "v_generator = test.batch(batch_size).as_numpy_iterator()\n",
    "\n",
    "model.fit(\n",
    "    x=x_generator ,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    steps_per_epoch=num_x_batches_per_epoch,\n",
    "    #validation_data=v_generator,\n",
    "    #validation_steps=num_v_batches_per_epoch,\n",
    "    #validation_batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bef28b-3744-48b5-b9f5-e4863cf47276",
   "metadata": {},
   "source": [
    "---\n",
    "# Validation without validation_steps \n",
    "\n",
    "Although the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) says it is for **tf.data.Dataset**, it is required for **generator as well**.\n",
    "\n",
    "> Only relevant if validation_data is provided and **is a tf.data dataset**. Total ```number of steps (batches of samples)``` to draw before stopping when performing validation at the end of every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3c59140-2c11-48db-b792-7b0dc9ca2236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 0.3696 - sparse_categorical_accuracy: 0.9133 - val_loss: 0.4669 - val_sparse_categorical_accuracy: 0.9056\n",
      "Epoch 2/2\n",
      "  57/3750 [..............................] - ETA: 10s - loss: 0.4637 - sparse_categorical_accuracy: 0.8947"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 18:33:16.922099: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2933709128551705068\n",
      "2023-11-21 18:33:16.922163: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 897582143205307554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3741/3750 [============================>.] - ETA: 0s - loss: 0.3842 - sparse_categorical_accuracy: 0.9095WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1250 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 18:33:27.850811: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2933709128551705068\n",
      "2023-11-21 18:33:27.850832: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 897582143205307554\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1250 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750/3750 [==============================] - 11s 3ms/step - loss: 0.3836 - sparse_categorical_accuracy: 0.9096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f39a040dd80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_generator = train.batch(batch_size).as_numpy_iterator()\n",
    "v_generator = test.batch(batch_size).as_numpy_iterator()\n",
    "\n",
    "model.fit(\n",
    "    x=x_generator ,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    steps_per_epoch=num_x_batches_per_epoch,\n",
    "    validation_data=v_generator,\n",
    "    #validation_steps=num_v_batches_per_epoch,\n",
    "    #validation_batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd1df04-8ae2-419c-98dd-7e862b418acf",
   "metadata": {},
   "source": [
    "---\n",
    "# Validation with validation_steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e3f0f05-4d0e-4e62-877a-300898ce05f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3750/3750 [==============================] - 11s 3ms/step - loss: 0.3617 - sparse_categorical_accuracy: 0.9151 - val_loss: 0.4130 - val_sparse_categorical_accuracy: 0.9199\n",
      "Epoch 2/2\n",
      "3750/3750 [==============================] - 11s 3ms/step - loss: 0.4030 - sparse_categorical_accuracy: 0.9107 - val_loss: 0.3837 - val_sparse_categorical_accuracy: 0.9233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f39a040f400>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_generator = train.batch(batch_size).as_numpy_iterator()\n",
    "v_generator = test.batch(batch_size).as_numpy_iterator()\n",
    "\n",
    "model.fit(\n",
    "    x=x_generator ,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    steps_per_epoch=num_x_batches_per_epoch,\n",
    "    validation_data=v_generator,\n",
    "    validation_steps=num_v_batches_per_epoch,\n",
    "    validation_batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59173040-4380-46f5-bd9c-c1f140e1bc4f",
   "metadata": {},
   "source": [
    "---\n",
    "# validation_steps confusion\n",
    "\n",
    "validation_steps seems ```-1``` required.\n",
    "\n",
    "* [tensorflow - tf.keras.Model.fit causes run out of data for validation data with validation_steps being set](https://stackoverflow.com/questions/77520936/tensorflow-tf-keras-model-fit-causes-run-out-of-data-for-validation-data-with)\n",
    "\n",
    "* [tensorflow - tf.keras.Model.fit causes run out of data for validation data with validation_steps being set#62444](https://github.com/tensorflow/tensorflow/issues/62444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d706b857-54c9-4dd4-81b6-4993f405ae86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750 625\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "batch_size = 8\n",
    "\n",
    "num_x_batches_per_epoch = int(np.floor(num_total_train_records / batch_size / num_epochs))\n",
    "# without -1\n",
    "num_v_batches_per_epoch = int(np.floor(num_total_test_records / batch_size / num_epochs))\n",
    "\n",
    "print(num_x_batches_per_epoch, num_v_batches_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6777a8b0-54c4-4390-b712-20bb4ea1a96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3750/3750 [==============================] - 11s 3ms/step - loss: 0.3464 - sparse_categorical_accuracy: 0.9211 - val_loss: 0.6398 - val_sparse_categorical_accuracy: 0.9116\n",
      "Epoch 2/2\n",
      "3749/3750 [============================>.] - ETA: 0s - loss: 0.3644 - sparse_categorical_accuracy: 0.9141WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 625 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 18:34:13.022749: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2933709128551705068\n",
      "2023-11-21 18:34:13.022866: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 897582143205307554\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 625 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750/3750 [==============================] - 13s 3ms/step - loss: 0.3644 - sparse_categorical_accuracy: 0.9141 - val_loss: 0.4150 - val_sparse_categorical_accuracy: 0.9115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f39a04b2c20>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_generator = train.batch(batch_size).as_numpy_iterator()\n",
    "v_generator = test.batch(batch_size).as_numpy_iterator()\n",
    "\n",
    "model.fit(\n",
    "    x=x_generator ,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    steps_per_epoch=num_x_batches_per_epoch,\n",
    "    validation_data=v_generator,\n",
    "    validation_steps=num_v_batches_per_epoch,\n",
    "    validation_batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91bbddb-b1dd-4d1e-9e97-f6330b3e1cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
