{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "949bbc15",
   "metadata": {},
   "source": [
    "# Tensorflow Categorical Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7d6626",
   "metadata": {},
   "source": [
    "---\n",
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf80e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-01 21:52:56.654208: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='titanic',\n",
       "    full_name='titanic/2.0.0',\n",
       "    description=\"\"\"\n",
       "    Dataset describing the survival status of individual passengers on the Titanic. Missing values in the original dataset are represented using ?. Float and int missing values are replaced with -1, string missing values are replaced with 'Unknown'.\n",
       "    \"\"\",\n",
       "    homepage='https://www.openml.org/d/40945',\n",
       "    data_path='/home/oonisim/tensorflow_datasets/titanic/2.0.0',\n",
       "    download_size=114.98 KiB,\n",
       "    dataset_size=532.14 KiB,\n",
       "    features=FeaturesDict({\n",
       "        'features': FeaturesDict({\n",
       "            'age': tf.float32,\n",
       "            'boat': tf.string,\n",
       "            'body': tf.int32,\n",
       "            'cabin': tf.string,\n",
       "            'embarked': ClassLabel(shape=(), dtype=tf.int64, num_classes=4),\n",
       "            'fare': tf.float32,\n",
       "            'home.dest': tf.string,\n",
       "            'name': tf.string,\n",
       "            'parch': tf.int32,\n",
       "            'pclass': ClassLabel(shape=(), dtype=tf.int64, num_classes=3),\n",
       "            'sex': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "            'sibsp': tf.int32,\n",
       "            'ticket': tf.string,\n",
       "        }),\n",
       "        'survived': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "    }),\n",
       "    supervised_keys=('features', 'survived'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'train': <SplitInfo num_examples=1309, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@ONLINE {titanic,\n",
       "    author = \"Frank E. Harrell Jr., Thomas Cason\",\n",
       "    title  = \"Titanic dataset\",\n",
       "    month  = \"oct\",\n",
       "    year   = \"2017\",\n",
       "    url    = \"https://www.openml.org/d/40945\"\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Construct a tf.data.Dataset\n",
    "train, info = tfds.load(\n",
    "    'titanic:2.*.*',              # Name of the dataset\n",
    "    with_info=True,       # Information of the dataset\n",
    "    shuffle_files=True, \n",
    "    split='train[:90%]'\n",
    ")\n",
    "validation = tfds.load(\n",
    "    'titanic:2.*.*',              # Name of the dataset\n",
    "    with_info=False,       # Information of the dataset\n",
    "    shuffle_files=True, \n",
    "    split='train[:10%]'\n",
    ")\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38dde7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'age': TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
       "  'boat': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       "  'body': TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       "  'cabin': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       "  'embarked': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       "  'fare': TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
       "  'home.dest': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       "  'name': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       "  'parch': TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       "  'pclass': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       "  'sex': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       "  'sibsp': TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       "  'ticket': TensorSpec(shape=(), dtype=tf.string, name=None)},\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.map(lambda row: (row['features'], row['survived']))\n",
    "tf.data.experimental.get_structure(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29439c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-01 21:52:57.193937: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1178"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for row in train:\n",
    "    count +=1\n",
    "    \n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07af5f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = validation.map(lambda row: (row['features'], row['survived']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb10383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for row in validation:\n",
    "    count +=1\n",
    "    \n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573a124",
   "metadata": {},
   "source": [
    "## Examin dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85aa9331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['age', 'boat', 'body', 'cabin', 'embarked', 'fare', 'home.dest', 'name', 'parch', 'pclass', 'sex', 'sibsp', 'ticket']\n",
      "A batch of ages: tf.Tensor([30. 37. 28. 18. -1.], shape=(5,), dtype=float32)\n",
      "A batch of targets: tf.Tensor([0 0 1 0 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "[(train_features, label_batch)] = train.batch(5).take(1)\n",
    "print('Features:', list(train_features.keys()))\n",
    "print('A batch of ages:', train_features['age'])\n",
    "print('A batch of targets:', label_batch )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc4396",
   "metadata": {},
   "source": [
    "## Keras layer to convert categorical into MHE\n",
    "\n",
    "Convert a TF dataset categorical column (single TF Tensor) into MHE columns (single Tensor having multiple columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e0a4458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_encoding_layer(dataset, name, dtype, max_tokens=None, oov_token=None):\n",
    "    \"\"\"Create a Keras layer to convert a column into Multi Hot Encoding.\n",
    "    The layer function as below.\n",
    "    1. Convert string/integer in the target column (dataset[name]) into indices.\n",
    "       e.g. ['cat', 'dog', 'fish', 'bird', 'ant'] into [0,1,2,3,4]\n",
    "    2. Convert indices in the column into Multi Hot Encoding.\n",
    "    \n",
    "    Args:\n",
    "        dataset: TF Dataset that have the target column against which to create the category_encoding_layer.\n",
    "        name: The name that identifies the target column in the dataset.\n",
    "        max_tokens: \n",
    "            Use the top max_token most frequent tokens are used to create the vocabulary. \n",
    "            All others will be treated as out-of-vocabulary (OOV).\n",
    "\n",
    "    Returns: Keras layer to function as category encoder.\n",
    "    \"\"\"\n",
    "    if dtype == 'string':\n",
    "    # Create a layer that turns strings into integer indices.\n",
    "        oov_token = oov_token if oov_token is not None and isinstance(oov_token, str) else '[UNK]'\n",
    "        lookup = tf.keras.layers.StringLookup(max_tokens=max_tokens, oov_token=oov_token)\n",
    "    else:\n",
    "        # Otherwise, create a layer that turns integer values into integer indices.\n",
    "        oov_token = oov_token if oov_token is not None and isinstance(oov_token, (inf, float)) else -1\n",
    "        lookup = tf.keras.layers.IntegerLookup(max_tokens=max_tokens, oov_token=oov_token)\n",
    "\n",
    "    # Extract the target feature column by \"name\" from the \"dataset\"\n",
    "    feature = dataset.map(lambda features, label: features[name])\n",
    "\n",
    "    # Fit the lookup table (string -> int) to the values in the feature column.\n",
    "    lookup.adapt(feature)\n",
    "\n",
    "    # Encode the integer indices. Multi Hot to save the space.\n",
    "    # encoder = tf.keras.layers.CategoryEncoding(num_tokens=lookup.vocabulary_size(), output_mode='multi_hot')\n",
    "    encoder = tf.keras.layers.CategoryEncoding(num_tokens=lookup.vocabulary_size(), output_mode='multi_hot')\n",
    "\n",
    "    def f(column):\n",
    "        \"\"\"Apply multi-hot encoding\"\"\"\n",
    "        return encoder(lookup(column))\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a99a0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Unknown'] : [0. 1. 0. 0. 0.]\n",
      "[b'Unknown'] : [0. 1. 0. 0. 0.]\n",
      "[b'Unknown'] : [0. 1. 0. 0. 0.]\n",
      "[b'Unknown'] : [0. 1. 0. 0. 0.]\n",
      "[b'Unknown'] : [0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Test the string categorical 'Type' column conversion into MHE'\n",
    "tensor_column_categorical_cabin = tf.constant([\n",
    "    [cabin.numpy()] for cabin in train_features['cabin']\n",
    "])\n",
    "\n",
    "test_cabin_layer = get_category_encoding_layer(\n",
    "    dataset=train,\n",
    "    name='cabin',\n",
    "    dtype='string',\n",
    "    max_tokens=5\n",
    ")\n",
    "tensor_column_mhe_cabin = test_cabin_layer(tensor_column_categorical_cabin)\n",
    "\n",
    "for i in range(len(tensor_column_categorical_cabin)):\n",
    "    print(\"{} : {}\".format(\n",
    "    tensor_column_categorical_cabin[i].numpy(),\n",
    "    tensor_column_mhe_cabin[i].numpy()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbc226e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] : [0. 1. 0. 0. 0.]\n",
      "[2] : [0. 1. 0. 0. 0.]\n",
      "[2] : [0. 1. 0. 0. 0.]\n",
      "[2] : [0. 1. 0. 0. 0.]\n",
      "[0] : [0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "tensor_column_categorical_embarked = tf.constant([\n",
    "    [col.numpy()] for col in train_features['embarked']\n",
    "])\n",
    "\n",
    "test_embarked_layer = get_category_encoding_layer(\n",
    "    dataset=train,\n",
    "    name='embarked',\n",
    "    dtype='int64',\n",
    "    max_tokens=None\n",
    ")\n",
    "tensor_column_mhe_embarked = test_embarked_layer(tensor_column_categorical_embarked)\n",
    "\n",
    "for i in range(len(tensor_column_categorical_embarked)):\n",
    "    print(\"{} : {}\".format(\n",
    "    tensor_column_categorical_embarked[i].numpy(),\n",
    "    tensor_column_mhe_embarked[i].numpy()\n",
    "))\n",
    "    \n",
    "del test_embarked_layer, tensor_column_categorical_embarked, tensor_column_mhe_embarked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9f3511",
   "metadata": {},
   "source": [
    "## Keras layer to normalize numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "649da93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "    # Create a Normalization layer for the feature.\n",
    "    normalizer = tf.keras.layers.Normalization(axis=None)\n",
    "\n",
    "    # Prepare a Dataset that only yields the feature.\n",
    "    feature = dataset.map(lambda features, label: features[name])\n",
    "\n",
    "    # Learn the statistics of the data.\n",
    "    normalizer.adapt(feature)\n",
    "\n",
    "    return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04f8f1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.] : [0.33245727]\n",
      "[37.] : [0.7231315]\n",
      "[28.] : [0.22083609]\n",
      "[18.] : [-0.3372699]\n",
      "[-1.] : [-1.3976712]\n"
     ]
    }
   ],
   "source": [
    "tensor_column_categorical_age = tf.constant([\n",
    "    [col.numpy()] for col in train_features['age']\n",
    "])\n",
    "test_norm_layer = get_normalization_layer('age', train)\n",
    "tensor_column_mhe_age = test_norm_layer(tensor_column_categorical_age)\n",
    "\n",
    "for i in range(len(tensor_column_categorical_age)):\n",
    "    print(\"{} : {}\".format(\n",
    "    tensor_column_categorical_age[i].numpy(),\n",
    "    tensor_column_mhe_age[i].numpy()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db88465",
   "metadata": {},
   "source": [
    "---\n",
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f332aa",
   "metadata": {},
   "source": [
    "## Split data into training, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e988fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train = train.batch(batch_size).shuffle(buffer_size=32).prefetch(1)\n",
    "validation = validation.batch(batch_size).shuffle(buffer_size=32).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e02199",
   "metadata": {},
   "source": [
    "## Keras model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c11e1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e784ab9",
   "metadata": {},
   "source": [
    "### Holizontal Keras preprocessing layers for numerical normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2450898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features.\n",
    "for header in ['age', 'fare']:\n",
    "    numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "    normalization_layer = get_normalization_layer(header, train)\n",
    "    encoded_numeric_col = normalization_layer(numeric_col)\n",
    "    all_inputs.append(numeric_col)\n",
    "    encoded_features.append(encoded_numeric_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1da937",
   "metadata": {},
   "source": [
    "### Holizontal Keras preprocessing layers for numerical categorical into MHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "425ceb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for header in ['pclass', 'sex', 'sibsp']:\n",
    "    numeric_input_feature = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
    "    numeric_category_encoding_layer = get_category_encoding_layer(\n",
    "        name=header,\n",
    "        dataset=train,\n",
    "        dtype='int64',\n",
    "        max_tokens=None\n",
    "    )\n",
    "    categorically_encoded_feature = numeric_category_encoding_layer(numeric_input_feature)\n",
    "    all_inputs.append(numeric_input_feature)\n",
    "    encoded_features.append(categorically_encoded_feature)\n",
    "    \n",
    "for header in ['embarked']:\n",
    "    numeric_input_feature = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
    "    numeric_category_encoding_layer = get_category_encoding_layer(\n",
    "        name=header,\n",
    "        dataset=train,\n",
    "        dtype='int64',\n",
    "        max_tokens=None\n",
    "    )\n",
    "    categorically_encoded_feature = numeric_category_encoding_layer(numeric_input_feature)\n",
    "    all_inputs.append(numeric_input_feature)\n",
    "    encoded_features.append(categorically_encoded_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf9f90",
   "metadata": {},
   "source": [
    "### Holizontal Keras preprocessing layers for String categorical into MHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeaf639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_categorical_columns = [\n",
    "    'boat', 'cabin'\n",
    "]\n",
    "\n",
    "for column_name in string_categorical_columns:\n",
    "    string_input_feature = tf.keras.Input(shape=(1,), name=column_name, dtype='string')\n",
    "\n",
    "    # String category encoding layer\n",
    "    string_category_encoding_layer = get_category_encoding_layer(\n",
    "        name=column_name,\n",
    "        dataset=train,\n",
    "        dtype='string',\n",
    "        max_tokens=5,\n",
    "        oov_token='[UNK]'\n",
    "    )\n",
    "    # Categorical encoding\n",
    "    categorically_encoded_feature = string_category_encoding_layer(string_input_feature)\n",
    "\n",
    "    all_inputs.append(string_input_feature)\n",
    "    encoded_features.append(categorically_encoded_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e109748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "model = tf.keras.Model(all_inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0da08323",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ba4da",
   "metadata": {},
   "source": [
    "## Keras Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e331c207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "# Use `rankdir='LR'` to make the graph horizontal.\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c4c7a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 0.1045 - accuracy: 0.9686 - val_loss: 0.0748 - val_accuracy: 0.9847\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.1084 - accuracy: 0.9686 - val_loss: 0.0754 - val_accuracy: 0.9847\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.1043 - accuracy: 0.9703 - val_loss: 0.0741 - val_accuracy: 0.9847\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.1047 - accuracy: 0.9669 - val_loss: 0.0758 - val_accuracy: 0.9847\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 13ms/step - loss: 0.1062 - accuracy: 0.9669 - val_loss: 0.0740 - val_accuracy: 0.9847\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.1021 - accuracy: 0.9703 - val_loss: 0.0740 - val_accuracy: 0.9847\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 0.0884 - accuracy: 0.9771 - val_loss: 0.0733 - val_accuracy: 0.9847\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.1032 - accuracy: 0.9703 - val_loss: 0.0741 - val_accuracy: 0.9847\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 0.1017 - accuracy: 0.9686 - val_loss: 0.0737 - val_accuracy: 0.9847\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 0.0981 - accuracy: 0.9703 - val_loss: 0.0730 - val_accuracy: 0.9847\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 1s 19ms/step - loss: 0.0977 - accuracy: 0.9728 - val_loss: 0.0730 - val_accuracy: 0.9847\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0939 - accuracy: 0.9745 - val_loss: 0.0723 - val_accuracy: 0.9847\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.0958 - accuracy: 0.9745 - val_loss: 0.0734 - val_accuracy: 0.9847\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 1s 19ms/step - loss: 0.0902 - accuracy: 0.9745 - val_loss: 0.0732 - val_accuracy: 0.9847\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.0892 - accuracy: 0.9711 - val_loss: 0.0724 - val_accuracy: 0.9847\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 0.0895 - accuracy: 0.9762 - val_loss: 0.0736 - val_accuracy: 0.9847\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 0.0952 - accuracy: 0.9711 - val_loss: 0.0728 - val_accuracy: 0.9847\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0680e604f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=train, \n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            verbose=1, \n",
    "            mode='min',\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ],\n",
    "    validation_data=validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02793d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-01 21:53:17.796506: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/pet_classifier_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/pet_classifier_model/assets\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p model\n",
    "model.save('model/titanic_classifier_model')\n",
    "reloaded_model = tf.keras.models.load_model('model/titanic_classifier_model')\n",
    "\n",
    "#del train, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed55ca",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
