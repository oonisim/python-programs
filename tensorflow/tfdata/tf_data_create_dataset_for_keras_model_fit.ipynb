{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "310e9f2f",
   "metadata": {},
   "source": [
    "# Create Dataset for Keras mode.fit()\n",
    "\n",
    "* [tf.data: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data) (MUST)\n",
    "* [Load CSV data](https://www.tensorflow.org/tutorials/load_data/csv)\n",
    "* [tf.data.experimental.make_csv_dataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset)\n",
    "\n",
    "\n",
    "## Input format for model.fit()\n",
    "\n",
    "\n",
    "* [How to create the same structure of tf.data.experimental.make_csv_dataset from pandas\n",
    "](https://stackoverflow.com/questions/69777802/how-to-create-the-same-structure-of-tf-data-experimental-make-csv-dataset-from-p/69778344#69778344)\n",
    "\n",
    "> tensorflow.org/api_docs/python/tf/keras/Model#fit defines what should be passed to .fit() \n",
    "\n",
    "When passing TF dataset, it shoudl have ```(features, labels)``` format which is to be created by ```tf.data.Dataset.from_tensor_slices((features, labels))``` where **features** can be either:\n",
    "\n",
    "> * A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs).\n",
    "> * A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs).\n",
    "> * A dict mapping input names to the corresponding array/tensors, if the model has named inputs.\n",
    "> * A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights).\n",
    "\n",
    "* [tf.keras.Model - fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)\n",
    "\n",
    "> ```fit(x=None, y=None)```  \n",
    "> ### x: Input data.  \n",
    "> For a tf.data dataset, it should be **a tuple of either (inputs, targets) or (inputs, targets, sample_weights)**.\n",
    "> ###  y: Target data  \n",
    "> either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3d84466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37282a60",
   "metadata": {},
   "source": [
    "# Example CSV\n",
    "\n",
    "Titanic CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c284223",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_file = tf.keras.utils.get_file(\"titanic_train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe563962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone\r\n",
      "0,male,22.0,1,0,7.25,Third,unknown,Southampton,n\r\n",
      "1,female,38.0,1,0,71.2833,First,C,Cherbourg,n\r\n",
      "1,female,26.0,0,0,7.925,Third,unknown,Southampton,y\r\n",
      "1,female,35.0,1,0,53.1,First,C,Southampton,n\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5  ~/.keras/datasets/titanic_train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89b3d8b",
   "metadata": {},
   "source": [
    "# Dictionary to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeb44b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'name': <tf.Tensor: shape=(), dtype=string, numpy=b'hoge'>, 'age': <tf.Tensor: shape=(), dtype=int32, numpy=20>}, {'survived': <tf.Tensor: shape=(), dtype=int32, numpy=0>})\n",
      "({'name': <tf.Tensor: shape=(), dtype=string, numpy=b'tako'>, 'age': <tf.Tensor: shape=(), dtype=int32, numpy=99>}, {'survived': <tf.Tensor: shape=(), dtype=int32, numpy=1>})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-01 21:19:07.561343: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "features = {\n",
    "    \"name\": [\"hoge\", \"tako\"],\n",
    "    \"age\": [20, 99]\n",
    "}\n",
    "labels = {\n",
    "    \"survived\": [0, 1]\n",
    "}\n",
    "ds_from_dictionary = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "for record in ds_from_dictionary:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55d9f25",
   "metadata": {},
   "source": [
    "# Pandas to Dataset\n",
    "\n",
    "* [How to create the same structure of tf.data.experimental.make_csv_dataset from pandas\n",
    "](https://stackoverflow.com/questions/69777802/how-to-create-the-same-structure-of-tf-data-experimental-make-csv-dataset-from-p/69778344#69778344)\n",
    "\n",
    "If  data fits in memory, ```from_tensor_slices``` method works on dictionaries.\n",
    "\n",
    "```Pandas -> Python dictionary -> from_tensor_slices -> Dataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ecb2715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 627 entries, 0 to 626\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   survived            627 non-null    int64  \n",
      " 1   sex                 627 non-null    object \n",
      " 2   age                 627 non-null    float64\n",
      " 3   n_siblings_spouses  627 non-null    int64  \n",
      " 4   parch               627 non-null    int64  \n",
      " 5   fare                627 non-null    float64\n",
      " 6   class               627 non-null    object \n",
      " 7   deck                627 non-null    object \n",
      " 8   embark_town         627 non-null    object \n",
      " 9   alone               627 non-null    object \n",
      "dtypes: float64(2), int64(3), object(5)\n",
      "memory usage: 49.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(titanic_file)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fc01e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'sex': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       "  'age': TensorSpec(shape=(), dtype=tf.float64, name=None),\n",
       "  'n_siblings_spouses': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       "  'parch': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       "  'fare': TensorSpec(shape=(), dtype=tf.float64, name=None),\n",
       "  'class': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       "  'deck': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       "  'embark_town': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       "  'alone': TensorSpec(shape=(), dtype=tf.string, name=None)},\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataset of format (features, labels) where \n",
    "# - \"features\" is a dictionary or numpy array or generator.\n",
    "# - \n",
    "titanic_from_pandas = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(df.loc[:, df.columns != 'survived']),   # Multi-column features as dictionary or numpy array or Tensor\n",
    "    df.loc[:, 'survived']                        # Labels format depends on the number of classes.\n",
    "\n",
    "))\n",
    "tf.data.experimental.get_structure(titanic_from_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "408e4543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male']\n",
      "age                 : [22.]\n",
      "n_siblings_spouses  : [1]\n",
      "parch               : [0]\n",
      "fare                : [7.25]\n",
      "class               : [b'Third']\n",
      "deck                : [b'unknown']\n",
      "embark_town         : [b'Southampton']\n",
      "alone               : [b'n']\n",
      "label/survived      : [0]\n"
     ]
    }
   ],
   "source": [
    "for row in titanic_from_pandas.batch(1).take(1):  # Take the first batch \n",
    "    features = row[0]        # Diectionary\n",
    "    label = row[1]\n",
    "    \n",
    "    for feature, value in features.items():\n",
    "        print(f\"{feature:20s}: {value}\")\n",
    "    \n",
    "    print(f\"label/survived      : {label}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a7238",
   "metadata": {},
   "source": [
    "## Generalized function for pandas to TF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce29a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, label_column_name=\"label\", shuffle=True, batch_size=32):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(label_column_name)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(dataframe),   # <--- X: features\n",
    "        labels             # <--- Y: labels\n",
    "    ))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size).prefetch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebf97a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'sex': TensorSpec(shape=(None,), dtype=tf.string, name=None),\n",
       "  'age': TensorSpec(shape=(None,), dtype=tf.float64, name=None),\n",
       "  'n_siblings_spouses': TensorSpec(shape=(None,), dtype=tf.int64, name=None),\n",
       "  'parch': TensorSpec(shape=(None,), dtype=tf.int64, name=None),\n",
       "  'fare': TensorSpec(shape=(None,), dtype=tf.float64, name=None),\n",
       "  'class': TensorSpec(shape=(None,), dtype=tf.string, name=None),\n",
       "  'deck': TensorSpec(shape=(None,), dtype=tf.string, name=None),\n",
       "  'embark_town': TensorSpec(shape=(None,), dtype=tf.string, name=None),\n",
       "  'alone': TensorSpec(shape=(None,), dtype=tf.string, name=None)},\n",
       " TensorSpec(shape=(None,), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_from_pandas = df_to_dataset(dataframe=df, label_column_name=\"survived\", shuffle=False, batch_size=1)\n",
    "tf.data.experimental.get_structure(titanic_from_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2efc0b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male']\n",
      "age                 : [22.]\n",
      "n_siblings_spouses  : [1]\n",
      "parch               : [0]\n",
      "fare                : [7.25]\n",
      "class               : [b'Third']\n",
      "deck                : [b'unknown']\n",
      "embark_town         : [b'Southampton']\n",
      "alone               : [b'n']\n",
      "label/survived      : [0]\n"
     ]
    }
   ],
   "source": [
    "for row in titanic_from_pandas.take(1):  # Take the first batch \n",
    "    features = row[0]        # Diectionary\n",
    "    label = row[1]\n",
    "    \n",
    "    for feature, value in features.items():\n",
    "        print(f\"{feature:20s}: {value}\")\n",
    "    \n",
    "    print(f\"label/survived      : {label}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4362cdc",
   "metadata": {},
   "source": [
    "## tfdf.keras.pd_dataframe_to_tf_dataset\n",
    "\n",
    "Keras TF Decision Forests has its own implementation of pandas to TF dataset conversion.\n",
    "\n",
    "* [TensorFlow Decision Forests Installation](https://www.tensorflow.org/decision_forests/installation)\n",
    "* [tfdf.keras.pd_dataframe_to_tf_dataset](https://www.tensorflow.org/decision_forests/api_docs/python/tfdf/keras/pd_dataframe_to_tf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d1687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tensorflow_decision_forests --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abf0482",
   "metadata": {},
   "source": [
    "---\n",
    "# Read CSV into Dataset\n",
    "\n",
    "```make_csv_dataset``` method reads the csv and split them into ```(features, label)``` where ```features``` is a diectioay of ```(feature, value)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58094bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = tf.data.experimental.make_csv_dataset(\n",
    "    titanic_file,\n",
    "    label_name=\"survived\",\n",
    "    batch_size=1,   # To compre with the head of CSV\n",
    "    shuffle=False,  # To compre with the head of CSV\n",
    "    header=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c2c24d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male']\n",
      "age                 : [22.]\n",
      "n_siblings_spouses  : [1]\n",
      "parch               : [0]\n",
      "fare                : [7.25]\n",
      "class               : [b'Third']\n",
      "deck                : [b'unknown']\n",
      "embark_town         : [b'Southampton']\n",
      "alone               : [b'n']\n",
      "label/survived      : [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-01 21:19:08.618284: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "for row in titanic.take(1):  # Take the first batch \n",
    "    features = row[0]        # Diectionary\n",
    "    label = row[1]\n",
    "    \n",
    "    for feature, value in features.items():\n",
    "        print(f\"{feature:20s}: {value}\")\n",
    "    \n",
    "    print(f\"label/survived      : {label}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509ad73",
   "metadata": {},
   "source": [
    "# HuggingFace example\n",
    "\n",
    "* [How to fine-tune HuggingFace BERT model for Text Classification](https://stackoverflow.com/questions/69025750/how-to-fine-tune-huggingface-bert-model-for-text-classification/69025751#69025751)\n",
    "* [huggingface_fine_tuning.ipynb](https://nbviewer.org/github/omontasama/nlp-huggingface/blob/main/fine_tuning/huggingface_fine_tuning.ipynb)\n",
    "* [Hugging Face Transformers: Fine-tuning DistilBERT for Binary Classification Tasks](https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a76de583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generatred <class 'transformers.tokenization_utils_base.BatchEncoding'> with content:\n",
      "{'input_ids': <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
      "array([[ 101, 1045, 2360, 7592,  102,    0],\n",
      "       [ 101, 2017, 2360, 2204, 9061,  102]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 0],\n",
      "       [1, 1, 1, 1, 1, 1]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    TFDistilBertForSequenceClassification,\n",
    ")\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Tokenizer\n",
    "# --------------------------------------------------------------------------------\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "def tokenize(sentences, max_length=MAX_SEQUENCE_LENGTH, padding='max_length'):\n",
    "    \"\"\"Tokenize using the Huggingface tokenizer\n",
    "    Args:\n",
    "        sentences: String or list of string to tokenize\n",
    "        padding: Padding method ['do_not_pad'|'longest'|'max_length']\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        sentences,\n",
    "        truncation=True,\n",
    "        padding=padding,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "sample_tokens = tokenize(\n",
    "    [   # Two example seenteces\n",
    "        \"i say hello\", \n",
    "        \"you say good bye\",\n",
    "    ],\n",
    "    padding='longest'\n",
    ")\n",
    "print(f\"generatred {type(sample_tokens)} with content:\\n{sample_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1aee4f50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BATCH_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_96831/2002795906.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Convert BatchEncoding instance to dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_label\u001b[0m                  \u001b[0;31m# List[int32] to be converted into numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m )).batch(BATCH_SIZE).prefetch(1)\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'BATCH_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "DATA_COLUMN = 'comment_text'\n",
    "LABEL_COLUMN = 'toxic'\n",
    "\n",
    "raw_train = pd.read_csv(\"./data/train.csv\")\n",
    "train_data, validation_data, train_label, validation_label = train_test_split(\n",
    "    raw_train[DATA_COLUMN].tolist(),\n",
    "    raw_train[LABEL_COLUMN].tolist(),\n",
    "    test_size=.2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "X = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(tokenize(train_data)),  # Convert BatchEncoding instance to dictionary\n",
    "    train_label                  # List[int32] to be converted into numpy array\n",
    ")).batch(BATCH_SIZE).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa9fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.take(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
