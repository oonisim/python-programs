{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4a67cc",
   "metadata": {},
   "source": [
    "# Word indexing\n",
    "\n",
    "Convert words in a corpus into integer word indices as a preprocess to vectorize a word (1-gram), n-grams, or  a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03c41d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59520c22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Simple implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34f8cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Dict,\n",
    "    List\n",
    ")\n",
    "import re\n",
    "import string\n",
    "\n",
    "def standardize(text: str) -> str:\n",
    "    \"\"\"Standardize the text\n",
    "    1. Lower the string\n",
    "    2. Remove punctuation\n",
    "    3. Remove white space, new lines, carriage returns\n",
    "    Args:\n",
    "        text: sequence of words\n",
    "    Returns\n",
    "        standardized: standardized text\n",
    "    \"\"\"\n",
    "    assert isinstance(text, str) and len(text) > 0\n",
    "    replacement = \" \"\n",
    "    pattern: str = '[%s%s]+' % (re.escape(string.punctuation), r\"\\s\")\n",
    "\n",
    "    standardized: str = re.compile(pattern).sub(repl=replacement, string=text).lower().strip()\n",
    "    assert len(standardized) > 0, f\"Text [{text}] needs words other than punctuations.\"\n",
    "    return standardized\n",
    "\n",
    "\n",
    "def word_indexing(corpus: str):\n",
    "    \"\"\"Generate word indices\n",
    "    Args:\n",
    "        corpus: A string including sentences to process.\n",
    "    Returns:\n",
    "        vocabulary: unique words in the corpus\n",
    "        id_to_word: word index to word mapping\n",
    "        word_to_id: word to word index mapping\n",
    "    \"\"\"\n",
    "    words = standardize(corpus).split()\n",
    "    vocabulary = ['UNK'] + list(set(words))\n",
    "    id_to_word: Dict[int, str] = dict(enumerate(vocabulary))\n",
    "    word_to_id: Dict[str, int] = dict(zip(id_to_word.values(), id_to_word.keys()))\n",
    "\n",
    "    return words, vocabulary, id_to_word, word_to_id\n",
    "\n",
    "\n",
    "def text_to_sequence(\n",
    "        corpus,\n",
    "        word_to_id: dict\n",
    ") -> List[str]:\n",
    "    \"\"\"Generate integer sequence word\n",
    "    Args:\n",
    "        corpus: A string including sentences to process.\n",
    "        word_to_id: word to integer index mapping\n",
    "    Returns:\n",
    "        sequence:\n",
    "            word indices to every word in the originlal corpus as as they appear in it.\n",
    "            The objective of sequence is to preserve the original corpus but as numerical indices.\n",
    "    \"\"\"\n",
    "    return [word_to_id.get(w, 0) for w in standardize(corpus).split()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8fe2a732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UNK', 'announces', 'promising', 'greatest', 'room', 'up', 'children', 'his', 'visiting', 'commands', 'government', 'kingdom', 'throne', 'responsibilities', 'britain', 'the', 'share', 'daughters', 'him', 'them', 'plan', 'loves', 'spend', 'among', 'to', 'say', 'divide', 'most', 'and', 'give', 'old', 'that', 'enters', 'intends', 'ruler', 'of', 'daughter', 'he', 'age', 'three', 'which']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15,\n",
       " 34,\n",
       " 35,\n",
       " 14,\n",
       " 32,\n",
       " 7,\n",
       " 12,\n",
       " 4,\n",
       " 28,\n",
       " 1,\n",
       " 7,\n",
       " 20,\n",
       " 24,\n",
       " 26,\n",
       " 15,\n",
       " 11,\n",
       " 23,\n",
       " 7,\n",
       " 39,\n",
       " 17,\n",
       " 37,\n",
       " 33,\n",
       " 24,\n",
       " 29,\n",
       " 5,\n",
       " 15,\n",
       " 13,\n",
       " 35,\n",
       " 10,\n",
       " 28,\n",
       " 22,\n",
       " 7,\n",
       " 30,\n",
       " 38,\n",
       " 8,\n",
       " 7,\n",
       " 6,\n",
       " 37,\n",
       " 9,\n",
       " 7,\n",
       " 17,\n",
       " 24,\n",
       " 25,\n",
       " 40,\n",
       " 35,\n",
       " 19,\n",
       " 21,\n",
       " 18,\n",
       " 15,\n",
       " 27,\n",
       " 2,\n",
       " 24,\n",
       " 29,\n",
       " 15,\n",
       " 3,\n",
       " 16,\n",
       " 24,\n",
       " 31,\n",
       " 36]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = \"\"\"\n",
    "The ruler of Britain, enters his throne room and announces his plan to \n",
    "divide the kingdom among his three daughters. He intends to give up the \n",
    "responsibilities of government and spend his old age visiting his children. \n",
    "He commands his daughters to say which of them loves him the most, \n",
    "promising to give the greatest share to that daughter.\"\"\"\n",
    "\n",
    "words, vocabulary, id_to_word, word_to_id = word_indexing(corpus)\n",
    "print(vocabulary)\n",
    "text_to_sequence(corpus, word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a384ef1",
   "metadata": {},
   "source": [
    "# Keras TextVectorization layer\n",
    "Use TextVectorization to create word indices to the words in a corpus.\n",
    "\n",
    "> transforms a batch of strings into either \n",
    "> * a list of token indices (one sample = 1D tensor of integer token indices) or \n",
    "> * a dense representation (one sample = 1D tensor of float values representing data about the sample's \n",
    "\n",
    "\n",
    "* [TextVectorization layer](https://keras.io/api/layers/preprocessing_layers/core_preprocessing_layers/text_vectorization/)\n",
    "\n",
    "```\n",
    "tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"whitespace\",\n",
    "    ngrams=None,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=None,\n",
    "    pad_to_max_tokens=True,\n",
    "    vocabulary=None,\n",
    "    **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "1. standardize each sample (usually lowercasing + punctuation stripping) \n",
    "2. split each sample into substrings (usually words) \n",
    "3. recombine substrings into tokens (usually ngrams) \n",
    "4. index tokens (associate a unique int value with each token) \n",
    "5. transform each sample using this index, either into a vector of ints or a dense float vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f7489",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "lowercasing + punctuation stripping by default as ```standardize=\"lower_and_strip_punctuation\"```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7189312",
   "metadata": {},
   "source": [
    "## Output modes\n",
    "### Integer index\n",
    "\n",
    "Index to the word position placed in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efb8c948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus is \n",
      "[['This is the 1st sample.']\n",
      " [\"And here's the 2nd sample.\"]]\n",
      "\n",
      "vocabulary:['', '[UNK]', 'the', 'sample', 'this', 'is', 'heres', 'and', '2nd', '1st']\n",
      "\n",
      "word index sequence of the corpus:\n",
      "[[4 5 2 9 3]\n",
      " [7 6 2 8 3]]\n",
      "\n",
      "index_to_word:\n",
      "[[b'']\n",
      " [b'[UNK]']\n",
      " [b'the']\n",
      " [b'sample']\n",
      " [b'this']\n",
      " [b'is']\n",
      " [b'heres']\n",
      " [b'and']\n",
      " [b'2nd']\n",
      " [b'1st']]\n",
      "\n",
      "First sentence in the corpus is tf.Tensor(\n",
      "[[b'this']\n",
      " [b'is']\n",
      " [b'the']\n",
      " [b'1st']\n",
      " [b'sample']], shape=(5, 1), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "corpus = np.array([\n",
    "    [\"This is the 1st sample.\"], \n",
    "    [\"And here's the 2nd sample.\"]\n",
    "])\n",
    "print(f\"corpus is \\n{corpus}\\n\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Fit to the words in the corpus\n",
    "# --------------------------------------------------------------------------------\n",
    "vectorizer = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    ngrams=None           # 1 word = 1 token\n",
    ")\n",
    "vectorizer.adapt(corpus)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Indices to the words\n",
    "# --------------------------------------------------------------------------------\n",
    "word_indices = vectorizer(corpus)\n",
    "word_indices = tf.cast(word_indices, dtype=tf.int32)\n",
    "\n",
    "print(f\"vocabulary:{vectorizer.get_vocabulary()}\\n\")\n",
    "print(f\"word index sequence of the corpus:\\n{word_indices}\\n\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Invert the indices to words for the first sentence\n",
    "# --------------------------------------------------------------------------------\n",
    "index_to_word = tf.reshape(tf.constant(vectorizer.get_vocabulary()), (-1, 1))\n",
    "print(f\"index_to_word:\\n{index_to_word}\\n\")\n",
    "\n",
    "first_sentence_indeces = word_indices[0][:, tf.newaxis]\n",
    "print(\n",
    "    \"First sentence in the corpus is %s\" \n",
    "    % tf.gather_nd(index_to_word, indices=first_sentence_indeces)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473525e",
   "metadata": {},
   "source": [
    "### Integer index to n-gram\n",
    "\n",
    "Token is up-to N grams, e.g. \n",
    "\n",
    "Example: for a corpus \"I am a cat\" where N=2, both 1-gram and 2-grams will be tokens. Indices include both to the 1-gram and 2-gram tokens.\n",
    "\n",
    "* 1-gram tokens = (\"i\", \"am\", \"a\", \"cat\")\n",
    "* 2-gram tokens = (\"i am\", \"am a\", \"a cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18f1e2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus is \n",
      "[['This is the 1st sample.']\n",
      " [\"And here's the 2nd sample.\"]]\n",
      "\n",
      "vocabulary:['', '[UNK]', 'the', 'sample', 'this is', 'this', 'the 2nd', 'the 1st', 'is the', 'is', 'heres the', 'heres', 'and heres', 'and', '2nd sample', '2nd', '1st sample', '1st']\n",
      "\n",
      "word index sequence of the corpus:\n",
      "[[ 5  9  2 17  3  4  8  7 16]\n",
      " [13 11  2 15  3 12 10  6 14]]\n",
      "\n",
      "index_to_word:\n",
      "[[b'']\n",
      " [b'[UNK]']\n",
      " [b'the']\n",
      " [b'sample']\n",
      " [b'this is']\n",
      " [b'this']\n",
      " [b'the 2nd']\n",
      " [b'the 1st']\n",
      " [b'is the']\n",
      " [b'is']\n",
      " [b'heres the']\n",
      " [b'heres']\n",
      " [b'and heres']\n",
      " [b'and']\n",
      " [b'2nd sample']\n",
      " [b'2nd']\n",
      " [b'1st sample']\n",
      " [b'1st']]\n",
      "\n",
      "First sentence in the corpus is tf.Tensor(\n",
      "[[b'this']\n",
      " [b'is']\n",
      " [b'the']\n",
      " [b'1st']\n",
      " [b'sample']\n",
      " [b'this is']\n",
      " [b'is the']\n",
      " [b'the 1st']\n",
      " [b'1st sample']], shape=(9, 1), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "corpus = np.array([\n",
    "    [\"This is the 1st sample.\"], \n",
    "    [\"And here's the 2nd sample.\"]\n",
    "])\n",
    "print(f\"corpus is \\n{corpus}\\n\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Fit to the words in the corpus\n",
    "# --------------------------------------------------------------------------------\n",
    "vectorizer = TextVectorization(\n",
    "    output_mode=\"int\", \n",
    "    ngrams=2             # 1 token == 1-gram or 2-grams\n",
    ")\n",
    "vectorizer.adapt(corpus)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Indices to the words\n",
    "# --------------------------------------------------------------------------------\n",
    "word_indices = vectorizer(corpus)\n",
    "word_indices = tf.cast(word_indices, dtype=tf.int32)\n",
    "\n",
    "print(f\"vocabulary:{vectorizer.get_vocabulary()}\\n\")\n",
    "print(f\"word index sequence of the corpus:\\n{word_indices}\\n\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Invert the indices to words for the first sentence\n",
    "# --------------------------------------------------------------------------------\n",
    "index_to_word = tf.reshape(tf.constant(vectorizer.get_vocabulary()), (-1, 1))\n",
    "print(f\"index_to_word:\\n{index_to_word}\\n\")\n",
    "\n",
    "first_sentence_indeces = word_indices[0][:, tf.newaxis]\n",
    "print(\n",
    "    \"First sentence in the corpus is %s\" \n",
    "    % tf.gather_nd(index_to_word, indices=first_sentence_indeces)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87c4e50",
   "metadata": {},
   "source": [
    "### OHE to 2-grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d3f311e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integer encoded corpus:\n",
      "[[0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0.]]\n",
      "\n",
      "vocabulary:\n",
      "['[UNK]', 'the', 'sample', 'this is', 'this', 'the 2nd', 'the 1st', 'is the', 'is', 'heres the', 'heres', 'and heres', 'and', '2nd sample', '2nd', '1st sample', '1st']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "corpus = np.array([[\"This is the 1st sample.\"], [\"And here's the 2nd sample.\"]])\n",
    "vectorizer = TextVectorization(output_mode=\"binary\", ngrams=2)\n",
    "vectorizer.adapt(corpus)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# OHE bit-map to tell if the corresponding token is in the sentence.\n",
    "# --------------------------------------------------------------------------------\n",
    "# [0, 1, 1, ... ] means the sentence includes the 1st and 2nd but not 0th token.\n",
    "# 0-th: [UNK]\n",
    "# 1st: 'the'\n",
    "# 2nd: 'sample'\n",
    "# --------------------------------------------------------------------------------\n",
    "sequence = vectorizer(corpus)\n",
    "print(f\"integer encoded corpus:\\n{sequence}\\n\")\n",
    "print(f\"vocabulary:\\n{vectorizer.get_vocabulary()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadad07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1adbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edab1006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
