{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d12500",
   "metadata": {},
   "source": [
    "# Tensorflow Autodiff\n",
    "\n",
    "* [Introduction to gradients and automatic differentiation](https://www.tensorflow.org/guide/autodiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9baf77be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c99ab",
   "metadata": {},
   "source": [
    "# Word2vec sampling score with BoW (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee4596aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding is <tf.Variable 'Variable:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[0.5330119 , 0.09007072, 0.2937175 , 0.09215891, 0.91912687],\n",
      "       [0.7878784 , 0.8388704 , 0.23437726, 0.22551596, 0.37300646],\n",
      "       [0.44537175, 0.3073889 , 0.02727795, 0.12882948, 0.15287817],\n",
      "       [0.02237642, 0.63794255, 0.32387066, 0.6367707 , 0.48452032],\n",
      "       [0.53519464, 0.5502291 , 0.48767912, 0.68348444, 0.4968723 ]],\n",
      "      dtype=float32)>\n",
      "\n",
      "bow is [[0.4051274  0.7384065  0.27912396 0.43114334 0.4287634 ]]\n",
      "\n",
      "score is [[0.70276856]]\n",
      "\n",
      "Loss is [[0.40226826]]\n",
      "gradient dL/dW (embedding back prop) is IndexedSlices(indices=tf.Tensor([1 3], shape=(2,), dtype=int32), values=tf.Tensor(\n",
      "[[-0.03461591 -0.03589327 -0.03585096 -0.09037671 -0.06268762]\n",
      " [-0.03461591 -0.03589327 -0.03585096 -0.09037671 -0.06268762]], shape=(2, 5), dtype=float32), dense_shape=tf.Tensor([5 5], shape=(2,), dtype=int32))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Word embedding vectors on the BoW side\n",
    "# --------------------------------------------------------------------------------\n",
    "Win = tf.Variable(tf.random.uniform(shape=(5,5), dtype=tf.float32))\n",
    "print(f\"embedding is {Win}\\n\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Dummpy target word \n",
    "# --------------------------------------------------------------------------------\n",
    "target_word = tf.random.uniform(shape=(1, 5), dtype=tf.float32)\n",
    "T = tf.constant([1], dtype=tf.float32)\n",
    "eps = 1e-7\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Autodiff\n",
    "# --------------------------------------------------------------------------------\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(Win)  # Start recording the history of operations applied to `a`\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Forward path\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Bag of words (BoW) from the context of 3-grams frame around the target word\n",
    "    indices = [[1], [3]]\n",
    "    bow = tf.math.reduce_mean(tf.gather_nd(Win, indices), axis=0, keepdims=True)\n",
    "    print(f\"bow is {bow}\\n\")\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # score how close the BoW to the target word with dot product.\n",
    "    # --------------------------------------------------------------------------------\n",
    "    score = tf.linalg.matmul(bow, tf.transpose(target_word))\n",
    "    print(f\"score is {score}\\n\")\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Sigmoid log loss\n",
    "    # --------------------------------------------------------------------------------\n",
    "    L = -1.0 * (\n",
    "        T * tf.math.log(tf.nn.sigmoid(score)) + \n",
    "        (1-T) * tf.math.log(tf.nn.sigmoid(1-score))\n",
    "    )\n",
    "    print(f\"Loss is {L}\")\n",
    "    \n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Backward path/Autodiff\n",
    "    # --------------------------------------------------------------------------------\n",
    "    dLdW = tape.gradient(L, Win)\n",
    "    print(f\"gradient dL/dW (embedding back prop) is {dLdW}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
