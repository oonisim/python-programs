{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    TFBertForSequenceClassification, \n",
    "    BertForSequenceClassification, \n",
    "    glue_convert_examples_to_features\n",
    ")\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using BERT\n",
    "\n",
    "Google [BERT](https://github.com/google-research/bert) Github is obsolete (only works with TF 1.0).\n",
    "\n",
    "Use [Hugging Face](https://huggingface.co/transformers/installation.html).\n",
    "\n",
    "## Installation\n",
    "\n",
    "* (https://huggingface.co/transformers/installation.html#with-conda)\n",
    "\n",
    "> Since Transformers version v4.0.0, we now have a conda channel: huggingface.\n",
    "\n",
    "```\n",
    "conda install -c huggingface transformers\n",
    "```\n",
    "\n",
    "* [conda-forge / packages / transformers](https://anaconda.org/conda-forge/transformers)\n",
    "\n",
    "> State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained models\n",
    "\n",
    "* [Pretrained models](https://huggingface.co/transformers/pretrained_models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: \\ \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - defaults/noarch::tensorboard==2.4.0=pyhc547734_0\n",
      "  - defaults/linux-64::tensorflow-base==2.4.1=mkl_py38h43e0292_0\n",
      "  - defaults/noarch::tensorflow-estimator==2.4.1=pyheb71bc4_0\n",
      "  - defaults/linux-64::tensorflow==2.4.1=mkl_py38hb2083e0_0\n",
      "done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.2\n",
      "  latest version: 4.10.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/oonisim/conda/envs/python_programs\n",
      "\n",
      "  added / updated specs:\n",
      "    - transformers\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _libgcc_mutex-0.1          |      conda_forge           3 KB  conda-forge\n",
      "    _openmp_mutex-4.5          |            1_gnu          22 KB  conda-forge\n",
      "    ca-certificates-2020.12.5  |       ha878542_0         137 KB  conda-forge\n",
      "    certifi-2020.12.5          |   py38h578d9bd_1         143 KB  conda-forge\n",
      "    dataclasses-0.8            |     pyhc8e2a94_1           7 KB  conda-forge\n",
      "    filelock-3.0.12            |     pyh9f0ad1d_0          10 KB  conda-forge\n",
      "    huggingface_hub-0.0.9      |     pyhd8ed1ab_0          38 KB  conda-forge\n",
      "    joblib-1.0.1               |     pyhd8ed1ab_0         206 KB  conda-forge\n",
      "    libgcc-ng-9.3.0            |      h2828fa1_19         7.8 MB  conda-forge\n",
      "    libgomp-9.3.0              |      h2828fa1_19         376 KB  conda-forge\n",
      "    libstdcxx-ng-9.3.0         |      h6de172a_19         4.0 MB  conda-forge\n",
      "    openssl-1.1.1k             |       h7f98852_0         2.1 MB  conda-forge\n",
      "    python_abi-3.8             |           1_cp38           4 KB  conda-forge\n",
      "    regex-2021.4.4             |   py38h497a2fe_0         372 KB  conda-forge\n",
      "    sacremoses-0.0.43          |     pyh9f0ad1d_0         430 KB  conda-forge\n",
      "    tokenizers-0.10.1          |   py38hb63a372_0         2.8 MB  conda-forge\n",
      "    tqdm-4.60.0                |     pyhd8ed1ab_0          79 KB  conda-forge\n",
      "    transformers-4.6.0         |     pyhd8ed1ab_0         1.1 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        19.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-1_gnu\n",
      "  dataclasses        conda-forge/noarch::dataclasses-0.8-pyhc8e2a94_1\n",
      "  filelock           conda-forge/noarch::filelock-3.0.12-pyh9f0ad1d_0\n",
      "  huggingface_hub    conda-forge/noarch::huggingface_hub-0.0.9-pyhd8ed1ab_0\n",
      "  joblib             conda-forge/noarch::joblib-1.0.1-pyhd8ed1ab_0\n",
      "  libgomp            conda-forge/linux-64::libgomp-9.3.0-h2828fa1_19\n",
      "  protobuf           pkgs/main/linux-64::protobuf-3.14.0-py38h2531618_1\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.8-1_cp38\n",
      "  regex              conda-forge/linux-64::regex-2021.4.4-py38h497a2fe_0\n",
      "  sacremoses         conda-forge/noarch::sacremoses-0.0.43-pyh9f0ad1d_0\n",
      "  tokenizers         conda-forge/linux-64::tokenizers-0.10.1-py38hb63a372_0\n",
      "  tqdm               conda-forge/noarch::tqdm-4.60.0-pyhd8ed1ab_0\n",
      "  transformers       conda-forge/noarch::transformers-4.6.0-pyhd8ed1ab_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  certifi            pkgs/main::certifi-2020.12.5-py38h06a~ --> conda-forge::certifi-2020.12.5-py38h578d9bd_1\n",
      "  libgcc-ng           pkgs/main::libgcc-ng-9.1.0-hdf63c60_0 --> conda-forge::libgcc-ng-9.3.0-h2828fa1_19\n",
      "  libstdcxx-ng       pkgs/main::libstdcxx-ng-9.1.0-hdf63c6~ --> conda-forge::libstdcxx-ng-9.3.0-h6de172a_19\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  _libgcc_mutex           pkgs/main::_libgcc_mutex-0.1-main --> conda-forge::_libgcc_mutex-0.1-conda_forge\n",
      "  ca-certificates    pkgs/main::ca-certificates-2021.4.13-~ --> conda-forge::ca-certificates-2020.12.5-ha878542_0\n",
      "  openssl              pkgs/main::openssl-1.1.1k-h27cfd23_0 --> conda-forge::openssl-1.1.1k-h7f98852_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "_openmp_mutex-4.5    | 22 KB     | ##################################### | 100% \n",
      "certifi-2020.12.5    | 143 KB    | ##################################### | 100% \n",
      "tqdm-4.60.0          | 79 KB     | ##################################### | 100% \n",
      "regex-2021.4.4       | 372 KB    | ##################################### | 100% \n",
      "sacremoses-0.0.43    | 430 KB    | ##################################### | 100% \n",
      "huggingface_hub-0.0. | 38 KB     | ##################################### | 100% \n",
      "joblib-1.0.1         | 206 KB    | ##################################### | 100% \n",
      "_libgcc_mutex-0.1    | 3 KB      | ##################################### | 100% \n",
      "libgcc-ng-9.3.0      | 7.8 MB    | ##################################### | 100% \n",
      "transformers-4.6.0   | 1.1 MB    | ##################################### | 100% \n",
      "python_abi-3.8       | 4 KB      | ##################################### | 100% \n",
      "libstdcxx-ng-9.3.0   | 4.0 MB    | ##################################### | 100% \n",
      "tokenizers-0.10.1    | 2.8 MB    | ##################################### | 100% \n",
      "libgomp-9.3.0        | 376 KB    | ##################################### | 100% \n",
      "openssl-1.1.1k       | 2.1 MB    | ##################################### | 100% \n",
      "filelock-3.0.12      | 10 KB     | ##################################### | 100% \n",
      "dataclasses-0.8      | 7 KB      | ##################################### | 100% \n",
      "ca-certificates-2020 | 137 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge transformers -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████| 629/629 [00:00<00:00, 355kB/s]\n",
      "Downloading: 100%|███████████████████████████| 268M/268M [01:07<00:00, 3.94MB/s]\n",
      "2021-05-21 15:46:40.180256: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-05-21 15:46:40.181120: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-05-21 15:46:40.183650: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2021-05-21 15:46:40.239274: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Downloading: 100%|████████████████████████████| 232k/232k [00:01<00:00, 221kB/s]\n",
      "Downloading: 100%|███████████████████████████| 48.0/48.0 [00:00<00:00, 30.9kB/s]\n",
      "[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n"
     ]
    }
   ],
   "source": [
    "!python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b5b84afaea419a96459d6c6a94528e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4bd6cd39314567a17d3325b874c554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98c1e994a3f4997933df8cd7b81e18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(2, 14), dtype=int32, numpy=\n",
      "array([[  101,  2057,  2024,  2200,  3407,  2000,  2265,  2017,  1996,\n",
      "          100, 19081,  3075,  1012,   102],\n",
      "       [  101,  2057,  3246,  2017,  2123,  1005,  1056,  5223,  2009,\n",
      "         1012,   102,     0,     0,     0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(2, 14), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 14), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"We are very happy to show you the 🤗 Transformers library.\", \n",
    "    \"We hope you don't hate it.\"\n",
    "]\n",
    "tokens = tokenizer(\n",
    "    sentences,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"tf\"\n",
    ")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:[We are very happy to show you the 🤗 Transformers library.]\n",
      "words:[[  101  2057  2024  2200  3407  2000  2265  2017  1996   100 19081  3075  1012   102]]\n",
      "\n",
      "sentence:[We hope you don't hate it.]\n",
      "words:[[ 101 2057 3246 2017 2123 1005 1056 5223 2009 1012  102    0    0    0]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] we are very happy to show you the [UNK] transformers library. [SEP]'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sentence, words in zip(sentences, tokens['input_ids']):\n",
    "    print(f\"sentence:[{sentence}]\\nwords:[{words}]\\n\")\n",
    "\n",
    "tokenizer.decode(tokens['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "\n",
    "Convert the words into word indices (tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:[We              ] index:[     101]\n",
      "word:[are             ] index:[    2057]\n",
      "word:[very            ] index:[    2024]\n",
      "word:[happy           ] index:[    2200]\n",
      "word:[to              ] index:[    3407]\n",
      "word:[show            ] index:[    2000]\n",
      "word:[you             ] index:[    2265]\n",
      "word:[the             ] index:[    2017]\n",
      "word:[🤗               ] index:[    1996]\n",
      "word:[Transformers    ] index:[     100]\n",
      "word:[library.        ] index:[   19081]\n"
     ]
    }
   ],
   "source": [
    "indices = tokenizer.encode(\n",
    "    sentences[0],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "for word, index in zip(sentences[0].split(), indices):\n",
    "    print(f\"word:[{word:16s}] index:[{index:8d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding\n",
    "\n",
    "Convert the token (index) into the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] we are very happy to show you the [UNK] transformers library. [SEP]'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
