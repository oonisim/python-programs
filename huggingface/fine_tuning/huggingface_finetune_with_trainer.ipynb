{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "248355eb",
   "metadata": {},
   "source": [
    "# Huggingface Trainer\n",
    "\n",
    "* [Processing the data](https://huggingface.co/course/chapter3/2?fw=pt)\n",
    "\n",
    "> The last thing we will need to do is pad all the examples to the length of the longest element when we batch elements together â€” a technique we refer to as dynamic padding.\n",
    "> The function that is responsible for **putting together samples inside a batch** is called a **collate function**. The default being a function that will just **convert your samples to PyTorch tensors and concatenate them (recursively if your elements are lists, tuples, or dictionaries)**.\n",
    "> \n",
    "> Transformers library provides us with such a function via ```DataCollatorWithPadding```. It takes a tokenizer when you instantiate it (to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs) and will do everything you need:\n",
    "> ```\n",
    "> from transformers import DataCollatorWithPadding\n",
    "> \n",
    "> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "> batch = data_collator(samples)\n",
    "> ```\n",
    "> \n",
    "> Dynamic padding means the samples in this batch should all be padded to a length of 67, the maximum length inside the batch.\n",
    "\n",
    "## Trainer\n",
    "\n",
    "* [Fine-tuning a model with the Trainer API](https://huggingface.co/course/chapter3/3) ([YouTube](https://youtu.be/nvBXf7s7vTI))\n",
    "* [Trainer class](https://huggingface.co/docs/transformers/main_classes/trainer)\n",
    "\n",
    "## Tutorials\n",
    "\n",
    "* [Fine-tune a pretrained model](https://huggingface.co/docs/transformers/training)\n",
    "* [Summerization fine tuning](https://huggingface.co/docs/transformers/tasks/summarization)\n",
    "* [Question Answering fine tuning](https://huggingface.co/course/chapter7/7?fw=pt)\n",
    "* [Fine-tuning Bert for Abstractive Summarisation with the Curation Dataset](https://medium.com/curation-corporation/fine-tuning-bert-for-abstractive-summarisation-with-the-curation-dataset-79ea4b40a923) ([Github](https://github.com/CurationCorp/curation-corpus/blob/master/examples/bertextabs/finetune_bertabs_walkthrough.ipynb))\n",
    "* [ Text Summarization with Pretrained Encoders](https://github.com/nlpyang/PreSumm)\n",
    "\n",
    "> This code is for EMNLP 2019 paper Text Summarization [Pretrained Encoders](https://arxiv.org/abs/1908.08345)\n",
    "\n",
    "\n",
    "\n",
    "## Huggingface Notebooks\n",
    "\n",
    "* [How to train a new language model from scratch using Transformers and Tokenizers (Feb 2020)](https://huggingface.co/blog/how-to-train) ([Github notebook - 01_how_to_train.ipynb](https://github.com/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb))\n",
    "* [Pre-training SmallBERTa - A tiny model to train on a tiny dataset](https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b)\n",
    "\n",
    "* [Transformers Notebooks](https://huggingface.co/docs/transformers/main/notebooks)\n",
    "<img src=\"./image/huggingface_notebooks.png\" align=\"left\" width=200/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c863d04c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
