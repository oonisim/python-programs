{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune BLOOM for Summarization\n",
    "\n",
    "## Objective\n",
    "\n",
    "Experiment the way to custom tran the decoder only BLOOM/BLOOMZ models to generate summarization using prompt.\n",
    "\n",
    "## References\n",
    "### Hugginface BLOOM Discussion Forum & Github\n",
    "\n",
    "* [Huggingface Bloom Discussions](https://huggingface.co/bigscience/bloom/discussions)\n",
    "\n",
    "* [Text summarization with Bloom#122](https://huggingface.co/bigscience/bloom/discussions/122)\n",
    "\n",
    "* [Training or Fine-tuning the Bloom AI Model on my own Dataset#187](https://huggingface.co/bigscience/bloom/discussions/187)\n",
    "\n",
    "> In the [official example for text classification](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) README:\n",
    "> replace ```--model_name_or_path bert-base-multilingual-cased``` with ```--model_name_or_path bigscience/bloom-560m```\n",
    "\n",
    "* [Fine-tuning BLOOM for Summarization with Trainer API #234](https://huggingface.co/bigscience/bloom/discussions/234)\n",
    "\n",
    "* [Huge Num Epochs (9223372036854775807) when using Trainer API with streaming dataset #22757](https://github.com/huggingface/transformers/issues/22757)\n",
    "\n",
    "* [Data Collator class to use for BLOOM#238](https://huggingface.co/bigscience/bloom/discussions/238)\n",
    "\n",
    "* [TrainingArguments class - max_steps formula when using streaming dataset](https://discuss.huggingface.co/t/training-max-steps-formula-when-using-streaming-dataset/36531)\n",
    "\n",
    "### Huggingface Casual Language Model\n",
    "\n",
    "* [Huggingface Task Guide - Causal language modeling](https://huggingface.co/docs/transformers/tasks/language_modeling)\n",
    "\n",
    "### Huggingface Task Parameters \n",
    "\n",
    "* [Detailed parameters](https://huggingface.co/docs/api-inference/detailed_parameters#text2text-generation-task)\n",
    "\n",
    "## BLOOM Prompt Example\n",
    "\n",
    "* [Learn how to use Bloom like chatGPT for free.#183](https://huggingface.co/bigscience/bloom/discussions/183)\n",
    "\n",
    "```\n",
    "User: Number BLOOM is an autoregressive Large Language Model (LLM), trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. As such, it is able to output coherent text in 46 languages and 13 programming languages that is hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for, by casting them as text generation tasks.\n",
    "AI: \n",
    "```\n",
    "\n",
    "<img src=\"./image/bloom_prompt_example.png\" align=\"left\" width=400/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install torch transformers datasets evaluate scikit-learn rouge rouge-score promptsource --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "import datetime\n",
    "from typing import (\n",
    "    List,\n",
    "    Dict,\n",
    "    Callable,\n",
    ")\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    get_dataset_split_names\n",
    ")\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorWithPadding,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BloomForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback, \n",
    "    IntervalStrategy\n",
    ")\n",
    "import evaluate\n",
    "from promptsource.templates import (\n",
    "    DatasetTemplates,\n",
    "    Template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_datetime_string(\n",
    "        date_time: datetime.datetime = datetime.datetime.now(),\n",
    "        format_string: str = '%Y%m%d%H%M%S'\n",
    ") -> str:\n",
    "    \"\"\"Generate date/time string from datetime instance\n",
    "    Args:\n",
    "        date_time: datetime.datetime instance. default datetime.datetime.now()\n",
    "        format_string: default '%Y%m%d%H%M%S' e.g. 20230217113330 for 11:33:30 AM on 2023 Feb 17\n",
    "    Returns: date/time string\n",
    "    \"\"\"\n",
    "    return date_time.strftime(format_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_CPUS: int = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 18 10:16:45 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   43C    P0    25W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\n",
      "\n",
      "- `transformers` version: 4.28.1\n",
      "- Platform: Linux-4.14.311-233.529.amzn2.x86_64-x86_64-with-debian-10.6\n",
      "- Python version: 3.7.10\n",
      "- Huggingface_hub version: 0.13.4\n",
      "- Safetensors version: not installed\n",
      "- PyTorch version (GPU?): 1.13.1+cu117 (True)\n",
      "- Tensorflow version (GPU?): not installed (NA)\n",
      "- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n",
      "- Jax version: not installed\n",
      "- JaxLib version: not installed\n",
      "- Using GPU in script?: <fill in>\n",
      "- Using distributed or parallel set-up in script?: <fill in>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!transformers-cli env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230418101644\n"
     ]
    }
   ],
   "source": [
    "RUN_DATE_TIME: str = get_datetime_string()\n",
    "print(RUN_DATE_TIME)\n",
    "\n",
    "## Huggingface Datasets\n",
    "DATASET_NAME: str = \"xsum\"\n",
    "DATASET_TRAIN_NUM_ROWS: int = 204045      # Number of rows in the original train dataset\n",
    "DATASET_STREAMING: bool = True                    # If using Dataset streaming\n",
    "DATASET_TRAIN_NUM_SELECT: int = 4098       # Number of rows to use for training\n",
    "DATASET_VALIDATE_NUM_SELECT: int = 256\n",
    "\n",
    "# Huggingface Tokenizer (BLOOM default token length is 2048)\n",
    "MAX_TOKEN_LENGTH: int = 512         # Max token length to avoid out of memory\n",
    "MAX_RESPONSE_LENGTH: int = 50\n",
    "BUFFER = 64\n",
    "MAX_REQUEST_LENGTH: int = MAX_TOKEN_LENGTH - MAX_RESPONSE_LENGTH - BUFFER\n",
    "PER_DEVICE_BATCH_SIZE: int = 1       # GPU batch size\n",
    "\n",
    "# Huggingface Model\n",
    "# MODEL = \"bigscience/bloomz-560m\"\n",
    "MODEL = \"bigscience/bloomz-560m\"\n",
    "USE_FLOAT16: bool = True\n",
    "\n",
    "# Training\n",
    "LEARNING_RATE: float = 3e-5\n",
    "NUM_EPOCHS: int = 3\n",
    "MAX_STEPS: int = NUM_EPOCHS * DATASET_TRAIN_NUM_SELECT if DATASET_STREAMING else -1\n",
    "EVAL_STEPS: int = 500\n",
    "MODEL_DIR_FINE_TUNED: str = f\"finetuned_bloom_model_{RUN_DATE_TIME}\"\n",
    "MODEL_DIR_CHECKPOINT: str = \"bloom_fine_tuning\"\n",
    "RESUME_FROM_MODEL_DIR_CHECKPOINT: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noVMzFA4l6MC"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "Use Huggingface dataset [xsum](https://huggingface.co/datasets/xsum) which has PromptSource templates.\n",
    "\n",
    "<img src=\"./image/xsum.png\" align=\"left\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./image/xsum_promptsource_templates.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'validation', 'test']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dataset_split_names(path=DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.iterable_dataset.IterableDataset at 0x7fc801ef7d90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = load_dataset(\"xsum\", split=\"train\", streaming=DATASET_STREAMING)\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_p7bwEh1l6MF"
   },
   "source": [
    "There are two fields that you'll want to use:\n",
    "\n",
    "- `document`: the text of the news.\n",
    "- `summary`: a condensed version of `document` which'll be the model target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "AHzBjOvWl6ME",
    "outputId": "460f1509-40f8-4c90-e5e5-422d98a9b622",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 'The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\\nTrains on the west coast mainline face disruption due to damage at the Lamington Viaduct.\\nMany businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town.\\nFirst Minister Nicola Sturgeon visited the area to inspect the damage.\\nThe waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare.\\nJeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit.\\nHowever, she said more preventative work could have been carried out to ensure the retaining wall did not fail.\\n\"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said.\\n\"That may not be true but it is perhaps my perspective over the last few days.\\n\"Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out?\"\\nMeanwhile, a flood alert remains in place across the Borders because of the constant rain.\\nPeebles was badly hit by problems, sparking calls to introduce more defences in the area.\\nScottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs.\\nThe Labour Party\\'s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand.\\nHe said it was important to get the flood protection plan right but backed calls to speed up the process.\\n\"I was quite taken aback by the amount of damage that has been done,\" he said.\\n\"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses.\"\\nHe said it was important that \"immediate steps\" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans.\\nHave you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled. Email us on selkirk.news@bbc.co.uk or dumfries@bbc.co.uk.',\n",
       " 'summary': 'Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.',\n",
       " 'id': '35232142'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DATASET_STREAMING:\n",
    "    example: Dict[str, str]  = list(train.take(50))[0]\n",
    "else:\n",
    "    example: Dict[str, str] = train.select(range(DATASET_TRAIN_NUM_SELECT)).shuffle(seed=42)[49]\n",
    "\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template\n",
    "\n",
    "Use [PromptSource](https://github.com/bigscience-workshop/promptsource) summarization template for XSUM dataset to generate prompts and feed the prompts to the model as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DOC_boils_down_to_simple_idea_that',\n",
       " 'DOC_given_above_write_one_sentence',\n",
       " 'DOC_how_would_you_rephrase_few_words',\n",
       " 'DOC_tldr',\n",
       " 'DOC_write_summary_of_above',\n",
       " 'article_DOC_summary',\n",
       " 'college_roommate_asked_DOC_so_I_recap',\n",
       " 'read_below_DOC_write_abstract',\n",
       " 'summarize_DOC',\n",
       " 'summarize_this_DOC_summary']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_templates = DatasetTemplates( dataset_name=DATASET_NAME)  \n",
    "prompt_templates.all_template_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize: {{document}}|||\n",
      "{{summary}}\n"
     ]
    }
   ],
   "source": [
    "template: Template = prompt_templates['summarize_DOC']\n",
    "print(template.jinja)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt xample from the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Prompt\n",
      "--------------------------------------------------------------------------------\n",
      "Summarize: The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed. Repair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water. Trains on the west coast mainline face disruption due to damage at the Lamington Viaduct. Many businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town. First Minister Nicola Sturgeon visited the area to inspect the damage. The waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare. Jeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit. However, she said more preventative work could have been carried out to ensure the retaining wall did not fail. It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we re neglected or forgotten, she said. That may not be true but it is perhaps my perspective over the last few days. Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out? Meanwhile, a flood alert remains in place across the Borders because of the constant rain. Peebles was badly hit by problems, sparking calls to introduce more defences in the area. Scottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs. The Labour Party s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand. He said it was important to get the flood protection plan right but backed calls to speed up the process. I was quite taken aback by the amount of damage that has been done, he said. Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses. He said it was important that immediate steps were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans. Have you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled. Email us on selkirk.news@bbc.co.uk or dumfries@bbc.co.uk.\n",
      "--------------------------------------------------------------------------------\n",
      "Response\n",
      "--------------------------------------------------------------------------------\n",
      "Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.\n"
     ]
    }
   ],
   "source": [
    "prompt, response = template.apply(example=example, truncate=False)\n",
    "print('-' * 80)\n",
    "print(\"Prompt\")\n",
    "print('-' * 80)\n",
    "print(re.sub(r'[\\s\\'\\\"]+', ' ', prompt))\n",
    "\n",
    "print('-' * 80)\n",
    "print(\"Response\")\n",
    "print('-' * 80)\n",
    "print(re.sub(r'[\\s\\'\\\"]+', ' ', response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWEEix6ql6MF"
   },
   "source": [
    "---\n",
    "# Preprocess\n",
    "\n",
    "1. Generate prompt from the dataset.\n",
    "2. Remove characters e.g. quotes.\n",
    "3. Trim sentence to MAX length if exceeds it (note: BLOOMZ skips sentences that exceed 2048).\n",
    "4. Tokenize.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the preprocessing function over the entire dataset, use Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once.\n",
    "\n",
    "* [Datasets - select / filter](https://huggingface.co/docs/datasets/process#select-and-filter)\n",
    "* [Datasets - select](https://huggingface.co/docs/datasets/v2.11.0/en/package_reference/main_classes#datasets.Dataset.select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Framework Tensor Format\n",
    "\n",
    "* [Use with PyTorch - Dataset Format](https://huggingface.co/docs/datasets/use_with_pytorch)\n",
    "> By default, datasets return regular python objects: integers, floats, strings, lists, etc. To get PyTorch tensors instead, you can set the format of the dataset to pytorch using Dataset.with_format():\n",
    "\n",
    "```\n",
    "ds = ds.with_format(\"torch\")\n",
    "```\n",
    "\n",
    "* [Using Datasets with TensorFlow](https://huggingface.co/docs/datasets/use_with_tensorflow)\n",
    "\n",
    "> By default, datasets return regular Python objects: integers, floats, strings, lists, etc. To get TensorFlow tensors instead, you can set the format of the dataset to tf:\n",
    "\n",
    "```\n",
    "ds = ds.with_format(\"tf\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMSdggVdl6MG"
   },
   "source": [
    "The preprocessing function you want to create needs to:\n",
    "\n",
    "1. Prefix the input with a prompt so T5 knows this is a summarization task. Some models capable of multiple NLP tasks require prompting for specific tasks.\n",
    "2. Use the keyword `text_target` argument when tokenizing labels.\n",
    "3. Truncate sequences to be no longer than the maximum length set by the `max_length` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map function for prompt generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Y_PqJhjDl6MG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOT USING NOW\n",
    "def get_convert_to_request_response(template: Template) -> Callable:\n",
    "    def _convert_to_prompt_response(example: Dict[str, str]) -> Dict[str, str]:\n",
    "        \"\"\"Generate prompt, response as a dictionary:\n",
    "        {\n",
    "            \"prompt\": \"Summarize: ...\",\n",
    "            \"response\": \"...\"\n",
    "        }\n",
    "\n",
    "        NOTE: DO NOT use with dataset map function( batched=True). Use batch=False\n",
    "\n",
    "        Args:\n",
    "            example: single {document, summary} pair to be able to apply template\n",
    "        Returns: a dictionary of pro\n",
    "        \"\"\"\n",
    "        # assert isinstance(example, dict), f\"expected dict but {type(example)}.\\n{example}\" # does not work with streaming\n",
    "        assert isinstance(example['document'], str), f\"expected str but {type(example['document'])}.\"\n",
    "        \n",
    "        prompt, response = template.apply(example=example, truncate=False)\n",
    "        if len(prompt) <=1 or len(response) <= 1:\n",
    "            return {\n",
    "                \"prompt\": \"NA\",\n",
    "                \"response\": \"NA\"                \n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"prompt\": \" \".join(\n",
    "                re.sub(r'[\\s\\'\\\"]+', ' ', prompt).split(' ')[:MAX_REQUEST_LENGTH]\n",
    "            ),\n",
    "            \"response\": \" \".join(\n",
    "                re.sub(r'[\\s\\'\\\"]+', ' ', response).split(' ')[:MAX_RESPONSE_LENGTH]\n",
    "            )\n",
    "        }\n",
    "\n",
    "    return _convert_to_prompt_response\n",
    "\n",
    "convert_to_request_response: Callable = get_convert_to_request_response(template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_convert_to_prompt(template: Template) -> Callable:\n",
    "    def _convert_to_prompt(example: Dict[str, str]) -> Dict[str, str]:\n",
    "        \"\"\"Generate prompt as a dictionary:\n",
    "        {\n",
    "            \"prompt\": \"Summarize: <document>\\n<summary>\"\n",
    "        }\n",
    "\n",
    "        NOTE: DO NOT use dataset map function with  batched=True. Use batch=False\n",
    "\n",
    "        Args:\n",
    "            example: single {document, summary} pair to be able to apply template\n",
    "        Returns: a dictionary of prompt\n",
    "        \"\"\"\n",
    "        # assert isinstance(example, dict), f\"expected dict but {type(example)}.\\n{example}\"\n",
    "        assert isinstance(example['document'], str), f\"expected str but {type(example['document'])}.\"\n",
    "\n",
    "        prompt, response = template.apply(example=example, truncate=False)\n",
    "        if len(prompt) <=1 or len(response) <= 1:\n",
    "            return {\n",
    "                \"prompt\": \"NA\\nNA\\n\"\n",
    "            }\n",
    "        \n",
    "        # prompt and response are seprated with '\\n' character.\n",
    "        prompt = re.sub(r'^Summarize:', 'USER:', prompt)\n",
    "        return {\n",
    "            \"prompt\": \" \".join(\n",
    "                re.sub(r'[\\s\\'\\\"]+', ' ', prompt).split(' ')[:MAX_REQUEST_LENGTH-1]  # -1 for \\n\n",
    "            ) + \"\\nAI: \" + \" \".join(\n",
    "                re.sub(r'[\\s\\'\\\"]+', ' ', response).split(' ')[:MAX_RESPONSE_LENGTH-7] # -7 for \\nAI:<...>\\n\\n\n",
    "            ) + \"\\n\\n\"\n",
    "        }\n",
    "\n",
    "    return _convert_to_prompt\n",
    "\n",
    "convert_to_prompt: Callable = get_convert_to_prompt(template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'USER: The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed. Repair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water. Trains on the west coast mainline face disruption due to damage at the Lamington Viaduct. Many businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town. First Minister Nicola Sturgeon visited the area to inspect the damage. The waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare. Jeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit. However, she said more preventative work could have been carried out to ensure the retaining wall did not fail. It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we re neglected or forgotten, she said. That may not be true but it is perhaps my perspective over the last few days. Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out? Meanwhile, a flood alert remains in place across the Borders because of the constant rain. Peebles was badly hit by problems, sparking calls to introduce more defences in the area. Scottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs. The Labour Party s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand. He said it was important to get the flood protection plan right but backed calls to speed up the process. I was quite taken aback by the amount of damage that has been done, he said. Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses. He said it was important that immediate steps were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans. Have you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled.\\nAI: Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.\\n\\n'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = convert_to_prompt(example=example)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "* [Causal language modeling](https://huggingface.co/docs/transformers/tasks/language_modeling)\n",
    "\n",
    "> Now create a batch of examples using DataCollatorForLanguageModeling. It’s more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n",
    "> Use the end-of-sequence token as the padding token and set mlm=False. This will use the inputs as labels shifted to the right by one element:\n",
    "\n",
    "```\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "7wNu99KBl6MF",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloomz-560m/snapshots/25f241f41c04f08d658a1dd3b49ad41390109a8e/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloomz-560m/snapshots/25f241f41c04f08d658a1dd3b49ad41390109a8e/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloomz-560m/snapshots/25f241f41c04f08d658a1dd3b49ad41390109a8e/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "# TODO: Needs to verify if using EOS is correct for BLOOM/Z and with DataCollatorWithPadding\n",
    "# (perhaps should use DataCollatorForLanguageModeling which causes an error)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map function to tokkenize prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casual LM model internally shift one input to use as a label according to the explanation. As such, using the **summary** as the **label** for the sentence would not be correct. \n",
    "\n",
    "* [Huggnggface - Data processing for Causal Language Modeling](https://youtu.be/ma1TrR7gE7I?t=191)\n",
    "\n",
    "> Casual language model the input sequences themselves are the labels. Input sequence is the label just shifted by one. All **the shifting is handled by the model internally**.\n",
    "> <img src=\"./image/transformers_casual_lm_inputs_are_labels.png\" align=\"left\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should use ```(input,label)=(document,summary)``` and Seq2Seq task, however Huggingface Seq2Seq task nor pipeline does not accept the BLOOM model because class e.g. ```AutoModelForSeq2SeqLM``` expects encoder/decoder model such as T5. Need to find out how BLOOM/BLOOMZ train the models for seq2seq tasks.\n",
    "\n",
    "> The model 'BloomForCausalLM' is not supported for summarization. Supported models are ```['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration']```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOT USING NOW\n",
    "def tokenize_prompt_response(example):\n",
    "    \"\"\"Generate the model inputs in the dictionary with format:\n",
    "    {\n",
    "        \"input_ids\": List[int], \n",
    "        \"attention_mask\": List[int]\",\n",
    "        \"labels\": List[int]\n",
    "    }\n",
    "\n",
    "    This function only works with the Dataset.map(batched=False).\n",
    "\n",
    "    Note: Huggngface dataaset map(batched=True, batch_size=n) merges values of \n",
    "    n dictionarys into a values of the key. If you have n instances of {\"key\", \"v\"}, then\n",
    "    you will get {\"key\": [\"v\", \"v\", \"v\", ...] }.\n",
    "    \n",
    "    Args:\n",
    "        example:   a dictionary of format {\n",
    "            \"prompt\": prompt to summarize a sentence,\n",
    "            \"response\": summary for the sentence\n",
    "        }\n",
    "    \"\"\"    \n",
    "    # NOTE: It is a bug to use 'max_length=MAX_TOKEN_LENGTH' when using \"batched=True\"\n",
    "    # examples[\"prompt\"] with 'batched=True\" has N instances of prompts each of which \n",
    "    # can have MAX_TOKEN_LENGTH length. Chopping N * MAX_TOKEN_LENGTH to\n",
    "    # MAX_TOKEN_LENGTH means only using the first prompt out of N.\n",
    "    inputs: Dict[str, List[int]] = tokenizer(\n",
    "        text=examples[\"prompt\"], \n",
    "        max_length=MAX_TOKEN_LENGTH,\n",
    "        truncation=True,\n",
    "        # padding='max_length',\n",
    "        # padding=True\n",
    "    )\n",
    "\n",
    "    labels: Dict[str, List[int]] = tokenizer(\n",
    "        text=examples[\"response\"], \n",
    "        max_length=MAX_TOKEN_LENGTH,\n",
    "        truncation=True,\n",
    "        # DataCollatorWithPadding does not pad the \"labels\" element, hence need to pad here.\n",
    "        # padding='max_length',\n",
    "        padding=True\n",
    "    )\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_prompt(example):\n",
    "    \"\"\"Generate the model inputs in the dictionary with format:\n",
    "    {\n",
    "        \"input_ids\": List[int], \n",
    "        \"attention_mask\": List[int]\",\n",
    "        \"labels\": List[int]\n",
    "    }\n",
    "    \n",
    "    This function only works with the Dataset.map(batched=False).\n",
    "    \n",
    "    Note: Huggngface dataaset map(batched=True, batch_size=n) merges values of \n",
    "    n dictionarys into a values of the key. If you have n instances of {\"key\", \"v\"}, then\n",
    "    you will get {\"key\": [\"v\", \"v\", \"v\", ...] }.\n",
    "    \n",
    "    Args:\n",
    "        example:   a dictionary of format {\n",
    "            \"prompt\": \"Summarize:<document>\\n<summary>\\n\",\n",
    "        }\n",
    "    \"\"\"    \n",
    "    assert isinstance(example['prompt'], str), f\"expected str, got {type(example['prompt'])}\"\n",
    "    inputs: Dict[str, List[int]] = tokenizer(\n",
    "        example['prompt'], \n",
    "        max_length=MAX_TOKEN_LENGTH,\n",
    "        truncation=True,\n",
    "        # padding='max_length',\n",
    "        padding=True\n",
    "    )\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].copy()   # Casual LM get the same tokens as inputs and label\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USER: The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed. Repair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water. Trains on the west coast mainline face disruption due to damage at the Lamington Viaduct. Many businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town. First Minister Nicola Sturgeon visited the area to inspect the damage. The waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare. Jeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit. However, she said more preventative work could have been carried out to ensure the retaining wall did not fail. It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we re neglected or forgotten, she said. That may not be true but it is perhaps my perspective over the last few days. Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out? Meanwhile, a flood alert remains in place across the Borders because of the constant rain. Peebles was badly hit by problems, sparking calls to introduce more defences in the area. Scottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs. The Labour Party s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand. He said it was important to get the flood protection plan right but backed calls to speed up the process. I was quite taken aback by the amount of damage that has been done, he said. Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses. He said it was important that immediate steps were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans. Have you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled.\\nAI: Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.\\n\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized: Dict[str, List[int]] = tokenize_prompt(example=convert_to_prompt(example=example))\n",
    "tokenizer.decode(token_ids=tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hrFvR58l6MG"
   },
   "source": [
    "## Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DATASET_STREAMING:\n",
    "    train = train.take(DATASET_TRAIN_NUM_SELECT)\n",
    "else:\n",
    "    train = train.select(\n",
    "        indices=range(DATASET_TRAIN_NUM_SELECT)\n",
    "    )\n",
    "\n",
    "remove_column_names: List[str] = list(train.features.keys())\n",
    "\n",
    "tokenized_train = train.map(\n",
    "    function=convert_to_prompt, \n",
    "    batched=False,\n",
    "    #batch_size=2048,\n",
    "    #drop_last_batch=False,\n",
    "    remove_columns=remove_column_names,\n",
    "    #num_proc=NUM_CPUS\n",
    ").map(\n",
    "    function=tokenize_prompt, \n",
    "    batched=False,\n",
    "    # batch_size=32,\n",
    "    # drop_last_batch=True,\n",
    "    # remove_columns=['prompt', 'response']\n",
    "    remove_columns=['prompt'],\n",
    "    #num_proc=NUM_CPUS\n",
    ").shuffle(\n",
    "    seed=42\n",
    ").with_format(\n",
    "    \"torch\"\n",
    ")\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "GD_NBbXIl6MG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation =  load_dataset(\n",
    "    path=\"xsum\", \n",
    "    split=\"validation\", \n",
    "    streaming=DATASET_STREAMING\n",
    ")\n",
    "\n",
    "if DATASET_STREAMING:\n",
    "    validation =  validation.take(DATASET_VALIDATE_NUM_SELECT)\n",
    "else:\n",
    "    validation = validation.select(\n",
    "        indices=range(DATASET_VALIDATE_NUM_SELECT)\n",
    "    )\n",
    "\n",
    "tokenized_validation =  validation.map(\n",
    "    function=convert_to_prompt, \n",
    "    batched=False,\n",
    "    # batch_size=2048,\n",
    "    # drop_last_batch=False,\n",
    "    remove_columns=remove_column_names,\n",
    "    # num_proc=NUM_CPUS\n",
    ").map(\n",
    "    function=tokenize_prompt, \n",
    "    batched=False,\n",
    "    # batch_size=32,\n",
    "    # drop_last_batch=True,\n",
    "    remove_columns=['prompt'],\n",
    "    # num_proc=NUM_CPUS\n",
    ").shuffle(\n",
    "    seed=42\n",
    ").with_format(\n",
    "    \"torch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples = list(tokenized_train.take(50)) if DATASET_STREAMING else tokenized_train[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "prompt\n",
      "--------------------------------------------------------------------------------\n",
      "USER: Scotland s chief statistician estimated services grew by 0.5% and production by 0.3% between April and June, while construction contracted by 1.9%. UK output as a whole grew by 0.7% over the same period. Over the past year, the Scottish economy grew by 0.7% - a third of the UK rate of 2.1%. In the first three months of the year, there was no growth in Scotland. Scottish GDP per person - which takes population changes into account - grew by 0.3% during the second quarter, compared with 0.4% for the UK. The report indicated that growth in Scottish GDP over the past year was driven by growth in the services industry, particularly in business services and finance. However, that was tempered by contractions in the construction and production industries, especially electricity and gas, following the closure in March of Scotland s last coal-fired power station. It was estimated that the closure resulted in a reduction of Scottish GDP of about 0.2 percentage points in the second quarter. The economic report added: As this was a one-off closure it will not have an ongoing impact on the growth of the Scottish economy. Reacting to the figures, Scottish Chambers of Commerce chief executive Liz Cameron said it was good news that Scotland s economic growth rate had increased but added that there was still a great deal of work to be done . She said: To put this in perspective, the Scottish economy has grown in a year at almost the same rate that the UK economy has grown in just three months. These figures underline the fact that Scotland s economic performance has been significantly lower than that of the UK as a whole for a full year and, whilst we are now seeing welcome growth in our production and service sectors, construction has been contracting at a significant rate for two consecutive quarters. Colin Borland, head of external affairs in Scotland for the Federation of Small Businesses, said: These pre-referendum statistics might feel like a history lesson, but they teach us that Scottish growth was weak even before June s historic vote. Scotland needs to strive for growth levels at least as good as the UK average. We look forward to the SNP talking business when they meet in Glasgow this week. In addition, the UK government needs to put the welfare of the economy\n",
      "--------------------------------------------------------------------------------\n",
      "response\n",
      "--------------------------------------------------------------------------------\n",
      "AI: Output from the Scottish economy grew by 0.4% in the second quarter of this year but lagged behind the UK as a whole, according to official figures.\n"
     ]
    }
   ],
   "source": [
    "print('-' * 80)\n",
    "print(\"prompt\")\n",
    "print('-' * 80)\n",
    "\n",
    "if DATASET_STREAMING:\n",
    "    print(tokenizer.decode(token_ids=examples[49]['input_ids']).split('\\n')[0])\n",
    "else:\n",
    "    print(tokenizer.decode(token_ids=examples['input_ids'][49]).split('\\n')[0])\n",
    "\n",
    "print('-' * 80)\n",
    "print(\"response\")\n",
    "print('-' * 80)\n",
    "if DATASET_STREAMING:\n",
    "    print(tokenizer.decode(token_ids=examples[49]['input_ids']).split('\\n')[1])\n",
    "else:\n",
    "    print(tokenizer.decode(token_ids=examples['input_ids'][49]).split('\\n')[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2q-vpzbal6MI"
   },
   "source": [
    "---\n",
    "# Training\n",
    "\n",
    "Regarding the hyperparameter to use, need to investivage. Those used in [Fine-tune the model? #46 by NXBY - opened Jul 16, 2022](https://huggingface.co/bigscience/bloom/discussions/46#633d452d48ab6a0add2b61bd) might be a starting point.\n",
    "\n",
    "```\n",
    "!python run_qa.py \\\n",
    "  --model_name_or_path bigscience/bloom-560m \\\n",
    "  --dataset_name squad_v2 \\\n",
    "  --do_train \\\n",
    "  --per_device_train_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 2 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir /tmp/debug_seq2seq_squad/ \\\n",
    "  --eval_accumulation_steps 1 \\\n",
    "  --version_2_with_negative \\\n",
    "  --overwrite_output_dir\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model\n",
    "\n",
    "We may need to use a specific model class e.g.  ```AutoModelForSequenceClassification```  to use BERT for classifying pairs of sentences because BERT has not been pretrained on such a task, so the head of the pretrained model has been discarded and a new head suitable for sequence classification has been added instead in ```AutoModelForSequenceClassification```.\n",
    "\n",
    "```\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "```\n",
    "\n",
    "Note that BLOOM is a Decoder model, not Encoder-Decoder, hence cannot be used with ```AutoModelForSeq2SeqLM``` which causes:\n",
    "```\n",
    "ValueError: Unrecognized configuration class <class 'transformers.models.bloom.configuration_bloom.BloomConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM. Model type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, SwitchTransformersConfig, T5Config, XLMProphetNetConfig.\n",
    "```\n",
    "\n",
    "Have a solid understanding on the model architecture and the task to execute for the fine-tuning, and devise the appropriate model to use. BLOOM is still a new model and Decoder architecture such as GPT \n",
    "\n",
    "Note that we cannot use AutoModel as it causes the error:\n",
    "\n",
    "```\n",
    "TypeError: The current model class (BloomModel) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'BloomForCausalLM'}\n",
    "```\n",
    "\n",
    "<img src=\"./image/bloom_model_classes.png\" align=\"left\" width=200/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloomz-560m/snapshots/25f241f41c04f08d658a1dd3b49ad41390109a8e/config.json\n",
      "Model config BloomConfig {\n",
      "  \"_name_or_path\": \"bigscience/bloomz-560m\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"seq_length\": 2048,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bigscience--bloomz-560m/snapshots/25f241f41c04f08d658a1dd3b49ad41390109a8e/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing BloomForCausalLM.\n",
      "\n",
      "All the weights of BloomForCausalLM were initialized from the model checkpoint at bigscience/bloomz-560m.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BloomForCausalLM for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(250880, 1024)\n",
       "    (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (3): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (4): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (5): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (6): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (7): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (8): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (9): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (10): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (11): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (12): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (13): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (14): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (15): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (16): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (17): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (18): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (19): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (20): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (21): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (22): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (23): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = BloomForCausalLM.from_pretrained(MODEL)\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bloom.modeling_bloom.BloomForCausalLM"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_auto_class',\n",
       " '_backward_compatibility_gradient_checkpointing',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_convert_head_mask_to_5d',\n",
       " '_convert_to_bloom_cache',\n",
       " '_convert_to_standard_cache',\n",
       " '_create_repo',\n",
       " '_expand_inputs_for_generation',\n",
       " '_extract_past_from_model_output',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_from_config',\n",
       " '_get_backward_hooks',\n",
       " '_get_decoder_start_token_id',\n",
       " '_get_files_timestamps',\n",
       " '_get_logits_processor',\n",
       " '_get_logits_warper',\n",
       " '_get_name',\n",
       " '_get_resized_embeddings',\n",
       " '_get_resized_lm_head',\n",
       " '_get_stopping_criteria',\n",
       " '_hook_rss_memory_post_forward',\n",
       " '_hook_rss_memory_pre_forward',\n",
       " '_init_weights',\n",
       " '_initialize_weights',\n",
       " '_is_full_backward_hook',\n",
       " '_is_hf_initialized',\n",
       " '_keep_in_fp32_modules',\n",
       " '_keys_to_ignore_on_load_missing',\n",
       " '_keys_to_ignore_on_load_unexpected',\n",
       " '_keys_to_ignore_on_save',\n",
       " '_load_from_state_dict',\n",
       " '_load_pretrained_model',\n",
       " '_load_pretrained_model_low_mem',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_initialize_input_ids_for_generation',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_merge_criteria_processor_list',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_no_split_modules',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_prepare_attention_mask_for_generation',\n",
       " '_prepare_decoder_input_ids_for_generation',\n",
       " '_prepare_encoder_decoder_kwargs_for_generation',\n",
       " '_prepare_model_inputs',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_reorder_cache',\n",
       " '_replicate_for_data_parallel',\n",
       " '_resize_token_embeddings',\n",
       " '_save_to_state_dict',\n",
       " '_set_default_torch_dtype',\n",
       " '_set_gradient_checkpointing',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_tie_encoder_decoder_weights',\n",
       " '_tie_or_clone_weights',\n",
       " '_update_model_kwargs_for_generation',\n",
       " '_upload_modified_files',\n",
       " '_validate_model_class',\n",
       " '_validate_model_kwargs',\n",
       " '_version',\n",
       " 'add_memory_hooks',\n",
       " 'add_module',\n",
       " 'adjust_logits_during_generation',\n",
       " 'apply',\n",
       " 'base_model',\n",
       " 'base_model_prefix',\n",
       " 'beam_sample',\n",
       " 'beam_search',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'can_generate',\n",
       " 'children',\n",
       " 'compute_transition_scores',\n",
       " 'config',\n",
       " 'config_class',\n",
       " 'constrained_beam_search',\n",
       " 'contrastive_search',\n",
       " 'cpu',\n",
       " 'create_extended_attention_mask_for_decoder',\n",
       " 'cuda',\n",
       " 'device',\n",
       " 'disable_input_require_grads',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dummy_inputs',\n",
       " 'dump_patches',\n",
       " 'enable_input_require_grads',\n",
       " 'estimate_tokens',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'floating_point_ops',\n",
       " 'forward',\n",
       " 'framework',\n",
       " 'from_pretrained',\n",
       " 'generate',\n",
       " 'generation_config',\n",
       " 'get_buffer',\n",
       " 'get_extended_attention_mask',\n",
       " 'get_extra_state',\n",
       " 'get_head_mask',\n",
       " 'get_input_embeddings',\n",
       " 'get_memory_footprint',\n",
       " 'get_output_embeddings',\n",
       " 'get_parameter',\n",
       " 'get_position_embeddings',\n",
       " 'get_submodule',\n",
       " 'gradient_checkpointing_disable',\n",
       " 'gradient_checkpointing_enable',\n",
       " 'greedy_search',\n",
       " 'group_beam_search',\n",
       " 'half',\n",
       " 'init_weights',\n",
       " 'invert_attention_mask',\n",
       " 'ipu',\n",
       " 'is_gradient_checkpointing',\n",
       " 'is_loaded_in_8bit',\n",
       " 'is_parallelizable',\n",
       " 'lm_head',\n",
       " 'load_state_dict',\n",
       " 'main_input_name',\n",
       " 'modules',\n",
       " 'name_or_path',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_parameters',\n",
       " 'parameters',\n",
       " 'post_init',\n",
       " 'prepare_inputs_for_generation',\n",
       " 'prune_heads',\n",
       " 'push_to_hub',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_for_auto_class',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'reset_memory_hooks_state',\n",
       " 'resize_position_embeddings',\n",
       " 'resize_token_embeddings',\n",
       " 'retrieve_modules_from_names',\n",
       " 'sample',\n",
       " 'save_pretrained',\n",
       " 'set_extra_state',\n",
       " 'set_input_embeddings',\n",
       " 'set_output_embeddings',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'supports_gradient_checkpointing',\n",
       " 'tie_weights',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'transformer',\n",
       " 'type',\n",
       " 'warnings_issued',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomConfig {\n",
       "  \"_name_or_path\": \"bigscience/bloomz-560m\",\n",
       "  \"apply_residual_connection_post_layernorm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BloomForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"attention_softmax_in_fp32\": true,\n",
       "  \"bias_dropout_fusion\": true,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_dropout\": 0.0,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"masked_softmax_fusion\": true,\n",
       "  \"model_type\": \"bloom\",\n",
       "  \"n_head\": 16,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 24,\n",
       "  \"offset_alibi\": 100,\n",
       "  \"pad_token_id\": 3,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"seq_length\": 2048,\n",
       "  \"skip_bias_add\": true,\n",
       "  \"skip_bias_add_qkv\": false,\n",
       "  \"slow_but_exact\": false,\n",
       "  \"transformers_version\": \"4.28.1\",\n",
       "  \"unk_token_id\": 0,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 250880\n",
       "}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "See the predction parameters to use for the hugging face model tasks.\n",
    "\n",
    "* [Generation](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
    "\n",
    "See Huggingface BLOOM model discussion for **do_sample** parameter requirement.\n",
    "\n",
    "* [Change seed in interference API #131](https://huggingface.co/bigscience/bloom/discussions/131#6368f28950a665fa20d35cc0)\n",
    "\n",
    "> Yes, you need to provide the do_sample parameter as @TimeRobber explained. This endpoint only supports:\n",
    "> * temperature\n",
    "> * topK\n",
    "> * topP\n",
    "> * do_sample\n",
    "> * max_new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to create the summarization. For more details about the different text generation strategies and parameters for controlling generation, check out the [Text Generation](https://huggingface.co/docs/transformers/main/en/tasks/../main_classes/text_generation) API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, prompt) -> str:\n",
    "    inputs: Dict[str, List[int]] = tokenizer(\n",
    "        text=prompt, \n",
    "        max_length=MAX_TOKEN_LENGTH, \n",
    "        truncation=True,\n",
    "        # padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    length: int = tuple(inputs['input_ids'].shape)[1]\n",
    "    response_tokens = model.generate(\n",
    "        inputs[\"input_ids\"].cuda(), \n",
    "        min_new_tokens=32,\n",
    "        max_new_tokens=length+32,\n",
    "        do_sample=True, \n",
    "        top_k=50, \n",
    "        top_p=0.9,\n",
    "        num_return_sequences=1,\n",
    "        # return_full_text=False, Not available\n",
    "        repetition_penalty=50.0,\n",
    "    )[0]\n",
    "    response = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test prediction before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\"\n",
    "prompt = \"USER: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes. Now it's time for your friends that are richer than you do! As Americans' government is taking responsibility ... we must step back from our economic growth policies because they have failed us as people...\n"
     ]
    }
   ],
   "source": [
    "print(predict(model=model, prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collator\n",
    "\n",
    "Tensors going into a model must have the same shape. Hcne pad all the examples to the length of the longest element when we batch elements together — a technique we refer to as dynamic padding. We delay the padding to the last moment, otherwise we bring around padded data which waste the memory and computation time. \n",
    "\n",
    "The function that is responsible for packaging examples into a batch is a collate function, which you pass to a DataLoader as an argument when instantiate it. The collate function converts examples to PyTorch tensors and concatenate them (recursively if your elements are lists, tuples, or dictionaries).\n",
    "\n",
    "The collator is [DataCollatorWithPadding(tokenizer=tokenizer)](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorWithPadding) that takes a tokenizer as an argument to know which padding token to use, and whether the model expects padding to be on the left or on the right.\n",
    "\n",
    "```\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "```\n",
    "\n",
    "if there are eight tokenized sentences whose lengths are ```[50, 59, 47, 67, 59, 50, 62, 32]```, the collator will pad the sentences so that the length will be all 67 as ```[67,  67,  67,  67,  67,  67,  67,  67 ]```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "cRdkkna9l6MI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DataCollatorWithPadding does not pad 'labels' which causes an error at train()\n",
    "# https://stackoverflow.com/a/74228547/4281353\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, \n",
    "    # padding='max_length',\n",
    "    padding=True,\n",
    "    # pad_to_multiple_of=8,\n",
    "    # max_length=MAX_TOKEN_LENGTH,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```DataCollatorForLanguageModeling``` does not work with the error at Trainer.\n",
    "\n",
    "```\n",
    "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#    tokenizer=tokenizer, \n",
    "#    mlm=False,\n",
    "#    return_tensors='pt'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:512\n",
      "attention_mask:512\n",
      "labels:512\n"
     ]
    }
   ],
   "source": [
    "# This does not work with DataCollatorForLanguageModeling either.\n",
    "# Only works with DataCollatorWithPadding\n",
    "collated = data_collator(list(tokenized_train.take(1))[0]) if DATASET_STREAMING else data_collator(tokenized_train[0])\n",
    "for key in collated.keys():\n",
    "    print(f\"{key}:{len(collated[key])}\")\n",
    "    \n",
    "assert len(collated['input_ids']) == len(collated['labels']), \\\n",
    "    f\"expected the same length of input_ids:[{len(collated['input_ids'])}] and labels:{len(collated['labels'])}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USER: Carlos Vela and Juanmi, formerly of Arsenal and Southampton respectively, scored the hosts goals as Granada suffered a fourth successive defeat under Adams. We are all sad, the players, the fans, everybody, said Adams. There s been a lot of mistakes. We re going to try to rectify it and rebound very quickly. The 50-year-old, who took charge on 10 April, has a contract to the end of the current campaign. However Adams has been working at the Spanish club since November and is vice president of the company owned by Granada s club president. If the team played like this at the beginning of the season, there s no way we d be in this situation, he added. I thought they were incredible today, but it s not a day for incredible, it s too late, you re down, you re finished, it s over. Granada s relegation ends a six-season spell in the top flight. They play Real Madrid at home in their next match on 6 May with fans having walked out of previous defeats in protest at how the club is being run. Match ends, Real Sociedad 2, Granada CF 1. Second Half ends, Real Sociedad 2, Granada CF 1. Mikel Oyarzabal (Real Sociedad) wins a free kick in the attacking half. Foul by Uche (Granada CF). Jon Bautista (Real Sociedad) wins a free kick in the defensive half. Foul by Sverrir Ingi Ingason (Granada CF). Corner, Granada CF. Conceded by Mikel Oyarzabal. Hand ball by Juanmi (Real Sociedad). Corner, Real Sociedad. Conceded by Guillermo Ochoa. Attempt saved. Sergio Canales (Real Sociedad) left footed shot from outside the box is saved in the top left corner. Martin Hongla (Granada CF) is shown the yellow card. Juanmi (Real Sociedad) wins a free kick in the attacking half. Foul by Martin Hongla (Granada CF). Foul by Zaldúa (Real Sociedad). Andreas Pereira (Granada CF) wins a free kick in the defensive half. Goal! Real Sociedad 2, Granada CF 1. Juanmi (Real Sociedad) right footed shot from the centre of the box to the bottom left corner. Assisted by Sergio Canales with a through ball. Attempt missed. Raúl Navas (Real Sociedad) header from the centre of the box is close, but misses the top left corner. Assisted by Sergio Canales following a set piece situation. Ezequiel'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_ids=collated['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSdVWcHLl6MH"
   },
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "Use ROUGE for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtPrt_75l6MH"
   },
   "source": [
    "Including a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) metric (see the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "IzoFEZkQl6MH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hz3KKv9kl6MH"
   },
   "source": [
    "Then create a function that passes your predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate the ROUGE metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "ABz4BABll6MI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer API\n",
    "\n",
    "* [TrainingArguments class](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/trainer#transformers.TrainingArguments)\n",
    "\n",
    "The first step before we can define our Trainer is to define a TrainingArguments class that will contain all the hyperparameters the Trainer will use for training and evaluation. \n",
    "\n",
    "```\n",
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\"bloom-trainer\")\n",
    "```\n",
    "\n",
    "We can then define a Trainer by passing it all the objects constructed - the model, the training_args, the training and validation datasets, our data_collator, and our tokenizer.\n",
    "\n",
    "* [Trainer class](https://huggingface.co/docs/transformers/main_classes/trainer)\n",
    "\n",
    "> The ```Trainer``` class provides an API for training in **PyTorch**  for most standard use cases. It’s used in most of the [example scripts](https://github.com/huggingface/transformers/tree/main/examples). \n",
    "\n",
    "[TFTrainer is deprecated](https://discuss.huggingface.co/t/tensorflow-trainer/6383) for Tensorflow, and we should use Keras. See Huggingface [Tensorflow examples](https://github.com/huggingface/transformers/tree/main/examples/tensorflow) github.\n",
    "\n",
    "> TFTrainer will be deprecated and removed in v5, we will focus on better integrating with Keras (though the means of Keras callbacks if we need to add functionality). Checkout the new [classification example](https://github.com/huggingface/transformers/blob/main/examples/tensorflow/text-classification/run_text_classification.py) for an example of where we are going.\n",
    "\n",
    "The Trainer contains the basic training loop which supports the above features. You can subclass them and override the following methods:\n",
    "```\n",
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0]))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlO5sv9Wl6MI"
   },
   "source": [
    "At this point, only three steps remain:\n",
    "\n",
    "1. Define your training hyperparameters in [TrainingArguments](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.TrainingArguments). At the end of each epoch, the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) will evaluate the ROUGE metric and save the training checkpoint.\n",
    "2. Pass the training arguments to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n",
    "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Use the loss function associated to the Huggingface pretrained model. No need to provide.\n",
    "\n",
    "* [What is the loss function used in Trainer from the Transformers library of Hugging Face?](https://stackoverflow.com/a/71585375/4281353)\n",
    "* [Specify Loss for Trainer / TrainingArguments](https://discuss.huggingface.co/t/specify-loss-for-trainer-trainingarguments/10481)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### max_steps for streaming dataset\n",
    "\n",
    "* [TrainingArguments class - max_steps formula when using streaming dataset](https://discuss.huggingface.co/t/training-max-steps-formula-when-using-streaming-dataset/36531)\n",
    "* [Explicitly set number of training steps using Trainer](https://discuss.huggingface.co/t/explicitly-set-number-of-training-steps-using-trainer/1127)\n",
    "\n",
    "### num epochs for streaming dataset\n",
    "\n",
    "* [Huge Num Epochs (9223372036854775807) when using Trainer API with streaming dataset #22757](https://github.com/huggingface/transformers/issues/22757)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with streaming dataset\n",
    "\n",
    "* [Streaming Dataset of Sequence Length 2048](https://discuss.huggingface.co/t/streaming-dataset-of-sequence-length-2048/17649)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "* [Early stopping in Bert Trainer instances](https://stackoverflow.com/questions/69087044/early-stopping-in-bert-trainer-instances)\n",
    "\n",
    "> You need to:\n",
    "> * Use load_best_model_at_end = True (EarlyStoppingCallback() requires this to be True).\n",
    "> * evaluation_strategy = 'steps' or IntervalStrategy.STEPS instead of 'epoch'.\n",
    "> * eval_steps = 50 (evaluate the metrics after N steps).\n",
    "\n",
    "```\n",
    "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
    "...\n",
    "...\n",
    "# Defining the TrainingArguments() arguments\n",
    "args = TrainingArguments(\n",
    "   f\"training_with_callbacks\",\n",
    "   evaluation_strategy = IntervalStrategy.STEPS, # \"steps\"\n",
    "   eval_steps = 50, # Evaluation and Save happens every 50 steps\n",
    "   save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=batch_size,\n",
    "   per_device_eval_batch_size=batch_size,\n",
    "   num_train_epochs=5,\n",
    "   weight_decay=0.01,\n",
    "   push_to_hub=False,\n",
    "   metric_for_best_model = 'f1',\n",
    "   load_best_model_at_end=True)\n",
    "```\n",
    "\n",
    "> In your Trainer():\n",
    "\n",
    "```\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    ...\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "```\n",
    "\n",
    "> Of course, when you use compute_metrics(), for example it can be a function below:\n",
    "> The return of the compute_metrics() should be a dictionary and you can access whatever metric you want/compute inside the function and return.\n",
    "\n",
    "```\n",
    "def compute_metrics(p):    \n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)    \n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "```\n",
    "\n",
    "> Note: In newer transformers version, the usage of Enum IntervalStrategy.steps is recommended (see TrainingArguments()) instead of plain steps string, the latter being soon subject to deprecation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of Memory\n",
    "\n",
    "Using validation causes ```OutOfMemoryError: CUDA out of memory```. There are sum workaround suggested at Huggingface forums.\n",
    "\n",
    "* [CUDA out of memory only during validation not training](https://discuss.huggingface.co/t/cuda-out-of-memory-only-during-validation-not-training/18378)\n",
    "\n",
    "> Use ```eval_accumulation_steps``` to regularly offload the predictions on the GPU to the CPU (slower but will avoid this OOM error).  \n",
    "> Setting ```predict_with_generate``` to True in the training args seemed to solve it for me\n",
    "\n",
    "* [CUDA out of memory when using Trainer with compute_metrics](https://discuss.huggingface.co/t/cuda-out-of-memory-when-using-trainer-with-compute-metrics/2941/3)\n",
    "\n",
    "> use ```eval_accumulation_steps``` to set a number of steps after which your predictions are sent back to the CPU (slower but uses less device memory). This should avoid your OOM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Metrics Error\n",
    "\n",
    "Computing the metrics score cause the error ```Computing the metrics score cause the error ```. The workaroud suggested is ```predict_with_generate=True``` but it is not avilable for BLOOM/Z (causes ```unexpected keyword argument 'predict_with_generate'``` because it is the parameter of the class ```Seq2SeqTrainingArguments```.\n",
    "\n",
    "* [Type Error: list object cannot be interpreted as integer’ while evaluating a summarization model (seq2seq,BART)](https://discuss.huggingface.co/t/type-error-list-object-cannot-be-interpreted-as-integer-while-evaluating-a-summarization-model-seq2seq-bart/11590)\n",
    "\n",
    "> the code runs albeit without the metrics (rouge etc) \n",
    "\n",
    "> the reason why you get an error with ```predict_with_generate=False``` is because the Trainer won’t call the model’s ```generate()``` method in that case (it just computes the loss / logits, which is why you don’t see the metrics). So if you want to compute things like ROUGE during training, you’ll need to generate the summaries with ```predict_with_generate=True```\n",
    "\n",
    "Need to find out a way to use compute_metrics for BLOOM or examing the ```compute_metrics``` function if the implementation is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping Callback\n",
    "\n",
    "Early Stop requires the two training argument.\n",
    "\n",
    "1. metric_for_best_model='eval_<metric>'\n",
    "2. load_best_model_at_end=True,\n",
    "\n",
    "\n",
    "The ```eval_``` prefixed metric is to be specified to the training argument ```metric_for_best_model=\"eval_<metric>\"```. ```load_best_model_at_end=True``` ensures the best model checkpoint is preserved.\n",
    "\n",
    "* [Trainer not keeping best model checkpoint with save_total_limit=1 #15089](https://github.com/huggingface/transformers/issues/15089)\n",
    "\n",
    "> load_best_model_at_end=True makes sure the best model checkpoint is always kept. That means the absolute best model checkpoint, so if at step 500, you get a model worse then at step 450, and the best model checkpoint was at step 350, the Trainer will delete the checkpoint at step 450 indeed, and only keep the checkpoint at step 350 for the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "fmD5Z8Rtl6MI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# For steaming=True datasdt, *max_steps* is required to tell the total number of rows.\n",
    "# https://discuss.huggingface.co/t/streaming-dataset-into-trainer-does-not-implement-len-max-steps-has-to-be-specified/32893/5\n",
    "# ValueError: train_dataset does not implement __len__, max_steps has to be specified\n",
    "# \n",
    "# Enable evaluation cause OutOfMemory\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_DIR_CHECKPOINT,\n",
    "    overwrite_output_dir=True,\n",
    "    max_steps=MAX_STEPS,\n",
    "    num_train_epochs=-1 if DATASET_STREAMING else NUM_EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01, \n",
    "    fp16=USE_FLOAT16,\n",
    "    no_cuda=False,\n",
    "    # predict_with_generate=True,  # This is for Seq2SeqTrainingArguments only\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    eval_accumulation_steps=1,\n",
    "    # fp16_full_eval=True,\n",
    "    # save_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=10,\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end=True,\n",
    "    log_level=\"debug\",\n",
    "    disable_tqdm=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_validation,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # Disable compute_metrics\n",
    "    # Causes Error: argument 'ids': 'list' object cannot be interpreted as an integer\n",
    "    # compute_metrics=compute_metrics,\n",
    "    callbacks = [\n",
    "        EarlyStoppingCallback(early_stopping_patience=5)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch num will be set to a huge number when using streaming dataset.\n",
    "\n",
    "* [Huge Num Epochs (9223372036854775807) when using Trainer API with streaming dataset #22757](https://github.com/huggingface/transformers/issues/22757)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not to remove to resume from saved checkpoint\n",
    "# !rm -rf $MODEL_DIR_CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 12,294\n",
      "  Num Epochs = 9,223,372,036,854,775,807\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12,294\n",
      "  Number of trainable parameters = 559,214,592\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6500' max='12294' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6500/12294 1:09:57 < 1:02:22, 1.55 it/s, Epoch 1/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.782300</td>\n",
       "      <td>3.702616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.663800</td>\n",
       "      <td>3.560490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.552300</td>\n",
       "      <td>3.487336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.511900</td>\n",
       "      <td>3.422432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.486800</td>\n",
       "      <td>3.376068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.413000</td>\n",
       "      <td>3.345376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.392700</td>\n",
       "      <td>3.311706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.318700</td>\n",
       "      <td>3.292373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.668900</td>\n",
       "      <td>3.401282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.335600</td>\n",
       "      <td>3.401787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.300800</td>\n",
       "      <td>3.423522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.328200</td>\n",
       "      <td>3.420883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.332300</td>\n",
       "      <td>3.441765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bloom_fine_tuning/checkpoint-500\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-500/config.json\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-500/generation_config.json\n",
      "Model weights saved in bloom_fine_tuning/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in bloom_fine_tuning/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in bloom_fine_tuning/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [bloom_fine_tuning/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bloom_fine_tuning/checkpoint-1000\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-1000/config.json\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-1000/generation_config.json\n",
      "Model weights saved in bloom_fine_tuning/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in bloom_fine_tuning/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in bloom_fine_tuning/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [bloom_fine_tuning/checkpoint-10500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bloom_fine_tuning/checkpoint-1500\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-1500/config.json\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-1500/generation_config.json\n",
      "Model weights saved in bloom_fine_tuning/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in bloom_fine_tuning/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in bloom_fine_tuning/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [bloom_fine_tuning/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bloom_fine_tuning/checkpoint-2000\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-2000/config.json\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-2000/generation_config.json\n",
      "Model weights saved in bloom_fine_tuning/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in bloom_fine_tuning/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in bloom_fine_tuning/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [bloom_fine_tuning/checkpoint-11500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bloom_fine_tuning/checkpoint-2500\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-2500/config.json\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-2500/generation_config.json\n",
      "Model weights saved in bloom_fine_tuning/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in bloom_fine_tuning/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in bloom_fine_tuning/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [bloom_fine_tuning/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bloom_fine_tuning/checkpoint-3000\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-3000/config.json\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-3000/generation_config.json\n",
      "Model weights saved in bloom_fine_tuning/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in bloom_fine_tuning/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in bloom_fine_tuning/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [bloom_fine_tuning/checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bloom_fine_tuning/checkpoint-3500\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-3500/config.json\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-3500/generation_config.json\n",
      "Model weights saved in bloom_fine_tuning/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in bloom_fine_tuning/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in bloom_fine_tuning/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [bloom_fine_tuning/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bloom_fine_tuning/checkpoint-4000\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-4000/config.json\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-4000/generation_config.json\n",
      "Model weights saved in bloom_fine_tuning/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in bloom_fine_tuning/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in bloom_fine_tuning/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [bloom_fine_tuning/checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bloom_fine_tuning/checkpoint-4500\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-4500/config.json\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-4500/generation_config.json\n",
      "Model weights saved in bloom_fine_tuning/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in bloom_fine_tuning/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in bloom_fine_tuning/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [bloom_fine_tuning/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bloom_fine_tuning/checkpoint-5000\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-5000/config.json\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-5000/generation_config.json\n",
      "Model weights saved in bloom_fine_tuning/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in bloom_fine_tuning/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in bloom_fine_tuning/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [bloom_fine_tuning/checkpoint-2500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bloom_fine_tuning/checkpoint-5500\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-5500/config.json\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-5500/generation_config.json\n",
      "Model weights saved in bloom_fine_tuning/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in bloom_fine_tuning/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in bloom_fine_tuning/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [bloom_fine_tuning/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bloom_fine_tuning/checkpoint-6000\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-6000/config.json\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-6000/generation_config.json\n",
      "Model weights saved in bloom_fine_tuning/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in bloom_fine_tuning/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in bloom_fine_tuning/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [bloom_fine_tuning/checkpoint-3500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bloom_fine_tuning/checkpoint-6500\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-6500/config.json\n",
      "Configuration saved in bloom_fine_tuning/checkpoint-6500/generation_config.json\n",
      "Model weights saved in bloom_fine_tuning/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in bloom_fine_tuning/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in bloom_fine_tuning/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [bloom_fine_tuning/checkpoint-4500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from bloom_fine_tuning/checkpoint-4000 (score: 3.292372703552246).\n",
      "Saving model checkpoint to finetuned_bloom_model_20230418101644\n",
      "Configuration saved in finetuned_bloom_model_20230418101644/config.json\n",
      "Configuration saved in finetuned_bloom_model_20230418101644/generation_config.json\n",
      "Model weights saved in finetuned_bloom_model_20230418101644/pytorch_model.bin\n",
      "tokenizer config file saved in finetuned_bloom_model_20230418101644/tokenizer_config.json\n",
      "Special tokens file saved in finetuned_bloom_model_20230418101644/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "if RESUME_FROM_MODEL_DIR_CHECKPOINT:\n",
    "    trainer.train(MODEL_DIR_CHECKPOINT)  # Resume from MODEL_DIR_CHECKPOINT\n",
    "else:\n",
    "    trainer.train()\n",
    "\n",
    "trainer.save_model(MODEL_DIR_FINE_TUNED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pf5hwlSsl6MJ"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"USER: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "odTyveIWl6MK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes. At risk from losing your life now as part of an ongoing scheme at high levels with many small business owners living below 10% so that people do not look like this world is going into problems for our future . So are we too afraid? Why was all those who were affected forced out by austerity reduced or moved back home instead when they re paid something different over another 10 years? . I don t mean there arenics but here s what other parties believe about how it feel. \n",
      "AIer: All you need says Texas President Muhammad Ali after facing big data challenges.\n",
      "\n",
      "The government plans cut tax credits every single day before voters can vote yes - even\n"
     ]
    }
   ],
   "source": [
    "print(predict(model=model, prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file finetuned_bloom_model_20230418101644/config.json\n",
      "Model config BloomConfig {\n",
      "  \"_name_or_path\": \"finetuned_bloom_model_20230418101644\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"seq_length\": 2048,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n",
      "\n",
      "loading weights file finetuned_bloom_model_20230418101644/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing BloomForCausalLM.\n",
      "\n",
      "All the weights of BloomForCausalLM were initialized from the model checkpoint at finetuned_bloom_model_20230418101644.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BloomForCausalLM for predictions without further training.\n",
      "loading configuration file finetuned_bloom_model_20230418101644/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finetuned_model = AutoModelForCausalLM.from_pretrained(MODEL_DIR_FINE_TUNED).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"USER: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes. A big rise means that after about 5 years it becomes cheaper for everyone (from ordinary Americans); but this is likely not true with companies - rather many are surprised when they do actually have higher prices as long term . In recent seasons of rising house price sales were also an important part ways out policy changes such as: Mr Obama said we ve always made progress , But today he changed his mind by raising government money from next week s rate cap into 1p every 25th anniversary since 1999 because so much revenue had been collected than taxpaymarians could handle without paying any income or spending less. \\nAIis: President Barack Hallowes has\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model=finetuned_model, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# summarizer = pipeline(task=\"text2text-generation\", model=finetuned_model, config=model.config, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBxpeex5l6MK"
   },
   "source": [
    "The simplest way to try out your finetuned model RUN_DATE_TIME inference is to use it in a [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline). Instantiate a `pipeline` for summarization with your model, and pass your text to it:"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "colab": {
   "provenance": []
  },
  "instance_type": "ml.g4dn.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
