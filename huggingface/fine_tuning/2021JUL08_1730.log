2021JUN08_1730

MAX_SEQUENCE_LENGTH = 256
NUM_EPOCHS = 5
BATCH_SIZE = 32
LEARNING_RATE = 2e-05
L2 = 0.001
REDUCE_LR_PATIENCE = 1
EARLY_STOP_PATIENCE = 3
MODEL_DIRECTORY = /content/drive/MyDrive/home/repository/mon/huggingface/finetuning/output/run_2021JUL08_1730/model
LOG_DIRECTORY = /content/drive/MyDrive/home/repository/mon/huggingface/finetuning/output/run_2021JUL08_1730/log

Model: "2021JUL08_1730_DISTILBERT-BASE-UNCASED"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_ids (InputLayer)          [(None, 256)]        0
__________________________________________________________________________________________________
attention_mask (InputLayer)     [(None, 256)]        0
__________________________________________________________________________________________________
tf_distil_bert_model (TFDistilB TFBaseModelOutput(la 66362880    input_ids[0][0]
                                                                 attention_mask[0][0]
__________________________________________________________________________________________________
tf.__operators__.getitem (Slici (None, 768)          0           tf_distil_bert_model[0][0]
__________________________________________________________________________________________________
softmax (Dense)                 (None, 2)            1538        tf.__operators__.getitem[0][0]
==================================================================================================
Total params: 66,364,418
Trainable params: 66,364,418
Non-trainable params: 0