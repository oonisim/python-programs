{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Huggingface Basics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cd0a1075e3dc242"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab389ce4a8abd9d3"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    TFBertForSequenceClassification\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T00:50:52.791924360Z",
     "start_time": "2024-02-15T00:50:52.751362403Z"
    }
   },
   "id": "b4bc7d9fc1e1ab72"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Utility"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "868a303c712a6573"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def create_model(max_sequence, model_name, num_labels):\n",
    "    bert_model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    \n",
    "    # This is the input for the tokens themselves(words from the dataset after encoding):\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_sequence,), dtype=tf.int32, name='input_ids')\n",
    "\n",
    "    # attention_mask - is a binary mask which tells BERT which tokens to attend and which not to attend.\n",
    "    # Encoder will add the 0 tokens to the some sequence which smaller than MAX_SEQUENCE_LENGTH, \n",
    "    # and attention_mask, in this case, tells BERT where is the token from the original data and where is 0 pad token:\n",
    "    attention_mask = tf.keras.layers.Input((max_sequence,), dtype=tf.int32, name='attention_mask')\n",
    "    \n",
    "    # Use previous inputs as BERT inputs:\n",
    "    output = bert_model([input_ids, attention_mask])[0]\n",
    "\n",
    "    # We can also add dropout as regularization technique:\n",
    "    #output = tf.keras.layers.Dropout(rate=0.15)(output)\n",
    "\n",
    "    # Provide number of classes to the final layer:\n",
    "    output = tf.keras.layers.Dense(num_labels, activation='softmax')(output)\n",
    "\n",
    "    # Final model:\n",
    "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T00:44:07.571795833Z",
     "start_time": "2024-02-15T00:44:07.566229343Z"
    }
   },
   "id": "c5717b65f70f24e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Resources\n",
    "\n",
    "* [Quick Tour](https://huggingface.co/docs/transformers/main/en/quicktour)\n",
    "* [Transformers Notebooks](https://github.com/huggingface/transformers/tree/main/notebooks)\n",
    "\n",
    "> You can find here a list of the official notebooks provided by Hugging Face.\n",
    "\n",
    "* [Fine tuning the model](https://huggingface.co/docs/transformers/main/en/training)\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "* [Hugging Face course](https://huggingface.co/course/chapter0/1?fw=pt)\n",
    "\n",
    "> Welcome to the Hugging Face course! This introduction will guide you through setting up a working environment. If youâ€™re just starting the course, we recommend you first take a look at Chapter 1, then come back and set up your environment so you can try the code yourself.\n",
    "\n",
    "* [Huggingface Github - pipeline source](https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/__init__.py#L494)\n",
    "\n",
    "\n",
    "## Fine-tune a pretrained model  \n",
    "> There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. ðŸ¤— Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:\n",
    "> \n",
    "> * Fine-tune a pretrained model with ðŸ¤— Transformers Trainer.\n",
    "> * Fine-tune a pretrained model in TensorFlow with Keras.\n",
    "> * Fine-tune a pretrained model in native PyTorch.\n",
    "\n",
    "See: \n",
    "* [Transformers Notebooks](https://github.com/huggingface/transformers/tree/main/notebooks)\n",
    "* [Training TFBertForSequenceClassification with custom X and Y data](https://stackoverflow.com/a/63295240/4281353)\n",
    "\n",
    "\n",
    "## HuggingFace on SageMaker\n",
    "\n",
    "* [Hugging Face on Amazon SageMaker](https://huggingface.co/docs/sagemaker/index)\n",
    "* [Deploy models to Amazon SageMaker](https://huggingface.co/docs/sagemaker/inference)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94da98cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Terminologies\n",
    "\n",
    "## Pre-tokenized\n",
    "\n",
    "* [Pre-tokenized](https://huggingface.co/transformers/preprocessing.html#pre-tokenized-inputs)\n",
    "\n",
    "It is a ***list of string words*** e.g. ```[\"Hello\", \"I'm\", \"a\", \"single\", \"sentence\"]```. The usage of **tokenized** is misleading in the documentation.\n",
    "\n",
    "> Pre-tokenized does not mean your inputs are already tokenized (you wouldnâ€™t need to pass them through the tokenizer if that was the case) but just split into words.\n",
    "> If you want to use pre-tokenized inputs, you **MUST** set **```is_split_into_words=True```** when passing your inputs to the tokenizer.\n",
    "\n",
    "When passing ```[\"Hello\", \"I'm\", \"a\", \"single\", \"sentence\"]``` instead of a sentence ```\"Hello I'm a single sentence```, the list is called **Pre-tokenized inputs**.\n",
    "```\n",
    "encoded_input = tokenizer([\"Hello\", \"I'm\", \"a\", \"single\", \"sentence\"], is_split_into_words=True)\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3867330194c6df66"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Config file\n",
    "\n",
    "When instantiating a model, you need to define the model initialization parameters that are defined in the Transformers configuration file. The base class is PretrainedConfig.\n",
    "\n",
    "* [PretrainedConfig](https://huggingface.co/transformers/main_classes/configuration.html#pretrainedconfig)\n",
    "\n",
    "> Base class for all configuration classes. Handles a few parameters common to all modelsâ€™ configurations as well as methods for loading/downloading/saving configurations.\n",
    "\n",
    "Each sub class has its own parameters. For instance, Bert pretrained models have the BertConfig.\n",
    "\n",
    "* [BertConfig](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertConfig)\n",
    "\n",
    "> This is the configuration class to store the configuration of a BertModel or a TFBertModel. It is used to instantiate a BERT model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the BERT bert-base-uncased architecture.\n",
    "\n",
    "The ```num_labels``` parameter is from the [PretrainedConfig](https://huggingface.co/transformers/main_classes/configuration.html#transformers.PretrainedConfig)\n",
    "\n",
    "> num_labels (int, optional) â€“ Number of labels to use in the last layer added to the model, typically for a classification task.\n",
    "\n",
    "```\n",
    "TFBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "``` \n",
    "\n",
    "## Example Config File\n",
    "The configuration file for the model ```bert-base-uncased``` is published at [Huggingface model - bert-base-uncased - config.json](https://huggingface.co/bert-base-uncased/blob/main/config.json).\n",
    "\n",
    "```\n",
    "{\n",
    "  \"architectures\": [\n",
    "    \"BertForMaskedLM\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"gradient_checkpointing\": false,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-12,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"model_type\": \"bert\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"transformers_version\": \"4.6.0.dev0\",\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 30522\n",
    "}\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb6ca9aedfec24b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating model from config file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62771ae8c3506d5"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Max length of encoded string(including special tokens such as [CLS] and [SEP]):\n",
    "MAX_SEQUENCE_LENGTH = 64 \n",
    "\n",
    "# Standard BERT model with lowercase chars only:\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-uncased' \n",
    "\n",
    "# Batch size for fitting:\n",
    "BATCH_SIZE = 16 \n",
    "\n",
    "# Number of epochs:\n",
    "EPOCHS=5\n",
    "\n",
    "# Number of labels\n",
    "NUM_LABELS: int = 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T00:47:39.751528783Z",
     "start_time": "2024-02-15T00:47:39.742372181Z"
    }
   },
   "id": "44248bd9e0d55b98"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5c556fdd3ad4eb984e336edb48984bb"
      },
      "application/json": {
       "n": 0,
       "total": 570,
       "elapsed": 0.010780096054077148,
       "ncols": null,
       "nrows": null,
       "prefix": "config.json",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42365f86c4e54dca9fc072ff416fc3d7"
      },
      "application/json": {
       "n": 0,
       "total": 440449768,
       "elapsed": 0.0071828365325927734,
       "ncols": null,
       "nrows": null,
       "prefix": "model.safetensors",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 11:48:32.817997: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-15 11:48:32.846369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-15 11:48:32.846476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-15 11:48:32.848068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-15 11:48:32.848167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-15 11:48:32.848216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-15 11:48:32.885266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-15 11:48:32.885391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-15 11:48:32.885484: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-15 11:48:32.885566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4285 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-02-15 11:48:33.228729: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = create_model(MAX_SEQUENCE_LENGTH, PRETRAINED_MODEL_NAME, NUM_LABELS)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T00:48:37.192333724Z",
     "start_time": "2024-02-15T00:47:45.244253389Z"
    }
   },
   "id": "fe3b1707b8efa211"
  },
  {
   "cell_type": "markdown",
   "id": "f888ef34",
   "metadata": {},
   "source": [
    "---\n",
    "# Pipeline\n",
    "\n",
    "A utility class for convenience.\n",
    "\n",
    "* [pipeline](https://huggingface.co/transformers/main_classes/pipelines.html?highlight=pipeline#transformers.pipeline)\n",
    "\n",
    "> These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering. See the task summary for examples of use.\n",
    "><br>\n",
    "\n",
    "\n",
    "Based on the task specified, ```pipeline``` auto-loads the appropriate model pretrained for the task.\n",
    "\n",
    "## Tutorial\n",
    "\n",
    "See:\n",
    "* [Summary of the tasks](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/task_summary.ipynb)\n",
    "* [Quick Tour](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb)\n",
    "\n",
    "## Available Tasks\n",
    "\n",
    "> * \"feature-extraction\" -  FeatureExtractionPipeline\n",
    "> * \"text-classification\" -  TextClassificationPipeline\n",
    "> * \"sentiment-analysis\" (alias of \"text-classification\") TextClassificationPipeline\n",
    "> * \"token-classification\" -  TokenClassificationPipeline\n",
    "> * \"ner\" (alias of \"token-classification\") - TokenClassificationPipeline\n",
    "> * \"question-answering\" -  QuestionAnsweringPipeline\n",
    "> * \"fill-mask\" -  FillMaskPipeline\n",
    "> * \"summarization\" -  SummarizationPipeline\n",
    "> * \"translation_xx_to_yy\" -  TranslationPipeline\n",
    "> * \"text2text-generation\" -  Text2TextGenerationPipeline\n",
    "> * \"text-generation\" -  TextGenerationPipeline\n",
    "> * \"zero-shot-classification: -  ZeroShotClassificationPipeline\n",
    "> * \"conversational\" -  ConversationalPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88ace279",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T00:54:57.232410196Z",
     "start_time": "2024-02-15T00:54:26.381655285Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48d625cad6fd4793a309cb9288c19af0"
      },
      "application/json": {
       "n": 0,
       "total": 629,
       "elapsed": 0.006234645843505859,
       "ncols": null,
       "nrows": null,
       "prefix": "config.json",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee5b076fa4b84f62a729d380e852d668"
      },
      "application/json": {
       "n": 0,
       "total": 267832558,
       "elapsed": 0.010056257247924805,
       "ncols": null,
       "nrows": null,
       "prefix": "model.safetensors",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a79cb9624594421c9c30a147cc87f230"
      },
      "application/json": {
       "n": 0,
       "total": 48,
       "elapsed": 0.007873296737670898,
       "ncols": null,
       "nrows": null,
       "prefix": "tokenizer_config.json",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64c98306bd6b44af8e945ae2c53d0478"
      },
      "application/json": {
       "n": 0,
       "total": 231508,
       "elapsed": 0.008298635482788086,
       "ncols": null,
       "nrows": null,
       "prefix": "vocab.txt",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: NEGATIVE, with score: 0.9991\n",
      "label: POSITIVE, with score: 0.9999\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", framework='tf')\n",
    "\n",
    "result = classifier(\"I hate you\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n",
    "\n",
    "result = classifier(\"I love you\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82e7bf69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T00:52:31.454199786Z",
     "start_time": "2024-02-15T00:50:58.840349212Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to microsoft/DialoGPT-medium and revision 8bada3b (https://huggingface.co/microsoft/DialoGPT-medium).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8916afd2002d41618721a0b1625a2b45"
      },
      "application/json": {
       "n": 0,
       "total": 642,
       "elapsed": 0.0026488304138183594,
       "ncols": null,
       "nrows": null,
       "prefix": "config.json",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "pytorch_model.bin:   0%|          | 0.00/863M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cee40ddc6610472c8e876b348aa07696"
      },
      "application/json": {
       "n": 0,
       "total": 862955157,
       "elapsed": 0.009212970733642578,
       "ncols": null,
       "nrows": null,
       "prefix": "pytorch_model.bin",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d014833a9fd64978983efe1f70d0652b"
      },
      "application/json": {
       "n": 0,
       "total": 124,
       "elapsed": 0.010637044906616211,
       "ncols": null,
       "nrows": null,
       "prefix": "generation_config.json",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef971dcd312b44b696fc7eba1998e30f"
      },
      "application/json": {
       "n": 0,
       "total": 26,
       "elapsed": 0.007883548736572266,
       "ncols": null,
       "nrows": null,
       "prefix": "tokenizer_config.json",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2a092ec6c6f42f1a5fb320f385f8d31"
      },
      "application/json": {
       "n": 0,
       "total": 1042301,
       "elapsed": 0.008307456970214844,
       "ncols": null,
       "nrows": null,
       "prefix": "vocab.json",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e810a007585e4a06bd42fdd25a44ea91"
      },
      "application/json": {
       "n": 0,
       "total": 456318,
       "elapsed": 0.0076923370361328125,
       "ncols": null,
       "nrows": null,
       "prefix": "merges.txt",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "ConversationalPipeline, expects Conversation as inputs",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m Conversation \u001B[38;5;241m=\u001B[39m pipeline(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconversational\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m conversation_1 \u001B[38;5;241m=\u001B[39m \u001B[43mConversation\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mGoing to the movies tonight - any suggestions?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m conversation_2 \u001B[38;5;241m=\u001B[39m Conversation(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhat\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms the last book you have read?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/venv/ml/lib/python3.10/site-packages/transformers/pipelines/conversational.py:243\u001B[0m, in \u001B[0;36mConversationalPipeline.__call__\u001B[0;34m(self, conversations, num_workers, **kwargs)\u001B[0m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    224\u001B[0m \u001B[38;5;124;03mGenerate responses for the conversation(s) given as inputs.\u001B[39;00m\n\u001B[1;32m    225\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;124;03m    containing a new user input.\u001B[39;00m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;66;03m# XXX: num_workers==0 is required to be backward compatible\u001B[39;00m\n\u001B[1;32m    240\u001B[0m \u001B[38;5;66;03m# Otherwise the threads will require a Conversation copy.\u001B[39;00m\n\u001B[1;32m    241\u001B[0m \u001B[38;5;66;03m# This will definitely hinder performance on GPU, but has to be opted\u001B[39;00m\n\u001B[1;32m    242\u001B[0m \u001B[38;5;66;03m# in because of this BC change.\u001B[39;00m\n\u001B[0;32m--> 243\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconversations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_workers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    244\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mlist\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(outputs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    245\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/venv/ml/lib/python3.10/site-packages/transformers/pipelines/base.py:1120\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1112\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[1;32m   1113\u001B[0m         \u001B[38;5;28miter\u001B[39m(\n\u001B[1;32m   1114\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1117\u001B[0m         )\n\u001B[1;32m   1118\u001B[0m     )\n\u001B[1;32m   1119\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1120\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/venv/ml/lib/python3.10/site-packages/transformers/pipelines/base.py:1126\u001B[0m, in \u001B[0;36mPipeline.run_single\u001B[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[1;32m   1125\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[0;32m-> 1126\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1127\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(model_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params)\n\u001B[1;32m   1128\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(model_outputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpostprocess_params)\n",
      "File \u001B[0;32m~/venv/ml/lib/python3.10/site-packages/transformers/pipelines/conversational.py:250\u001B[0m, in \u001B[0;36mConversationalPipeline.preprocess\u001B[0;34m(self, conversation, min_length_for_response)\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpreprocess\u001B[39m(\u001B[38;5;28mself\u001B[39m, conversation: Conversation, min_length_for_response\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[1;32m    249\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(conversation, Conversation):\n\u001B[0;32m--> 250\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConversationalPipeline, expects Conversation as inputs\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    251\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m conversation\u001B[38;5;241m.\u001B[39mnew_user_input \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    252\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    253\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConversation with UUID \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(conversation\u001B[38;5;241m.\u001B[39muuid)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not contain new user input to process. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    254\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAdd user inputs with the conversation\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms `add_user_input` method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    255\u001B[0m         )\n",
      "\u001B[0;31mValueError\u001B[0m: ConversationalPipeline, expects Conversation as inputs"
     ]
    }
   ],
   "source": [
    "Conversation = pipeline(\"conversational\")\n",
    "\n",
    "conversation_1 = Conversation(\"Going to the movies tonight - any suggestions?\")\n",
    "conversation_2 = Conversation(\"What's the last book you have read?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conversation_1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T00:52:31.455524164Z",
     "start_time": "2024-02-15T00:52:31.455239063Z"
    }
   },
   "id": "e0f988b0b23b2dc9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conversational_pipeline([conversation_1, conversation_2])\n",
    "\n",
    "conversation_1.add_user_input(\"Is it an action movie?\")\n",
    "conversation_2.add_user_input(\"What is the genre of this book?\")\n",
    "\n",
    "conversational_pipeline([conversation_1, conversation_2])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c630c459e1aaed71"
  },
  {
   "cell_type": "markdown",
   "id": "49efdb1b",
   "metadata": {},
   "source": [
    "---\n",
    "# Using specific Model\n",
    "\n",
    "Use ```TFAutoModelForSequenceClassification``` and ```AutoTokenizer``` to load the pretrained model and itâ€™s associated tokenizer.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "classifier(\"Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ðŸ¤— Transformers.\")\n",
    "```\n",
    "\n",
    "See for details:\n",
    "* [Auto Model](https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Fine-Tuning (Transfer Learning)\n",
    "\n",
    "* [Training TFBertForSequenceClassification with custom X and Y data](https://stackoverflow.com/a/68171171/4281353)\n",
    "\n",
    "For instance, utilize the [Sequence Classification](https://huggingface.co/transformers/task_summary.html#sequence-classification) capabilty of BERT for the text classification by fine-tuing the pre-trained BERT model upon the data provided. \n",
    "\n",
    "* [Fine-tuning a pretrained model](https://huggingface.co/transformers/training.html)\n",
    "> How to fine-tune a pretrained model from the Transformers library. In TensorFlow, models can be directly trained using Keras and the fit method. \n",
    "\n",
    "* [Fine-tuning with custom datasets](https://huggingface.co/transformers/custom_datasets.html)\n",
    "> This tutorial will take you through several examples of using ðŸ¤— Transformers models with your own datasets.<br>\n",
    "> [Fine-tuning with native PyTorch/TensorFlow](https://huggingface.co/transformers/custom_datasets.html#fine-tuning-with-native-pytorch-tensorflow)\n",
    "> ```\n",
    "> from transformers import TFDistilBertForSequenceClassification\n",
    "> \n",
    "> model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "> \n",
    "> optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "> model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn\n",
    "> model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)\n",
    "> ```\n",
    "\n",
    "* [HuggingFace Text classification examples](https://github.com/huggingface/transformers/tree/master/examples/tensorflow/text-classification)\n",
    "> This folder contains some scripts showing examples of text classification with the hugs Transformers library. \n",
    "\n",
    "[run_text_classification.py](https://github.com/huggingface/transformers/blob/master/examples/tensorflow/text-classification/run_text_classification.py) is the example for text classification fine-tuning for TensorFlow(https://huggingface.co/transformers/custom_datasets.html)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0fef760124a3679"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "92c1a2add0eaf766"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
