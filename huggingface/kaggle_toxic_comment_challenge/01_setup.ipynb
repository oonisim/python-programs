{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "![toxic_comment_classification_flow.png](data:image/png;base64,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)\n",
    "\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "1. Text comments are in English\n",
    "\n",
    "\n",
    "## Notes on Related Resources\n",
    "\n",
    "* [How AI Is Learning to Identify Toxic Online Content](https://www.scientificamerican.com/article/can-ai-identify-toxic-online-content/)\n",
    ">  This is how our team built Detoxify, an open-source, user-friendly comment detection library to identify inappropriate or harmful text online. Its intended use is to help researchers and practitioners identify potential toxic comments. ... Each model can be easily accessed in one line of code and all models and training code are [publicly available on GitHub (Detoxify)]((https://github.com/unitaryai/detoxify)).\n",
    "\n",
    "* [Detecting toxic comments with Keras and interpreting the model with ELI5](https://medium.com/@armandj.olivares/detecting-toxic-comments-with-keras-and-interpreting-the-model-with-eli5-dbe734f3e86b) - ([Github](https://github.com/ArmandDS/toxic_detection/blob/master/toxic_detection.ipynb))\n",
    "> Develop an estimator with a neural network model for a text classification problem and used ELI5 library for explain the predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3KFBkjcknCD"
   },
   "source": [
    "---\n",
    "# Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3dPmX5_ZwzV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import re\n",
    "import string\n",
    "import datetime\n",
    "import logging\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6by6Rbv4_Z9",
    "outputId": "1ddc707a-d4cc-4145-8b15-58a789627cfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution timestamp 2021JUL012322\n"
     ]
    }
   ],
   "source": [
    "# Project ID\n",
    "PROJECT = \"toxic_comment_classification\"\n",
    "\n",
    "# To reduce the data volumen to run through the training in short timeframe.\n",
    "TEST_MODE = False\n",
    "\n",
    "## Execution recording (e.g. 2021JUL012322)\n",
    "#TIMESTAMP = datetime.datetime.now().strftime(\"%Y%b%d%H%M\").upper()\n",
    "TIMESTAMP = input(\"Enter TIMESTAMP\") \n",
    "print(f\"Execution timestamp {TIMESTAMP}\")\n",
    "\n",
    "# Directory to manage the data. \n",
    "# Place jigsaw-toxic-comment-classification-challenge.zip in DATA_DIR\n",
    "DATA_DIR = \".\"\n",
    "OUTPUT_DIR = \".\"\n",
    "\n",
    "# Flag to clear data or not\n",
    "CLEANING_FOR_ANALYSIS = True\n",
    "CLEANING_FOR_TRAINING = False\n",
    "\n",
    "# Flag to overwrite the cleaned data\n",
    "FORCE_OVERWRITE = False\n",
    "\n",
    "# Labbels that classifies the type of the comment.\n",
    "CATEGORIES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6fCHrZNTePV"
   },
   "source": [
    "## Environment Specifics\n",
    "### Google Colab\n",
    "\n",
    "Google Colab specific operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzCt--KyGh8i"
   },
   "outputs": [],
   "source": [
    "def google_colab_info():\n",
    "    \"\"\"Information on the Google Colab environment\n",
    "    \"\"\"\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # GPU\n",
    "    # --------------------------------------------------------------------------------\n",
    "    gpu_info = !nvidia-smi\n",
    "    gpu_info = '\\n'.join(gpu_info)\n",
    "    if gpu_info.find('failed') >= 0:\n",
    "        print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "        print('and then re-execute this cell.')\n",
    "    else:\n",
    "        print(gpu_info)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Memory\n",
    "    # --------------------------------------------------------------------------------\n",
    "    from psutil import virtual_memory\n",
    "    ram_gb = virtual_memory().total / 1e9\n",
    "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "    if ram_gb < 20:\n",
    "        print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
    "        print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
    "        print('re-execute this cell.')\n",
    "    else:\n",
    "        print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zP1Ln4gxTgio",
    "outputId": "434e2337-3b3b-490f-e27b-92d6f674b30b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Google Colab environment.\n",
      "Thu Jul  1 23:22:45 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   38C    P0    27W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Your runtime has 27.3 gigabytes of available RAM\n",
      "\n",
      "You are using a high-RAM runtime!\n",
      "Mounted at /content/drive\n",
      "Requirement already up-to-date: spacy[cuda102] in /usr/local/lib/python3.7/dist-packages (3.0.6)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (2.4.1)\n",
      "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (3.0.5)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (57.0.0)\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (20.9)\n",
      "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (2.11.3)\n",
      "Requirement already satisfied, skipping upgrade: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (0.3.2)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (1.7.4)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (8.0.7)\n",
      "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (0.8.2)\n",
      "Requirement already satisfied, skipping upgrade: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (3.0.6)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (2.0.5)\n",
      "Requirement already satisfied, skipping upgrade: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (2.0.4)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: cupy-cuda102<9.0.0,>=5.0.0b4; extra == \"cuda102\" in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (8.6.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy[cuda102]) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy[cuda102]) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy[cuda102]) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy[cuda102]) (5.1.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda102]) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda102]) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda102]) (2021.5.30)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda102]) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy[cuda102]) (3.4.1)\n",
      "Requirement already satisfied, skipping upgrade: fastrlock>=0.3 in /usr/local/lib/python3.7/dist-packages (from cupy-cuda102<9.0.0,>=5.0.0b4; extra == \"cuda102\"->spacy[cuda102]) (0.6)\n",
      "2021-07-01 23:22:50.861793: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Requirement already satisfied: en-core-web-sm==3.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl#egg=en_core_web_sm==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.6.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.4)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (57.0.0)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (5.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
      "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Check if the environment is Google Colab.\n",
    "    # --------------------------------------------------------------------------------\n",
    "    import google.colab\n",
    "    IN_GOOGLE_COLAB = True\n",
    "    print(\"Using Google Colab environment.\")\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Show Google Colab information\n",
    "    # --------------------------------------------------------------------------------\n",
    "    google_colab_info()\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Mount Google drive\n",
    "    # --------------------------------------------------------------------------------\n",
    "    google.colab.drive.mount('/content/drive', force_remount=True)\n",
    "    \n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Directory to manage the data\n",
    "    # --------------------------------------------------------------------------------\n",
    "    DATA_DIR = f\"/content/drive/MyDrive/home/data/kaggle/{PROJECT}\"\n",
    "    PICKLE_DIR = DATA_DIR\n",
    "    BASE_DIR = f\"/content/drive/MyDrive/home/repository/mon/kaggle/{PROJECT}\"\n",
    "    OUTPUT_DIR = BASE_DIR\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # GPU Optimized modules\n",
    "    # --------------------------------------------------------------------------------\n",
    "    !pip install -U spacy[cuda102]\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    \n",
    "except ModuleNotFoundError as e:\n",
    "    # if str(e) == \"No module named 'google.colab'\":\n",
    "    IN_GOOGLE_COLAB = False    \n",
    "    print(\"Not using Google Colab environment.\")\n",
    "\n",
    "except ValueError as e:\n",
    "    # https://stackoverflow.com/a/68203799/4281353\n",
    "    print(f\"{str(e)}: possible known issue https://stackoverflow.com/a/68203799/4281353\")\n",
    "    if os.path.isdir('/content/drive'):\n",
    "        !ls -lrt /content/drive\n",
    "        # !rm -rf /content/drive\n",
    "        # google.colab.drive.flush_and_unmount()\n",
    "    raise e\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcefqEMkA2IV"
   },
   "source": [
    "### Kaggle\n",
    "* [How to detect the environment that I'm running the notebook](https://www.kaggle.com/getting-started/147177)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FWdp6UNWA2OL",
    "outputId": "d02f26ee-ec39-4b82-d2d0-7b1197586afa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ.get('KAGGLE_KERNEL_RUN_TYPE','Localhost') == 'Localhost'\n",
      "os.environ.get('KAGGLE_KERNEL_RUN_TYPE','') == ''                   | We are running code on Localhost\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f\"os.environ.get('KAGGLE_KERNEL_RUN_TYPE','Localhost') == '{os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Localhost')}'\")\n",
    "\n",
    "if os.environ.get('KAGGLE_KERNEL_RUN_TYPE',''):\n",
    "    print(\"os.environ.get('KAGGLE_KERNEL_RUN_TYPE','')                         | We are running a Kaggle Notebook/Script - Could be Interactive or Batch Mode\")  \n",
    "    IN_KAGGLE = True\n",
    "\n",
    "if os.environ.get('KAGGLE_KERNEL_RUN_TYPE','') == 'Interactive':\n",
    "    print(\"os.environ.get('KAGGLE_KERNEL_RUN_TYPE','') == 'Interactive'        | We are running a Kaggle Notebook/Script - Interactive Mode\")\n",
    "    IN_KAGGLE = True\n",
    "\n",
    "if os.environ.get('KAGGLE_KERNEL_RUN_TYPE','') == 'Batch':\n",
    "    print(\"os.environ.get('KAGGLE_KERNEL_RUN_TYPE','') == 'Batch'              | We are running a Kaggle Notebook/Script - Batch Mode\")\n",
    "    IN_KAGGLE = True\n",
    "\n",
    "if os.environ.get('KAGGLE_KERNEL_RUN_TYPE','') == '':\n",
    "    print(\"os.environ.get('KAGGLE_KERNEL_RUN_TYPE','') == ''                   | We are running code on Localhost\")    \n",
    "    IN_KAGGLE = False\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Directory to manage the data\n",
    "    # --------------------------------------------------------------------------------\n",
    "    DATA_DIR = \"/kaggle/input/jigsaw-toxic-comment-classification-challenge\"\n",
    "    BASE_DIR = \"/kaggle/working\"\n",
    "    PICKLE_DIR = BASE_DIR\n",
    "    OUTPUT_DIR = BASE_DIR\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # GPU Optimized modules\n",
    "    # --------------------------------------------------------------------------------\n",
    "    !pip install -U spacy[cuda102]\n",
    "    !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not IN_GOOGLE_COLAB) and (not IN_KAGGLE):\n",
    "    !pip install h5py pandas matplotlib \n",
    "    !pip install -U spacy\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    !python -m spacy validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVDkx-1pky8C"
   },
   "source": [
    "## Modules\n",
    "\n",
    "Install and load Python modules required for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brzFWR6YhqCF",
    "outputId": "cad672c5-265d-40f3-9a9d-02b86d5a0849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: setuptools in /usr/local/lib/python3.7/dist-packages (57.0.0)\n",
      "Requirement already up-to-date: wheel in /usr/local/lib/python3.7/dist-packages (0.36.2)\n",
      "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
      "Requirement already up-to-date: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
      "Requirement already up-to-date: pandas in /usr/local/lib/python3.7/dist-packages (1.2.5)\n",
      "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.24.2)\n",
      "Requirement already up-to-date: h5py in /usr/local/lib/python3.7/dist-packages (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
      "Requirement already satisfied, skipping upgrade: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.34.1)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
      "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow) (57.0.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.31.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
      "Requirement already up-to-date: clean-text in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
      "Requirement already up-to-date: unidecode in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
      "Requirement already up-to-date: nltk in /usr/local/lib/python3.7/dist-packages (3.6.2)\n",
      "Requirement already up-to-date: wordcloud in /usr/local/lib/python3.7/dist-packages (1.8.1)\n",
      "Requirement already up-to-date: gensim in /usr/local/lib/python3.7/dist-packages (4.0.1)\n",
      "Requirement already satisfied, skipping upgrade: ftfy<7.0,>=6.0 in /usr/local/lib/python3.7/dist-packages (from clean-text) (6.0.3)\n",
      "Requirement already satisfied, skipping upgrade: emoji in /usr/local/lib/python3.7/dist-packages (from clean-text) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.7/dist-packages (from wordcloud) (3.4.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from wordcloud) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.5)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Requirement already up-to-date: matplotlib in /usr/local/lib/python3.7/dist-packages (3.4.2)\n",
      "Requirement already up-to-date: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.1)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.2.5)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2018.9)\n",
      "Requirement already up-to-date: line_profiler in /usr/local/lib/python3.7/dist-packages (3.3.0)\n",
      "Requirement already up-to-date: memory_profiler in /usr/local/lib/python3.7/dist-packages (0.58.0)\n",
      "Requirement already satisfied, skipping upgrade: IPython>=0.13; python_version >= \"3.7\" in /usr/local/lib/python3.7/dist-packages (from line_profiler) (5.5.0)\n",
      "Requirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n",
      "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (0.7.5)\n",
      "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (2.6.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (57.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (4.8.0)\n",
      "Requirement already satisfied, skipping upgrade: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (5.0.5)\n",
      "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (4.4.2)\n",
      "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (1.0.18)\n",
      "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->IPython>=0.13; python_version >= \"3.7\"->line_profiler) (0.7.0)\n",
      "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->IPython>=0.13; python_version >= \"3.7\"->line_profiler) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython>=0.13; python_version >= \"3.7\"->line_profiler) (0.2.5)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython>=0.13; python_version >= \"3.7\"->line_profiler) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U setuptools wheel\n",
    "!pip install -U tensorflow transformers scikit-learn \n",
    "!pip install -U clean-text unidecode nltk wordcloud gensim\n",
    "!pip install -U line_profiler memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jaR9e-xkqNc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from wordcloud import (\n",
    "    WordCloud, \n",
    "    STOPWORDS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qpx-pzGWasel"
   },
   "source": [
    "## Logging\n",
    "\n",
    "Control the logging outputs to supress the warning and information to prevent the execution results from being cluttered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofY8HgJPcu7K"
   },
   "outputs": [],
   "source": [
    "logging.disable(logging.WARNING)\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-Oj3ltY_T8m"
   },
   "source": [
    "## Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yz9YyZnU_ZdE"
   },
   "outputs": [],
   "source": [
    "SEP = (\"\".join(([\"-\"] * 80))) + \"\\n\"\n",
    "\n",
    "# Matplotlib utilities\n",
    "def plot_on_ax(ax, X, Y, label=None, color=None, title=None, xlabel=None, ylabel=None, legend=True, scale=None, limits=None):\n",
    "    ax.set_xlim(limits[0:2]) if limits is not None else None\n",
    "    ax.set_ylim(limits[2:4]) if limits is not None else None\n",
    "    ax.set_xlabel(xlabel) if xlabel else ...\n",
    "    ax.set_ylabel(ylabel) if ylabel else ...\n",
    "    ax.set_title(title) if title is not None else ...\n",
    "    ax.plot(X, Y, color=color, label=label)\n",
    "    ax.legend() if legend else None\n",
    "    ax.grid(which='major', b=False, linestyle='--')\n",
    "    ax.grid(which='minor', alpha=0.2, linestyle='--')\n",
    "    ax.set_xscale(scale) if scale else ...\n",
    "    ax.xaxis.get_ticklocs(minor=True)\n",
    "    ax.minorticks_on()\n",
    "\n",
    "def hist_on_ax(ax, X, Y, label=None, color=None, alpha=0.5, title=None, xlabel=None, ylabel=None, legend=True, scale=None, limits=None):\n",
    "    ax.set_xlim(limits[0:2]) if limits is not None else None\n",
    "    ax.set_ylim(limits[2:4]) if limits is not None else None\n",
    "    ax.set_xlabel(xlabel) if xlabel else ...\n",
    "    ax.set_ylabel(ylabel) if ylabel else ...\n",
    "    ax.set_title(title) if title is not None else ...\n",
    "    ax.hist(Y, bins=X, alpha=alpha, color=color,label=label)\n",
    "    ax.grid(which='both')\n",
    "    ax.grid(which='major', b=False, linestyle='--')\n",
    "    ax.grid(which='minor', alpha=0.2, linestyle='--')\n",
    "    ax.legend() if legend else None\n",
    "    ax.set_xscale(scale) if scale else ...\n",
    "\n",
    "def plotter(ax, x, hy, uy, hcolor, ucolor, hlabel, ulabel, xlabel, ylabel, title, limits=None, single=False):\n",
    "    plot_on_ax(ax, x, hy, color=hcolor, label=hlabel, xlabel=xlabel, ylabel=ylabel, title=title, limits=limits)\n",
    "    plot_on_ax(ax, x, uy, color=ucolor, label=ulabel) if not single else None\n",
    "    return\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)       \n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.grid()\n",
    "    ax.legend() if legend else None\n",
    "    plt.axis(limits) if limits is not None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZZbTtUGWx9S"
   },
   "source": [
    "## Jupyter Notebook\n",
    "\n",
    "Jupyter cell format configurations. Align the cell output to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AFiq3iQjVDEL",
    "outputId": "8d4cc2e3-48bd-4c6f-86a3-72c95e542094"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n",
       "\n",
       "np.set_printoptions(threshold=sys.maxsize)\n",
       "np.set_printoptions(linewidth=1000) "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leheXoY8q8b_"
   },
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CzdpqIDq7oA",
    "outputId": "e53b1b47-c85e-4076-edaa-fa229fdf005f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from cleantext import clean\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UPio2cmTcWO"
   },
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLeUHOaJ1bt5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 1000   # Allow long string content in a cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRq43r5RSoCA"
   },
   "source": [
    "## TensorFlow\n",
    "\n",
    "Control TensorFlow logging.\n",
    "\n",
    "| TF_CPP_MIN_LOG_LEVEL | Description|          \n",
    "| - |------------- | \n",
    "|0| Suppress all messages are logged (default behavior)|\n",
    "|1 |Suppress INFO messages are not printed|\n",
    "|2 |Suppress INFO and WARNING messages are not printed|\n",
    "|3 |Suppress INFO, WARNING, and ERROR messages are not printed|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJfDGer2Sqkh"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXx4rCOeakbp"
   },
   "source": [
    "## Transformers\n",
    "\n",
    "[HuggingFace](https://huggingface.co/transformers/) offers the libarary for NLP (Natural Language Processing) based on the Transfoemer architecture introduced in the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) by Google.\n",
    "\n",
    "> Transformers provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between Jax, PyTorch and TensorFlow.\n",
    "\n",
    "### Transfer Learning (Fine-Tuning)\n",
    "\n",
    "Utilize the [Sequence Classification](https://huggingface.co/transformers/task_summary.html#sequence-classification) capabilty of BERT for the text classification by fine-tuing the pre-trained BERT model upon the data provided. \n",
    "\n",
    "* [Fine-tuning a pretrained model](https://huggingface.co/transformers/training.html)\n",
    "> How to fine-tune a pretrained model from the Transformers library. In TensorFlow, models can be directly trained using Keras and the fit method. \n",
    "\n",
    "* [Fine-tuning with custom datasets](https://huggingface.co/transformers/custom_datasets.html)\n",
    "> This tutorial will take you through several examples of using 🤗 Transformers models with your own datasets.\n",
    "\n",
    "* [HuggingFace Text classification examples](https://github.com/huggingface/transformers/tree/master/examples/tensorflow/text-classification)\n",
    "> This folder contains some scripts showing examples of text classification with the hugs Transformers library. \n",
    "\n",
    "The code in this notebook is based on the [run_text_classification.py](https://github.com/huggingface/transformers/blob/master/examples/tensorflow/text-classification/run_text_classification.py) example for TensorFlow and the code in the documentation [Fine-tuning with custom datasets](https://huggingface.co/transformers/custom_datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Fct7lHwa1yg"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    DistilBertTokenizerFast,\n",
    "    TFDistilBertForSequenceClassification,\n",
    "    TFTrainer,\n",
    "    TFTrainingArguments\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Control log level (https://huggingface.co/transformers/main_classes/logging.html)\n",
    "# --------------------------------------------------------------------------------\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = \"error\"\n",
    "import transformers\n",
    "transformers.logging.set_verbosity(transformers.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcngPOCBW4R1"
   },
   "source": [
    "## Output Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ga9RQLSUWuk3"
   },
   "outputs": [],
   "source": [
    "# Result output directory\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "RESULT_DIRECTORY = f\"{OUTPUT_DIR}/toxicity_classification_{TIMESTAMP}\"\n",
    "Path(RESULT_DIRECTORY).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not os.access(RESULT_DIRECTORY, os.W_OK):\n",
    "    raise RuntimeError(f\"{RESULT_DIRECTORY} not writable\")\n",
    "if not os.access(PICKLE_DIR, os.W_OK):\n",
    "    raise RuntimeError(f\"{PICKLE_DIR} not writable\")\n",
    "\n",
    "print(f\"DATA_DIR: {DATA_DIR}\")\n",
    "print(f\"BASE_DIR: {BASE_DIR}\")\n",
    "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")\n",
    "print(f\"RESULT_DIRECTORY: {RESULT_DIRECTORY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PFau2osjvx-"
   },
   "source": [
    "---\n",
    "# Ingenstion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CByGibL5II6O"
   },
   "source": [
    "\n",
    "## Data\n",
    "### DATA_PATH\n",
    "**DATA_PATH** variable points to the location of the data package for [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data) in the Google Drive or in the local directory. Unzip the data package to extract the data for training and testing.\n",
    "\n",
    "* train.csv\n",
    "* test.csv\n",
    "* test_labels.csv - 0/1 binary labels to identify the comment is rated for each category (e.g. toxici)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXvCh3GqXtw7"
   },
   "outputs": [],
   "source": [
    "if IN_GOOGLE_COLAB:\n",
    "    DATA_PATH=f\"{DATA_DIR}/jigsaw-toxic-comment-classification-challenge.zip\"\n",
    "elif IN_KAGGLE:\n",
    "    pass\n",
    "else:\n",
    "    DATA_PATH = input(\"Enter the data archive path\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sz-5NlxtoB34",
    "outputId": "9161104b-05e6-40c2-ee88-716b3b1bbc1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/drive/MyDrive/data/jigsaw-toxic-comment-classification-challenge.zip\n",
      "  inflating: /content/drive/MyDrive/data/sample_submission.csv.zip  \n",
      "  inflating: /content/drive/MyDrive/data/test.csv.zip  \n",
      "  inflating: /content/drive/MyDrive/data/test_labels.csv.zip  \n",
      "  inflating: /content/drive/MyDrive/data/train.csv.zip  \n",
      "Archive:  /content/drive/MyDrive/data/train.csv.zip\n",
      "  inflating: ./train.csv             \n",
      "Archive:  /content/drive/MyDrive/data/test.csv.zip\n",
      "  inflating: ./test.csv              \n",
      "Archive:  /content/drive/MyDrive/data/test_labels.csv.zip\n",
      "  inflating: ./test_labels.csv       \n"
     ]
    }
   ],
   "source": [
    "if not IN_KAGGLE:\n",
    "    !unzip -o $DATA_PATH -d $DATA_DIR || echo\n",
    "\n",
    "!unzip -o $DATA_DIR/train.csv.zip -d .\n",
    "!unzip -o $DATA_DIR/test.csv.zip -d .\n",
    "!unzip -o $DATA_DIR/test_labels.csv.zip -d ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQievs3ZNwkz"
   },
   "source": [
    "## Raw Dataframes\n",
    "\n",
    "Load the original data from the CSV files into ```raw_``` dataframes.\n",
    "\n",
    "* raw_train is from train.csv\n",
    "* raw_test is merged from test.csv and test_labels.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7Id3UZspT7h"
   },
   "outputs": [],
   "source": [
    "def load_raw_data(test_mode=False):\n",
    "    raw_train = pd.read_csv(\"./train.csv\")\n",
    "    raw_test_data = pd.read_csv(\"./test.csv\")\n",
    "    raw_test_label = pd.read_csv(\"./test_labels.csv\")\n",
    "    raw_test = pd.merge(raw_test_data, raw_test_label, left_on='id', right_on='id', how='inner')\n",
    "    del raw_test_data, raw_test_label\n",
    "\n",
    "    if test_mode:\n",
    "        raw_train = raw_train.head(256)\n",
    "        raw_test = raw_test.head(128)\n",
    "\n",
    "    return raw_train, raw_test\n",
    "\n",
    "raw_train, raw_test = load_raw_data(TEST_MODE)\n",
    "print(f\"raw_train records: {raw_train['id'].count()} raw_test records: {raw_test['id'].count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqRp3qdczPM5"
   },
   "source": [
    "### Raw data (train.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "QKAe7wXMzOMd",
    "outputId": "346118bc-4d72-44ff-8f1e-5f5c25325601"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  ... identity_hate\n",
       "0  0000997932d777bf  ...             0\n",
       "1  000103f0d9cfb60f  ...             0\n",
       "2  000113f07ec002fd  ...             0\n",
       "\n",
       "[3 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If in Google colab runtime and got an error, restart the runtime.\n",
    "# \"AttributeError: 'NotebookFormatter' object has no attribute 'get_result'\"\"\n",
    "# https://stackoverflow.com/questions/66412776\n",
    "raw_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "Wfx5DKry2KjZ",
    "outputId": "3ca68f1f-9887-48a8-ea22-905ddd42c285"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic  ...         insult  identity_hate\n",
       "count  159571.000000  159571.000000  ...  159571.000000  159571.000000\n",
       "mean        0.095844       0.009996  ...       0.049364       0.008805\n",
       "std         0.294379       0.099477  ...       0.216627       0.093420\n",
       "min         0.000000       0.000000  ...       0.000000       0.000000\n",
       "25%         0.000000       0.000000  ...       0.000000       0.000000\n",
       "50%         0.000000       0.000000  ...       0.000000       0.000000\n",
       "75%         0.000000       0.000000  ...       0.000000       0.000000\n",
       "max         1.000000       1.000000  ...       1.000000       1.000000\n",
       "\n",
       "[8 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "-NaP9KvTzJuZ",
    "outputId": "368bf310-8038-4eb4-c78f-92b77ca735d2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0005c987bdfc9d4b</td>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\\n\\nAsk Sityush to clean up his behavior than issue me nonsensical warnings...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0007e25b2121310b</td>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming back! Tosser.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  ... identity_hate\n",
       "6   0002bcb3da6cb337  ...             0\n",
       "12  0005c987bdfc9d4b  ...             0\n",
       "16  0007e25b2121310b  ...             0\n",
       "\n",
       "[3 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train[raw_train['toxic'] > 0].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0jSqXxxQrUj"
   },
   "source": [
    "### Raw data (test.csv)\n",
    "Remove the rows where the label value is -1 as as the meaning is not clearly defined.\n",
    "\n",
    "> test_labels.csv - labels for the test data; value of -1 indicates it was not used for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ve713cdzWptk",
    "outputId": "f579a843-6956-40f5-bf05-6fb89b944896"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>63978.000000</td>\n",
       "      <td>63978.000000</td>\n",
       "      <td>63978.000000</td>\n",
       "      <td>63978.000000</td>\n",
       "      <td>63978.000000</td>\n",
       "      <td>63978.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095189</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.053565</td>\n",
       "      <td>0.011129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.293478</td>\n",
       "      <td>0.075522</td>\n",
       "      <td>0.233161</td>\n",
       "      <td>0.057334</td>\n",
       "      <td>0.225160</td>\n",
       "      <td>0.104905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              toxic  severe_toxic  ...        insult  identity_hate\n",
       "count  63978.000000  63978.000000  ...  63978.000000   63978.000000\n",
       "mean       0.095189      0.005736  ...      0.053565       0.011129\n",
       "std        0.293478      0.075522  ...      0.225160       0.104905\n",
       "min        0.000000      0.000000  ...      0.000000       0.000000\n",
       "25%        0.000000      0.000000  ...      0.000000       0.000000\n",
       "50%        0.000000      0.000000  ...      0.000000       0.000000\n",
       "75%        0.000000      0.000000  ...      0.000000       0.000000\n",
       "max        1.000000      1.000000  ...      1.000000       1.000000\n",
       "\n",
       "[8 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#raw_test.describe()\n",
    "raw_test[(raw_test['toxic'] >= 0)].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YVyle97CWy7A",
    "outputId": "f810a085-79fd-4d8f-d9c8-878f2c280485"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>Thank you for understanding. I think very highly of you and would not revert without discussion.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0002f87b16116a7f</td>\n",
       "      <td>\"::: Somebody will invariably try to add Religion?  Really??  You mean, the way people have invariably kept adding \"\"Religion\"\" to the Samuel Beckett infobox?  And why do you bother bringing up the long-dead completely non-existent \"\"Influences\"\" issue?  You're just flailing, making up crap on the fly. \\n ::: For comparison, the only explicit acknowledgement in the entire Amos Oz article that he is personally Jewish is in the categories!    \\n\\n \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  ... identity_hate\n",
       "5   0001ea8717f6de06  ...             0\n",
       "7   000247e83dcc1211  ...             0\n",
       "11  0002f87b16116a7f  ...             0\n",
       "\n",
       "[3 rows x 8 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test[(raw_test['toxic'] >= 0)].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wwZFMlD6pIJ"
   },
   "source": [
    "---\n",
    "# Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ZCFG5Bv_53c",
    "outputId": "4215047c-8997-4887-f865-d457016930d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep the raw non-mutated whereas train/test are mutated.\n",
    "train = raw_train.copy()\n",
    "test = raw_test.copy()\n",
    "\n",
    "# To avoid OOM, remove raw_train but keep raw_test for testing/submissin.\n",
    "del raw_train   \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPVocLsNlz6i"
   },
   "source": [
    "## Enrichment\n",
    "### Unhealthiness label \n",
    "\n",
    "Use the sum of labels as the unhealthiness level of the comment (0: healthy, 6: the most unhealthy). The ```unhealthiness``` is 2 for a comment labeled as ```toxic``` and ```insult```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPbVcApY0AIy"
   },
   "outputs": [],
   "source": [
    "train['unhealthiness'] = train.iloc[:, 1:].sum(axis=1)\n",
    "train['unhealthy'] = train['unhealthiness'].apply(lambda x: int(x > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVwVH7nm7Tjn"
   },
   "source": [
    "### Comment length\n",
    "Add the comment length to analyze the co-relation the content with its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-qGfbsl7dJy"
   },
   "outputs": [],
   "source": [
    "train['length'] = train['comment_text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjcbdEAItiw5"
   },
   "source": [
    "\n",
    "### Data Argumentation (**Not implemented**)\n",
    "\n",
    "Permutate the data to provide additional data and address the skewness of the data representation.\n",
    "\n",
    "* [Data Augmentation in NLP\n",
    "](https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28) - nlpaug\n",
    "> We explore different authors how they leverage augmentation to tickle NLP tasks via generating more text data to boost up the models.  \n",
    "\n",
    "* Github [nlpaug](https://github.com/makcedward/nlpaug)\n",
    "> This python library helps you with augmenting nlp for your machine learning projects. Visit this introduction to understand about Data Augmentation in NLP. Augmenter is the basic element of augmentation while Flow is a pipeline to orchestra multi augmenter together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmN1y1eitrz_"
   },
   "source": [
    "## Cleaning\n",
    "\n",
    "### De-contraction\n",
    "\n",
    "Restore contraction (e.g. I'll) into de-contracted form (I will) to have the valid words in place.\n",
    "\n",
    "### Stop words removal\n",
    "\n",
    "Remove the stop words (e.g. EOL, an, the, us) that has neither positive nor negative influence on the sentence.\n",
    "\n",
    "* EOL (End of line)\n",
    "* Numbers, digits\n",
    "* Email, URL, phone number, currency symbols\n",
    "* Optional\n",
    "\n",
    "### Lemmatization\n",
    "Reducing the inflectional forms (e.g. having, had, has) of a word into its base (have) to focus on the core meaning, not the form of the words.\n",
    "\n",
    "### Case consistency\n",
    "Lower the case to remove variations of the same word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UoBuLSZlGZCy"
   },
   "outputs": [],
   "source": [
    "def decontracted(sentences):\n",
    "    \"\"\"Restore the contracted words\"\"\"\n",
    "    # specific\n",
    "    sentences = re.sub(r\"won\\'t\", \"will not\", sentences, flags=re.IGNORECASE)\n",
    "    sentences = re.sub(r\"can\\'t\", \"can not\", sentences, flags=re.IGNORECASE)\n",
    "    # general\n",
    "    sentences = re.sub(r\"n\\'t\", \" not\", sentences, flags=re.IGNORECASE)\n",
    "    sentences = re.sub(r\"\\'re\", \" are\", sentences, flags=re.IGNORECASE)\n",
    "    sentences = re.sub(r\"\\'s\", \" is\", sentences, flags=re.IGNORECASE)\n",
    "    sentences = re.sub(r\"\\'d\", \" would\", sentences, flags=re.IGNORECASE)\n",
    "    sentences = re.sub(r\"\\'ll\", \" will\", sentences, flags=re.IGNORECASE)\n",
    "    sentences = re.sub(r\"\\'t\", \" not\", sentences, flags=re.IGNORECASE)\n",
    "    sentences = re.sub(r\"\\'ve\", \" have\", sentences, flags=re.IGNORECASE)\n",
    "    sentences = re.sub(r\"\\'m\", \" am\", sentences, flags=re.IGNORECASE)\n",
    "    return sentences\n",
    "\n",
    "def remove_noises(sentences):\n",
    "    \"\"\"Clean up noises in the text\n",
    "    Not use no_punct as it is too slow\n",
    "    \"\"\"\n",
    "    sentences = clean(sentences,\n",
    "        fix_unicode=True,               # fix various unicode errors\n",
    "        to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "        lower=True,                     # lowercase text\n",
    "        no_line_breaks=True,            # fully strip line breaks as opposed to only normalizing them\n",
    "        no_urls=True,                   # replace all URLs with a special token\n",
    "        no_emails=True,                 # replace all email addresses with a special token\n",
    "        no_phone_numbers=True,          # replace all phone numbers with a special token\n",
    "        no_numbers=True,                # replace all numbers with a special token\n",
    "        no_digits=True,                 # replace all digits with a special token\n",
    "        no_currency_symbols=True,       # replace all currency symbols with a special token\n",
    "        no_punct=False,                 # remove punctuations\n",
    "        replace_with_punct=\"\" ,         # instead of removing punctuations you may replace them\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_phone_number=\"<PHONE>\",\n",
    "        replace_with_number=\"\",\n",
    "        replace_with_digit=\"\",\n",
    "        replace_with_currency_symbol=\"\",\n",
    "        lang=\"en\"                       # set to 'de' for German special handling\n",
    "    )\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Remove punctuations\n",
    "    # --------------------------------------------------------------------------------\n",
    "    punctuation = re.escape(string.punctuation)\n",
    "    pattern = '[%s%s]+' % (punctuation, r'\\s')\n",
    "    sentences = re.sub(pattern, \" \", sentences)  \n",
    "    return sentences\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(sentence):\n",
    "    return \" \".join([\n",
    "        lemmatizer.lemmatize(word, pos=\"v\") \n",
    "        for word in nltk.word_tokenize(sentence.lower()) \n",
    "        if word not in stopwords.words('english')\n",
    "    ])\n",
    "\n",
    "def clean_comment_text_without_lemmatize(sentences):\n",
    "    return lemmatize(remove_noises(decontracted(sentences)))\n",
    "\n",
    "def clean_comment_text(sentences):\n",
    "    return lemmatize(remove_noises(decontracted(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_data(train, test):\n",
    "    train_pickle_path = f\"{PICKLE_DIR}/train_lemmatized_{int(TEST_MODE)}.pkl\"\n",
    "    if os.path.isfile(train_pickle_path) and (not FORCE_OVERWRITE):\n",
    "        del train\n",
    "        gc.collect()\n",
    "        train = pd.read_pickle(train_pickle_path)\n",
    "    else:\n",
    "        train['comment_text'] = train['comment_text'].apply(clean_comment_text)\n",
    "        train.to_pickle(train_pickle_path)\n",
    "\n",
    "    test_pickle_path = f\"{PICKLE_DIR}/test_lemmatized_{int(TEST_MODE)}.pkl\"\n",
    "    if os.path.isfile(test_pickle_path) and (not FORCE_OVERWRITE):\n",
    "        del test\n",
    "        gc.collect()\n",
    "        test = pd.read_pickle(test_pickle_path)\n",
    "    else:\n",
    "        test['comment_text'] = test['comment_text'].apply(clean_comment_text)\n",
    "        test.to_pickle(test_pickle_path)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "g61Kn3ljtrOx",
    "outputId": "905a55aa-9bfe-45f6-a171-3ccbd180f8d7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>unhealthiness</th>\n",
       "      <th>unhealthy</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>cocksucker piss around work</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0005c987bdfc9d4b</td>\n",
       "      <td>hey talk exclusive group wp talibanswho good destroy selfappointed purist gang one ask question abt antisocial destructive noncontribution wp ask sityush clean behavior issue nonsensical warn</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0007e25b2121310b</td>\n",
       "      <td>bye look come think comming back tosser</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  ... length\n",
       "6   0002bcb3da6cb337  ...     44\n",
       "12  0005c987bdfc9d4b  ...    319\n",
       "16  0007e25b2121310b  ...     57\n",
       "\n",
       "[3 rows x 11 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if CLEANING_FOR_ANALYSIS or CLEANING_FOR_TRAINING:\n",
    "    train, test = load_clean_data(train, test)\n",
    "\n",
    "if test['id'].count() >= train['id'].count():\n",
    "raise RuntimeError(\"Invalid counts. Verfity the train/test data.\")\n",
    "\n",
    "train[train['toxic'] > 0].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NZjDr9VH6CoJ",
    "outputId": "54430c6c-06ea-4161-d17e-d60b4fa6a86b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the rows with -1. ['toxic'] >= 0 is sufficient\n",
    "test = test[test['toxic'] >= 0]\n",
    "test.head(3)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train records: {train['id'].count()} Test records: {test['id'].count()}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "o-Oj3ltY_T8m",
    "lZZbTtUGWx9S",
    "Qpx-pzGWasel",
    "gcefqEMkA2IV",
    "7UPio2cmTcWO",
    "QcngPOCBW4R1",
    "k0jSqXxxQrUj"
   ],
   "machine_shape": "hm",
   "name": "toxic_comment_01_setup.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
