{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdZo50sxLBuX"
   },
   "source": [
    "---\n",
    "# Technology Selection\n",
    "\n",
    "What algorithms to use and what technologies are available.\n",
    "\n",
    "## Nature of the problem\n",
    "\n",
    "* Multi-label Binary Classification \n",
    "\n",
    "It is a binary classification task where multiple althorithms have been developed and applied in the real life e.g. SPAM fileter.\n",
    "\n",
    "* Naive Bayes - [Naive Bayes and Text Classification](https://arxiv.org/abs/1410.5329)\n",
    "* CNN - [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf)\n",
    "* DNN Language Model - [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "## Asssessment\n",
    "\n",
    "Skipped due to the time constraint.\n",
    "\n",
    "## Decision\n",
    "\n",
    "**Transformer Deep Neural Network Architecture** transfer-learning (fine-tuning) on the pre-trained language model.\n",
    "\n",
    "1. State of the art algorithms being actively researched.\n",
    "2. Pre-trained models for text classification e.g text sentiment analysis are available. \n",
    "3. Other well-explored althorithms have been well tested as published in Kaggle. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOtHJ1edOFoS"
   },
   "source": [
    "---\n",
    "# Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rB2cjqCZhcE9"
   },
   "source": [
    "## ML Model for Fine Tuning\n",
    "\n",
    "### Framework\n",
    "* Google TensorFlow 2.x \n",
    "* Keras for training the model\n",
    "* Huggingface Transformer library\n",
    "\n",
    "### Data allocation\n",
    "Utilize [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to :\n",
    "1. shuffle the train data \n",
    "2. allocate the ratio R of the data for validation. R=0.2\n",
    "3 apply the model training on (1-R) ratio of the data for training\n",
    "\n",
    "Apply the trained model on testing data for evaluation.\n",
    "\n",
    "### Hyper parameter search\n",
    "* Learning rate (5e-5, 5e-4, 5e-3) as the start value\n",
    "\n",
    "### Epoch\n",
    "Number of times to go through the entire training data set N. N=10 due to the time constraint.\n",
    "\n",
    "### Early stopping\n",
    "Utilize Keras [EarlyStopping](https://keras.io/api/callbacks/early_stopping/) to stop the training when no improvement is achieved N times. N=5.\n",
    "\n",
    "### Reduce learning rate at no improvement\n",
    "Utilize Keras [ReduceLROnPlateau](https://keras.io/api/callbacks/reduce_lr_on_plateau/) to reduce the learning rate when no improvement is achieved N times. N=3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bp2VT2ohkKD"
   },
   "source": [
    "### Keras Callbacks\n",
    "\n",
    "Utilize [Keras Callbacks API](https://keras.io/api/callbacks/) to apply Eary Stopping, Reduce Learning Rate, and TensorBoard during the model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Fc67_HTf0zyO"
   },
   "outputs": [],
   "source": [
    "class SavePretrainedCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    This is only for directly working on the Huggingface models.\n",
    "    \n",
    "    Hugging Face models have a save_pretrained() method that saves both \n",
    "    the weights and the necessary metadata to allow them to be loaded as \n",
    "    a pretrained model in future. This is a simple Keras callback that \n",
    "    saves the model with this method after each epoch.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, output_dir, **kwargs):\n",
    "        super().__init__()\n",
    "        self.output_dir = output_dir\n",
    "        self.lowest_val_loss=np.inf\n",
    "        self.best_epoch = -1\n",
    "        self.verbose = kwargs['verbose'] if 'verbose' in kwargs else False\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        Save only the best model\n",
    "        - https://stackoverflow.com/a/68042600/4281353\n",
    "        - https://www.tensorflow.org/guide/keras/custom_callback\n",
    "        \n",
    "        TODO: \n",
    "        save_pretrained() method is in the HuggingFace model only.\n",
    "        Need to implement an logic to update for Keras model saving.\n",
    "        \"\"\"\n",
    "        val_loss=logs.get('val_loss')\n",
    "        if (self.best_epoch < 0) or (val_loss < self.lowest_val_loss):\n",
    "            if self.verbose:\n",
    "                print(f\"Model val_loss improved: [{val_loss} < {self.lowest_val_loss}]\")\n",
    "                print(f\"Saving to {self.output_dir}\")\n",
    "            self.lowest_val_loss = val_loss\n",
    "            self.best_epoch = epoch\n",
    "            self.model.save_pretrained(self.output_dir)\n",
    "\n",
    "\n",
    "class TensorBoardCallback(tf.keras.callbacks.TensorBoard):\n",
    "    \"\"\"TensorBoard visualization of the model training\n",
    "    See https://keras.io/api/callbacks/tensorboard/\n",
    "    \"\"\"\n",
    "    def __init__(self, output_directory):\n",
    "        super().__init__(\n",
    "            log_dir=output_directory,\n",
    "            write_graph=True,\n",
    "            write_images=True,\n",
    "            histogram_freq=1,     # log histogram visualizations every 1 epoch\n",
    "            embeddings_freq=1,    # log embedding visualizations every 1 epoch\n",
    "            update_freq=\"epoch\",  # every epoch\n",
    "        )\n",
    "\n",
    "\n",
    "class EarlyStoppingCallback(tf.keras.callbacks.EarlyStopping):\n",
    "    \"\"\"Stop training when no progress on the metric to monitor\n",
    "    https://keras.io/api/callbacks/early_stopping/\n",
    "    https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "\n",
    "    Using val_loss to monitor. \n",
    "    https://datascience.stackexchange.com/a/49594/68313\n",
    "    Prefer the loss to the accuracy. Why? The loss quantify how certain \n",
    "    the model is about a prediction. The accuracy merely account for \n",
    "    the number of correct predictions. Similarly, any metrics using hard \n",
    "    predictions rather than probabilities have the same problem.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=3):\n",
    "        assert patience > 0\n",
    "        super().__init__(\n",
    "            monitor='val_loss', \n",
    "            mode='min', \n",
    "            verbose=1, \n",
    "            patience=patience,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "\n",
    "class ModelCheckpointCallback(tf.keras.callbacks.ModelCheckpoint):\n",
    "    \"\"\"Check point to save the model\n",
    "    See https://keras.io/api/callbacks/model_checkpoint/\n",
    "\n",
    "    NOTE: Did not work with HuggingFace with the error.\n",
    "        NotImplementedError: Saving the model to HDF5 format requires the model \n",
    "        to be a Functional model or a Sequential model. \n",
    "        It does not work for subclassed models, because such models are defined \n",
    "        via the body of a Python method, which isn't safely serializable. \n",
    "    \"\"\"\n",
    "    def __init__(self, path_to_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path_to_file: path to the model file to save at check points\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            filepath=path_to_file, \n",
    "            monitor='val_loss', \n",
    "            mode='min', \n",
    "            save_best_only=True,\n",
    "            save_freq=\"epoch\",\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "\n",
    "class ReduceLRCallback(tf.keras.callbacks.ReduceLROnPlateau):\n",
    "    \"\"\"Reduce learning rate when a metric has stopped improving.\n",
    "    See https://keras.io/api/callbacks/reduce_lr_on_plateau/\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=3):\n",
    "        assert patience > 0\n",
    "        super().__init__(\n",
    "            monitor=\"val_loss\",\n",
    "            factor=0.2,\n",
    "            patience=patience,\n",
    "            verbose=1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0Rpsc7IjWFd"
   },
   "source": [
    "### Fine Tuning Runner\n",
    "\n",
    "The Runner class implements the fine-tuning based on the [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) pretrained model. Each classification category e.g. ```toxic``` will have a dedicated Runner class instance. The reason for using the ***Distilled*** BERT model is to run the training on the limited resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "xXWNRYbes8V6"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import (\n",
    "    Sequential\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "\n",
    "class Runner:\n",
    "    \"\"\"Fine tuning implementation class\n",
    "    See:\n",
    "    - https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
    "    - https://stackoverflow.com/questions/68172891/\n",
    "    - https://stackoverflow.com/a/68172992/4281353\n",
    "\n",
    "    The TF/Keras model has the base model, e.g distilbert for DistiBERT which is\n",
    "    from the base model TFDistilBertModel.\n",
    "    https://huggingface.co/transformers/model_doc/distilbert.html#tfdistilbertmodel\n",
    "\n",
    "    TFDistilBertForSequenceClassification has classification layers added on top\n",
    "    of TFDistilBertModel, hence not required to add fine-tuning layers by users.\n",
    "    _________________________________________________________________\n",
    "    Layer (type)                 Output Shape              Param #   \n",
    "    =================================================================\n",
    "    distilbert (TFDistilBertMain multiple                  66362880  \n",
    "    _________________________________________________________________\n",
    "    pre_classifier (Dense)       multiple                  590592    \n",
    "    _________________________________________________________________\n",
    "    classifier (Dense)           multiple                  1538      \n",
    "    _________________________________________________________________\n",
    "    dropout_59 (Dropout)         multiple                  0         \n",
    "    =================================================================\n",
    "    \"\"\"\n",
    "    # ================================================================================\n",
    "    # Class\n",
    "    # ================================================================================\n",
    "    USE_HF_TRAINER = False\n",
    "    TOKENIZER_LOWER_CASE = True\n",
    "    # _model_name = 'distilbert-base-cased'\n",
    "    _model_name = 'distilbert-base-uncased'\n",
    "    _model_base_name = 'distilbert'\n",
    "    _tokenizer = DistilBertTokenizerFast.from_pretrained(\n",
    "        _model_name, \n",
    "        do_lower_case=TOKENIZER_LOWER_CASE\n",
    "    )\n",
    "\n",
    "    # ================================================================================\n",
    "    # Instance\n",
    "    # ================================================================================\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Instance properties\n",
    "    # --------------------------------------------------------------------------------\n",
    "    @property\n",
    "    def category(self):\n",
    "        \"\"\"Category of the text comment classification, e.g. toxic\"\"\"\n",
    "        return self._category\n",
    "\n",
    "    @property\n",
    "    def num_labels(self):\n",
    "        \"\"\"Number of labels to classify\"\"\"\n",
    "        assert self._num_labels > 0\n",
    "        return self._num_labels\n",
    "\n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        \"\"\"BERT tokenizer. The Tokenzer must match the pretrained model\"\"\"\n",
    "        return self._tokenizer\n",
    "\n",
    "    @property\n",
    "    def max_sequence_length(self):\n",
    "        \"\"\"Maximum token length for the BERT tokenizer can accept. Max 512\n",
    "        \"\"\"\n",
    "        assert 128 <= self._max_sequence_length <= 512\n",
    "        return self._max_sequence_length\n",
    "\n",
    "    @property\n",
    "    def X(self):\n",
    "        \"\"\"Training TensorFlow DataSet\"\"\"\n",
    "        return self._X\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        \"\"\"Validation TensorFlow DataSet\"\"\"\n",
    "        return self._V\n",
    "\n",
    "    @property\n",
    "    def model_name(self):\n",
    "        \"\"\"HuggingFace pretrained model name\"\"\"\n",
    "        return self._model_name\n",
    "\n",
    "    @property\n",
    "    def model_base_name(self):\n",
    "        \"\"\"HuggingFace pretrained base model name\"\"\"\n",
    "        return self._model_base_name\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        \"\"\"TensorFlow/Keras Model instance\"\"\"\n",
    "        return self._model\n",
    "\n",
    "    @property\n",
    "    def freeze_pretrained_base_model(self):\n",
    "        \"\"\"Boolean to freeze the base model\"\"\"\n",
    "        return self._freeze_pretrained_base_model\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        \"\"\"Mini batch size during the training\"\"\"\n",
    "        assert self._batch_size > 0\n",
    "        return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        \"\"\"Training learning rate\"\"\"\n",
    "        return self._learning_rate\n",
    "\n",
    "    @property\n",
    "    def reduce_lr_patience(self):\n",
    "        \"\"\"Training patience for reducing learinig rate\"\"\"\n",
    "        return self._reduce_lr_patience\n",
    "\n",
    "    @property\n",
    "    def early_stop_patience(self):\n",
    "        \"\"\"Training patience for early stopping\"\"\"\n",
    "        return self._early_stop_patience\n",
    "\n",
    "    @property\n",
    "    def num_epochs(self):\n",
    "        \"\"\"Number of maximum epochs to run for the training\"\"\"\n",
    "        return self._num_epochs\n",
    "\n",
    "    @property\n",
    "    def output_directory(self):\n",
    "        \"\"\"Parent directory to manage training artefacts\"\"\"\n",
    "        return self._output_directory\n",
    "\n",
    "    @property\n",
    "    def model_directory(self):\n",
    "        \"\"\"Directory to save the trained models\"\"\"\n",
    "        return self._model_directory\n",
    "\n",
    "    @property\n",
    "    def log_directory(self):\n",
    "        \"\"\"Directory to save logs, e.g. TensorBoard logs\"\"\"\n",
    "        return self._log_directory\n",
    "\n",
    "    @property\n",
    "    def model_metric_names(self):\n",
    "        \"\"\"Model mtrics\n",
    "        The attribute model.metrics_names gives labels for the scalar metrics\n",
    "        to be returned from model.evaluate().\n",
    "        \"\"\"\n",
    "        return self.model.metrics_names\n",
    "\n",
    "    @property\n",
    "    def history(self):\n",
    "        \"\"\"The history object returned from model.fit(). \n",
    "        The object holds a record of the loss and metric during training\n",
    "        \"\"\"\n",
    "        assert self._history is not None\n",
    "        return self._history\n",
    "\n",
    "    @property\n",
    "    def trainer(self):\n",
    "        \"\"\"HuggingFace trainer instance\n",
    "        HuggingFace offers an optimized Trainer because PyTorch does not have\n",
    "        the training loop as Keras/Model has. It is available for TensorFlow\n",
    "        as well, hence to be able to hold the instance in case using it.\n",
    "        \"\"\"\n",
    "        return self._trainer\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Instance initialization\n",
    "    # --------------------------------------------------------------------------------\n",
    "    def __init__(\n",
    "            self,\n",
    "            category,\n",
    "            training_data,\n",
    "            training_label,\n",
    "            validation_data,\n",
    "            validation_label,\n",
    "            num_labels=2,\n",
    "            max_sequence_length=256,\n",
    "            freeze_pretrained_base_model=False,\n",
    "            batch_size=16,\n",
    "            learning_rate=5e-5,\n",
    "            early_stop_patience=5,\n",
    "            reduce_lr_patience=2,\n",
    "            num_epochs=3,\n",
    "            output_directory=\"./output\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            category: \n",
    "            traininig_data: \n",
    "            training_label:\n",
    "            validation_data:\n",
    "            validation_label:\n",
    "            num_labels: Number of labels\n",
    "            max_sequence_length=256: maximum tokens for tokenizer\n",
    "            freeze_pretrained_base_model: flag to freeze pretrained model base layer\n",
    "            batch_size:\n",
    "            learning_rate:\n",
    "            early_stop_patience:\n",
    "            reduce_lr_patience:\n",
    "            num_epochs:\n",
    "            output_directory: Directory to save the outputs\n",
    "        \"\"\"\n",
    "        self._category = category\n",
    "        self._trainer = None\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Model training configurations\n",
    "        # --------------------------------------------------------------------------------\n",
    "        assert 128 <= max_sequence_length <= 512, \"Current max sequenth length is 512\"\n",
    "        self._max_sequence_length = max_sequence_length\n",
    "\n",
    "        assert num_labels > 0\n",
    "        self._num_labels = num_labels\n",
    "\n",
    "        assert isinstance(freeze_pretrained_base_model, bool)\n",
    "        self._freeze_pretrained_base_model = freeze_pretrained_base_model\n",
    "\n",
    "        assert learning_rate > 0.0\n",
    "        self._learning_rate = learning_rate\n",
    "        self._model = None\n",
    "\n",
    "        assert num_epochs > 0\n",
    "        self._num_epochs = num_epochs\n",
    "\n",
    "        assert batch_size > 0\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        assert early_stop_patience > 0\n",
    "        self._early_stop_patience = early_stop_patience\n",
    "        self._reduce_lr_patience = reduce_lr_patience\n",
    "\n",
    "        # model.fit() result holder\n",
    "        self._history = None  \n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Output directories\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Parent directory\n",
    "        self._output_directory = output_directory\n",
    "        Path(self.output_directory).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Model directory\n",
    "        self._model_directory = \"{parent}/model_C{category}_B{size}_L{length}\".format(\n",
    "            parent=self.output_directory,\n",
    "            category=self.category,\n",
    "            size=self.batch_size,\n",
    "            length=self.max_sequence_length\n",
    "        )\n",
    "        Path(self.model_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Log directory\n",
    "        self._log_directory = \"{parent}/log_C{category}_B{size}_L{length}\".format(\n",
    "            parent=self.output_directory,\n",
    "            category=self.category,\n",
    "            size=self.batch_size,\n",
    "            length=self.max_sequence_length\n",
    "        )\n",
    "        Path(self.log_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # TensorFlow DataSet\n",
    "        # --------------------------------------------------------------------------------\n",
    "        assert np.all(np.isin(training_label, np.arange(self.num_labels)))\n",
    "        assert np.all(np.isin(validation_label, np.arange(self.num_labels)))\n",
    "        self._X = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(self.tokenize(training_data)),\n",
    "            training_label\n",
    "        ))\n",
    "        self._V = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(self.tokenize(validation_data)),\n",
    "            validation_label\n",
    "        ))\n",
    "        del training_data, validation_data\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Model\n",
    "        # --------------------------------------------------------------------------------\n",
    "        config_file = self.model_directory + os.path.sep + \"config.json\"\n",
    "        if os.path.isfile(config_file) and os.access(config_file, os.R_OK):\n",
    "            # Load the saved model\n",
    "            print(f\"loading the saved model from {self.model_directory}...\")\n",
    "            self._pretrained_model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "                self.model_directory,\n",
    "                num_labels=num_labels\n",
    "            )\n",
    "        else:\n",
    "            # Download the model from Huggingface\n",
    "            self._pretrained_model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "                self.model_name,\n",
    "                num_labels=num_labels,            \n",
    "            )\n",
    "\n",
    "        # Freeze base model if required\n",
    "        if self.freeze_pretrained_base_model:\n",
    "            for _layer in self._pretrained_model.layers:\n",
    "                if _layer.name == self.model_base_name:\n",
    "                    _layer.trainable = False\n",
    "\n",
    "        self._model = self._pretrained_model\n",
    "\n",
    "        # The number of classes in the output must match the num_labels\n",
    "        _output = self._pretrained_model(self.tokenize([\"i say hello\"]))\n",
    "        assert _output['logits'].shape[-1] == self.num_labels, \"Number of labels mismatch\"\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Build the model\n",
    "        #     from_logits in SparseCategoricalCrossentropy(from_logits=[True|False])\n",
    "        #     True  when the input is logits not  normalized by softmax.\n",
    "        #     False when the input is probability normalized by softmax\n",
    "        # --------------------------------------------------------------------------------\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        self.model.compile(\n",
    "            optimizer=optimizer, \n",
    "            # loss=self.model.compute_loss,\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            # loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "            # [\"accuracy\", \"AUC\"] causes an error:\n",
    "            # ValueError: Shapes (None, 1) and (None, 2) are incompatible\n",
    "            metrics = [\"accuracy\"]  \n",
    "        )\n",
    "        self.model.summary()\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Instance methods\n",
    "    # --------------------------------------------------------------------------------\n",
    "    def tokenize(self, sentences, truncation=True, padding='longest'):\n",
    "        \"\"\"Tokenize using the Huggingface tokenizer\n",
    "        Args: \n",
    "            sentences: String or list of string to tokenize\n",
    "            padding: Padding method ['do_not_pad'|'longest'|'max_length']\n",
    "        \"\"\"\n",
    "        return self.tokenizer(\n",
    "            sentences,\n",
    "            truncation=truncation,\n",
    "            padding=padding,\n",
    "            max_length=self.max_sequence_length,\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return tokenizer.decode(tokens)\n",
    "\n",
    "    def _hf_train(self):\n",
    "        \"\"\"Train the model using HuggingFace Trainer\"\"\"\n",
    "        self._training_args = TFTrainingArguments(\n",
    "            output_dir='./results',             # output directory\n",
    "            num_train_epochs=3,                 # total number of training epochs\n",
    "            per_device_train_batch_size=self.batch_size,     # batch size per device during training\n",
    "            per_device_eval_batch_size=self.batch_size,      # batch size for evaluation\n",
    "            warmup_steps=500,                   # number of warmup steps for learning rate scheduler\n",
    "            weight_decay=0.01,                  # strength of weight decay\n",
    "            logging_dir='./logs',               # directory for storing logs\n",
    "            logging_steps=10,\n",
    "        )\n",
    "\n",
    "        # with self._training_args.strategy.scope():\n",
    "        #     self._model = TFDistilBertForSequenceClassification.from_pretrained(self.model_name)\n",
    "\n",
    "        self._trainer = TFTrainer(\n",
    "            model=self.model,\n",
    "            args=self._training_args,   # training arguments\n",
    "            train_dataset=self.X,       # training dataset\n",
    "            eval_dataset=self.V         # evaluation dataset\n",
    "        )\n",
    "        self.trainer.train()\n",
    "\n",
    "    def _keras_train(self):\n",
    "        \"\"\"Train the model using Keras\n",
    "        \"\"\"\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Train the model\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self._history = self.model.fit(\n",
    "            self.X.shuffle(1000).batch(self.batch_size).prefetch(1),\n",
    "            epochs=self.num_epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            validation_data=self.V.shuffle(1000).batch(self.batch_size).prefetch(1),\n",
    "            callbacks=[\n",
    "                EarlyStoppingCallback(patience=self.early_stop_patience),\n",
    "                ReduceLRCallback(patience=self.reduce_lr_patience),\n",
    "                TensorBoardCallback(self.log_directory),\n",
    "                SavePretrainedCallback(output_dir=self.model_directory, verbose=True),\n",
    "            ]\n",
    "        )\n",
    "        # del self._X, self._V\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Run the model trainig\"\"\"\n",
    "        if self.USE_HF_TRAINER:\n",
    "            self._hf_train()\n",
    "        else:\n",
    "            self._keras_train()\n",
    "\n",
    "    def evaluate(self, data, label):\n",
    "        \"\"\"Evaluate the model on the given data and label.\n",
    "        https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate\n",
    "        The attribute model.metrics_names gives labels for the scalar metrics\n",
    "        to be returned from model.evaluate().\n",
    "\n",
    "        Args:\n",
    "            data: data to run the prediction\n",
    "            label: label for the data\n",
    "        Returns: \n",
    "            scalar loss if the model has a single output and no metrics, OR \n",
    "            list of scalars (if the model has multiple outputs and/or metrics). \n",
    "        \"\"\"\n",
    "        assert np.all(np.isin(label, np.arange(self.num_labels)))\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(self.tokenize(data)),\n",
    "            label\n",
    "        ))\n",
    "        evaluation = self.model.evaluate(\n",
    "            test_dataset.shuffle(1000).batch(self.batch_size).prefetch(1)\n",
    "        )\n",
    "        return evaluation\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"Calcuate the prediction for the data\n",
    "        Args:\n",
    "            data: text data to classify\n",
    "        Returns: Probabilities for label value 0 and 1\n",
    "        \"\"\"\n",
    "        tokens = dict(self.tokenizer(\n",
    "            data,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=self.max_sequence_length,\n",
    "            return_tensors=\"tf\"\n",
    "        ))\n",
    "        logits = self.model.predict(tokens)[\"logits\"]\n",
    "        return tf.nn.softmax(logits)\n",
    "        # return logits\n",
    "\n",
    "    def save(self, path_to_dir=None):\n",
    "        \"\"\"Save the model from the HuggingFace. \n",
    "        - config.json \n",
    "        - tf_model.h5  \n",
    "\n",
    "        Args:\n",
    "            path_to_dir: directory path to save the HuggingFace model artefacts\n",
    "        \"\"\"\n",
    "\n",
    "        if path_to_dir is None or len(path_to_dir) == 0:\n",
    "            path_to_dir = self.model_directory\n",
    "        Path(path_to_dir).mkdir(parents=True, exist_ok=True)\n",
    "        if self.USE_HF_TRAINER:\n",
    "            self.trainer.save_model(path_to_dir)  \n",
    "        else:\n",
    "            # TODO: \n",
    "            #   save_pretrained() method is in the HuggingFace model only.\n",
    "            #   Need to update for custom model saving.\n",
    "            self.model.save_pretrained(path_to_dir)\n",
    "\n",
    "    def load(self, path_to_dir):\n",
    "        \"\"\"Load the model as the HuggingFace format.\n",
    "        Args:\n",
    "            path_to_dir: Directory path from where to load config.json and .h5.\n",
    "        \"\"\"\n",
    "        if os.path.isdir(path_to_dir) and os.access(path_to_dir, os.R_OK):\n",
    "            self._model = TFDistilBertForSequenceClassification.from_pretrained(path_to_dir)\n",
    "        else:\n",
    "            raise RuntimeError(f\"{path_to_dir} does not exit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "z0xIlT_ugzL4"
   },
   "outputs": [],
   "source": [
    "def balance(\n",
    "    df, \n",
    "    data_col_name,\n",
    "    label_col_name,\n",
    "    retain_columns,\n",
    "    max_replication_ratio=sys.maxsize\n",
    "):\n",
    "    \"\"\"Balance the data volumes of positives and negatives\n",
    "    The negatives (label==0) has more volume than the positives has, hence\n",
    "    causing skewed data representation. To avoid the model from adapting to the\n",
    "    majority (negative), naively balance the volumes so that they have same size.\n",
    "\n",
    "    For the ratio = (negatives / positives), replicate positives 'ratio' times \n",
    "    to match the volume of negatives if ratio < max_replication_ratio.\n",
    "    When ratio > max_replication_ratio, replicate max_replication_ratio times\n",
    "    to the size = (positive_size * max_replication_ratio). Then take 'size'\n",
    "    volume randomly from negatives.\n",
    "\n",
    "    A portion of the negatives will not be used because of this balancing.\n",
    "\n",
    "    Args:\n",
    "        df: Pandas dataframe \n",
    "        data_col_name: Column name for the data\n",
    "        label_col_name: Column name for the label\n",
    "        retain_columns: Columns to retain in the dataframe to return\n",
    "    Returns: \n",
    "        Pandas dataframe with the ratin_columns.\n",
    "    \"\"\"\n",
    "    positive_indices = df.index[df[label_col_name]==1].tolist()\n",
    "    negative_indices = df.index[df[label_col_name]==0].tolist()\n",
    "    assert not bool(set(positive_indices) & set(negative_indices))\n",
    "\n",
    "    positive_size = len(positive_indices)\n",
    "    negative_size = len(negative_indices)\n",
    "    ratio = np.minimum(negative_size // positive_size, max_replication_ratio)\n",
    "\n",
    "    if ratio >= 2:\n",
    "        # Generate equal size of indices for positives and negatives. \n",
    "        target_positive_indices = ratio * positive_indices\n",
    "        target_negative_indices = np.random.choice(\n",
    "            a=negative_indices, \n",
    "            size=ratio * positive_size,\n",
    "            replace=False\n",
    "        ).tolist()\n",
    "        indices = target_positive_indices + target_negative_indices\n",
    "\n",
    "        # Extract [data, label] with equal size of positives and negatives\n",
    "        data = df.iloc[indices][\n",
    "            df.columns[df.columns.isin(retain_columns)]\n",
    "        ]\n",
    "\n",
    "    else: \n",
    "        data = df[\n",
    "            df.columns[df.columns.isin(retain_columns)]\n",
    "        ]\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_runner(\n",
    "    train,\n",
    "    category,\n",
    "    max_sequence_length,\n",
    "    freeze_pretrained_base_model,\n",
    "    num_labels,\n",
    "    batch_size,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    early_stop_patience,\n",
    "    reduce_lr_patience,\n",
    "    output_directory,\n",
    "    max_replication_ratio = sys.maxsize\n",
    "):\n",
    "    \"\"\"Wrapper to create the Runnler instances for the respective category\n",
    "    Args:\n",
    "        train: Pandas dataframe containing entire training data\n",
    "        category: unhealthy comment category, e.g. 'toxic'\n",
    "        max_sequence_length:\n",
    "        batch_size:\n",
    "        num_epochs:\n",
    "        learning_rate:\n",
    "        early_stop_patience:\n",
    "        reduce_lr_patience:\n",
    "        output_directory:\n",
    "        max_replication_ratio: ratio up to which to replicate the skewed volume\n",
    "    \"\"\"\n",
    "    print(\"\\n--------------------------------------------------------------------------------\")\n",
    "    print(f\"Build runner for [{category}]\")\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "    balanced = balance(\n",
    "        df=train, \n",
    "        data_col_name='comment_text', \n",
    "        label_col_name=category,\n",
    "        retain_columns=['id', 'comment_text', category]\n",
    "    )\n",
    "    data = balanced['comment_text'].tolist()\n",
    "    label = balanced[category].tolist()\n",
    "    del balanced\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Split data into training and validation\n",
    "    # --------------------------------------------------------------------------------\n",
    "    train_data, validation_data, train_label, validation_label = train_test_split(\n",
    "        data,\n",
    "        label,\n",
    "        test_size=.2,\n",
    "        shuffle=True\n",
    "    )\n",
    "    del data, label\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Instantiate the model trainer\n",
    "    # --------------------------------------------------------------------------------\n",
    "    runner = Runner(\n",
    "        category=category,\n",
    "        training_data=train_data,\n",
    "        training_label=train_label,\n",
    "        validation_data=validation_data,\n",
    "        validation_label=validation_label,\n",
    "        max_sequence_length=max_sequence_length,\n",
    "        freeze_pretrained_base_model=freeze_pretrained_base_model,\n",
    "        num_labels=num_labels,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        early_stop_patience=early_stop_patience,\n",
    "        reduce_lr_patience=reduce_lr_patience,\n",
    "        output_directory=output_directory\n",
    "    )\n",
    "    return runner\n",
    "    \n",
    "def generate_category_runner(category):\n",
    "    def f(train):\n",
    "        return generate_runner(\n",
    "            category=category,\n",
    "            train=train,\n",
    "            max_replication_ratio = sys.maxsize,\n",
    "            freeze_pretrained_base_model=FREEZE_BASE_MODEL,\n",
    "            num_labels=NUM_LABELS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            early_stop_patience=EARLY_STOP_PATIENCE,\n",
    "            reduce_lr_patience=REDUCE_LR_PATIENCE,\n",
    "            output_directory=RESULT_DIRECTORY\n",
    "        )\n",
    "    return f\n",
    "\n",
    "def evaluate(runner, test):\n",
    "    \"\"\"\n",
    "    Evaluate the model of the runner\n",
    "    Args:\n",
    "        runner: Runner instance\n",
    "        test: Pandas dataframe holding entire data\n",
    "    \"\"\"\n",
    "    print(\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print(f\"Model evaluation on [{runner.category}]\")\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    test_data = test['comment_text'].tolist()\n",
    "    test_label = test[category].tolist()\n",
    "    evaluation = runner.evaluate(test_data, test_label)\n",
    "\n",
    "    print(f\"Evaluation: {runner.model_metric_names}:{evaluation}\")\n",
    "    del test_data, test_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wV_uiVhBNM7n"
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEANING_FOR_ANALYSIS and (not CLEANING_FOR_TRAINING):\n",
    "    # Data has been clearned but training needs non cleaned data\n",
    "    train, test = load_raw_data(TEST_MODE)\n",
    "    print(f\"Data records for training [{train['id'].count()}]\")\n",
    "\n",
    "# Drop the rows with -1. ['toxic'] >= 0 is sufficient\n",
    "test = test[test['toxic'] >= 0]\n",
    "gc.collect()\n",
    "\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGFHYUT2gzL3",
    "outputId": "c4c1f98e-6828-4cd0-de1e-f57ef1bce82f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAX_SEQUENCE_LENGTH = 256\n",
      "FREEZE_BASE_MODEL = False\n",
      "NUM_EPOCHS = 10\n",
      "BATCH_SIZE = 32\n",
      "LEARNING_RATE = 5e-05\n",
      "REDUCE_LR_PATIENCE = 3\n",
      "EARLY_STOP_PATIENCE = 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace\n",
    "MAX_SEQUENCE_LENGTH = 256   # Max token length to accept. 512 taks 1 hour/epoch on Google Colab\n",
    "\n",
    "# Model training\n",
    "NUM_LABELS = 2\n",
    "FREEZE_BASE_MODEL = False\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 5e-5  # Must be small to avoid catastrophic forget\n",
    "REDUCE_LR_PATIENCE = 3\n",
    "EARLY_STOP_PATIENCE = 5\n",
    "\n",
    "print(\"\"\"\n",
    "MAX_SEQUENCE_LENGTH = {}\n",
    "FREEZE_BASE_MODEL = {}\n",
    "NUM_EPOCHS = {}\n",
    "BATCH_SIZE = {}\n",
    "LEARNING_RATE = {}\n",
    "REDUCE_LR_PATIENCE = {}\n",
    "EARLY_STOP_PATIENCE = {}\n",
    "\"\"\".format(\n",
    "    MAX_SEQUENCE_LENGTH,\n",
    "    FREEZE_BASE_MODEL,\n",
    "    NUM_EPOCHS,\n",
    "    BATCH_SIZE,\n",
    "    LEARNING_RATE,\n",
    "    REDUCE_LR_PATIENCE,\n",
    "    EARLY_STOP_PATIENCE\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "IMjy-emngzL4"
   },
   "outputs": [],
   "source": [
    "# runners = {}      # To save the Runner instance for each category.\n",
    "# evaluations = {}  # Evaluation results for each category\n",
    "#     for category in CATEGORIES:\n",
    "#         runners[category], evaluations[category] = run(\n",
    "#             category=category, \n",
    "#             train=train,\n",
    "#             test=test,\n",
    "#             batch_size=BATCH_SIZE,\n",
    "#             max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "#             num_epochs=NUM_EPOCHS,\n",
    "#             learning_rate=LEARNING_RATE,\n",
    "#             early_stop_patience=EARLY_STOP_PATIENCE,\n",
    "#             reduce_lr_patience=REDUCE_LR_PATIENCE,\n",
    "#             output_directory=RESULT_DIRECTORY\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Y3KFBkjcknCD",
    "o-Oj3ltY_T8m",
    "lZZbTtUGWx9S",
    "Qpx-pzGWasel",
    "gcefqEMkA2IV",
    "7UPio2cmTcWO",
    "QcngPOCBW4R1",
    "_PFau2osjvx-",
    "k0jSqXxxQrUj",
    "1wwZFMlD6pIJ",
    "nUYPD0TQvEaI",
    "pFWPmiCkaVsN",
    "ShId8bvTGlER",
    "s4jNfuchQiPy",
    "f-RWZFJj8lJZ",
    "5bp2VT2ohkKD",
    "w0Rpsc7IjWFd",
    "XJyvUG3cGylz",
    "SvZ2Z7JQ9jsS"
   ],
   "machine_shape": "hm",
   "name": "toxic_comment_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
