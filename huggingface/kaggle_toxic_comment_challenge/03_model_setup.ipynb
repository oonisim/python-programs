{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdZo50sxLBuX"
   },
   "source": [
    "---\n",
    "# Technology Selection\n",
    "\n",
    "What algorithms to use and what technologies are available.\n",
    "\n",
    "## Nature of the problem\n",
    "\n",
    "* Multi-label Binary Classification \n",
    "\n",
    "It is a binary classification task where multiple althorithms have been developed and applied in the real life e.g. SPAM fileter.\n",
    "\n",
    "* Naive Bayes - [Naive Bayes and Text Classification](https://arxiv.org/abs/1410.5329)\n",
    "* CNN - [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf)\n",
    "* DNN Language Model - [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "## Asssessment\n",
    "\n",
    "Skipped due to the time constraint.\n",
    "\n",
    "## Decision\n",
    "\n",
    "**Transformer Deep Neural Network Architecture** transfer-learning (fine-tuning) on the pre-trained language model.\n",
    "\n",
    "1. State of the art algorithms being actively researched.\n",
    "2. Pre-trained models for text classification e.g text sentiment analysis are available. \n",
    "3. Other well-explored althorithms have been well tested as published in Kaggle. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOtHJ1edOFoS"
   },
   "source": [
    "---\n",
    "# Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rB2cjqCZhcE9"
   },
   "source": [
    "## ML Model for Fine Tuning\n",
    "\n",
    "### Framework\n",
    "* Google TensorFlow 2.x \n",
    "* Keras for training the model\n",
    "* Huggingface Transformer library\n",
    "\n",
    "### Data allocation\n",
    "Utilize [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to :\n",
    "1. shuffle the train data \n",
    "2. allocate the ratio R of the data for validation. R=0.2\n",
    "3 apply the model training on (1-R) ratio of the data for training\n",
    "\n",
    "Apply the trained model on testing data for evaluation.\n",
    "\n",
    "### Hyper parameter search\n",
    "* Learning rate (5e-5, 5e-4, 5e-3) as the start value\n",
    "\n",
    "### Epoch\n",
    "Number of times to go through the entire training data set N. N=10 due to the time constraint.\n",
    "\n",
    "### Early stopping\n",
    "Utilize Keras [EarlyStopping](https://keras.io/api/callbacks/early_stopping/) to stop the training when no improvement is achieved N times. N=5.\n",
    "\n",
    "### Reduce learning rate at no improvement\n",
    "Utilize Keras [ReduceLROnPlateau](https://keras.io/api/callbacks/reduce_lr_on_plateau/) to reduce the learning rate when no improvement is achieved N times. N=3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bp2VT2ohkKD"
   },
   "source": [
    "### Keras Callbacks\n",
    "\n",
    "Utilize [Keras Callbacks API](https://keras.io/api/callbacks/) to apply Eary Stopping, Reduce Learning Rate, and TensorBoard during the model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Fc67_HTf0zyO"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from keras import backend\n",
    "\n",
    "\n",
    "class ROCCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"calculate ROC&AUC\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        validation_data, \n",
    "        validation_label, \n",
    "        output_path, \n",
    "        reduce_lr_patience = 3,\n",
    "        reduce_lr_factor = 0.2,\n",
    "        early_stop_patience=sys.maxsize, \n",
    "        verbose=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert 0.0 < reduce_lr_factor < 1.0\n",
    "        assert 0 < reduce_lr_patience\n",
    "        assert 0 < early_stop_patience        \n",
    "\n",
    "        self.x = validation_data\n",
    "        self.y = validation_label\n",
    "        self.output_path = output_path\n",
    "        self.reduce_lr_patience = reduce_lr_patience\n",
    "        self.reduce_lr_factor = reduce_lr_factor\n",
    "        self.early_stop_patience = early_stop_patience\n",
    "        self.verbose = verbose    \n",
    "\n",
    "        self.max_value = 0\n",
    "        self.best_epoch = -1\n",
    "        self.successive_no_improvement = 0\n",
    "        self.total_no_improvement = 0\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def _reduce_learning_rate(self):\n",
    "        old_lr = backend.get_value(self.model.optimizer.lr)\n",
    "        new_lr = old_lr * self.reduce_lr_factor\n",
    "        backend.set_value(self.model.optimizer.lr, new_lr)\n",
    "        self.successive_no_improvement = 0\n",
    "        if self.verbose:\n",
    "            print(f\"Reducing learning rate to {new_lr}.\")\n",
    "            \n",
    "    def _early_stop(self):\n",
    "        self.model.stop_training = True\n",
    "        self.total_no_improvement = 0\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                \"Early stopping: total no improvement [%s] times. best epoch [%s] AUC [%s]\" % \n",
    "                (self.total_no_improvement, self.best_epoch, self.max_value)\n",
    "            )\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # [print(f\"{k}:{v}\") for k, v in logs.items()]\n",
    "        predictions = self.model.predict(self.x)\n",
    "        score = roc_auc_score(self.y, predictions)\n",
    "\n",
    "        if (self.best_epoch < 0) or (score > self.max_value):\n",
    "            # --------------------------------------------------------------------------------\n",
    "            # Save the model upon improvement\n",
    "            # --------------------------------------------------------------------------------\n",
    "            if self.verbose:\n",
    "                print(f\"Model auc improved [{score} > {self.max_value:0.5f}]: Saving to {self.output_path}\")\n",
    "\n",
    "            self.max_value = score\n",
    "            self.best_epoch = epoch\n",
    "            self.model.save_weights(\n",
    "                self.output_path, overwrite=True, save_format='h5'\n",
    "            )\n",
    "\n",
    "            # Reset counters\n",
    "            self.successive_no_improvement = 0\n",
    "            \n",
    "            # The AUC can not improve than 1.0. Early stop.\n",
    "            if score > (1.0 - 1e-10):\n",
    "                self._early_stop()\n",
    "                if self.verbose:\n",
    "                    print(\"Stopped as no AUC improvement can be made beyond 1.0\")\n",
    " \n",
    "        else:\n",
    "            print(f\"AUC did not improve from [{self.max_value:0.5f}].\")\n",
    "            # --------------------------------------------------------------------------------\n",
    "            # Reduce LR\n",
    "            # --------------------------------------------------------------------------------\n",
    "            self.successive_no_improvement += 1\n",
    "            if self.successive_no_improvement >= self.reduce_lr_patience:\n",
    "                self._reduce_learning_rate()\n",
    "                \n",
    "            # --------------------------------------------------------------------------------\n",
    "            # Early Stop\n",
    "            # --------------------------------------------------------------------------------\n",
    "            self.total_no_improvement += 1\n",
    "            if self.total_no_improvement >= self.early_stop_patience:\n",
    "                self._early_stop()\n",
    "        \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "\n",
    "class SavePretrainedCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    This is only for directly working on the Huggingface models.\n",
    "    \n",
    "    Hugging Face models have a save_pretrained() method that saves both \n",
    "    the weights and the necessary metadata to allow them to be loaded as \n",
    "    a pretrained model in future. This is a simple Keras callback that \n",
    "    saves the model with this method after each epoch.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, output_dir, monitor='val_loss', mode='auto', verbose=True):\n",
    "        super().__init__()\n",
    "        self.output_dir = output_dir\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.lowest_val_loss=np.inf\n",
    "        self.best_epoch = -1\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \"\"\"\n",
    "        Save only the best model\n",
    "        - https://stackoverflow.com/a/68042600/4281353\n",
    "        - https://www.tensorflow.org/guide/keras/custom_callback\n",
    "        \n",
    "        TODO: \n",
    "        save_pretrained() method is in the HuggingFace model only.\n",
    "        Need to implement an logic to update for Keras model saving.\n",
    "        \"\"\"\n",
    "        assert self.monitor in logs, \\\n",
    "            f\"monitor metric {self.monitor} not in valid metrics {logs.keys()}\"\n",
    "        \n",
    "        val_loss = logs.get(self.monitor)\n",
    "        if (self.best_epoch < 0) or (val_loss < self.lowest_val_loss):\n",
    "            if self.verbose:\n",
    "                print(f\"Model val_loss improved: [{val_loss} < {self.lowest_val_loss}]\")\n",
    "                print(f\"Saving to {self.output_dir}\")\n",
    "            self.lowest_val_loss = val_loss\n",
    "            self.best_epoch = epoch\n",
    "            self.model.save_pretrained(self.output_dir)\n",
    "\n",
    "\n",
    "class TensorBoardCallback(tf.keras.callbacks.TensorBoard):\n",
    "    \"\"\"TensorBoard visualization of the model training\n",
    "    See https://keras.io/api/callbacks/tensorboard/\n",
    "    \"\"\"\n",
    "    def __init__(self, output_directory):\n",
    "        super().__init__(\n",
    "            log_dir=output_directory,\n",
    "            write_graph=True,\n",
    "            write_images=True,\n",
    "            histogram_freq=1,     # log histogram visualizations every 1 epoch\n",
    "            embeddings_freq=1,    # log embedding visualizations every 1 epoch\n",
    "            update_freq=\"epoch\",  # every epoch\n",
    "        )\n",
    "\n",
    "class EarlyStoppingCallback(tf.keras.callbacks.EarlyStopping):\n",
    "    \"\"\"Stop training when no progress on the metric to monitor\n",
    "    https://keras.io/api/callbacks/early_stopping/\n",
    "    https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "\n",
    "    Using val_loss to monitor. \n",
    "    https://datascience.stackexchange.com/a/49594/68313\n",
    "    Prefer the loss to the accuracy. Why? The loss quantify how certain \n",
    "    the model is about a prediction. The accuracy merely account for \n",
    "    the number of correct predictions. Similarly, any metrics using hard \n",
    "    predictions rather than probabilities have the same problem.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=3, monitor='val_loss', mode='auto'):\n",
    "        assert patience > 0\n",
    "        super().__init__(\n",
    "            monitor=monitor, \n",
    "            mode=mode, \n",
    "            verbose=1, \n",
    "            patience=patience,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        assert self.monitor in logs, \\\n",
    "            f\"monitor metric {self.monitor} not in valid metrics {logs.keys()}\"\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "\n",
    "class ModelCheckpointCallback(tf.keras.callbacks.ModelCheckpoint):\n",
    "    \"\"\"Check point to save the model\n",
    "    See https://keras.io/api/callbacks/model_checkpoint/\n",
    "\n",
    "    NOTE: \n",
    "        Did not work with the HuggingFace native model with the error.\n",
    "        NotImplementedError: Saving the model to HDF5 format requires the model \n",
    "        to be a Functional model or a Sequential model. \n",
    "        It does not work for subclassed models, because such models are defined \n",
    "        via the body of a Python method, which isn't safely serializable.\n",
    "        \n",
    "        Did not work with the tf.keras.models.save_model nor model.save()\n",
    "        as causing out-of-index errors or load_model() failures. Hence use\n",
    "        save_weights_only=True.\n",
    "    \"\"\"\n",
    "    def __init__(self, path_to_file, monitor='val_loss', mode='auto'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path_to_file: path to the model file to save at check points\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            filepath=path_to_file, \n",
    "            monitor=monitor, \n",
    "            mode=mode, \n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,  # Cannot save entire model.\n",
    "            save_freq=\"epoch\",\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        assert self.monitor in logs, \\\n",
    "            f\"monitor metric {self.monitor} not in valid metrics {logs.keys()}\"\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "        \n",
    "\n",
    "class ReduceLRCallback(tf.keras.callbacks.ReduceLROnPlateau):\n",
    "    \"\"\"Reduce learning rate when a metric has stopped improving.\n",
    "    See https://keras.io/api/callbacks/reduce_lr_on_plateau/\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=3, monitor='val_loss', mode='auto'):\n",
    "        assert patience > 0\n",
    "        super().__init__(\n",
    "            monitor=monitor, \n",
    "            mode=mode, \n",
    "            factor=0.2,\n",
    "            patience=patience,\n",
    "            verbose=1\n",
    "        )\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        assert self.monitor in logs, \\\n",
    "            f\"monitor metric {self.monitor} not in valid metrics {logs.keys()}\"\n",
    "        super().on_epoch_end(epoch, logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0Rpsc7IjWFd"
   },
   "source": [
    "### Fine Tuning Runner\n",
    "\n",
    "The Runner class implements the fine-tuning based on the [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) pretrained model. Each classification category e.g. ```toxic``` will have a dedicated Runner class instance. The reason for using the ***Distilled*** BERT model is to run the training on the limited resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xXWNRYbes8V6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-10 13:48:20.222860: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-07-10 13:48:20.222924: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/oonisim/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3441, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_235198/2692233486.py\", line 9, in <module>\n",
      "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
      "NameError: name 'DistilBertTokenizerFast' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/oonisim/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/oonisim/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/oonisim/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/oonisim/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/oonisim/conda/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/oonisim/conda/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/oonisim/conda/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/oonisim/conda/lib/python3.8/inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/oonisim/conda/lib/python3.8/posixpath.py\", line 391, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/oonisim/conda/lib/python3.8/posixpath.py\", line 425, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/oonisim/conda/lib/python3.8/posixpath.py\", line 167, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_235198/2692233486.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistilBertTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'distilbert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DistilBertTokenizerFast' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2060\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NameError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2061\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2063\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2064\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import (\n",
    "    Sequential\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "\n",
    "class Runner:\n",
    "    \"\"\"Fine tuning implementation class\n",
    "    See:\n",
    "    - https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
    "    - https://stackoverflow.com/questions/68172891/\n",
    "    - https://stackoverflow.com/a/68172992/4281353\n",
    "\n",
    "    The TF/Keras model has the base model, e.g distilbert for DistiBERT which is\n",
    "    from the base model TFDistilBertModel.\n",
    "    https://huggingface.co/transformers/model_doc/distilbert.html#tfdistilbertmodel\n",
    "\n",
    "    TFDistilBertForSequenceClassification has classification layers added on top\n",
    "    of TFDistilBertModel, hence not required to add fine-tuning layers by users.\n",
    "    _________________________________________________________________\n",
    "    Layer (type)                 Output Shape              Param #   \n",
    "    =================================================================\n",
    "    distilbert (TFDistilBertMain multiple                  66362880  \n",
    "    _________________________________________________________________\n",
    "    pre_classifier (Dense)       multiple                  590592    \n",
    "    _________________________________________________________________\n",
    "    classifier (Dense)           multiple                  1538      \n",
    "    _________________________________________________________________\n",
    "    dropout_59 (Dropout)         multiple                  0         \n",
    "    =================================================================\n",
    "    \"\"\"\n",
    "    # ================================================================================\n",
    "    # Class\n",
    "    # ================================================================================\n",
    "    USE_HF_TRAINER = False\n",
    "    USE_CUSTOM_MODEL = True\n",
    "    USE_METRIC_AUC = False\n",
    "    TOKENIZER_LOWER_CASE = True\n",
    "    # _model_name = 'distilbert-base-cased'\n",
    "    _model_name = 'distilbert-base-uncased'\n",
    "    _model_base_name = 'distilbert'\n",
    "    _tokenizer = DistilBertTokenizerFast.from_pretrained(\n",
    "        _model_name, \n",
    "        do_lower_case=TOKENIZER_LOWER_CASE\n",
    "    )\n",
    "\n",
    "    # ================================================================================\n",
    "    # Instance\n",
    "    # ================================================================================\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Instance properties\n",
    "    # --------------------------------------------------------------------------------\n",
    "    @property\n",
    "    def category(self):\n",
    "        \"\"\"Category of the text comment classification, e.g. toxic\"\"\"\n",
    "        return self._category\n",
    "\n",
    "    @property\n",
    "    def num_labels(self):\n",
    "        \"\"\"Number of labels to classify\"\"\"\n",
    "        assert self._num_labels > 0\n",
    "        return self._num_labels\n",
    "\n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        \"\"\"BERT tokenizer. The Tokenzer must match the pretrained model\"\"\"\n",
    "        return self._tokenizer\n",
    "\n",
    "    @property\n",
    "    def max_sequence_length(self):\n",
    "        \"\"\"Maximum token length for the BERT tokenizer can accept. Max 512\n",
    "        \"\"\"\n",
    "        assert 128 <= self._max_sequence_length <= 512\n",
    "        return self._max_sequence_length\n",
    "\n",
    "    @property\n",
    "    def X(self):\n",
    "        \"\"\"Training TensorFlow DataSet\"\"\"\n",
    "        return self._X\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        \"\"\"Validation TensorFlow DataSet\"\"\"\n",
    "        return self._V\n",
    "\n",
    "    @property\n",
    "    def model_name(self):\n",
    "        \"\"\"HuggingFace pretrained model name\"\"\"\n",
    "        return self._model_name\n",
    "\n",
    "    @property\n",
    "    def model_base_name(self):\n",
    "        \"\"\"HuggingFace pretrained base model name\"\"\"\n",
    "        return self._model_base_name\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        \"\"\"TensorFlow/Keras Model instance\"\"\"\n",
    "        return self._model\n",
    "\n",
    "    @property\n",
    "    def freeze_pretrained_base_model(self):\n",
    "        \"\"\"Boolean to freeze the base model\"\"\"\n",
    "        return self._freeze_pretrained_base_model\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        \"\"\"Mini batch size during the training\"\"\"\n",
    "        assert self._batch_size > 0\n",
    "        return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        \"\"\"Training learning rate\"\"\"\n",
    "        return self._learning_rate\n",
    "\n",
    "    @property\n",
    "    def l2(self):\n",
    "        \"\"\"Regularizer decay rate\"\"\"\n",
    "        return self._l2\n",
    "\n",
    "    @property\n",
    "    def reduce_lr_patience(self):\n",
    "        \"\"\"Training patience for reducing learinig rate\"\"\"\n",
    "        return self._reduce_lr_patience\n",
    "\n",
    "    @property\n",
    "    def early_stop_patience(self):\n",
    "        \"\"\"Training patience for early stopping\"\"\"\n",
    "        return self._early_stop_patience\n",
    "\n",
    "    @property\n",
    "    def num_epochs(self):\n",
    "        \"\"\"Number of maximum epochs to run for the training\"\"\"\n",
    "        return self._num_epochs\n",
    "\n",
    "    @property\n",
    "    def output_directory(self):\n",
    "        \"\"\"Parent directory to manage training artefacts\"\"\"\n",
    "        return self._output_directory\n",
    "\n",
    "    @property\n",
    "    def model_directory(self):\n",
    "        \"\"\"Directory to save the trained models\"\"\"\n",
    "        return self._model_directory\n",
    "\n",
    "    @property\n",
    "    def log_directory(self):\n",
    "        \"\"\"Directory to save logs, e.g. TensorBoard logs\"\"\"\n",
    "        return self._log_directory\n",
    "\n",
    "    @property\n",
    "    def model_metric_names(self):\n",
    "        \"\"\"Model mtrics\n",
    "        The attribute model.metrics_names gives labels for the scalar metrics\n",
    "        to be returned from model.evaluate().\n",
    "        \"\"\"\n",
    "        return self.model.metrics_names\n",
    "\n",
    "    @property\n",
    "    def history(self):\n",
    "        \"\"\"The history object returned from model.fit(). \n",
    "        The object holds a record of the loss and metric during training\n",
    "        \"\"\"\n",
    "        assert self._history is not None\n",
    "        return self._history\n",
    "\n",
    "    @property\n",
    "    def trainer(self):\n",
    "        \"\"\"HuggingFace trainer instance\n",
    "        HuggingFace offers an optimized Trainer because PyTorch does not have\n",
    "        the training loop as Keras/Model has. It is available for TensorFlow\n",
    "        as well, hence to be able to hold the instance in case using it.\n",
    "        \"\"\"\n",
    "        return self._trainer\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Instance initialization\n",
    "    # --------------------------------------------------------------------------------\n",
    "    def build_custom_model_acc_callbacks(self):\n",
    "        \"\"\"Callbacks for accuracy\"\"\"\n",
    "        return [\n",
    "            EarlyStoppingCallback(\n",
    "                patience=self.early_stop_patience, \n",
    "                monitor=self._monitor_metric, \n",
    "                mode=self._monitor_mode\n",
    "            ),\n",
    "            ReduceLRCallback(\n",
    "                patience=self.reduce_lr_patience, \n",
    "                monitor=self._monitor_metric, \n",
    "                mode=self._monitor_mode\n",
    "            ),\n",
    "            ModelCheckpointCallback(\n",
    "                self.model_directory + os.path.sep + 'model.h5', \n",
    "                monitor=self._monitor_metric, \n",
    "                mode=self._monitor_mode\n",
    "            ),\n",
    "            TensorBoardCallback(self.log_directory),\n",
    "        ]\n",
    "\n",
    "    def build_custom_model_auc_callbacks(self, validation_data, validation_label):\n",
    "        \"\"\"Callbacks for ROC AUC\"\"\"\n",
    "        return [\n",
    "            ROCCallback(\n",
    "                validation_data=dict(self.tokenize(validation_data)), \n",
    "                validation_label=validation_label,\n",
    "                output_path=self.model_directory + os.path.sep + 'model.h5', \n",
    "                reduce_lr_patience = self.reduce_lr_patience,\n",
    "                reduce_lr_factor = 0.2,\n",
    "                early_stop_patience=self.early_stop_patience, \n",
    "                verbose=True\n",
    "            ),\n",
    "            TensorBoardCallback(self.log_directory),\n",
    "        ]\n",
    "        \n",
    "    def build_huggingface_model(self):\n",
    "        \"\"\"Build model based on TFDistilBertForSequenceClassification which has\n",
    "        classification heads added on top of the base BERT model.\n",
    "        \"\"\"\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Base model\n",
    "        # --------------------------------------------------------------------------------\n",
    "        config_file = self.model_directory + os.path.sep + \"config.json\"\n",
    "        if os.path.isfile(config_file) and os.access(config_file, os.R_OK):\n",
    "            # Load the saved model\n",
    "            print(f\"\\nloading the saved huggingface model from {self.model_directory}...\\n\")\n",
    "            self._pretrained_model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "                self.model_directory,\n",
    "                num_labels=self.num_labels\n",
    "            )\n",
    "        else:\n",
    "            # Download the model from Huggingface\n",
    "            self._pretrained_model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "                self.model_name,\n",
    "                num_labels=self.num_labels,            \n",
    "            )\n",
    "\n",
    "        # Freeze base model if required\n",
    "        if self.freeze_pretrained_base_model:\n",
    "            for _layer in self._pretrained_model.layers:\n",
    "                if _layer.name == self.model_base_name:\n",
    "                    _layer.trainable = False\n",
    "\n",
    "        self._model = self._pretrained_model\n",
    "\n",
    "        # The number of classes in the output must match the num_labels\n",
    "        _output = self._pretrained_model(self.tokenize([\"i say hello\"]))\n",
    "        assert _output['logits'].shape[-1] == self.num_labels, \"Number of labels mismatch\"\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Model Metrics\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self._metric_name = \"accuracy\"\n",
    "        self._monitor_metric = \"val_loss\"\n",
    "        self._monitor_mode = 'min'\n",
    "        self._callbacks = self.build_custom_model_acc_callbacks()\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Build the model\n",
    "        #     from_logits in SparseCategoricalCrossentropy(from_logits=[True|False])\n",
    "        #     True  when the input is logits not  normalized by softmax.\n",
    "        #     False when the input is probability normalized by softmax\n",
    "        # --------------------------------------------------------------------------------\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        self.model.compile(\n",
    "            optimizer=optimizer, \n",
    "            # loss=self.model.compute_loss,\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            # loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "            # [\"accuracy\", \"AUC\"] causes an error:\n",
    "            # ValueError: Shapes (None, 1) and (None, 2) are incompatible\n",
    "            metrics = [\"accuracy\"]  \n",
    "        )\n",
    "    \n",
    "    def build_custom_model(self, validation_data, validation_label):\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Input layer (token indices and attention masks)\n",
    "        # --------------------------------------------------------------------------------\n",
    "        input_ids = tf.keras.layers.Input(shape=(self.max_sequence_length,), dtype=tf.int32, name='input_ids')\n",
    "        attention_mask = tf.keras.layers.Input((self.max_sequence_length,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Base layer\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # TFBaseModelOutput.last_hidden_state has shape (batch_size, max_sequence_length, 768)\n",
    "        # Each sequence has [CLS]...[SEP] structure of shape (max_sequence_length, 768)\n",
    "        # Extract [CLS] embeddings of shape (batch_size, 768) as last_hidden_state[:, 0, :]\n",
    "        # --------------------------------------------------------------------------------\n",
    "        base = TFDistilBertModel.from_pretrained(\n",
    "            self.model_name,\n",
    "        )\n",
    "        # Freeze the base model weights.\n",
    "        if self.freeze_pretrained_base_model:\n",
    "            for layer in base.layers:\n",
    "                layer.trainable = False\n",
    "\n",
    "        base.summary()\n",
    "        output = base([input_ids, attention_mask]).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # TODO:\n",
    "        #    Need to verify the effect of regularizers. \n",
    "        #\n",
    "        #    [bias regularizer]\n",
    "        #    It looks bias_regularizer adjusts the ROC threshold towards 0.5. \n",
    "        #    Without it, the threshold of the ROC with BinaryCrossEntropy loss was approx 0.02.\n",
    "        #    With    it, the threshold of the ROC with BinaryCrossEntropy loss was approx 0.6.\n",
    "        # --------------------------------------------------------------------------------\n",
    "        activation = \"sigmoid\" if self.num_labels == 1 else \"softmax\"\n",
    "        output = tf.keras.layers.Dense(\n",
    "            units=self.num_labels,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(l2=self.l2),\n",
    "            bias_regularizer=tf.keras.regularizers.l2(l2=self.l2),\n",
    "            activity_regularizer=tf.keras.regularizers.l2(l2=self.l2/10.0),\n",
    "            activation=activation,\n",
    "            name=activation\n",
    "        )(output)        \n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Loss layer\n",
    "        # --------------------------------------------------------------------------------\n",
    "        if self.num_labels == 1:    # Binary classification\n",
    "            loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        else:                       # Categorical classification\n",
    "            loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Model Metrics\n",
    "        # --------------------------------------------------------------------------------\n",
    "        if self.num_labels == 1:\n",
    "            if self.USE_METRIC_AUC:             # ROC/AUC\n",
    "                # AUC is for Binary Classification. Error if used for categorical\"\n",
    "                # \"alueError: Shapes (None, <num_classes>) and (None, 1) are incompatible\"\n",
    "                # Because AUC is expecting shape(None, 1) as binary input into the loss fn.\n",
    "                self._metric_name = \"auc\"\n",
    "                self._monitor_metric = f\"val_{self._metric_name}\"\n",
    "                self._monitor_mode = 'max'\n",
    "                self._metrics=[tf.keras.metrics.AUC(from_logits=False, name=self._metric_name), \"accuracy\"]\n",
    "                self._callbacks = self.build_custom_model_auc_callbacks(\n",
    "                    validation_data, \n",
    "                    validation_label\n",
    "                )\n",
    "            else:\n",
    "                self._metric_name = \"recall\"    # Recall\n",
    "                self._monitor_metric = f\"val_{self._metric_name}\"\n",
    "                self._monitor_mode = 'auto'\n",
    "                self._metrics=[tf.keras.metrics.Recall(name=self._metric_name), \"accuracy\"]\n",
    "                self._callbacks = self.build_custom_model_acc_callbacks()\n",
    "                \n",
    "        else:                       # Validation loss\n",
    "            self._metric_name = \"accuracy\"\n",
    "            self._monitor_metric = \"val_loss\"\n",
    "            self._monitor_mode = 'min'\n",
    "            # metrics=[tf.keras.metrics.Accuracy(name=metric_name)]\n",
    "            self._metrics=[self._metric_name]\n",
    "            self._callbacks = self.build_custom_model_acc_callbacks()\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Build model\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # TODO: Replace TIMESTAMP with instance variable\n",
    "        name = f\"{TIMESTAMP}_{self.model_name.upper()}\"\n",
    "        self._model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output, name=name)\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
    "            loss=loss_fn,\n",
    "            metrics=self._metrics\n",
    "        )\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Load model parameters if the saved weight file exits\n",
    "        # --------------------------------------------------------------------------------\n",
    "        path_to_h5 = self.model_directory + os.path.sep + \"model.h5\"\n",
    "        if os.path.isfile(path_to_h5) and os.access(path_to_h5, os.R_OK):\n",
    "            print(f\"\\nloading the saved model parameters from {path_to_h5}...\\n\")\n",
    "            self.model.load_weights(path_to_h5)\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            category,\n",
    "            training_data,\n",
    "            training_label,\n",
    "            validation_data,\n",
    "            validation_label,\n",
    "            num_labels=2,\n",
    "            max_sequence_length=256,\n",
    "            freeze_pretrained_base_model=False,\n",
    "            batch_size=16,\n",
    "            learning_rate=5e-5,\n",
    "            l2=1e-3,\n",
    "            metric_name = \"accuracy\",\n",
    "            monitor_metric='val_loss',\n",
    "            monitor_mode='min',\n",
    "            early_stop_patience=5,\n",
    "            reduce_lr_patience=2,\n",
    "            num_epochs=3,\n",
    "            output_directory=\"./output\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            category: \n",
    "            training_data: \n",
    "            training_label:\n",
    "            validation_data:\n",
    "            validation_label:\n",
    "            num_labels: Number of labels\n",
    "            max_sequence_length=256: maximum tokens for tokenizer\n",
    "            freeze_pretrained_base_model: flag to freeze pretrained model base layer\n",
    "            batch_size:\n",
    "            learning_rate:\n",
    "            l2: L2 regularizer decay rate\n",
    "            metric_name: metric for the model\n",
    "            monitor_metric: metric to monitor for callbacks\n",
    "            monitor_mode: auto|min|max\n",
    "            early_stop_patience:\n",
    "            reduce_lr_patience:\n",
    "            num_epochs:\n",
    "            output_directory: Directory to save the outputs\n",
    "        \"\"\"\n",
    "        self._category = category\n",
    "        self._trainer = None\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Model training configurations\n",
    "        # --------------------------------------------------------------------------------\n",
    "        assert 128 <= max_sequence_length <= 512, \"Current max sequenth length is 512\"\n",
    "        self._max_sequence_length = max_sequence_length\n",
    "\n",
    "        assert num_labels > 0\n",
    "        self._num_labels = num_labels\n",
    "\n",
    "        assert isinstance(freeze_pretrained_base_model, bool)\n",
    "        self._freeze_pretrained_base_model = freeze_pretrained_base_model\n",
    "\n",
    "        assert (0.0 < learning_rate) and (0 <= l2 < 1.0)\n",
    "        self._learning_rate = learning_rate\n",
    "        self._l2 = l2\n",
    "        self._model = None\n",
    "\n",
    "        assert num_epochs > 0\n",
    "        self._num_epochs = num_epochs\n",
    "\n",
    "        assert batch_size > 0\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        assert early_stop_patience > 0\n",
    "        self._metric_name = metric_name\n",
    "        self._monitor_metric = monitor_metric\n",
    "        self._monitor_mode = monitor_mode\n",
    "        self._early_stop_patience = early_stop_patience\n",
    "        self._reduce_lr_patience = reduce_lr_patience\n",
    "\n",
    "        # model.fit() result holder\n",
    "        self._history = None  \n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Output directories\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Parent directory\n",
    "        self._output_directory = output_directory\n",
    "        Path(self.output_directory).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Model directory\n",
    "        self._model_directory = \"{parent}/model_C{category}_B{size}_L{length}\".format(\n",
    "            parent=self.output_directory,\n",
    "            category=self.category,\n",
    "            size=self.batch_size,\n",
    "            length=self.max_sequence_length\n",
    "        )\n",
    "        Path(self.model_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Log directory\n",
    "        self._log_directory = \"{parent}/log_C{category}_B{size}_L{length}\".format(\n",
    "            parent=self.output_directory,\n",
    "            category=self.category,\n",
    "            size=self.batch_size,\n",
    "            length=self.max_sequence_length\n",
    "        )\n",
    "        Path(self.log_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # TensorFlow DataSet\n",
    "        # --------------------------------------------------------------------------------\n",
    "        if self.num_labels == 1:\n",
    "            assert np.all(np.isin(training_label, [0,1]))\n",
    "            assert np.all(np.isin(validation_label, [0,1]))\n",
    "        else:\n",
    "            assert np.all(np.isin(training_label, np.arange(self.num_labels)))\n",
    "            assert np.all(np.isin(validation_label, np.arange(self.num_labels)))\n",
    "\n",
    "        self._X = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(self.tokenize(training_data)),\n",
    "            training_label\n",
    "        ))\n",
    "        self._V = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(self.tokenize(validation_data)),\n",
    "            validation_label\n",
    "        ))\n",
    "        del training_data, training_label\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Model\n",
    "        # --------------------------------------------------------------------------------\n",
    "        if self.USE_CUSTOM_MODEL:\n",
    "            self.build_custom_model(validation_data, validation_label)\n",
    "            self._train_fn = self._keras_train_custom_model\n",
    "            self._save_fn = self._save_custom_model\n",
    "            self._load_fn = self._load_custom_model\n",
    "            if self.num_labels == 1:\n",
    "                self._predict_fn = self._keras_predict_custom_model_binary\n",
    "            else:\n",
    "                self._predict_fn = self._keras_predict_custom_model_categorical\n",
    "        else:\n",
    "            self.build_huggingface_model()\n",
    "            self._train_fn = self._keras_train_huggingface_model\n",
    "            self._predict_fn = self._keras_predict_huggnigface_model\n",
    "            self._save_fn = self._save_huggingface_model\n",
    "            self._load_fn = self._load_huggingface_model\n",
    "\n",
    "        del validation_data, validation_label\n",
    "        self.model.summary()\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Model validations\n",
    "        # --------------------------------------------------------------------------------\n",
    "        test_sentences = [\n",
    "            \"i am a cat who has no name.\",\n",
    "            \"to be or not to be.\"\n",
    "        ]\n",
    "        test_tokens = self.tokenize(test_sentences, padding='max_length')\n",
    "        TEST_BATCH_SIZE = len(test_tokens)\n",
    "\n",
    "        # Model generates predictions for all the classes/labels.\n",
    "        # including the binary classification where num_labels == 1\n",
    "        test_model_output = self.model(test_tokens)\n",
    "        assert test_model_output.shape == (TEST_BATCH_SIZE, self.num_labels), \\\n",
    "            \"test_model_output type[%s] data [%s]\" % \\\n",
    "            (type(test_model_output), test_model_output)\n",
    "\n",
    "        # predict returns probabilities for the target class/label only \n",
    "        # in a np array of shape (batch_size, 1). The probability value\n",
    "        # is between 0 and 1.\n",
    "        test_predictions = self.predict(test_sentences)\n",
    "        assert test_predictions.shape == (TEST_BATCH_SIZE, 1), \\\n",
    "            \"test_predictions shape[%s] data [%s]\" % \\\n",
    "            (test_predictions.shape, test_predictions)\n",
    "        assert np.all(0 < test_predictions) and np.all(test_predictions < 1)            \n",
    "            \n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Instance methods\n",
    "    # --------------------------------------------------------------------------------\n",
    "    def tokenize(self, sentences, truncation=True, padding='longest'):\n",
    "        \"\"\"Tokenize using the Huggingface tokenizer\n",
    "        Args: \n",
    "            sentences: String or list of string to tokenize\n",
    "            padding: Padding method ['do_not_pad'|'longest'|'max_length']\n",
    "        \"\"\"\n",
    "        return self.tokenizer(\n",
    "            sentences,\n",
    "            truncation=truncation,\n",
    "            padding=padding,\n",
    "            max_length=self.max_sequence_length,\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return tokenizer.decode(tokens)\n",
    "\n",
    "    def _hf_train(self):\n",
    "        \"\"\"Train the model using HuggingFace Trainer\"\"\"\n",
    "        self._training_args = TFTrainingArguments(\n",
    "            output_dir='./results',             # output directory\n",
    "            num_train_epochs=3,                 # total number of training epochs\n",
    "            per_device_train_batch_size=self.batch_size,     # batch size per device during training\n",
    "            per_device_eval_batch_size=self.batch_size,      # batch size for evaluation\n",
    "            warmup_steps=500,                   # number of warmup steps for learning rate scheduler\n",
    "            weight_decay=0.01,                  # strength of weight decay\n",
    "            logging_dir='./logs',               # directory for storing logs\n",
    "            logging_steps=10,\n",
    "        )\n",
    "\n",
    "        # with self._training_args.strategy.scope():\n",
    "        #     self._model = TFDistilBertForSequenceClassification.from_pretrained(self.model_name)\n",
    "\n",
    "        self._trainer = TFTrainer(\n",
    "            model=self.model,\n",
    "            args=self._training_args,   # training arguments\n",
    "            train_dataset=self.X,       # training dataset\n",
    "            eval_dataset=self.V         # evaluation dataset\n",
    "        )\n",
    "        self.trainer.train()\n",
    "\n",
    "    def _keras_train_huggingface_model(self):\n",
    "        \"\"\"Train the model using Keras\n",
    "        \"\"\"\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Train the model\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self._history = self.model.fit(\n",
    "            self.X.shuffle(1000).batch(self.batch_size).prefetch(1),\n",
    "            epochs=self.num_epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            validation_data=self.V.shuffle(1000).batch(self.batch_size).prefetch(1),\n",
    "            callbacks=[\n",
    "                SavePretrainedCallback(output_dir=self.model_directory, verbose=True),\n",
    "                TensorBoardCallback(self.log_directory),\n",
    "                EarlyStoppingCallback(patience=self.early_stop_patience),\n",
    "                ReduceLRCallback(patience=self.reduce_lr_patience),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _keras_train_custom_model(self):\n",
    "        \"\"\"Train the model using Keras\n",
    "        \"\"\"\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Train the model\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self._history = self.model.fit(\n",
    "            self.X.shuffle(1000).batch(self.batch_size).prefetch(1),\n",
    "            epochs=self.num_epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            validation_data=self.V.shuffle(1000).batch(self.batch_size).prefetch(1),\n",
    "            callbacks=self._callbacks\n",
    "        )        \n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Run the model trainig\"\"\"\n",
    "        self._train_fn()\n",
    "\n",
    "    def evaluate(self, data, label):\n",
    "        \"\"\"Evaluate the model on the given data and label.\n",
    "        https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate\n",
    "        The attribute model.metrics_names gives labels for the scalar metrics\n",
    "        to be returned from model.evaluate().\n",
    "\n",
    "        Args:\n",
    "            data: data to run the prediction\n",
    "            label: label for the data\n",
    "        Returns: \n",
    "            scalar loss if the model has a single output and no metrics, OR \n",
    "            list of scalars (if the model has multiple outputs and/or metrics). \n",
    "        \"\"\"\n",
    "        if self.num_labels == 1:\n",
    "            assert np.all(np.isin(label, [0,1]))\n",
    "        else:\n",
    "            assert np.all(np.isin(label, np.arange(self.num_labels)))\n",
    "\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(self.tokenize(data)),\n",
    "            label\n",
    "        ))\n",
    "        evaluation = self.model.evaluate(\n",
    "            test_dataset.shuffle(1000).batch(self.batch_size).prefetch(1)\n",
    "        )\n",
    "        return evaluation\n",
    "\n",
    "    def _keras_predict_custom_model_binary(self, data):\n",
    "        \"\"\"Calcuate the prediction for the data\n",
    "        Args:\n",
    "            data: sentences to tokenize of type List[str]\n",
    "        Returns: Probabilities for label 1 as numpy array of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        tokens = dict(self.tokenize(data, padding='max_length'))\n",
    "        probabilities = self.model.predict(tokens)\n",
    "        assert isinstance(probabilities, np.ndarray)\n",
    "        return probabilities\n",
    "\n",
    "    def _keras_predict_custom_model_categorical(self, data):\n",
    "        \"\"\"Calcuate the prediction for the data\n",
    "        Args:\n",
    "            data: sentences to tokenize of type List[str]\n",
    "        Returns: Probabilities for label 1 as numpy array of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        tokens = dict(self.tokenize(data, padding='max_length'))\n",
    "        probabilities = self.model.predict(tokens)\n",
    "        assert isinstance(probabilities, np.ndarray) and probabilities.ndim == 2\n",
    "        return probabilities[:, 1:2]\n",
    "\n",
    "    def _keras_predict_huggnigface_model(self, data):\n",
    "        \"\"\"Calcuate the probability for each label\n",
    "        Args:\n",
    "            data: sentences to tokenize of type List[str]\n",
    "        Returns: Probabilities for label 1 as numpy array of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        tokens = dict(self.tokenize(data))\n",
    "        logits = self.model.predict(tokens)[\"logits\"]\n",
    "        # [:, 1:2] -> TensorFlow Tensor indices to select column 1 for all rows\n",
    "        return tf.nn.softmax(logits)[:, 1:2].numpy()\n",
    "\n",
    "    def predict(self, sentences):\n",
    "        \"\"\"Generate prediction (probabilities) for the target label\n",
    "        Args:\n",
    "            sentences: text sentences of type str or List[str]\n",
    "        Return:\n",
    "            normalized probabilities in numpy array via sigmoid or softmax \n",
    "        \"\"\"\n",
    "        result = self._predict_fn(sentences)\n",
    "        assert isinstance(result, np.ndarray) and result.shape[-1] == 1, \\\n",
    "            f\"Expected np.ndarray but {type(result)} {result}\"\n",
    "        return result\n",
    "                \n",
    "    def _save_huggingface_model(self, path_to_dir):\n",
    "        \"\"\"Save Keras model in h5 format\n",
    "        \"\"\"\n",
    "        self.model.save_pretrained(path_to_dir)\n",
    "\n",
    "    def _save_custom_model(self, path_to_dir):\n",
    "        \"\"\"Save Keras model in h5 format\n",
    "        \"\"\"\n",
    "        path_to_h5 = path_to_dir + os.path.sep + 'model.h5'\n",
    "        self.model.save_weights(\n",
    "            path_to_h5, overwrite=True, save_format='h5'\n",
    "        )\n",
    "\n",
    "    def save(self, path_to_dir=None):\n",
    "        \"\"\"Save the model from the HuggingFace. \n",
    "        - config.json \n",
    "        - tf_model.h5  \n",
    "\n",
    "        Args:\n",
    "            path_to_dir: directory path to save the HuggingFace model artefacts\n",
    "        \"\"\"\n",
    "        if path_to_dir is None or len(path_to_dir) == 0:\n",
    "            path_to_dir = self.model_directory\n",
    "        Path(path_to_dir).mkdir(parents=True, exist_ok=True)\n",
    "        self._save_fn(path_to_dir)\n",
    "\n",
    "    def _load_huggingface_model(self, path_to_dir):\n",
    "        self._model = TFDistilBertForSequenceClassification.from_pretrained(path_to_dir)\n",
    "\n",
    "    def _load_custom_model(self, path_to_dir):\n",
    "        path_to_h5 = path_to_dir + os.path.sep + 'model.h5'\n",
    "        self.model.load_weights(path_to_h5)\n",
    "        \n",
    "    def load(self, path_to_dir):\n",
    "        \"\"\"Load the model as the HuggingFace format.\n",
    "        Args:\n",
    "            path_to_dir: Directory path from where to load config.json and .h5.\n",
    "        \"\"\"\n",
    "        if os.path.isdir(path_to_dir) and os.access(path_to_dir, os.R_OK):\n",
    "            self._load_fn(path_to_dir)\n",
    "        else:\n",
    "            raise RuntimeError(f\"{path_to_dir} does not exit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "z0xIlT_ugzL4"
   },
   "outputs": [],
   "source": [
    "def balance(\n",
    "    df, \n",
    "    data_col_name,\n",
    "    label_col_name,\n",
    "    retain_columns,\n",
    "    max_replication_ratio=sys.maxsize\n",
    "):\n",
    "    \"\"\"Balance the data volumes of positives and negatives\n",
    "    The negatives (label==0) has more volume than the positives has, hence\n",
    "    causing skewed data representation. To avoid the model from adapting to the\n",
    "    majority (negative), naively balance the volumes so that they have same size.\n",
    "\n",
    "    For the ratio = (negatives / positives), replicate positives 'ratio' times \n",
    "    to match the volume of negatives if ratio < max_replication_ratio.\n",
    "    When ratio > max_replication_ratio, replicate max_replication_ratio times\n",
    "    to the size = (positive_size * max_replication_ratio). Then take 'size'\n",
    "    volume randomly from negatives.\n",
    "\n",
    "    A portion of the negatives will not be used because of this balancing.\n",
    "\n",
    "    Args:\n",
    "        df: Pandas dataframe \n",
    "        data_col_name: Column name for the data\n",
    "        label_col_name: Column name for the label\n",
    "        retain_columns: Columns to retain in the dataframe to return\n",
    "        max_replication_ratio\n",
    "    Returns: \n",
    "        Pandas dataframe with the ratin_columns.\n",
    "    \"\"\"\n",
    "    positive_indices = df.index[df[label_col_name]==1].tolist()\n",
    "    negative_indices = df.index[df[label_col_name]==0].tolist()\n",
    "    assert not bool(set(positive_indices) & set(negative_indices))\n",
    "\n",
    "    positive_size = len(positive_indices)\n",
    "    negative_size = len(negative_indices)\n",
    "    ratio = np.minimum(negative_size // positive_size, max_replication_ratio)\n",
    "\n",
    "    if ratio >= 2:\n",
    "        # Generate equal size of indices for positives and negatives. \n",
    "        target_positive_indices = ratio * positive_indices\n",
    "        target_negative_indices = np.random.choice(\n",
    "            a=negative_indices, \n",
    "            size=ratio * positive_size,\n",
    "            replace=False\n",
    "        ).tolist()\n",
    "        indices = target_positive_indices + target_negative_indices\n",
    "\n",
    "        # Extract [data, label] with equal size of positives and negatives\n",
    "        data = df.iloc[indices][\n",
    "            df.columns[df.columns.isin(retain_columns)]\n",
    "        ]\n",
    "\n",
    "    else: \n",
    "        data = df[\n",
    "            df.columns[df.columns.isin(retain_columns)]\n",
    "        ]\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_runner(\n",
    "    train,\n",
    "    category,\n",
    "    max_sequence_length,\n",
    "    freeze_pretrained_base_model,\n",
    "    num_labels,\n",
    "    batch_size,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    early_stop_patience,\n",
    "    reduce_lr_patience,\n",
    "    output_directory,\n",
    "    max_replication_ratio=sys.maxsize\n",
    "):\n",
    "    \"\"\"Wrapper to create the Runner instances for the respective category\n",
    "    Args:\n",
    "        train: Pandas dataframe containing entire training data\n",
    "        category: unhealthy comment category, e.g. 'toxic'\n",
    "        max_sequence_length:\n",
    "        freeze_pretrained_base_model: flat to freeze the base model\n",
    "        num_labels: Number of classes to classify\n",
    "        batch_size:\n",
    "        num_epochs:\n",
    "        learning_rate:\n",
    "        early_stop_patience:\n",
    "        reduce_lr_patience:\n",
    "        output_directory:\n",
    "        max_replication_ratio: ratio up to which to replicate the skewed volume\n",
    "    \"\"\"\n",
    "    print(\"\\n--------------------------------------------------------------------------------\")\n",
    "    print(f\"Build runner for [{category}]\")\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "    balanced = balance(\n",
    "        df=train, \n",
    "        data_col_name='comment_text', \n",
    "        label_col_name=category,\n",
    "        retain_columns=['id', 'comment_text', category]\n",
    "    )\n",
    "    data = balanced['comment_text'].tolist()\n",
    "    label = balanced[category].tolist()\n",
    "    del balanced\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Split data into training and validation\n",
    "    # --------------------------------------------------------------------------------\n",
    "    train_data, validation_data, train_label, validation_label = train_test_split(\n",
    "        data,\n",
    "        label,\n",
    "        test_size=.2,\n",
    "        shuffle=True\n",
    "    )\n",
    "    del data, label\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Instantiate the model trainer\n",
    "    # --------------------------------------------------------------------------------\n",
    "    runner = Runner(\n",
    "        category=category,\n",
    "        training_data=train_data,\n",
    "        training_label=train_label,\n",
    "        validation_data=validation_data,\n",
    "        validation_label=validation_label,\n",
    "        max_sequence_length=max_sequence_length,\n",
    "        freeze_pretrained_base_model=freeze_pretrained_base_model,\n",
    "        num_labels=num_labels,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        early_stop_patience=early_stop_patience,\n",
    "        reduce_lr_patience=reduce_lr_patience,\n",
    "        output_directory=output_directory\n",
    "    )\n",
    "    return runner\n",
    "\n",
    "\n",
    "def generate_category_runner(category):\n",
    "    def f(train):\n",
    "        return generate_runner(\n",
    "            category=category,\n",
    "            train=train,\n",
    "            max_replication_ratio = sys.maxsize,\n",
    "            freeze_pretrained_base_model=FREEZE_BASE_MODEL,\n",
    "            num_labels=NUM_LABELS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            early_stop_patience=EARLY_STOP_PATIENCE,\n",
    "            reduce_lr_patience=REDUCE_LR_PATIENCE,\n",
    "            output_directory=RESULT_DIRECTORY\n",
    "        )\n",
    "    return f\n",
    "\n",
    "\n",
    "def evaluate(runner, test, category):\n",
    "    \"\"\"\n",
    "    Evaluate the model of the runner\n",
    "    Args:\n",
    "        runner: Runner instance\n",
    "        test: Pandas dataframe holding entire data\n",
    "    \"\"\"\n",
    "    print(\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print(f\"Model evaluation on [{runner.category}]\")\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    test_data = test['comment_text'].tolist()\n",
    "    test_label = test[category].tolist()\n",
    "    evaluation = runner.evaluate(test_data, test_label)\n",
    "\n",
    "    print(f\"Evaluation: {runner.model_metric_names}:{evaluation}\")\n",
    "    del test_data, test_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wV_uiVhBNM7n"
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEANING_FOR_ANALYSIS and (not CLEANING_FOR_TRAINING):\n",
    "    # Data has been clearned but training needs non cleaned data\n",
    "    train, test = load_raw_data(TEST_MODE)\n",
    "    print(f\"Data records for training [{train['id'].count()}]\")\n",
    "\n",
    "# Drop the rows with -1. ['toxic'] >= 0 is sufficient\n",
    "test = test[test['toxic'] >= 0]\n",
    "gc.collect()\n",
    "\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGFHYUT2gzL3",
    "outputId": "c4c1f98e-6828-4cd0-de1e-f57ef1bce82f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAX_SEQUENCE_LENGTH = 256\n",
      "FREEZE_BASE_MODEL = False\n",
      "NUM_EPOCHS = 10\n",
      "BATCH_SIZE = 32\n",
      "LEARNING_RATE = 5e-05\n",
      "REDUCE_LR_PATIENCE = 3\n",
      "EARLY_STOP_PATIENCE = 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace\n",
    "MAX_SEQUENCE_LENGTH = 256   # Max token length to accept. 512 taks 1 hour/epoch on Google Colab\n",
    "\n",
    "# Model training\n",
    "NUM_LABELS = 1\n",
    "FREEZE_BASE_MODEL = False\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 3e-5  # Must be small to avoid catastrophic forget\n",
    "REDUCE_LR_PATIENCE = 3\n",
    "EARLY_STOP_PATIENCE = 5\n",
    "\n",
    "print(\"\"\"\n",
    "TIMESTAMP = {}\n",
    "MAX_SEQUENCE_LENGTH = {}\n",
    "FREEZE_BASE_MODEL = {}\n",
    "NUM_LABELS = {}\n",
    "NUM_EPOCHS = {}\n",
    "BATCH_SIZE = {}\n",
    "LEARNING_RATE = {}\n",
    "REDUCE_LR_PATIENCE = {}\n",
    "EARLY_STOP_PATIENCE = {}\n",
    "RESULT_DIRECTORY = {}\n",
    "\"\"\".format(\n",
    "    TIMESTAMP,\n",
    "    MAX_SEQUENCE_LENGTH,\n",
    "    FREEZE_BASE_MODEL,\n",
    "    NUM_LABELS,\n",
    "    NUM_EPOCHS,\n",
    "    BATCH_SIZE,\n",
    "    LEARNING_RATE,\n",
    "    REDUCE_LR_PATIENCE,\n",
    "    EARLY_STOP_PATIENCE,\n",
    "    RESULT_DIRECTORY,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "IMjy-emngzL4"
   },
   "outputs": [],
   "source": [
    "# runners = {}      # To save the Runner instance for each category.\n",
    "# evaluations = {}  # Evaluation results for each category\n",
    "#     for category in CATEGORIES:\n",
    "#         runners[category], evaluations[category] = run(\n",
    "#             category=category, \n",
    "#             train=train,\n",
    "#             test=test,\n",
    "#             batch_size=BATCH_SIZE,\n",
    "#             max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "#             num_epochs=NUM_EPOCHS,\n",
    "#             learning_rate=LEARNING_RATE,\n",
    "#             early_stop_patience=EARLY_STOP_PATIENCE,\n",
    "#             reduce_lr_patience=REDUCE_LR_PATIENCE,\n",
    "#             output_directory=RESULT_DIRECTORY\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Y3KFBkjcknCD",
    "o-Oj3ltY_T8m",
    "lZZbTtUGWx9S",
    "Qpx-pzGWasel",
    "gcefqEMkA2IV",
    "7UPio2cmTcWO",
    "QcngPOCBW4R1",
    "_PFau2osjvx-",
    "k0jSqXxxQrUj",
    "1wwZFMlD6pIJ",
    "nUYPD0TQvEaI",
    "pFWPmiCkaVsN",
    "ShId8bvTGlER",
    "s4jNfuchQiPy",
    "f-RWZFJj8lJZ",
    "5bp2VT2ohkKD",
    "w0Rpsc7IjWFd",
    "XJyvUG3cGylz",
    "SvZ2Z7JQ9jsS"
   ],
   "machine_shape": "hm",
   "name": "toxic_comment_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
