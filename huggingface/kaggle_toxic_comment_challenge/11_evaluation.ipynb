{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJyvUG3cGylz"
   },
   "source": [
    "---\n",
    "# Utility (Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u32zWvUKG1ef"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "\n",
    "def evaluate_roc(predictions, labels, title):\n",
    "    preds = predictions\n",
    "    fpr, tpr, threshold = roc_curve(labels, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "\n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(labels, y_pred)\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "    # Plot ROC AUC\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f'Receiver Operating Characteristic ({category})')\n",
    "    plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.grid(which='major', b=False, linestyle='--')\n",
    "    plt.grid(which='minor', alpha=0.2, linestyle='--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Z59L6LtXQU7"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "Apply the model on the testing data.\n",
    "\n",
    "* Accuracy - As done in the model training completion.\n",
    "* ROC - Evaluate Receiver Operating Characteristic (ROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvZ2Z7JQ9jsS"
   },
   "source": [
    "### Instantiate predictors from the save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BwOtYdWqgyL"
   },
   "outputs": [],
   "source": [
    "dummy_data = [\"dummy\"]\n",
    "dummy_label = [0]\n",
    "id=\"\"\n",
    "predictors = {}\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    predictor = Runner(\n",
    "        category=category,\n",
    "        training_data=dummy_data,\n",
    "        training_label=dummy_label,\n",
    "        validation_data=dummy_data,\n",
    "        validation_label=dummy_label,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    )\n",
    "    path_to_dir = \"{parent}/model_C{category}_B{size}_L{length}\".format(\n",
    "        parent=RESULT_DIRECTORY,\n",
    "        category=category,\n",
    "        size=BATCH_SIZE,\n",
    "        length=MAX_SEQUENCE_LENGTH\n",
    "    )\n",
    "    predictor.load(path_to_dir)\n",
    "    predictors[category] = predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GG21clJX9p6Q"
   },
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uT3gxBooKh0B"
   },
   "outputs": [],
   "source": [
    "test_data = test['comment_text'].tolist()\n",
    "test_label = test[category].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UZMD-JBYIPj"
   },
   "outputs": [],
   "source": [
    "row = {}\n",
    "index = np.random.randint(0, len(test_data))\n",
    "data = test_data[index]\n",
    "row['data'] = data\n",
    "for category in CATEGORIES:\n",
    "    row[category] = np.argmax(predictors[category].predict(data).numpy().tolist()[0])\n",
    "\n",
    "pd.DataFrame([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ij0hmoTM9xSq"
   },
   "source": [
    "#### True Ratings\n",
    "\n",
    "True results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOwh3MqN7zvZ"
   },
   "outputs": [],
   "source": [
    "raw_test[(raw_test['toxic'] >= 0)].iloc[[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjqiMFA1UXTS"
   },
   "outputs": [],
   "source": [
    "del test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwnsYjYvJX5m"
   },
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3kzypAKJVzL"
   },
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for category in CATEGORIES:\n",
    "    labels = test[category].tolist()\n",
    "    predictions[category] = predictors[category].predict(test['comment_text'].tolist())[:, 1:2].numpy()\n",
    "    evaluate_roc(predictions[category], labels, category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgjVx3uEc_F5"
   },
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hfu7dRNe9GtT"
   },
   "outputs": [],
   "source": [
    "# Restore the test data including -1 label values\n",
    "if CLEANING_FOR_TRAINING:\n",
    "    _, submission = load_clean_data(train, test)\n",
    "    del _\n",
    "else:\n",
    "    # submission = pd.read_pickle(test_pickle_path).loc[:, ['id', 'comment_text']]\n",
    "    _, submission = load_raw_data(TEST_MODE)\n",
    "    del _\n",
    "assert submission['toxic'].count() > 0\n",
    "\n",
    "submission = submission.loc[:, ['id', 'comment_text']]\n",
    "for category in CATEGORIES:\n",
    "    # [:, 1:2] -> TensorFlow Tensor indices to select column 1 for all rows\n",
    "    # Add to 'category' column as numpy array.\n",
    "    submission[category] = predictors[category].predict(submission['comment_text'].tolist())[:, 1:2].numpy()\n",
    "\n",
    "submission.drop('comment_text',axis = 1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cwGjd9MgY3a"
   },
   "outputs": [],
   "source": [
    "submission.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0o9gDq_ZD80"
   },
   "outputs": [],
   "source": [
    "review = pd.merge(test, submission, on='id')\n",
    "review.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqFhQ98KgfvJ"
   },
   "outputs": [],
   "source": [
    "for category in CATEGORIES:\n",
    "    # NP: Negative Predictions\n",
    "    # PP: Positive Predictions\n",
    "    PP = review[(review[f'{category}_y'] > 0.5)]['id'].count()\n",
    "    NP = review[(review[f'{category}_y'] <= 0.5)]['id'].count()\n",
    "    ALL = PP + NP\n",
    "\n",
    "    # TP: True Positive Prediction\n",
    "    # AP: Actual Positive\n",
    "    # TN: True Negative Prediction\n",
    "    # AN: Actual Negative\n",
    "    TP = review[(review[f'{category}_x'] == 1) & (review[f'{category}_y'] > 0.5)]['id'].count()\n",
    "    AP = review[(review[f'{category}_x'] == 1)]['id'].count()\n",
    "    TN = review[(review[f'{category}_x'] == 0) & (review[f'{category}_y'] <= 0.3)]['id'].count()\n",
    "    AN = review[(review[f'{category}_x'] == 0)]['id'].count()\n",
    "\n",
    "    print(f\"[{category:13s}] TP {TP/ALL:0.3f} FP {(PP-TP)/ALL:0.3f} TN: {TN/ALL:0.3f} FN {(NP-TN)/ALL:0.3f}\" ) \n",
    "    if AP > 0:\n",
    "        print(f\"[{category:13s}] Positive : Recall {TP/AP:0.3f}\" ) \n",
    "    if AN > 0:\n",
    "        print(f\"[{category:13s}] Negative : Recall {TN/AN:0.3f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMM-OICj659E"
   },
   "source": [
    "---\n",
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niEPjf57IyTn"
   },
   "outputs": [],
   "source": [
    "submission.to_csv(f\"{RESULT_DIRECTORY}/{'submission.csv'}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hI3JCVSjUSG3"
   },
   "outputs": [],
   "source": [
    "del submission"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Y3KFBkjcknCD",
    "o-Oj3ltY_T8m",
    "lZZbTtUGWx9S",
    "Qpx-pzGWasel",
    "gcefqEMkA2IV",
    "7UPio2cmTcWO",
    "QcngPOCBW4R1",
    "_PFau2osjvx-",
    "k0jSqXxxQrUj",
    "1wwZFMlD6pIJ",
    "nUYPD0TQvEaI",
    "pFWPmiCkaVsN",
    "ShId8bvTGlER",
    "s4jNfuchQiPy",
    "f-RWZFJj8lJZ",
    "5bp2VT2ohkKD",
    "w0Rpsc7IjWFd",
    "XJyvUG3cGylz",
    "SvZ2Z7JQ9jsS"
   ],
   "machine_shape": "hm",
   "name": "toxic_comment_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
