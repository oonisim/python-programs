import os
import sys
import pathlib
import logging

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import (
    train_test_split
)
from transformers import (
    DistilBertTokenizerFast,
    TFDistilBertModel,
    AdamWeightDecay
)

INPUT_IDS = "input_ids"
ATTENTION_MASK = "attention_mask"


class TFToxicDataSetsFactory:
    """Manage training dataset X:(data, label) and validation dataset V:(data, label)
    for toxic comment classification
    X, V are TF dataset and the data is the tokenized sequence with the structure
    {
        "INPUT_IDS": Tensor,
        "ATTENTION_MASK": Tensor
    }
    """
    @property
    def category(self):
        """Unhealthy comment category"""
        return self._category

    @property
    def input_ids(self):
        """TF Tensor of integer token ID sequence generated by Tokenizer"""
        return self._input_ids

    @property
    def attention_mask(self):
        """TF Tensor of 0/1 mask sequence generated by Tokenizer"""
        return self._attention_mask

    @property
    def labels(self):
        """Classification labels (1 or 0) for the comments as a numpy array"""
        return self._labels

    @property
    def positive_indices(self):
        """Indices to the rows where the classification == 1"""
        return self._positive_indices

    @property
    def negative_indices(self):
        """Indices to the rows where the classification == 0"""
        return self._negative_indices

    @property
    def num_positives(self):
        """Indices to the rows where the classification == 1"""
        return len(self.positive_indices)

    @property
    def split_ratio(self):
        """
        Ratio of the validation data when splitting the data into
        training and validation
        """
        return self._split_ratio

    @property
    def X(self) -> tf.data.Dataset:
        assert self._X is not None
        return self._X

    @property
    def X_size(self) -> int:
        assert self._X_size > 0
        return self._X_size

    @property
    def V(self) -> tf.data.Dataset:
        assert self._V is not None
        return self._V

    @property
    def V_size(self) -> int:
        assert self._V_size > 0
        return self._V_size

    @property
    def batch_size(self):
        """Batch size to which the size of training and validation data is aligned
        If the training data size is 66 and the batch size is 32, then the data is
        truncated into 64. If the value is 0 or less, then no adjustment.
        """
        return self._batch_size

    def __init__(
            self,
            df,
            category: str,
            data_column_name: str,
            tokenize_fn: callable,
            split_ratio: float = 0.2,
            batch_size=0
    ):
        """
        Args:
            df: pandas dataframe for the training data
            category: comment category e.g. 'toxic'
            data_column_name: name of the data column in df
            tokenize_fn: Tokenizer function
            split_ratio: ratio to split the data into training and validation
            batch_size: batch size, the multiple of which the training and
                        validation data is aligned. No alignment if 0 or less.
        """
        assert isinstance(df, pd.DataFrame)
        assert callable(tokenize_fn)
        assert category in df.columns and data_column_name in df.columns
        assert df[category].count() > 2
        assert 0.0 < split_ratio < 1.0

        self._category = category
        self._split_ratio = split_ratio
        self._batch_size = batch_size

        # --------------------------------------------------------------------------------
        # INPUT_IDS, ATTENTION_MASK are TF Tensor of shape(num_rows, max_sequence_size)
        # --------------------------------------------------------------------------------
        sentences = df[data_column_name].tolist()
        tokens = tokenize_fn(sentences)
        self._input_ids = tokens[INPUT_IDS]
        self._attention_mask = tokens[ATTENTION_MASK]
        del tokens

        # --------------------------------------------------------------------------------
        # labels of the comments as numpy
        # --------------------------------------------------------------------------------
        self._labels = df[category].values
        assert isinstance(self.labels, np.ndarray) and self.labels.ndim == 1

        # --------------------------------------------------------------------------------
        # Indices to the rows where label is 1 (positive) and 0 (negative)
        # --------------------------------------------------------------------------------
        self._positive_indices = df.index[df[category] == 1].values
        self._negative_indices = df.index[df[category] == 0].values
        assert isinstance(self.positive_indices, np.ndarray) and self.positive_indices.ndim == 1, \
            f"1D array is expected but shape {self.positive_indices.shape}"
        assert len(self.negative_indices) > len(self.positive_indices), \
            "Expected skewness where positives << negatives"

        # --------------------------------------------------------------------------------
        # Training and validation data
        # --------------------------------------------------------------------------------
        self._X = None
        self._X_size = 0
        self._V = None
        self._V_size = 0

        # --------------------------------------------------------------------------------
        # Initialize
        # --------------------------------------------------------------------------------
        self.on_epoch()

    def on_epoch(self):
        """Setup (X, V) where X is data for training and V for validation.
        The size of positives and negatives are equaled so that there will no skewness.
        The 'num_positives' size of data is randomly selected from negatives.

        Returns: (X, V) where X, V are TF Dataset.
        """
        # --------------------------------------------------------------------------------
        # Indices to positives and negatives having the same volume
        # --------------------------------------------------------------------------------
        indices = np.concatenate((
            self.positive_indices,
            np.random.choice(self.negative_indices, self.num_positives, replace=False)
        ))

        # --------------------------------------------------------------------------------
        # Split data into training and validation
        # --------------------------------------------------------------------------------
        x_indices, v_indices = train_test_split(indices, test_size=self.split_ratio)
        assert len(x_indices) > 0 and len(v_indices) > 0
        del indices

        # --------------------------------------------------------------------------------
        # Batch size alignment (adjust the size to be multiple of the batch size)
        # --------------------------------------------------------------------------------
        if self.batch_size > 0:
            x_len = len(x_indices)
            v_len = len(v_indices)
            assert x_len >= v_len >= self.batch_size, "Not enough size in the data"
            x_indices = x_indices[:x_len - (x_len % self.batch_size)]
            v_indices = v_indices[:v_len - (v_len % self.batch_size)]

        # --------------------------------------------------------------------------------
        # Train data X
        # --------------------------------------------------------------------------------
        x_data = {
            INPUT_IDS: tf.gather_nd(self.input_ids, x_indices.reshape((-1, 1))),
            ATTENTION_MASK: tf.gather_nd(self.attention_mask, x_indices.reshape((-1, 1)))
        }
        x_labels = self.labels[x_indices]
        self._X = tf.data.Dataset.from_tensor_slices((x_data, x_labels))
        self._X_size = len(x_indices)
        del x_data, x_labels

        # --------------------------------------------------------------------------------
        # Validation data V
        # --------------------------------------------------------------------------------
        v_data = {
            INPUT_IDS: tf.gather_nd(self.input_ids, v_indices.reshape((-1, 1))),
            ATTENTION_MASK: tf.gather_nd(self.attention_mask, v_indices.reshape((-1, 1)))
        }
        v_labels = self.labels[v_indices]
        self._V = tf.data.Dataset.from_tensor_slices((v_data, v_labels))
        self._V_size = len(v_indices)
        del v_data, v_labels


class TFToxicDataSetSequence(tf.keras.utils.Sequence):
    """Keras Sequence """
    @property
    def factory(self):
        return self._factory

    @property
    def dataset(self):
        """TF Dataset"""
        assert self._dataset is not None
        return self._dataset

    @property
    def batch_size(self):
        assert self.factory.batch_size > 0
        return self.factory.batch_size

    @property
    def size(self) -> int:
        assert self._size > 0, "size not initialized"
        return self._size

    @property
    def num_batches(self) -> int:
        assert (self.size // self.batch_size) > 0
        return self.size // self.batch_size

    @property
    def batch_index(self) -> int:
        """Index to the next batch to be returned from next() method"""
        assert 0 <= self._batch_index < self.num_batches
        return self._batch_index

    def next(self):
        """The next batch from the batched dataset
        The dataset is chunked into a sequence of batches already at __init__
        """
        batch = self.dataset.skip(self.batch_index).take(1)
        self._batch_index += 1
        return batch

    def __init__(self, factory: TFToxicDataSetsFactory, dataset: tf.data.Dataset, size: int):
        assert isinstance(factory, TFToxicDataSetsFactory) and isinstance(dataset, tf.data.Dataset)
        assert size > 0

        self._factory = factory
        self._dataset = dataset.shuffle(1000).batch(self.batch_size).prefetch(1)
        self._size = size
        self._batch_index = 0

    def __len__(self):
        return self.num_batches

    def __getitem__(self, idx):
        return self.next()
        

class TrainingSequence(TFToxicDataSetSequence):
    def __init__(self, factory: TFToxicDataSetsFactory):
        super().__init__(factory=factory, dataset=factory.X, size=factory.X_size)

    def on_epoch_end(self):
        """Acquire the new dataset from the Factory
        """
        super().__init__(factory=self.factory, dataset=self.factory.X, size=self.factory.X_size)


class ValidationSequence(TFToxicDataSetSequence):
    def __init__(self, factory: TFToxicDataSetsFactory):
        super().__init__(factory=factory, dataset=factory.V, size=factory.V_size)

    def on_epoch_end(self):
        """Acquire the new dataset from the Factory
        TODO:
            If callbacks are using the validation data e.g. to calculate AUC,
            you need to update the data there as well.
        """
        super().__init__(factory=self.factory, dataset=self.factory.V, size=self.factory.V_size)


tokenizer = DistilBertTokenizerFast.from_pretrained(
    'distilbert-base-uncased', do_lower_case=True
)


# --------------------------------------------------------------------------------
# Test
# --------------------------------------------------------------------------------
def tokenize(sentences, max_length=256, padding='max_length'):
    """Tokenize using the Huggingface tokenizer
    Args:
        sentences: String or list of string to tokenize
        max_length: maximum token length that the tokenizer generates
        padding: Padding method ['do_not_pad'|'longest'|'max_length']
    """
    return tokenizer(
        sentences,
        truncation=True,
        padding=padding,
        max_length=max_length,
        return_tensors="tf"
    )


def decode(tokens):
    """Decode token ids back to string sequence
    Args:
        tokens: output of the tokenizer having the structure:
                {'input_ids': <tf.Tensor>, 'attention_mask': <tf.Tensor>}
    Returns: List of string sentence
    """
    sentences = []
    if isinstance(tokens, list) or tf.is_tensor(tokens):
        for sequence in tokens:
            sentences.append(tokenizer.decode(sequence))
    elif 'input_ids' in tokens:
        for sequence in tokens['input_ids']:
            sentences.append(tokenizer.decode(sequence))
    return sentences


def test():
    # --------------------------------------------------------------------------------
    # Training data CSV for toxic comment classification
    # --------------------------------------------------------------------------------
    df = pd.read_csv('../data/train.csv.zip').head(100)

    # --------------------------------------------------------------------------------
    # Factory to generate TF datasets (X, V). X for training V for validation as
    # DataSet( sequences, labels) where sequence and labels are TF Tensor.
    # sequences are rows of tokenized sentences generated by the Tokenizer with the
    # structure { "input_ids": input_ids, "attention_mask": attention_mask }.
    # --------------------------------------------------------------------------------
    batch_size = 5
    factory = TFToxicDataSetsFactory(
        df=df,
        category='toxic',
        data_column_name='comment_text',
        tokenize_fn=tokenize,
        batch_size=batch_size
    )

    # --------------------------------------------------------------------------------
    # Constraint.
    # 1. X from the factory has (sequence, label) structure.
    # 2. "sequence" has the { "input_ids": input_ids, "attention_mask": attention_mask }
    #    structure, and the Tokenizer can decode "input_ids" in the sequence.
    # --------------------------------------------------------------------------------
    count = 0
    sentences = []
    for sequence, label in factory.X:
        sentences.append(decode(sequence))
        print(f"sentence: {sentences[-1]}")
        count += 1

    # --------------------------------------------------------------------------------
    # Constraint.
    # 1. Number of sequences in X matches with the X_size property of the factory.
    # --------------------------------------------------------------------------------
    assert factory.X_size == count, \
        "Number of records in X %s must match X_size %s" % \
        (count, factory.X_size)

    # --------------------------------------------------------------------------------
    # Constraint.
    # 1. TFToxicDataSetSequence provides a TF dataset via next() method that has
    #    batch_size rows. The dataset has (sequence, label) structure as produced by
    #    the TFToxicDataSetsFactory.
    # --------------------------------------------------------------------------------
    train_data_seq = TrainingSequence(factory)
    batch = train_data_seq.next()
    sequences = list(batch)[0][0]['input_ids']
    print(sequences)

    count_in_batch = 0
    for sentence, sequence in zip(sentences, sequences):
        # Note the batch has been shuffled, hence the original order is not there.
        print(decode(sequence))
        count_in_batch += 1

    assert count_in_batch == batch_size
