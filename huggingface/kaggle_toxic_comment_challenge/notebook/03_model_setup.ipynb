{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdZo50sxLBuX"
   },
   "source": [
    "---\n",
    "# Technology Selection\n",
    "\n",
    "What algorithms to use and what technologies are available.\n",
    "\n",
    "## Nature of the problem\n",
    "\n",
    "* Multi-label Binary Classification \n",
    "\n",
    "It is a binary classification task where multiple althorithms have been developed and applied in the real life e.g. SPAM fileter.\n",
    "\n",
    "* Naive Bayes - [Naive Bayes and Text Classification](https://arxiv.org/abs/1410.5329)\n",
    "* CNN - [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf)\n",
    "* DNN Language Model - [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "## Asssessment\n",
    "\n",
    "Skipped due to the time constraint.\n",
    "\n",
    "## Decision\n",
    "\n",
    "**Transformer Deep Neural Network Architecture** transfer-learning (fine-tuning) on the pre-trained language model.\n",
    "\n",
    "1. State of the art algorithms being actively researched.\n",
    "2. Pre-trained models for text classification e.g text sentiment analysis are available. \n",
    "3. Other well-explored althorithms have been well tested as published in Kaggle. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOtHJ1edOFoS"
   },
   "source": [
    "---\n",
    "# Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rB2cjqCZhcE9"
   },
   "source": [
    "## ML Model for Fine Tuning\n",
    "\n",
    "### Framework\n",
    "* Google TensorFlow 2.x \n",
    "* Keras for training the model\n",
    "* Huggingface Transformer library\n",
    "\n",
    "### Data allocation\n",
    "Utilize [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to :\n",
    "1. shuffle the train data \n",
    "2. allocate the ratio R of the data for validation. R=0.2\n",
    "3 apply the model training on (1-R) ratio of the data for training\n",
    "\n",
    "Apply the trained model on testing data for evaluation.\n",
    "\n",
    "### Hyper parameter search\n",
    "* Learning rate (5e-5, 5e-4, 5e-3) as the start value\n",
    "\n",
    "### Epoch\n",
    "Number of times to go through the entire training data set N. N=10 due to the time constraint.\n",
    "\n",
    "### Early stopping\n",
    "Utilize Keras [EarlyStopping](https://keras.io/api/callbacks/early_stopping/) to stop the training when no improvement is achieved N times. N=5.\n",
    "\n",
    "### Reduce learning rate at no improvement\n",
    "Utilize Keras [ReduceLROnPlateau](https://keras.io/api/callbacks/reduce_lr_on_plateau/) to reduce the learning rate when no improvement is achieved N times. N=3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bp2VT2ohkKD"
   },
   "source": [
    "### Keras Callbacks\n",
    "\n",
    "Utilize [Keras Callbacks API](https://keras.io/api/callbacks/) to apply Eary Stopping, Reduce Learning Rate, and TensorBoard during the model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Fc67_HTf0zyO"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "class ROCCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Take actions on the model training based on ROC/AUC\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            validation_data,\n",
    "            validation_label,\n",
    "            output_path,\n",
    "            output_format='h5',\n",
    "            criterion=1,\n",
    "            reduce_lr_patience=2,\n",
    "            reduce_lr_factor=0.2,\n",
    "            early_stop_patience=sys.maxsize,\n",
    "            verbose=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            validation_data: data to generate prediction to calculate ROC/AUC\n",
    "            validation_label: label to calculate ROC/AUC\n",
    "            output_path: path to save the model upon improvement\n",
    "            output_format: model save format 'tf' or 'h5'\n",
    "            reduce_lr_patience: number of consecutive no-improvement upon which to reduce LR.\n",
    "            reduce_lr_factor: new learning rate = reduce_lr_factor * old learning rate\n",
    "            early_stop_patience: total number of no-improvements upon which to stop the training\n",
    "            verbose: [True|False] to output extra information\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert 0.0 < reduce_lr_factor < 1.0\n",
    "        assert 0 < reduce_lr_patience\n",
    "        assert 0 < early_stop_patience\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # ROC/AUC calculation data\n",
    "        # TODO:\n",
    "        #    When recreating the data e.g. in the on_epoch_end in tf.keras.utils.Sequence\n",
    "        #    then need to update the x, y accordingly here.\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self.x = validation_data\n",
    "        self.y = validation_label\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Training control parameters\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self.criterion = criterion\n",
    "        self.reduce_lr_patience = reduce_lr_patience\n",
    "        self.reduce_lr_factor = reduce_lr_factor\n",
    "        self.early_stop_patience = early_stop_patience\n",
    "        self.output_path = output_path\n",
    "        self.output_format = output_format\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Statistics\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self.min_val_loss = np.inf\n",
    "        self.max_roc_auc = -1\n",
    "        self.max_pr_auc = -1\n",
    "        self.max_f1 = -1\n",
    "        self.best_epoch = -1\n",
    "        self.successive_no_improvement = 0\n",
    "        self.total_no_improvement = 0\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        \"\"\"Reset the statistics.\n",
    "        The class instance can be re-used throughout multiple training runs\n",
    "        \"\"\"\n",
    "        self.successive_no_improvement = 0\n",
    "        self.total_no_improvement = 0\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # DO NOT reset the best metric values that the model have achieved.\n",
    "        # If restart the training on the same model, improvements needs to be measured\n",
    "        # with the last best metrics of the model, not the initial values e.g -1 or np.inf.\n",
    "        #\n",
    "        # TODO:\n",
    "        #    Save the best model metrics when saving the model as Keras config file.\n",
    "        #    Reload the best metrics of the model when loading the model itself.\n",
    "        #\n",
    "        #    If the saved best model is re-loaded, the best metric values that the\n",
    "        #    model achieved need to be re-loaded as well. Otherwise the first epoch\n",
    "        #    result, even if the metrics are worse than the best metrics achieved,\n",
    "        #    will become the best results and the best model will be overwritten with\n",
    "        #    the inferior model.\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # self.max_roc_auc = -1\n",
    "        # self.min_val_loss = np.inf\n",
    "        # self.best_epoch = -1\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        pass\n",
    "\n",
    "    def _reduce_learning_rate(self):\n",
    "        old_lr = tf.keras.backend.get_value(self.model.optimizer.lr)\n",
    "        new_lr = old_lr * self.reduce_lr_factor\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, new_lr)\n",
    "        self.successive_no_improvement = 0\n",
    "        if self.verbose:\n",
    "            print(f\"Reducing learning rate to {new_lr}.\")\n",
    "\n",
    "    def _stop_early(self):\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                \"Early stopping: no improvement [%s] times. best epoch [%s] AUC [%5f] val_loss [%5f]\" %\n",
    "                (self.total_no_improvement, self.best_epoch + 1, self.max_roc_auc, self.min_val_loss)\n",
    "            )\n",
    "        self.model.stop_training = True\n",
    "        self.total_no_improvement = 0\n",
    "        self.successive_no_improvement = 0\n",
    "\n",
    "    def _handle_improvement(self, epoch, roc_auc, roc_auc_prev, val_loss, val_loss_prev):\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                \"Model improved auc [%5f > %5f] val_loss [%5f < %5f]. Saving to %s\" %\n",
    "                (roc_auc, roc_auc_prev, val_loss, val_loss_prev, self.output_path)\n",
    "            )\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Update statistics\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self.best_epoch = epoch\n",
    "        self.successive_no_improvement = 0\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Save the model upon improvement\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self.model.save_weights(\n",
    "            self.output_path, overwrite=True, save_format=self.output_format\n",
    "        )\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Stop when no more AUC improvement expected better than 1.0\n",
    "        # --------------------------------------------------------------------------------\n",
    "        if roc_auc_prev > (1.0 - 1e-10):\n",
    "            self._stop_early()\n",
    "            if self.verbose:\n",
    "                print(\"Stopped as no AUC improvement can be made beyond 1.0\")\n",
    "\n",
    "    def _handle_no_improvement(self, epoch, roc_auc, roc_auc_prev, val_loss, val_loss_prev):\n",
    "        if self.verbose:\n",
    "            if roc_auc <= roc_auc_prev:\n",
    "                print(f\"AUC [%5f] did not improve from [%5f].\" % (roc_auc, roc_auc_prev))\n",
    "            if val_loss >= val_loss_prev:\n",
    "                print(f\"val_loss [%5f] did not improve from [%5f].\" % (val_loss, val_loss_prev))\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Reduce LR\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self.successive_no_improvement += 1\n",
    "        if self.successive_no_improvement >= self.reduce_lr_patience:\n",
    "            self._reduce_learning_rate()\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Early Stop\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self.total_no_improvement += 1\n",
    "        if self.total_no_improvement >= self.early_stop_patience:\n",
    "            self._stop_early()\n",
    "\n",
    "    def _has_improved(self, roc_auc, roc_auc_prev, val_loss, val_loss_prev):\n",
    "        \"\"\"Decide if an improvement has been achieved\n",
    "        Criteria:\n",
    "            1: Both AUC and val_loss improved\n",
    "            2: AUC improved\n",
    "            3: val_loss improved\n",
    "        \"\"\"\n",
    "        if self.criterion == 1:\n",
    "            return (roc_auc > roc_auc_prev) and (val_loss < val_loss_prev)\n",
    "        if self.criterion == 2:\n",
    "            return roc_auc > roc_auc_prev\n",
    "        if self.criterion == 3:\n",
    "            return val_loss < val_loss_prev\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \"\"\"Verify the performance improvement metrics and make decisions on:\n",
    "        - Reduce learning rate if count of no consecutive improvement >= reduce_lr_patience\n",
    "        - Early stopping if total count of no improvement >= early_stop_patience\n",
    "\n",
    "        TODO:\n",
    "            If validation data is recreated e.g. at on_epoch_end tf.keras.utils.Sequence,\n",
    "            NEED to update the self.x, self.y accordingly.\n",
    "        \"\"\"\n",
    "        # [print(f\"{k}:{v}\") for k, v in logs.items()]\n",
    "        predictions = self.model.predict(self.x)\n",
    "        val_loss = logs.get('val_loss')\n",
    "        roc_auc = sklearn.metrics.roc_auc_score(self.y, predictions)\n",
    "        f1 = sklearn.metrics.f1_score(y_true=self.y, y_pred=predictions, average='binary')\n",
    "        precision, recall, thresholds = sklearn.metrics.precision_recall_curve(\n",
    "            self.y, y_pred \n",
    "        )\n",
    "        pr_auc = auc(recall, precision)\n",
    "\n",
    "        val_loss_prev = self.min_val_loss\n",
    "        roc_auc_prev = self.max_roc_auc\n",
    "        pr_auc_prev = self.max_pr_auc\n",
    "        f1_prev = self.max_f1\n",
    "        self.min_val_loss = np.minimum(val_loss, self.min_val_loss)\n",
    "        self.max_roc_auc = np.maximum(roc_auc, self.max_roc_auc)\n",
    "        self.max_pr_auc = np.maximum(pr_auc, self.max_pr_auc)\n",
    "        self.max_f1 = np.maximum(f1, self.max_f1)\n",
    "\n",
    "        if self._has_improved(roc_auc, roc_auc_prev, val_loss, val_loss_prev):\n",
    "            self._handle_improvement(epoch, roc_auc, roc_auc_prev, val_loss, val_loss_prev)\n",
    "        else:\n",
    "            self._handle_no_improvement(epoch, roc_auc, roc_auc_prev, val_loss, val_loss_prev)\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        pass\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SavePretrainedCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    This is only for directly working on the Huggingface models.\n",
    "\n",
    "    Hugging Face models have a save_pretrained() method that saves both\n",
    "    the weights and the necessary metadata to allow them to be loaded as\n",
    "    a pretrained model in future. This is a simple Keras callback that\n",
    "    saves the model with this method after each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dir, monitor_metric='val_loss', monitor_mode='min', verbose=True):\n",
    "        assert monitor_mode in ['min', 'max']\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dir = output_dir\n",
    "        self.monitor_metric = monitor_metric\n",
    "        self.monitor_mode = monitor_mode\n",
    "        self.best_metric_value = np.inf if monitor_mode == 'min' else -1\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.lowest_val_loss = np.inf\n",
    "        self.best_epoch = -1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \"\"\"\n",
    "        Save only the best model\n",
    "        - https://stackoverflow.com/a/68042600/4281353\n",
    "        - https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\n",
    "        TODO:\n",
    "        save_pretrained() method is in the HuggingFace model only.\n",
    "        Need to implement an logic to update for Keras model saving.\n",
    "        \"\"\"\n",
    "        assert self.monitor_metric in logs, \\\n",
    "            f\"monitor metric {self.monitor_metric} not in valid metrics {logs.keys()}\"\n",
    "\n",
    "        metric_value = logs.get(self.monitor_metric)\n",
    "        previous_best = self.best_metric_value\n",
    "        if self.monitor_mode == 'min':\n",
    "            self.best_metric_value = np.minimum(metric_value, self.best_metric_value)\n",
    "\n",
    "        elif self.monitor_mode == 'max':\n",
    "            self.best_metric_value = np.maximum(metric_value, self.best_metric_value)\n",
    "            \n",
    "        if previous_best != self.best_metric_value:\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    \"Model %s improved from [%.5f] to [%.5f]\" % \n",
    "                    (self.monitor_metric, previous_best, self.best_metric_value)\n",
    "                )\n",
    "                print(f\"Saving to {self.output_dir}\")\n",
    "            self.best_epoch = epoch\n",
    "            self.model.save_pretrained(self.output_dir)\n",
    "\n",
    "\n",
    "class TensorBoardCallback(tf.keras.callbacks.TensorBoard):\n",
    "    \"\"\"TensorBoard visualization of the model training\n",
    "    See https://keras.io/api/callbacks/tensorboard/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_directory):\n",
    "        super().__init__(\n",
    "            log_dir=output_directory,\n",
    "            write_graph=True,\n",
    "            write_images=True,\n",
    "            histogram_freq=1,  # log histogram visualizations every 1 epoch\n",
    "            embeddings_freq=1,  # log embedding visualizations every 1 epoch\n",
    "            update_freq=\"epoch\",  # every epoch\n",
    "        )\n",
    "\n",
    "\n",
    "class EarlyStoppingCallback(tf.keras.callbacks.EarlyStopping):\n",
    "    \"\"\"Stop training when no progress on the metric to monitor\n",
    "    https://keras.io/api/callbacks/early_stopping/\n",
    "    https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "\n",
    "    Using val_loss to monitor.\n",
    "    https://datascience.stackexchange.com/a/49594/68313\n",
    "    Prefer the loss to the accuracy. Why? The loss quantify how certain\n",
    "    the model is about a prediction. The accuracy merely account for\n",
    "    the number of correct predictions. Similarly, any metrics using hard\n",
    "    predictions rather than probabilities have the same problem.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience=3, monitor='val_loss', mode='auto'):\n",
    "        assert patience > 0\n",
    "        super().__init__(\n",
    "            monitor=monitor,\n",
    "            mode=mode,\n",
    "            verbose=1,\n",
    "            patience=patience,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        assert self.monitor in logs, \\\n",
    "            f\"monitor metric {self.monitor} not in valid metrics {logs.keys()}\"\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "\n",
    "class ModelCheckpointCallback(tf.keras.callbacks.ModelCheckpoint):\n",
    "    \"\"\"Check point to save the model\n",
    "    See https://keras.io/api/callbacks/model_checkpoint/\n",
    "\n",
    "    NOTE:\n",
    "        Did not work with the HuggingFace native model with the error.\n",
    "        NotImplementedError: Saving the model to HDF5 format requires the model\n",
    "        to be a Functional model or a Sequential model.\n",
    "        It does not work for subclassed models, because such models are defined\n",
    "        via the body of a Python method, which isn't safely serializable.\n",
    "\n",
    "        Did not work with the tf.keras.models.save_model nor model.save()\n",
    "        as causing out-of-index errors or load_model() failures. Hence use\n",
    "        save_weights_only=True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path_to_file, monitor='val_loss', mode='auto'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path_to_file: path to the model file to save at check points\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            filepath=path_to_file,\n",
    "            monitor=monitor,\n",
    "            mode=mode,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,  # Cannot save entire model.\n",
    "            save_freq=\"epoch\",\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        assert self.monitor in logs, \\\n",
    "            f\"monitor metric {self.monitor} not in valid metrics {logs.keys()}\"\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "\n",
    "class ReduceLRCallback(tf.keras.callbacks.ReduceLROnPlateau):\n",
    "    \"\"\"Reduce learning rate when a metric has stopped improving.\n",
    "    See https://keras.io/api/callbacks/reduce_lr_on_plateau/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience=3, monitor='val_loss', mode='auto'):\n",
    "        assert patience > 0\n",
    "        super().__init__(\n",
    "            monitor=monitor,\n",
    "            mode=mode,\n",
    "            factor=0.2,\n",
    "            patience=patience,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        assert self.monitor in logs, \\\n",
    "            f\"monitor metric {self.monitor} not in valid metrics {logs.keys()}\"\n",
    "        super().on_epoch_end(epoch, logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0Rpsc7IjWFd"
   },
   "source": [
    "### Fine Tuning Runner\n",
    "\n",
    "The Runner class implements the fine-tuning based on the [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) pretrained model. Each classification category e.g. ```toxic``` will have a dedicated Runner class instance. The reason for using the ***Distilled*** BERT model is to run the training on the limited resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXWNRYbes8V6"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Dense\n",
    ")\n",
    "\n",
    "\n",
    "class Runner:\n",
    "    \"\"\"Fine tuning implementation class\n",
    "    TODO:\n",
    "        Need to refactor as the implementation is messy.\n",
    "        - Encapsulate common logic and process in the base class.\n",
    "        - Implement specifics in a subclassfor (custom/Keras or huggingface) \n",
    "        - Separate common functions in a library and make them re-usable.\n",
    "        - Make function state-less. No reference to state/memory.\n",
    "        - Eliminate magic numbers e.g. 512 for max BERT sequence length.\n",
    "    \n",
    "    See:\n",
    "    - https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
    "    - https://stackoverflow.com/questions/68172891/\n",
    "    - https://stackoverflow.com/a/68172992/4281353\n",
    "\n",
    "    The TF/Keras model has the base model, e.g distilbert for DistiBERT which is\n",
    "    from the base model TFDistilBertModel.\n",
    "    https://huggingface.co/transformers/model_doc/distilbert.html#tfdistilbertmodel\n",
    "\n",
    "    TFDistilBertForSequenceClassification has classification layers added on top\n",
    "    of TFDistilBertModel, hence not required to add fine-tuning layers by users.\n",
    "    _________________________________________________________________\n",
    "    Layer (type)                 Output Shape              Param #   \n",
    "    =================================================================\n",
    "    distilbert (TFDistilBertMain multiple                  66362880  \n",
    "    _________________________________________________________________\n",
    "    pre_classifier (Dense)       multiple                  590592    \n",
    "    _________________________________________________________________\n",
    "    classifier (Dense)           multiple                  1538      \n",
    "    _________________________________________________________________\n",
    "    dropout_59 (Dropout)         multiple                  0         \n",
    "    =================================================================\n",
    "    \"\"\"\n",
    "    \n",
    "    # ================================================================================\n",
    "    # Class\n",
    "    # ================================================================================\n",
    "    USE_HF_TRAINER = False\n",
    "    USE_CUSTOM_MODEL = False\n",
    "    TOKENIZER_LOWER_CASE = True\n",
    "\n",
    "    # ================================================================================\n",
    "    # Instance\n",
    "    # ================================================================================\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Instance properties\n",
    "    # --------------------------------------------------------------------------------\n",
    "    @property\n",
    "    def category(self):\n",
    "        \"\"\"Category of the text comment classification, e.g. toxic\"\"\"\n",
    "        return self._category\n",
    "\n",
    "    @property\n",
    "    def num_labels(self):\n",
    "        \"\"\"Number of labels to classify\"\"\"\n",
    "        assert self._num_labels > 0\n",
    "        return self._num_labels\n",
    "\n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        \"\"\"BERT tokenizer. The Tokenzer must match the pretrained model\"\"\"\n",
    "        return self._tokenizer\n",
    "\n",
    "    @property\n",
    "    def max_sequence_length(self):\n",
    "        \"\"\"Maximum token length for the BERT tokenizer can accept. Max 512\n",
    "        \"\"\"\n",
    "        assert 128 <= self._max_sequence_length <= 512\n",
    "        return self._max_sequence_length\n",
    "\n",
    "    @property\n",
    "    def X(self):\n",
    "        \"\"\"Training TensorFlow DataSet\"\"\"\n",
    "        return self._X\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        \"\"\"Validation TensorFlow DataSet\"\"\"\n",
    "        return self._V\n",
    "\n",
    "    @property\n",
    "    def model_base_class(self):\n",
    "        \"\"\"HuggingFace base class of the pretrained model class\"\"\"\n",
    "        return self._model_base_class\n",
    "\n",
    "    @property\n",
    "    def model_class(self):\n",
    "        \"\"\"HuggingFace pretrained model class\"\"\"\n",
    "        return self._model_class\n",
    "\n",
    "    @property\n",
    "    def model_name(self):\n",
    "        \"\"\"HuggingFace pretrained model name\"\"\"\n",
    "        return self._model_name\n",
    "\n",
    "    @property\n",
    "    def model_base_name(self):\n",
    "        \"\"\"HuggingFace pretrained base model name\"\"\"\n",
    "        return self._model_base_name\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        \"\"\"TensorFlow/Keras Model instance\"\"\"\n",
    "        return self._model\n",
    "\n",
    "    @property\n",
    "    def freeze_pretrained_base_model(self):\n",
    "        \"\"\"Boolean to freeze the base model\"\"\"\n",
    "        return self._freeze_pretrained_base_model\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        \"\"\"Mini batch size during the training\"\"\"\n",
    "        assert self._batch_size > 0\n",
    "        return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        \"\"\"Training learning rate\"\"\"\n",
    "        return self._learning_rate\n",
    "\n",
    "    @property\n",
    "    def l2(self):\n",
    "        \"\"\"Regularizer decay rate\"\"\"\n",
    "        return self._l2\n",
    "\n",
    "    @property\n",
    "    def reduce_lr_patience(self):\n",
    "        \"\"\"Training patience for reducing learinig rate\"\"\"\n",
    "        return self._reduce_lr_patience\n",
    "\n",
    "    @property\n",
    "    def reduce_lr_factor(self):\n",
    "        \"\"\"Factor to reduce the learinig rate\"\"\"\n",
    "        return self._reduce_lr_factor\n",
    "\n",
    "    @property\n",
    "    def early_stop_patience(self):\n",
    "        \"\"\"Training patience for early stopping\"\"\"\n",
    "        return self._early_stop_patience\n",
    "\n",
    "    @property\n",
    "    def num_epochs(self):\n",
    "        \"\"\"Number of maximum epochs to run for the training\"\"\"\n",
    "        return self._num_epochs\n",
    "\n",
    "    @property\n",
    "    def output_directory(self):\n",
    "        \"\"\"Parent directory to manage training artefacts\"\"\"\n",
    "        return self._output_directory\n",
    "\n",
    "    @property\n",
    "    def output_format(self):\n",
    "        \"\"\"Model output format 'h5' or 'tf'\"\"\"\n",
    "        return self._output_format\n",
    "    \n",
    "    @property\n",
    "    def output_path(self):\n",
    "        \"\"\"file path to save the model\"\"\"\n",
    "        return self.model_directory + os.path.sep + 'model.h5'\n",
    "\n",
    "    @property\n",
    "    def model_directory(self):\n",
    "        \"\"\"Directory to save the trained models\"\"\"\n",
    "        return self._model_directory\n",
    "\n",
    "    @property\n",
    "    def log_directory(self):\n",
    "        \"\"\"Directory to save logs, e.g. TensorBoard logs\"\"\"\n",
    "        return self._log_directory\n",
    "\n",
    "    @property\n",
    "    def model_metric_names(self):\n",
    "        \"\"\"Model mtrics\n",
    "        The attribute model.metrics_names gives labels for the scalar metrics\n",
    "        to be returned from model.evaluate().\n",
    "        \"\"\"\n",
    "        return self.model.metrics_names\n",
    "\n",
    "    @property\n",
    "    def history(self):\n",
    "        \"\"\"The history object returned from model.fit(). \n",
    "        The object holds a record of the loss and metric during training\n",
    "        \"\"\"\n",
    "        assert self._history is not None\n",
    "        return self._history\n",
    "\n",
    "    @property\n",
    "    def trainer(self):\n",
    "        \"\"\"HuggingFace trainer instance\n",
    "        HuggingFace offers an optimized Trainer because PyTorch does not have\n",
    "        the training loop as Keras/Model has. It is available for TensorFlow\n",
    "        as well, hence to be able to hold the instance in case using it.\n",
    "        \"\"\"\n",
    "        return self._trainer\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Instance initialization\n",
    "    # --------------------------------------------------------------------------------\n",
    "    def _build_output_directories(self, output_directory):\n",
    "        # Parent directory\n",
    "        Path(self.output_directory).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Model directory\n",
    "        self._model_directory = \"{parent}/model_C{category}_B{size}_L{length}\".format(\n",
    "            parent=self.output_directory,\n",
    "            category=self.category,\n",
    "            size=self.batch_size,\n",
    "            length=self.max_sequence_length\n",
    "        )\n",
    "        Path(self.model_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Log directory\n",
    "        self._log_directory = \"{parent}/log_C{category}_B{size}_L{length}\".format(\n",
    "            parent=self.output_directory,\n",
    "            category=self.category,\n",
    "            size=self.batch_size,\n",
    "            length=self.max_sequence_length\n",
    "        )\n",
    "        Path(self.log_directory).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def _build_huggingface_model_callbacks(self):\n",
    "        return [\n",
    "            SavePretrainedCallback(\n",
    "                monitor_metric=self._monitor_metric,\n",
    "                monitor_mode=self._monitor_mode,\n",
    "                output_dir=self.model_directory, \n",
    "                verbose=True\n",
    "            ),\n",
    "            ReduceLRCallback(patience=self.reduce_lr_patience),\n",
    "            EarlyStoppingCallback(patience=self.early_stop_patience),\n",
    "            # TensorBoardCallback(self.log_directory),\n",
    "        ]\n",
    "        \n",
    "    def _build_huggingface_model_auc_callbacks(self, validation_data, validation_label):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _build_huggingface_model(self):\n",
    "        \"\"\"Build model based on TFDistilBertForSequenceClassification which has\n",
    "        classification heads added on top of the base BERT model.\n",
    "        \"\"\"\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Base model\n",
    "        # --------------------------------------------------------------------------------\n",
    "        config_file = self.model_directory + os.path.sep + \"config.json\"\n",
    "        if os.path.isfile(config_file) and os.access(config_file, os.R_OK):\n",
    "            # Load the saved model\n",
    "            print(f\"\\nloading the saved huggingface model from {self.model_directory}...\\n\")\n",
    "            self._pretrained_model = self.model_class.from_pretrained(\n",
    "                self.model_directory,\n",
    "                num_labels=self.num_labels\n",
    "            )\n",
    "        else:\n",
    "            # Download the model from Huggingface\n",
    "            self._pretrained_model = self.model_class.from_pretrained(\n",
    "                self.model_name,\n",
    "                num_labels=self.num_labels,            \n",
    "            )\n",
    "\n",
    "        # Freeze base model if required\n",
    "        if self.freeze_pretrained_base_model:\n",
    "            for _layer in self._pretrained_model.layers:\n",
    "                if _layer.name == self.model_base_name:\n",
    "                    _layer.trainable = False\n",
    "\n",
    "        self._model = self._pretrained_model\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Loss layer\n",
    "        # --------------------------------------------------------------------------------\n",
    "        if self.num_labels == 1:    # Binary classification\n",
    "            loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        else:                       # Categorical classification\n",
    "            loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "            \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Build the model\n",
    "        #     from_logits in SparseCategoricalCrossentropy(from_logits=[True|False])\n",
    "        #     True  when the input is logits not  normalized by softmax.\n",
    "        #     False when the input is probability normalized by softmax\n",
    "        # --------------------------------------------------------------------------------\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        self.model.compile(\n",
    "            optimizer=optimizer, \n",
    "            # loss=self.model.compute_loss,\n",
    "            loss=loss_fn,\n",
    "            metrics=self._metrics \n",
    "        )\n",
    "\n",
    "    def _build_custom_model_acc_callbacks(self):\n",
    "        \"\"\"Callbacks for accuracy\"\"\"\n",
    "        return [\n",
    "            EarlyStoppingCallback(\n",
    "                patience=self.early_stop_patience, \n",
    "                monitor=self._monitor_metric, \n",
    "                mode=self._monitor_mode\n",
    "            ),\n",
    "            ReduceLRCallback(\n",
    "                patience=self.reduce_lr_patience, \n",
    "                monitor=self._monitor_metric, \n",
    "                mode=self._monitor_mode\n",
    "            ),\n",
    "            ModelCheckpointCallback(\n",
    "                self.output_path, \n",
    "                monitor=self._monitor_metric, \n",
    "                mode=self._monitor_mode\n",
    "            ),\n",
    "            # TensorBoardCallback(self.log_directory),\n",
    "        ]\n",
    "\n",
    "    def _build_custom_model_auc_callbacks(self, validation_data, validation_label):\n",
    "        \"\"\"Callbacks for ROC AUC\"\"\"\n",
    "        return [\n",
    "            ROCCallback(\n",
    "                validation_data=dict(self.tokenize(validation_data)), \n",
    "                validation_label=validation_label,\n",
    "                output_path=self.output_path,\n",
    "                reduce_lr_patience = self.reduce_lr_patience,\n",
    "                reduce_lr_factor = self.reduce_lr_factor,\n",
    "                early_stop_patience=self.early_stop_patience, \n",
    "                verbose=True\n",
    "            ),\n",
    "            # TensorBoardCallback(self.log_directory),\n",
    "        ]\n",
    "        \n",
    "    def _build_custom_model(self, validation_data, validation_label):\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Input layer (token indices and attention masks)\n",
    "        # --------------------------------------------------------------------------------\n",
    "        input_ids = tf.keras.layers.Input(shape=(self.max_sequence_length,), dtype=tf.int32, name='input_ids')\n",
    "        attention_mask = tf.keras.layers.Input((self.max_sequence_length,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Base layer\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # TFBaseModelOutput.last_hidden_state has shape (batch_size, max_sequence_length, 768)\n",
    "        # Each sequence has [CLS]...[SEP] structure of shape (max_sequence_length, 768)\n",
    "        # Extract [CLS] embeddings of shape (batch_size, 768) as last_hidden_state[:, 0, :]\n",
    "        # --------------------------------------------------------------------------------\n",
    "        base = self.model_base_class.from_pretrained(\n",
    "            self.model_name,\n",
    "        )\n",
    "        # Freeze the base model weights.\n",
    "        if self.freeze_pretrained_base_model:\n",
    "            for layer in base.layers:\n",
    "                layer.trainable = False\n",
    "\n",
    "        base.summary()\n",
    "        output = base([input_ids, attention_mask]).last_hidden_state[:, 0, :]\n",
    "\n",
    "        if USE_CLASSIFICATION_LAYER:\n",
    "            # -------------------------------------------------------------------------------\n",
    "            # Classifiation leayer 01\n",
    "            # --------------------------------------------------------------------------------\n",
    "            output = tf.keras.layers.Dropout(\n",
    "                rate=0.20,\n",
    "                name=\"01_dropout\",\n",
    "            )(output)\n",
    "\n",
    "            output = tf.keras.layers.Dense(\n",
    "                units=NUM_BASE_MODEL_OUTPUT,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                activation=None,\n",
    "                name=\"01_dense_relu_no_regularizer\",\n",
    "            )(output)\n",
    "            output = tf.keras.layers.BatchNormalization(\n",
    "                name=\"01_bn\"\n",
    "            )(output)\n",
    "            output = tf.keras.layers.Activation(\n",
    "                \"relu\",\n",
    "                name=\"01_relu\"\n",
    "            )(output)\n",
    "\n",
    "            # --------------------------------------------------------------------------------\n",
    "            # Classifiation leayer 02\n",
    "            # --------------------------------------------------------------------------------\n",
    "            output = tf.keras.layers.Dense(\n",
    "                units=NUM_BASE_MODEL_OUTPUT,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                activation=None,\n",
    "                name=\"02_dense_relu_no_regularizer\",\n",
    "            )(output)\n",
    "            output = tf.keras.layers.BatchNormalization(\n",
    "                name=\"02_bn\"\n",
    "            )(output)\n",
    "            output = tf.keras.layers.Activation(\n",
    "                \"relu\",\n",
    "                name=\"02_relu\"\n",
    "            )(output)\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # TODO:\n",
    "        #    Need to verify the effect of regularizers. \n",
    "        #\n",
    "        #    [bias regularizer]\n",
    "        #    It looks bias_regularizer adjusts the ROC threshold towards 0.5. \n",
    "        #    Without it, the threshold of the ROC with BinaryCrossEntropy loss was approx 0.02.\n",
    "        #    With    it, the threshold of the ROC with BinaryCrossEntropy loss was approx 0.6.\n",
    "        # --------------------------------------------------------------------------------\n",
    "        activation = \"sigmoid\" if self.num_labels == 1 else \"softmax\"\n",
    "        output = tf.keras.layers.Dense(\n",
    "            units=self.num_labels,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            # https://huggingface.co/transformers/v4.3.3/main_classes/optimizer_schedules.html#adamweightdecay-tensorflow\n",
    "            # kernel_regularizer=tf.keras.regularizers.l2(l2=self.l2),\n",
    "            # bias_regularizer=tf.keras.regularizers.l2(l2=self.l2),\n",
    "            # activity_regularizer=tf.keras.regularizers.l2(l2=self.l2/10.0),\n",
    "            activation=activation,\n",
    "            name=activation\n",
    "        )(output)        \n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Loss layer\n",
    "        # --------------------------------------------------------------------------------\n",
    "        if self.num_labels == 1:    # Binary classification\n",
    "            loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        else:                       # Categorical classification\n",
    "            loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Build model\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # TODO: Replace TIMESTAMP with instance variable\n",
    "        name = f\"{TIMESTAMP}_{self.model_name.upper()}\"\n",
    "        self._model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output, name=name)\n",
    "        self.model.compile(\n",
    "            # https://huggingface.co/transformers/v4.3.3/main_classes/optimizer_schedules.html#adamweightdecay-tensorflow\n",
    "            # optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
    "            optimizer=transformers.AdamWeightDecay(learning_rate=self.learning_rate),\n",
    "            loss=loss_fn,\n",
    "            metrics=self._metrics\n",
    "        )\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Load model parameters if the saved weight file exits\n",
    "        # --------------------------------------------------------------------------------\n",
    "        path_to_h5 = self.model_directory + os.path.sep + \"model.h5\"\n",
    "        if os.path.isfile(path_to_h5) and os.access(path_to_h5, os.R_OK):\n",
    "            print(f\"\\nloading the saved model parameters from {path_to_h5}...\\n\")\n",
    "            self.model.load_weights(path_to_h5)\n",
    "\n",
    "    def _build_huggingface_model_monitor_metrics(self, validation_data, validation_label):\n",
    "        \"\"\"\n",
    "        Callback Monitor Configurations: (metric_name, monitor_metric, monitor_mode)\n",
    "        \"\"\"\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Model Metrics\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self._metrics=[\n",
    "            \"accuracy\",\n",
    "        ]\n",
    "        if self._metric_name in  [\"auc\", \"recall\", \"precision\"]:\n",
    "            assert self.num_labels == 1, \"AUC/Recall/Precision apparrently works only with binary\"\n",
    "            self._monitor_metric = f\"val_{self._metric_name}\"\n",
    "            self._monitor_mode = 'max'\n",
    "            self._metrics=[\n",
    "                \"accuracy\",\n",
    "                tf.keras.metrics.AUC(\n",
    "                    name=\"auc\",\n",
    "                    curve=\"PR\",   # 'ROC' or 'PR'\n",
    "                    multi_label=True if self.num_labels > 1 else False,\n",
    "                    num_labels=self.num_labels,\n",
    "                    from_logits=False\n",
    "                ), \n",
    "                tf.keras.metrics.Recall(\n",
    "                    name=\"recall\",\n",
    "                    class_id=1 if self.num_labels > 1 else None\n",
    "                ), \n",
    "                tf.keras.metrics.Precision(\n",
    "                    name=\"precision\",\n",
    "                    class_id=1 if self.num_labels > 1 else None\n",
    "                )\n",
    "            ]\n",
    "            self._callbacks = self._build_huggingface_model_auc_callbacks(\n",
    "                validation_data, \n",
    "                validation_label\n",
    "            )\n",
    "            \n",
    "        elif self._metric_name == \"loss\":\n",
    "            self._monitor_metric = f\"val_{self._metric_name}\"\n",
    "            self._monitor_mode='min'\n",
    "            self._callbacks = self._build_huggingface_model_callbacks()\n",
    "            \n",
    "        elif self._metric_name == \"accuracy\":\n",
    "            self._monitor_metric = f\"val_{self._metric_name}\"\n",
    "            self._monitor_mode='max'\n",
    "            self._callbacks = self._build_huggingface_model_callbacks()\n",
    "            \n",
    "        elif self._metric_name == \"sca\":   # Sparce Categorical Accuracy\n",
    "            self._monitor_metric = f\"val_{self._metric_name}\"\n",
    "            self._monitor_mode='max'\n",
    "            self._metrics=[\n",
    "                \"accuracy\",\n",
    "                tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "                    name='sca'\n",
    "                )\n",
    "            ]\n",
    "            self._callbacks = self._build_huggingface_model_callbacks()\n",
    "            self._callbacks = self._build_huggingface_model_callbacks()\n",
    "\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unknown monitor metric: {self._metric_name}\")\n",
    "            \n",
    "    def _build_custom_model_monitor_metrics(self, validation_data, validation_label):\n",
    "        \"\"\"\n",
    "        Callback Monitor Configurations: (metric_name, monitor_metric, monitor_mode)\n",
    "        \"\"\"\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Model Metrics\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self._metrics=[\n",
    "            \"accuracy\",\n",
    "        ]\n",
    "        if self._metric_name in  [\"auc\", \"recall\", \"precision\"]:\n",
    "            assert self.num_labels == 1, \"AUC/Recall/Precision apparrently works only with binary\"\n",
    "            self._monitor_metric = f\"val_{self._metric_name}\"\n",
    "            self._monitor_mode = 'max'\n",
    "            self._metrics=[\n",
    "                \"accuracy\",\n",
    "                tf.keras.metrics.AUC(\n",
    "                    name=\"auc\",\n",
    "                    multi_label=True if self.num_labels > 1 else False,\n",
    "                    num_labels=self.num_labels,\n",
    "                    from_logits=False\n",
    "                ), \n",
    "                tf.keras.metrics.Recall(\n",
    "                    name=\"recall\",\n",
    "                    class_id=1 if self.num_labels > 1 else None\n",
    "                ), \n",
    "                tf.keras.metrics.Precision(\n",
    "                    name=\"precision\",\n",
    "                    class_id=1 if self.num_labels > 1 else None\n",
    "                )\n",
    "            ]\n",
    "            self._callbacks = self._build_custom_model_auc_callbacks(\n",
    "                validation_data, \n",
    "                validation_label\n",
    "            )\n",
    "            \n",
    "        elif self._metric_name == \"loss\":\n",
    "            self._monitor_metric = f\"val_{self._metric_name}\"\n",
    "            self._monitor_mode='min'\n",
    "            self._callbacks = self._build_custom_model_acc_callbacks()\n",
    "            \n",
    "        elif self._metric_name == \"accuracy\":\n",
    "            self._monitor_metric = f\"val_{self._metric_name}\"\n",
    "            self._monitor_mode='max'\n",
    "            self._callbacks = self._build_custom_model_acc_callbacks()\n",
    "            \n",
    "        elif self._metric_name == \"sca\":   # Sparce Categorical Accuracy\n",
    "            self._monitor_metric = f\"val_{self._metric_name}\"\n",
    "            self._monitor_mode='max'\n",
    "            self._metrics=[\n",
    "                \"accuracy\",\n",
    "                tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "                    name='sca'\n",
    "                )\n",
    "            ]\n",
    "            self._callbacks = self._build_custom_model_acc_callbacks()\n",
    "\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unknown monitor metric: {self._metric_name}\")\n",
    "\n",
    "    def _build_model(self, validation_data, validation_label):\n",
    "        if self.USE_CUSTOM_MODEL:\n",
    "            self._build_custom_model_monitor_metrics(validation_data, validation_label)\n",
    "\n",
    "            self._build_custom_model(validation_data, validation_label)\n",
    "            self._validate_model = self._validate_custom_model\n",
    "            self._train_fn = self._train_custom_model\n",
    "            self._save_fn = self._save_custom_model\n",
    "            self._load_fn = self._load_custom_model\n",
    "            \n",
    "            # --------------------------------------------------------------------------------\n",
    "            # Prediction function to return logits.\n",
    "            # sigmoid output when num_labels == 1\n",
    "            # Probability for label==1 from softmax when num_labels > 1\n",
    "            # --------------------------------------------------------------------------------\n",
    "            if self.num_labels == 1:\n",
    "                self._predict_fn = self._predict_custom_model_binary\n",
    "            elif self.num_labels > 1:\n",
    "                self._predict_fn = self._predict_custom_model_categorical\n",
    "            else:\n",
    "                assert False, \"Invalid num_labels\"\n",
    "            \n",
    "        else:\n",
    "            self._build_huggingface_model_monitor_metrics(validation_data, validation_label)\n",
    "\n",
    "            self._build_huggingface_model()\n",
    "            self._validate_model = self._validate_huggingface_model\n",
    "            self._train_fn = self._train_huggingface_model\n",
    "            self._save_fn = self._save_huggingface_model\n",
    "            self._load_fn = self._load_huggingface_model\n",
    "            \n",
    "            if self.num_labels == 1:\n",
    "                self._predict_fn = self._predict_huggingface_model_binary\n",
    "            elif self.num_labels > 1:\n",
    "                self._predict_fn = self._predict_huggingface_model_categorical\n",
    "            else:\n",
    "                assert False, \"Invalid num_labels\"\n",
    "\n",
    "    def _validate_huggingface_model(self):\n",
    "        \"\"\"Validate the huggingface model\n",
    "        \"\"\"\n",
    "        # The number of classes in the output must match the num_labels\n",
    "        test_sentences = [\n",
    "            \"i am a cat who has no name.\",\n",
    "            \"to be or not to be.\"\n",
    "        ]\n",
    "        test_tokens = self.tokenize(test_sentences, padding='max_length')\n",
    "        TEST_BATCH_SIZE = len(test_tokens)\n",
    "        \n",
    "        # Huggingface model output is based on TFSequenceClassifierOutput\n",
    "        # which has 'loss' and 'logits' keys.\n",
    "        test_model_output = self.model(test_tokens)['logits']\n",
    "        assert test_model_output.shape == (TEST_BATCH_SIZE, self.num_labels), \\\n",
    "            \"test_model_output type[%s] data [%s]\" % \\\n",
    "            (type(test_model_output), test_model_output)\n",
    "\n",
    "        # predict returns probabilities for the target class/label only \n",
    "        # in a np array of shape (batch_size, 1). The probability value\n",
    "        # is between 0 and 1.\n",
    "        test_predictions = self.predict(test_sentences)\n",
    "        assert test_predictions.shape == (TEST_BATCH_SIZE, 1), \\\n",
    "            \"test_predictions shape[%s] data [%s]\" % \\\n",
    "            (test_predictions.shape, test_predictions)\n",
    "        assert np.all(0 < test_predictions) and np.all(test_predictions < 1)\n",
    "    \n",
    "    def _validate_custom_model(self):\n",
    "        \"\"\"Validate the custom model\n",
    "        \"\"\"\n",
    "        test_sentences = [\n",
    "            \"i am a cat who has no name.\",\n",
    "            \"to be or not to be.\"\n",
    "        ]\n",
    "        test_tokens = self.tokenize(test_sentences, padding='max_length')\n",
    "        TEST_BATCH_SIZE = len(test_tokens)\n",
    "\n",
    "        # Model generates predictions for all the classes/labels.\n",
    "        # including the binary classification where num_labels == 1\n",
    "        test_model_output = self.model(test_tokens)\n",
    "        assert test_model_output.shape == (TEST_BATCH_SIZE, self.num_labels), \\\n",
    "            \"test_model_output type[%s] data [%s]\" % \\\n",
    "            (type(test_model_output), test_model_output)\n",
    "\n",
    "        # predict returns probabilities for the target class/label only \n",
    "        # in a np array of shape (batch_size, 1). The probability value\n",
    "        # is between 0 and 1.\n",
    "        test_predictions = self.predict(test_sentences)\n",
    "        assert test_predictions.shape == (TEST_BATCH_SIZE, 1), \\\n",
    "            \"test_predictions shape[%s] data [%s]\" % \\\n",
    "            (test_predictions.shape, test_predictions)\n",
    "        assert np.all(0 < test_predictions) and np.all(test_predictions < 1)\n",
    "\n",
    "    def _build_dataset(self, training_data, training_label, validation_data, validation_label):\n",
    "        # TODO: \n",
    "        #    Do not generate data here but provide a utility to generate X, V\n",
    "        if self.num_labels == 1:\n",
    "            assert np.all(np.isin(training_label, [0,1]))\n",
    "            assert np.all(np.isin(validation_label, [0,1]))\n",
    "        else:\n",
    "            assert np.all(np.isin(training_label, np.arange(self.num_labels)))\n",
    "            assert np.all(np.isin(validation_label, np.arange(self.num_labels)))\n",
    "\n",
    "        self._X = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(self.tokenize(training_data)),\n",
    "            training_label\n",
    "        ))\n",
    "        self._V = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(self.tokenize(validation_data)),\n",
    "            validation_label\n",
    "        ))\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            category,\n",
    "            training_data,\n",
    "            training_label,\n",
    "            validation_data,\n",
    "            validation_label,\n",
    "            model_base_class=TFDistilBertModel,\n",
    "            model_class=TFDistilBertForSequenceClassification,\n",
    "            tokenizer_class=DistilBertTokenizerFast,\n",
    "            tokenizer_lower=True,\n",
    "            model_name='distilbert-base-uncased',\n",
    "            model_base_name='distilbert',\n",
    "            num_labels=2,\n",
    "            max_sequence_length=256,\n",
    "            freeze_pretrained_base_model=False,\n",
    "            batch_size=32,\n",
    "            learning_rate=2e-5,\n",
    "            l2=1e-4,\n",
    "            metric_name=\"accuracy\",\n",
    "            early_stop_patience=5,\n",
    "            reduce_lr_patience=1,\n",
    "            reduce_lr_factor=0.2,\n",
    "            num_epochs=20,\n",
    "            output_directory=\"./output\",\n",
    "            output_format='h5',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        NOTE:\n",
    "            https://arxiv.org/abs/2006.04884 indicated that over-fitting is not an issue and \n",
    "            recommends longer iterations with small learning rate and early stop on val_accuracy\n",
    "            - learning_rate=2e-5\n",
    "            - monitor_metric='val_accuracy'\n",
    "            - monitor_mode='max'\n",
    "\n",
    "        Args:\n",
    "            category: \n",
    "            training_data: \n",
    "            training_label:\n",
    "            validation_data:\n",
    "            validation_label:\n",
    "            model_name: Huggingface pre-trained model class\n",
    "            model_base_class: Base class of the Pre-trained model\n",
    "            model_name: Huggingface model name\n",
    "            model_base_name: Pre-trained base model name\n",
    "            num_labels: Number of labels\n",
    "            max_sequence_length=256: maximum tokens for tokenizer\n",
    "            freeze_pretrained_base_model: flag to freeze pretrained model base layer\n",
    "            batch_size:\n",
    "            learning_rate:\n",
    "            l2: L2 regularizer decay rate\n",
    "            metric_name: metric for the model\n",
    "            early_stop_patience:\n",
    "            reduce_lr_patience:\n",
    "            reduce_lr_factor:\n",
    "            num_epochs:\n",
    "            output_directory: Directory to save the outputs\n",
    "            output_format: Model save format ['h5' | 'tf']\n",
    "        \"\"\"\n",
    "        self._category = category\n",
    "        self._trainer = None\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Model to use\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self._model_name = model_name\n",
    "        self._model_base_name = model_base_name\n",
    "        self._model_class = model_class\n",
    "        self._model_base_class = model_base_class\n",
    "        self._tokenizer = tokenizer_class.from_pretrained(\n",
    "            model_name, \n",
    "            do_lower_case=self.TOKENIZER_LOWER_CASE\n",
    "        )\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Model training configurations\n",
    "        # --------------------------------------------------------------------------------\n",
    "        assert 128 <= max_sequence_length <= 512, \"Current max sequenth length for BERT is 512\"\n",
    "        self._max_sequence_length = max_sequence_length\n",
    "\n",
    "        assert num_labels > 0\n",
    "        self._num_labels = num_labels\n",
    "\n",
    "        assert isinstance(freeze_pretrained_base_model, bool)\n",
    "        self._freeze_pretrained_base_model = freeze_pretrained_base_model\n",
    "\n",
    "        assert (0.0 < learning_rate) and (0 <= l2 < 1.0)\n",
    "        self._learning_rate = learning_rate\n",
    "        self._l2 = l2\n",
    "        self._model = None\n",
    "\n",
    "        assert num_epochs > 0\n",
    "        self._num_epochs = num_epochs\n",
    "\n",
    "        assert batch_size > 0\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        assert early_stop_patience > 0\n",
    "        self._metric_name = metric_name\n",
    "        self._early_stop_patience = early_stop_patience\n",
    "        self._reduce_lr_patience = reduce_lr_patience\n",
    "        self._reduce_lr_factor = reduce_lr_factor\n",
    "\n",
    "        # model.fit() result holder\n",
    "        self._history = None  \n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Model output\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self._output_directory = output_directory\n",
    "        self._output_format = output_format\n",
    "        self._build_output_directories(output_directory)\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Data\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self._build_dataset(training_data, training_label, validation_data, validation_label)\n",
    "        del training_data, training_label\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Model\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self._build_model(validation_data, validation_label)\n",
    "        del validation_data, validation_label\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Validations\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self._validate_model()\n",
    "        self.model.summary()\n",
    "            \n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Instance methods\n",
    "    # --------------------------------------------------------------------------------\n",
    "    def tokenize(self, sentences, truncation=True, padding='longest'):\n",
    "        \"\"\"Tokenize using the Huggingface tokenizer\n",
    "        Args: \n",
    "            sentences: String or list of string to tokenize\n",
    "            padding: Padding method ['do_not_pad'|'longest'|'max_length']\n",
    "        \"\"\"\n",
    "        return self.tokenizer(\n",
    "            sentences,\n",
    "            truncation=truncation,\n",
    "            padding=padding,\n",
    "            max_length=self.max_sequence_length,\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        sentences = []\n",
    "        if isinstance(tokens, list) or tf.is_tensor(tokens):\n",
    "            for sequence in tokens:\n",
    "                sentences.append(self.tokenizer.decode(sequence))\n",
    "        elif 'input_ids' in tokens:\n",
    "            for sequence in tokens['input_ids']:\n",
    "                sentences.append(self.tokenizer.decode(sequence))\n",
    "        return sentences\n",
    "\n",
    "    def _hf_train(self):\n",
    "        \"\"\"Train the model using HuggingFace Trainer\"\"\"\n",
    "        self._training_args = TFTrainingArguments(\n",
    "            output_dir='./results',             # output directory\n",
    "            num_train_epochs=3,                 # total number of training epochs\n",
    "            per_device_train_batch_size=self.batch_size,     # batch size per device during training\n",
    "            per_device_eval_batch_size=self.batch_size,      # batch size for evaluation\n",
    "            warmup_steps=500,                   # number of warmup steps for learning rate scheduler\n",
    "            weight_decay=0.01,                  # strength of weight decay\n",
    "            logging_dir='./logs',               # directory for storing logs\n",
    "            logging_steps=10,\n",
    "        )\n",
    "\n",
    "        # with self._training_args.strategy.scope():\n",
    "        #     self._model = TFDistilBertForSequenceClassification.from_pretrained(self.model_name)\n",
    "\n",
    "        self._trainer = TFTrainer(\n",
    "            model=self.model,\n",
    "            args=self._training_args,   # training arguments\n",
    "            train_dataset=self.X,       # training dataset\n",
    "            eval_dataset=self.V         # evaluation dataset\n",
    "        )\n",
    "        self.trainer.train()\n",
    "\n",
    "    def _train_huggingface_model(self):\n",
    "        \"\"\"Train the model using Keras\n",
    "        \"\"\"\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Train the model\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self._history = self.model.fit(\n",
    "            self.X.shuffle(1000).batch(self.batch_size).prefetch(tf.data.experimental.AUTOTUNE),\n",
    "            epochs=self.num_epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            validation_data=self.V.shuffle(1000).batch(self.batch_size).prefetch(tf.data.experimental.AUTOTUNE),\n",
    "            callbacks=self._callbacks\n",
    "        )\n",
    "\n",
    "    def _train_custom_model(self):\n",
    "        \"\"\"Train the model using Keras\n",
    "        \"\"\"\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Train the model\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self._history = self.model.fit(\n",
    "            self.X.shuffle(1000).batch(self.batch_size).prefetch(tf.data.experimental.AUTOTUNE),\n",
    "            epochs=self.num_epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            validation_data=self.V.shuffle(1000).batch(self.batch_size).prefetch(tf.data.experimental.AUTOTUNE),\n",
    "            callbacks=self._callbacks\n",
    "        )        \n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Run the model trainig\"\"\"\n",
    "        self._train_fn()\n",
    "\n",
    "    def evaluate(self, data, label):\n",
    "        \"\"\"Evaluate the model on the given data and label.\n",
    "        https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate\n",
    "        The attribute model.metrics_names gives labels for the scalar metrics\n",
    "        to be returned from model.evaluate().\n",
    "\n",
    "        Args:\n",
    "            data: data to run the prediction\n",
    "            label: label for the data\n",
    "        Returns: \n",
    "            scalar loss if the model has a single output and no metrics, OR \n",
    "            list of scalars (if the model has multiple outputs and/or metrics). \n",
    "        \"\"\"\n",
    "        if self.num_labels == 1:\n",
    "            assert np.all(np.isin(label, [0,1]))\n",
    "        else:\n",
    "            assert np.all(np.isin(label, np.arange(self.num_labels)))\n",
    "\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(self.tokenize(data)),\n",
    "            label\n",
    "        ))\n",
    "        evaluation = self.model.evaluate(\n",
    "            test_dataset.shuffle(1000).batch(self.batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        )\n",
    "        return evaluation\n",
    "\n",
    "    def _predict_custom_model_binary(self, data):\n",
    "        \"\"\"Calcuate the binary classification predictions\n",
    "        Args:\n",
    "            data: sentences to tokenize of type List[str]\n",
    "        Returns: Probabilities as numpy array of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        tokens = dict(self.tokenize(data, padding='max_length'))\n",
    "        probabilities = self.model.predict(tokens)\n",
    "        assert isinstance(probabilities, np.ndarray)\n",
    "        return probabilities\n",
    "\n",
    "    def _predict_custom_model_categorical(self, data):\n",
    "        \"\"\"Calcuate the categorical classification predictions\n",
    "        Args:\n",
    "            data: sentences to tokenize of type List[str]\n",
    "        Returns: Probabilities for label 1 as numpy array of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        tokens = dict(self.tokenize(data, padding='max_length'))\n",
    "        probabilities = self.model.predict(tokens)\n",
    "        assert isinstance(probabilities, np.ndarray) and probabilities.ndim == 2\n",
    "        return probabilities[:, 1:2]\n",
    "\n",
    "    def _predict_huggingface_model_binary(self, data):\n",
    "        \"\"\"Calcuate the binary classification predictions\n",
    "        Args:\n",
    "            data: sentences to tokenize of type List[str]\n",
    "        Returns: Probabilities as numpy array of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        tokens = dict(self.tokenize(data))\n",
    "        probabilities = self.model.predict(tokens)[\"logits\"]\n",
    "        return probabilities \n",
    "    \n",
    "    def _predict_huggingface_model_categorical(self, data):\n",
    "        \"\"\"Calcuate the categorical classification predictions\n",
    "        Args:\n",
    "            data: sentences to tokenize of type List[str]\n",
    "        Returns: Probabilities for label 1 as numpy array of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        tokens = dict(self.tokenize(data))\n",
    "        logits = self.model.predict(tokens)[\"logits\"]\n",
    "        # [:, 1:2] -> TensorFlow Tensor indices to select column 1 for all rows\n",
    "        return tf.nn.softmax(logits)[:, 1:2].numpy()\n",
    "\n",
    "    def predict(self, sentences):\n",
    "        \"\"\"Generate prediction (probabilities) for the target label\n",
    "        Args:\n",
    "            sentences: text sentences of type str or List[str]\n",
    "        Return:\n",
    "            normalized probabilities in numpy array via sigmoid or softmax \n",
    "        \"\"\"\n",
    "        result = self._predict_fn(sentences)\n",
    "        assert isinstance(result, np.ndarray) and result.shape[-1] == 1, \\\n",
    "            f\"Expected np.ndarray but {type(result)} {result}\"\n",
    "        return result\n",
    "                \n",
    "    def _save_huggingface_model(self, path_to_dir):\n",
    "        \"\"\"Save Keras model in huggingface format\n",
    "        \"\"\"\n",
    "        self.model.save_pretrained(path_to_dir)\n",
    "\n",
    "    def _save_custom_model(self, path_to_dir):\n",
    "        \"\"\"Save Keras model in \"tf\" format for explicit save.\n",
    "        Use h5 for auto-save model during the trainig to avoid overwriting \n",
    "        the best model saved during the training.\n",
    "        \"\"\"\n",
    "        self.model.save_weights(\n",
    "            self.output_path, overwrite=True, save_format=self.output_format\n",
    "        )\n",
    "\n",
    "    def save(self, path_to_dir):\n",
    "        \"\"\"Save the model from the HuggingFace. \n",
    "        - config.json \n",
    "        - tf_model.h5  \n",
    "\n",
    "        TODO:\n",
    "            Save the best model metrics when saving the model as Keras config file.\n",
    "            Reload the best metrics of the model when loading the model itself.\n",
    "\n",
    "            If the saved best model is re-loaded, the best metric values that the\n",
    "            model achieved need to be re-loaded as well. Otherwise the first epoch\n",
    "            result, even if the metrics are worse than the best metrics achieved,\n",
    "            will become the best results and the best model will be overwritten with\n",
    "            the inferior model.\n",
    "\n",
    "        Args:\n",
    "            path_to_dir: directory path to save the model artefacts\n",
    "        \"\"\"\n",
    "        # path_to_dir is mandatory to avoid overwriting the best model saved\n",
    "        # during the training\n",
    "        Path(path_to_dir).mkdir(parents=True, exist_ok=True)\n",
    "        self._save_fn(path_to_dir)\n",
    "\n",
    "    def _load_huggingface_model(self, path_to_dir):\n",
    "        self._model = self.model_class.from_pretrained(path_to_dir)\n",
    "\n",
    "    def _load_custom_model(self, path_to_dir):\n",
    "        path_to_file = path_to_dir + os.path.sep + 'model.h5'\n",
    "        self.model.load_weights(path_to_file)\n",
    "        \n",
    "    def load(self, path_to_dir):\n",
    "        \"\"\"Load the model as the HuggingFace format.\n",
    "        TODO:\n",
    "            Reload the best metrics of the model when loading the model itself.\n",
    "            If the saved best model is re-loaded, the best metric values that the\n",
    "            model achieved need to be re-loaded as well. Otherwise the first epoch\n",
    "            result, even if the metrics are worse than the best metrics achieved,\n",
    "            will become the best results and the best model will be overwritten with\n",
    "            the inferior model.\n",
    "\n",
    "        Args:\n",
    "            path_to_dir: Directory path from where to load config.json and .h5.\n",
    "        \"\"\"\n",
    "        if os.path.isdir(path_to_dir) and os.access(path_to_dir, os.R_OK):\n",
    "            self._load_fn(path_to_dir)\n",
    "        else:\n",
    "            raise RuntimeError(f\"{path_to_dir} does not exit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "z0xIlT_ugzL4"
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "\n",
    "def balance(\n",
    "    df, \n",
    "    data_col_name,\n",
    "    label_col_name,\n",
    "    retain_columns,\n",
    "    positive_negative_ratio=1.0,\n",
    "    negative_replication_factor=1.0\n",
    "):\n",
    "    \"\"\"Balance the data volumes of positives(label=1) and negatives/0.\n",
    "    negatives (label==0) has more data than positives, causing skewness. \n",
    "    Replicate positives so that they have positive_negative_ratio \n",
    "    times more data than negatives.\n",
    "\n",
    "    This is a naive way. Ideally better to have proper data argumentation.\n",
    "\n",
    "    Args:\n",
    "        df: Pandas dataframe \n",
    "        data_col_name: Column name for the data\n",
    "        label_col_name: Column name for the label\n",
    "        retain_columns: Columns to retain in the dataframe to return\n",
    "        positive_negative_ratio: how many times more the volume of the positves to be than negatives\n",
    "        negative_replication_factor: adjust the negative volume to negative_replication_factor * negative_size\n",
    "    Returns: \n",
    "        Pandas dataframe with the ratin_columns.\n",
    "    \"\"\"\n",
    "    assert 0.0 < positive_negative_ratio <= 10.0\n",
    "    assert 0.0 < negative_replication_factor <= 10.0\n",
    "\n",
    "    positive_indices = df.index[df[label_col_name]==1].tolist()\n",
    "    negative_indices = df.index[df[label_col_name]==0].tolist()\n",
    "    assert not bool(set(positive_indices) & set(negative_indices))\n",
    "\n",
    "    # Adjust the volume of negatives\n",
    "    negative_size = ceil(len(negative_indices) * negative_replication_factor)\n",
    "\n",
    "    # Positives to have positive_negative_ratio times more than negatives\n",
    "    positive_size = ceil(negative_size * positive_negative_ratio)\n",
    "\n",
    "    # Random shuffle and select positives\n",
    "    target_positive_indices = np.random.choice(\n",
    "        a=positive_indices,\n",
    "        size=positive_size,\n",
    "        replace=True\n",
    "    ).tolist()\n",
    "    # Random shuffle and select negatives\n",
    "    target_negative_indices = np.random.choice(\n",
    "        a=negative_indices, \n",
    "        size=negative_size,\n",
    "        replace=True\n",
    "    ).tolist()\n",
    "    assert len(target_positive_indices) >= len(target_negative_indices) * positive_negative_ratio\n",
    "    \n",
    "    # Further shuffle the indices\n",
    "    indices = np.random.choice(\n",
    "        a=target_positive_indices + target_negative_indices,\n",
    "        size=negative_size+positive_size,\n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Extract [data, label]\n",
    "    data = df.iloc[indices][\n",
    "        df.columns[df.columns.isin(retain_columns)]\n",
    "    ]\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_runner(\n",
    "    train,\n",
    "    category,\n",
    "    max_sequence_length,\n",
    "    positive_negative_ratio,\n",
    "    negative_replication_factor,\n",
    "    freeze_pretrained_base_model,\n",
    "    num_labels,\n",
    "    batch_size,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    l2,\n",
    "    metric_name,\n",
    "    early_stop_patience,\n",
    "    reduce_lr_patience,\n",
    "    output_directory,\n",
    "):\n",
    "    \"\"\"Wrapper to create the Runner instances for the respective category\n",
    "    Args:\n",
    "        train: Pandas dataframe containing entire training data\n",
    "        category: unhealthy comment category, e.g. 'toxic'\n",
    "        max_sequence_length:\n",
    "        positive_negative_ratio: how many times more the volume of the positves to be than negatives\n",
    "        negative_replication_factor: adjust the negative volume to negative_replication_factor * negative_size\n",
    "        freeze_pretrained_base_model: flat to freeze the base model\n",
    "        num_labels: Number of classes to classify\n",
    "        batch_size:\n",
    "        num_epochs:\n",
    "        learning_rate:\n",
    "        l2: regularizer weight decay\n",
    "        metric_name: metric for the callbacks to monitor\n",
    "        early_stop_patience:\n",
    "        reduce_lr_patience:\n",
    "        output_directory:\n",
    "    \"\"\"\n",
    "    print(\"\\n--------------------------------------------------------------------------------\")\n",
    "    print(f\"Build runner for [{category}]\")\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "    balanced = balance(\n",
    "        df=train, \n",
    "        data_col_name='comment_text', \n",
    "        label_col_name=category,\n",
    "        retain_columns=['id', 'comment_text', category],\n",
    "        positive_negative_ratio=positive_negative_ratio,\n",
    "        negative_replication_factor=negative_replication_factor\n",
    "    )\n",
    "    data = balanced['comment_text'].tolist()\n",
    "    label = balanced[category].tolist()\n",
    "    del balanced\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Split data into training and validation\n",
    "    # --------------------------------------------------------------------------------\n",
    "    train_data, validation_data, train_label, validation_label = train_test_split(\n",
    "        data,\n",
    "        label,\n",
    "        test_size=.2,\n",
    "        shuffle=True\n",
    "    )\n",
    "    del data, label\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Instantiate the model trainer\n",
    "    # --------------------------------------------------------------------------------\n",
    "    runner = Runner(\n",
    "        category=category,\n",
    "        training_data=train_data,\n",
    "        training_label=train_label,\n",
    "        validation_data=validation_data,\n",
    "        validation_label=validation_label,\n",
    "        max_sequence_length=max_sequence_length,\n",
    "        freeze_pretrained_base_model=freeze_pretrained_base_model,\n",
    "        num_labels=num_labels,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        l2=l2,\n",
    "        metric_name = metric_name,\n",
    "        early_stop_patience=early_stop_patience,\n",
    "        reduce_lr_patience=reduce_lr_patience,\n",
    "        output_directory=output_directory\n",
    "    )\n",
    "    return runner\n",
    "\n",
    "\n",
    "def generate_category_runner(category):\n",
    "    def f(train):\n",
    "        return generate_runner(\n",
    "            category=category,\n",
    "            train=train,\n",
    "            positive_negative_ratio=5.0,\n",
    "            negative_replication_factor=0.2,\n",
    "            freeze_pretrained_base_model=FREEZE_BASE_MODEL,\n",
    "            num_labels=NUM_LABELS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            l2=L2,\n",
    "            metric_name=METRIC_NAME,\n",
    "            early_stop_patience=EARLY_STOP_PATIENCE,\n",
    "            reduce_lr_patience=REDUCE_LR_PATIENCE,\n",
    "            output_directory=RESULT_DIRECTORY\n",
    "        )\n",
    "    return f\n",
    "\n",
    "\n",
    "def evaluate(runner, test, category):\n",
    "    \"\"\"\n",
    "    Evaluate the model of the runner\n",
    "    Args:\n",
    "        runner: Runner instance\n",
    "        test: Pandas dataframe holding entire data\n",
    "    \"\"\"\n",
    "    print(\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print(f\"Model evaluation on [{runner.category}]\")\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    test_data = test['comment_text'].tolist()\n",
    "    test_label = test[category].tolist()\n",
    "    evaluation = runner.evaluate(test_data, test_label)\n",
    "\n",
    "    print(f\"Evaluation: {runner.model_metric_names}:{evaluation}\")\n",
    "    del test_data, test_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wV_uiVhBNM7n"
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEANING_FOR_ANALYSIS and (not CLEANING_FOR_TRAINING):\n",
    "    # Data has been clearned but training needs non cleaned data\n",
    "    train, test = load_raw_data(TEST_MODE)\n",
    "    print(f\"Data records for training [{train['id'].count()}]\")\n",
    "\n",
    "# Drop the rows with -1. ['toxic'] >= 0 is sufficient\n",
    "test = test[test['toxic'] >= 0]\n",
    "gc.collect()\n",
    "\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGFHYUT2gzL3",
    "outputId": "c4c1f98e-6828-4cd0-de1e-f57ef1bce82f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAX_SEQUENCE_LENGTH = 256\n",
      "FREEZE_BASE_MODEL = False\n",
      "NUM_EPOCHS = 10\n",
      "BATCH_SIZE = 32\n",
      "LEARNING_RATE = 5e-05\n",
      "REDUCE_LR_PATIENCE = 3\n",
      "EARLY_STOP_PATIENCE = 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace\n",
    "MAX_SEQUENCE_LENGTH = 256   # Max token length to accept. 512 taks 1 hour/epoch on Google Colab\n",
    "NUM_BASE_MODEL_OUTPUT = 768\n",
    "USE_CLASSIFICATION_LAYER = False\n",
    "\n",
    "# Model training\n",
    "FREEZE_BASE_MODEL = False\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "NUM_LABELS = 2\n",
    "LEARNING_RATE = 2e-5  # Must be small to avoid catastrophic forget\n",
    "L2 = 1e-4\n",
    "METRIC_NAME = 'loss'\n",
    "REDUCE_LR_PATIENCE = 1\n",
    "EARLY_STOP_PATIENCE = 3\n",
    "\n",
    "print(\"\"\"\n",
    "TIMESTAMP = {}\n",
    "CLEANING_FOR_TRAINING = {}\n",
    "MAX_SEQUENCE_LENGTH = {}\n",
    "FREEZE_BASE_MODEL = {}\n",
    "NUM_LABELS = {}\n",
    "NUM_EPOCHS = {}\n",
    "BATCH_SIZE = {}\n",
    "LEARNING_RATE = {}\n",
    "L2 = {}\n",
    "METRIC_NAME = {}\n",
    "REDUCE_LR_PATIENCE = {}\n",
    "EARLY_STOP_PATIENCE = {}\n",
    "RESULT_DIRECTORY = {}\n",
    "\"\"\".format(\n",
    "    TIMESTAMP,\n",
    "    CLEANING_FOR_TRAINING,\n",
    "    MAX_SEQUENCE_LENGTH,\n",
    "    FREEZE_BASE_MODEL,\n",
    "    NUM_LABELS,\n",
    "    NUM_EPOCHS,\n",
    "    BATCH_SIZE,\n",
    "    LEARNING_RATE,\n",
    "    L2,\n",
    "    METRIC_NAME,\n",
    "    REDUCE_LR_PATIENCE,\n",
    "    EARLY_STOP_PATIENCE,\n",
    "    RESULT_DIRECTORY,\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Y3KFBkjcknCD",
    "o-Oj3ltY_T8m",
    "lZZbTtUGWx9S",
    "Qpx-pzGWasel",
    "gcefqEMkA2IV",
    "7UPio2cmTcWO",
    "QcngPOCBW4R1",
    "_PFau2osjvx-",
    "k0jSqXxxQrUj",
    "1wwZFMlD6pIJ",
    "nUYPD0TQvEaI",
    "pFWPmiCkaVsN",
    "ShId8bvTGlER",
    "s4jNfuchQiPy",
    "f-RWZFJj8lJZ",
    "5bp2VT2ohkKD",
    "w0Rpsc7IjWFd",
    "XJyvUG3cGylz",
    "SvZ2Z7JQ9jsS"
   ],
   "machine_shape": "hm",
   "name": "toxic_comment_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
