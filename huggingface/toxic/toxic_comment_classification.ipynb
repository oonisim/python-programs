{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "toxic_comment_classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDHpPdvDZ0bG"
      },
      "source": [
        "# Toxic Comment Classification \n",
        "\n",
        "[Kaggle - Toxic Comment Classification Challenage](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview)\n",
        "\n",
        "> Build a multi-headed model to detect different types of of toxicity like threats, obscenity, insults, and identity-based. \n",
        "\n",
        "## Notes on Related Resources\n",
        "\n",
        "* [How AI Is Learning to Identify Toxic Online Content](https://www.scientificamerican.com/article/can-ai-identify-toxic-online-content/)\n",
        ">  This is how our team built Detoxify, an open-source, user-friendly comment detection library to identify inappropriate or harmful text online. Its intended use is to help researchers and practitioners identify potential toxic comments. ... Each model can be easily accessed in one line of code and all models and training code are [publicly available on GitHub (Detoxify)]((https://github.com/unitaryai/detoxify)).\n",
        "\n",
        "* [Detecting toxic comments with Keras and interpreting the model with ELI5](https://medium.com/@armandj.olivares/detecting-toxic-comments-with-keras-and-interpreting-the-model-with-eli5-dbe734f3e86b) - ([Github](https://github.com/ArmandDS/toxic_detection/blob/master/toxic_detection.ipynb))\n",
        "> Develop an estimator with a neural network model for a text classification problem and used ELI5 library for explain the predictions\n",
        "\n",
        "* [Data Augmentation in NLP\n",
        "](https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28) - nlpaug\n",
        "> We explore different authors how they leverage augmentation to tickle NLP tasks via generating more text data to boost up the models.  \n",
        "\n",
        "* Github [nlpaug](https://github.com/makcedward/nlpaug)\n",
        "> This python library helps you with augmenting nlp for your machine learning projects. Visit this introduction to understand about Data Augmentation in NLP. Augmenter is the basic element of augmentation while Flow is a pipeline to orchestra multi augmenter together.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3KFBkjcknCD"
      },
      "source": [
        "---\n",
        "# Setups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6by6Rbv4_Z9"
      },
      "source": [
        "# To reduce the data volumen to run through the training in short timeframe.\n",
        "TEST_MODE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVDkx-1pky8C"
      },
      "source": [
        "## Modules\n",
        "\n",
        "Install and load Python modules required for the task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brzFWR6YhqCF"
      },
      "source": [
        "!pip install tensorflow transformers pandas scikit-learn spacy wordcloud gensim h5py \n",
        "!pip install clean-text unidecode nltk\n",
        "!pip install matplotlib seaborn\n",
        "!pip install line_profiler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jaR9e-xkqNc"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZZbTtUGWx9S"
      },
      "source": [
        "## Jupyter Notebook\n",
        "\n",
        "Jupyter cell format configurations. Align the cell output to the left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFiq3iQjVDEL"
      },
      "source": [
        "%%html\n",
        "<style>\n",
        "table {float:left}\n",
        "</style>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qpx-pzGWasel"
      },
      "source": [
        "## Python logging\n",
        "\n",
        "Control the logging outputs to supress the warning and information to prevent the execution results from being cluttered. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofY8HgJPcu7K"
      },
      "source": [
        "logging.disable(logging.WARNING)\n",
        "logging.basicConfig(level=logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leheXoY8q8b_"
      },
      "source": [
        "## Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CzdpqIDq7oA"
      },
      "source": [
        "from cleantext import clean\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6fCHrZNTePV"
      },
      "source": [
        "## Google Colab\n",
        "\n",
        "Google Colab specific operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzCt--KyGh8i"
      },
      "source": [
        "def google_colab_info():\n",
        "    \"\"\"Information on the Google Colab environment\n",
        "    \"\"\"\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # GPU\n",
        "    # --------------------------------------------------------------------------------\n",
        "    gpu_info = !nvidia-smi\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "        print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "        print('and then re-execute this cell.')\n",
        "    else:\n",
        "        print(gpu_info)\n",
        "\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # Memory\n",
        "    # --------------------------------------------------------------------------------\n",
        "    from psutil import virtual_memory\n",
        "    ram_gb = virtual_memory().total / 1e9\n",
        "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "    if ram_gb < 20:\n",
        "        print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "        print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "        print('re-execute this cell.')\n",
        "    else:\n",
        "        print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP1Ln4gxTgio"
      },
      "source": [
        "try:\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # Check if the environment is Google Colab.\n",
        "    # --------------------------------------------------------------------------------\n",
        "    import google.colab\n",
        "    IN_GOOGLE_COLAB = True\n",
        "    print(\"Using Google Colab environment.\")\n",
        "\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # Show Google Colab information\n",
        "    # --------------------------------------------------------------------------------\n",
        "    google_colab_info()\n",
        "    \n",
        "except:\n",
        "    IN_GOOGLE_COLAB = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UPio2cmTcWO"
      },
      "source": [
        "## Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLeUHOaJ1bt5"
      },
      "source": [
        "import pandas as pd\n",
        "pd.options.display.max_colwidth = 1000   # Allow long string content in a cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRq43r5RSoCA"
      },
      "source": [
        "## TensorFlow\n",
        "\n",
        "Control TensorFlow logging.\n",
        "\n",
        "| TF_CPP_MIN_LOG_LEVEL | Description|          \n",
        "| - |------------- | \n",
        "|0| Suppress all messages are logged (default behavior)|\n",
        "|1 |Suppress INFO messages are not printed|\n",
        "|2 |Suppress INFO and WARNING messages are not printed|\n",
        "|3 |Suppress INFO, WARNING, and ERROR messages are not printed|\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJfDGer2Sqkh"
      },
      "source": [
        "import tensorflow as tf\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXx4rCOeakbp"
      },
      "source": [
        "## Transformers\n",
        "\n",
        "[HuggingFace](https://huggingface.co/transformers/) offers the libarary for NLP (Natural Language Processing) based on the Transfoemer architecture introduced in the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) by Google.\n",
        "\n",
        "> Transformers provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between Jax, PyTorch and TensorFlow.\n",
        "\n",
        "### Transfer Learning (Fine-Tuning)\n",
        "\n",
        "Utilize the [Sequence Classification](https://huggingface.co/transformers/task_summary.html#sequence-classification) capabilty of BERT for the text classification by fine-tuing the pre-trained BERT model upon the data provided. \n",
        "\n",
        "* [Fine-tuning a pretrained model](https://huggingface.co/transformers/training.html)\n",
        "> How to fine-tune a pretrained model from the Transformers library. In TensorFlow, models can be directly trained using Keras and the fit method. \n",
        "\n",
        "* [Fine-tuning with custom datasets](https://huggingface.co/transformers/custom_datasets.html)\n",
        "> This tutorial will take you through several examples of using ðŸ¤— Transformers models with your own datasets.\n",
        "\n",
        "* [HuggingFace Text classification examples](https://github.com/huggingface/transformers/tree/master/examples/tensorflow/text-classification)\n",
        "> This folder contains some scripts showing examples of text classification with the hugs Transformers library. \n",
        "\n",
        "The code in this notebook is based on the [run_text_classification.py](https://github.com/huggingface/transformers/blob/master/examples/tensorflow/text-classification/run_text_classification.py) example for TensorFlow and the code in the documentation [Fine-tuning with custom datasets](https://huggingface.co/transformers/custom_datasets.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Fct7lHwa1yg"
      },
      "source": [
        "from transformers import (\n",
        "    PreTrainedModel,\n",
        "    DistilBertTokenizerFast,\n",
        "    TFDistilBertForSequenceClassification,\n",
        "    TFTrainer,\n",
        "    TFTrainingArguments\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# Control log level (https://huggingface.co/transformers/main_classes/logging.html)\n",
        "# --------------------------------------------------------------------------------\n",
        "os.environ['TRANSFORMERS_VERBOSITY'] = \"error\"\n",
        "import transformers\n",
        "transformers.logging.set_verbosity(transformers.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB2cjqCZhcE9"
      },
      "source": [
        "## Fine Tuning Implementation\n",
        "\n",
        "Utilize TensorFlow 2.x Keras for training the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bp2VT2ohkKD"
      },
      "source": [
        "#### Keras Callbacks\n",
        "\n",
        "Utilize [Keras Callbacks API](https://keras.io/api/callbacks/) to apply Eary Stopping, Reduce Learning Rate, and TensorBoard during the model training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc67_HTf0zyO"
      },
      "source": [
        "class SavePretrainedCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Hugging Face models have a save_pretrained() method that saves both \n",
        "    the weights and the necessary metadata to allow them to be loaded as \n",
        "    a pretrained model in future. This is a simple Keras callback that \n",
        "    saves the model with this method after each epoch.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dir, **kwargs):\n",
        "        super().__init__()\n",
        "        self.output_dir = output_dir\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.model.save_pretrained(self.output_dir)\n",
        "\n",
        "class TensorBoardCallback(tf.keras.callbacks.TensorBoard):\n",
        "    \"\"\"TensorBoard visualization of the model training\n",
        "    See https://keras.io/api/callbacks/tensorboard/\n",
        "    \"\"\"\n",
        "    def __init__(self, output_directory):\n",
        "        super().__init__(\n",
        "            log_dir=output_directory,\n",
        "            write_graph=True,\n",
        "            write_images=True,\n",
        "            histogram_freq=1,     # log histogram visualizations every 1 epoch\n",
        "            embeddings_freq=1,    # log embedding visualizations every 1 epoch\n",
        "            update_freq=\"epoch\",      # every epoch\n",
        "        )\n",
        "\n",
        "class EaryStoppingCallback(tf.keras.callbacks.EarlyStopping):\n",
        "    \"\"\"Stop training when no progress on the metric to monitor\n",
        "    See: \n",
        "    https://keras.io/api/callbacks/early_stopping/\n",
        "    https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=3):\n",
        "        assert patience > 0\n",
        "        super().__init__(\n",
        "            monitor='val_loss', \n",
        "            mode='min', \n",
        "            verbose=1, \n",
        "            patience=patience,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "class ModelCheckpointCallback(tf.keras.callbacks.ModelCheckpoint):\n",
        "    \"\"\"Check point to save the model\n",
        "    See https://keras.io/api/callbacks/model_checkpoint/\n",
        "\n",
        "    NOTE: Did not work with HuggingFace with the error.\n",
        "        NotImplementedError: Saving the model to HDF5 format requires the model \n",
        "        to be a Functional model or a Sequential model. \n",
        "        It does not work for subclassed models, because such models are defined \n",
        "        via the body of a Python method, which isn't safely serializable. \n",
        "    \"\"\"\n",
        "    def __init__(self, path_to_file):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            path_to_file: path to the model file to save at check points\n",
        "        \"\"\"\n",
        "        super().__init__(\n",
        "            filepath=path_to_file, \n",
        "            monitor='val_loss', \n",
        "            mode='min', \n",
        "            save_best_only=True\n",
        "        )\n",
        "\n",
        "class ReduceLRCallback(tf.keras.callbacks.ReduceLROnPlateau):\n",
        "    \"\"\"Reduce learning rate when a metric has stopped improving.\n",
        "    See https://keras.io/api/callbacks/reduce_lr_on_plateau/\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=3):\n",
        "        assert patience > 0\n",
        "        super().__init__(\n",
        "            monitor=\"val_loss\",\n",
        "            factor=0.3,\n",
        "            patience=patience,\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0Rpsc7IjWFd"
      },
      "source": [
        "### Fine Tuning Runner\n",
        "\n",
        "The Runner class implements the fine-tuning based on the [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) pretrained model. Each classification category e.g. ```toxic``` will have a dedicated Runner class instance. The reason for using the ***Distilled*** BERT model is to run the training on the limited resources\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXWNRYbes8V6"
      },
      "source": [
        "class Runner:\n",
        "    \"\"\"Fine tuning implementation class\n",
        "    See https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
        "    \"\"\"\n",
        "    # ================================================================================\n",
        "    # Class\n",
        "    # ================================================================================\n",
        "    USE_HF_TRAINER = False\n",
        "    # _model_name = 'distilbert-base-cased'\n",
        "    _model_name = 'distilbert-base-uncased'\n",
        "    _tokenizer = DistilBertTokenizerFast.from_pretrained(_model_name)\n",
        "\n",
        "    # ================================================================================\n",
        "    # Instance\n",
        "    # ================================================================================\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # Instance properties\n",
        "    # --------------------------------------------------------------------------------\n",
        "    @property\n",
        "    def category(self):\n",
        "        \"\"\"Category of the text comment classification, e.g. toxic\"\"\"\n",
        "        return self._category\n",
        "\n",
        "    @property\n",
        "    def batch_size(self):\n",
        "        \"\"\"Mini batch size during the training\"\"\"\n",
        "        assert self._batch_size > 0\n",
        "        return self._batch_size\n",
        "\n",
        "    @property\n",
        "    def X(self):\n",
        "        \"\"\"Training TensorFlow DataSet\"\"\"\n",
        "        return self._X\n",
        "\n",
        "    @property\n",
        "    def V(self):\n",
        "        \"\"\"Validation TensorFlow DataSet\"\"\"\n",
        "        return self._V\n",
        "\n",
        "    @property\n",
        "    def model_name(self):\n",
        "        \"\"\"HuggingFace pretrained model name\"\"\"\n",
        "        return self._model_name\n",
        "\n",
        "    @property\n",
        "    def model(self):\n",
        "        \"\"\"TensorFlow/Keras Model instance\"\"\"\n",
        "        return self._model\n",
        "\n",
        "    @property\n",
        "    def model_metric_names(self):\n",
        "        \"\"\"Model mtrics\n",
        "        The attribute model.metrics_names gives labels for the scalar metrics\n",
        "        to be returned from model.evaluate().\n",
        "        \"\"\"\n",
        "        return self.model.metrics_names\n",
        "\n",
        "    @property\n",
        "    def history(self):\n",
        "        \"\"\"The history object returned from model.fit(). \n",
        "        The object holds a record of the loss and metric during training\n",
        "        \"\"\"\n",
        "        assert self._history is not None\n",
        "        return self._history\n",
        "\n",
        "    @property\n",
        "    def learning_rate(self):\n",
        "        \"\"\"Training learning rate\"\"\"\n",
        "        return self._learning_rate\n",
        "\n",
        "    @property\n",
        "    def num_epochs(self):\n",
        "        \"\"\"Number of maximum epochs to run for the training\"\"\"\n",
        "        return self._num_epochs\n",
        "\n",
        "    @property\n",
        "    def tokenizer(self):\n",
        "        \"\"\"BERT tokenizer. The Tokenzer must match the pretrained model\"\"\"\n",
        "        return self._tokenizer\n",
        "\n",
        "    @property\n",
        "    def max_sequence_length(self):\n",
        "        \"\"\"Maximum token length for the BERT tokenizer can accept. Max 512\n",
        "        \"\"\"\n",
        "        assert 128 <= self._max_sequence_length <= 512\n",
        "        return self._max_sequence_length\n",
        "\n",
        "    @property\n",
        "    def trainer(self):\n",
        "        \"\"\"HuggingFace trainer instance\n",
        "        HuggingFace offers an optimized Trainer because PyTorch does not have\n",
        "        the training loop as Keras/Model has. It is available for TensorFlow\n",
        "        as well, hence to be able to hold the instance in case using it.\n",
        "        \"\"\"\n",
        "        return self._trainer\n",
        "\n",
        "    @property\n",
        "    def output_directory(self):\n",
        "        \"\"\"Parent directory to manage training artefacts\"\"\"\n",
        "        return self._output_directory\n",
        "\n",
        "    @property\n",
        "    def model_directory(self):\n",
        "        \"\"\"Directory to save the trained models\"\"\"\n",
        "        return self._model_directory\n",
        "\n",
        "    @property\n",
        "    def log_directory(self):\n",
        "        \"\"\"Directory to save logs, e.g. TensorBoard logs\"\"\"\n",
        "        return self._log_directory\n",
        "\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # Instance initialization\n",
        "    # --------------------------------------------------------------------------------\n",
        "    def __init__(\n",
        "            self,\n",
        "            category,\n",
        "            training_data,\n",
        "            training_label,\n",
        "            validation_data,\n",
        "            validation_label,\n",
        "            max_sequence_length=256,\n",
        "            batch_size=16,\n",
        "            learning_rate=5e-5,\n",
        "            num_epochs=3,\n",
        "            output_directory=\"./output\"\n",
        "    ):\n",
        "        self._category = category\n",
        "\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # Keras Model\n",
        "        # --------------------------------------------------------------------------------\n",
        "        assert learning_rate > 0.0\n",
        "        self._learning_rate = learning_rate\n",
        "        self._model = None\n",
        "\n",
        "        assert num_epochs > 0\n",
        "        self._num_epochs = num_epochs\n",
        "\n",
        "        assert batch_size > 0\n",
        "        self._batch_size = batch_size\n",
        "        \n",
        "        # model.fit() result holder\n",
        "        self._history = None  \n",
        "\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # HuggingFace artefacts\n",
        "        # --------------------------------------------------------------------------------\n",
        "        assert 128 <= max_sequence_length <= 512\n",
        "        self._max_sequence_length = max_sequence_length\n",
        "        self._model = TFDistilBertForSequenceClassification.from_pretrained(self.model_name)\n",
        "        self._trainer = None\n",
        "\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # Output directories\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # Parent directory\n",
        "        self._output_directory = output_directory\n",
        "        Path(self.output_directory).mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Model directory\n",
        "        self._model_directory = \"{parent}/model_C{category}_B{size}_L{length}\".format(\n",
        "            parent=self.output_directory,\n",
        "            category=self.category,\n",
        "            size=self.batch_size,\n",
        "            length=self.max_sequence_length\n",
        "        )\n",
        "        Path(self.model_directory).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Log directory\n",
        "        self._log_directory = \"{parent}/log_C{category}_B{size}_L{length}\".format(\n",
        "            parent=self.output_directory,\n",
        "            category=self.category,\n",
        "            size=self.batch_size,\n",
        "            length=self.max_sequence_length\n",
        "        )\n",
        "        Path(self.log_directory).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # TensorFlow DataSet\n",
        "        # --------------------------------------------------------------------------------\n",
        "        assert np.all(np.isin(training_label, [0, 1]))\n",
        "        assert np.all(np.isin(validation_label, [0, 1]))\n",
        "        self._X = tf.data.Dataset.from_tensor_slices((\n",
        "            dict(self.tokenizer(\n",
        "                training_data,\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=self.max_sequence_length,\n",
        "                return_tensors=\"tf\"\n",
        "            )),\n",
        "            training_label\n",
        "        ))\n",
        "        self._V = tf.data.Dataset.from_tensor_slices((\n",
        "            dict(self.tokenizer(\n",
        "                validation_data,\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=self.max_sequence_length,\n",
        "                return_tensors=\"tf\"\n",
        "            )),\n",
        "            validation_label\n",
        "        ))\n",
        "\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # Instance methods\n",
        "    # --------------------------------------------------------------------------------\n",
        "    def _hf_train(self):\n",
        "        \"\"\"Train the model using HuggingFace Trainer\"\"\"\n",
        "        self._training_args = TFTrainingArguments(\n",
        "            output_dir='./results',             # output directory\n",
        "            num_train_epochs=3,                 # total number of training epochs\n",
        "            per_device_train_batch_size=self.batch_size,     # batch size per device during training\n",
        "            per_device_eval_batch_size=self.batch_size,      # batch size for evaluation\n",
        "            warmup_steps=500,                   # number of warmup steps for learning rate scheduler\n",
        "            weight_decay=0.01,                  # strength of weight decay\n",
        "            logging_dir='./logs',               # directory for storing logs\n",
        "            logging_steps=10,\n",
        "        )\n",
        "\n",
        "        # with self._training_args.strategy.scope():\n",
        "        #     self._model = TFDistilBertForSequenceClassification.from_pretrained(self.model_name)\n",
        "\n",
        "        self._trainer = TFTrainer(\n",
        "            model=self.model,\n",
        "            args=self._training_args,   # training arguments\n",
        "            train_dataset=self.X,       # training dataset\n",
        "            eval_dataset=self.V         # evaluation dataset\n",
        "        )\n",
        "        self.trainer.train()\n",
        "\n",
        "    def _keras_train(self):\n",
        "        \"\"\"Train the model using Keras\n",
        "        \"\"\"\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # Build the model\n",
        "        # --------------------------------------------------------------------------------\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "        self.model.compile(\n",
        "            optimizer=optimizer, \n",
        "            loss=self.model.compute_loss,\n",
        "            # loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            # loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "            # [\"accuracy\", \"AUC\"] causes an error:\n",
        "            # ValueError: Shapes (None, 1) and (None, 2) are incompatible\n",
        "            metrics = [\"accuracy\"]  \n",
        "        )\n",
        "        self.model.summary()\n",
        "\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # Train the model\n",
        "        # --------------------------------------------------------------------------------\n",
        "        self._history = self.model.fit(\n",
        "            self.X.shuffle(1000).batch(self.batch_size).prefetch(1),\n",
        "            epochs=self.num_epochs,\n",
        "            batch_size=self.batch_size,\n",
        "            validation_data=self.V.shuffle(1000).batch(self.batch_size).prefetch(1),\n",
        "            callbacks=[\n",
        "                EaryStoppingCallback(patience=3),\n",
        "                # SavePretrainedCallback(output_dir=self.output_directory),\n",
        "                ReduceLRCallback(patience=2),\n",
        "                TensorBoardCallback(self.log_directory)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Run the model trainig\"\"\"\n",
        "        if self.USE_HF_TRAINER:\n",
        "            self._hf_train()\n",
        "        else:\n",
        "            self._keras_train()\n",
        "\n",
        "    def evaluate(self, data, label):\n",
        "        \"\"\"Evaluate the model on the given data and label.\n",
        "        https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate\n",
        "        The attribute model.metrics_names gives labels for the scalar metrics\n",
        "        to be returned from model.evaluate().\n",
        "\n",
        "        Args:\n",
        "            data: data to run the prediction\n",
        "            label: label for the data\n",
        "        Returns: \n",
        "            scalar loss if the model has a single output and no metrics, OR \n",
        "            list of scalars (if the model has multiple outputs and/or metrics). \n",
        "        \"\"\"\n",
        "        assert np.all(np.isin(label, [0, 1]))\n",
        "        test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "            dict(self.tokenizer(\n",
        "                data,\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=self.max_sequence_length,\n",
        "                return_tensors=\"tf\"\n",
        "            )),\n",
        "            label\n",
        "        ))\n",
        "        evaluation = self.model.evaluate(\n",
        "            test_dataset.shuffle(1000).batch(self.batch_size).prefetch(1)\n",
        "        )\n",
        "        return evaluation\n",
        "\n",
        "    def predict(self, data):\n",
        "        \"\"\"Calcuate the prediction for the data\n",
        "        Args:\n",
        "            data: text data to classify\n",
        "        Returns: Probabilities for label value 0 and 1\n",
        "        \"\"\"\n",
        "        tokens = dict(self.tokenizer(\n",
        "            data,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=self.max_sequence_length,\n",
        "            return_tensors=\"tf\"\n",
        "        ))\n",
        "        logits = self.model.predict(tokens)[\"logits\"]\n",
        "        # return tf.nn.softmax(logits)\n",
        "        return logits\n",
        "\n",
        "    def save(self, path_to_dir=None):\n",
        "        \"\"\"Save the model from the HuggingFace. \n",
        "        - config.json \n",
        "        - tf_model.h5  \n",
        "\n",
        "        Args:\n",
        "            path_to_dir: directory path to save the HuggingFace model artefacts\n",
        "        \"\"\"\n",
        "\n",
        "        if path_to_dir is None or len(path_to_dir) == 0:\n",
        "            path_to_dir = self.model_directory\n",
        "        Path(path_to_dir).mkdir(parents=True, exist_ok=True)\n",
        "        if self.USE_HF_TRAINER:\n",
        "            self.trainer.save_model(path_to_dir)  \n",
        "        else:\n",
        "            self.model.save_pretrained(path_to_dir)\n",
        "\n",
        "    def load(self, path_to_dir):\n",
        "        \"\"\"Load the model as the HuggingFace format.\n",
        "        Args:\n",
        "            path_to_dir: Directory path from where to load config.json and .h5.\n",
        "        \"\"\"\n",
        "        if os.path.isdir(path_to_dir) and os.access(path_to_dir, os.R_OK):\n",
        "            self._model = TFDistilBertForSequenceClassification.from_pretrained(path_to_dir)\n",
        "        else:\n",
        "            raise RuntimeError(f\"{path_to_dir} does not exit\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PFau2osjvx-"
      },
      "source": [
        "---\n",
        "# Data\n",
        "\n",
        "**DATA_PATH** variable points to the location of the data package for [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data) in the Google Drive or in the local directory. Unzip the data package to extract the data for training and testing.\n",
        "\n",
        "* train.csv\n",
        "* test.csv\n",
        "* test_labels.csv - 0/1 binary labels to identify the comment is rated for each category (e.g. toxici)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXvCh3GqXtw7"
      },
      "source": [
        "if IN_GOOGLE_COLAB:\n",
        "    google.colab.drive.mount('/content/drive')\n",
        "    DATA_PATH=\"/content/drive/MyDrive/data/jigsaw-toxic-comment-classification-challenge.zip\"\n",
        "else:\n",
        "    DATA_PATH = input(\"Enter the data archive path\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz-5NlxtoB34"
      },
      "source": [
        "!unzip -o $DATA_PATH\n",
        "!unzip -o train.csv.zip\n",
        "!unzip -o test.csv.zip\n",
        "!unzip -o test_labels.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7Id3UZspT7h"
      },
      "source": [
        "raw_train = pd.read_csv(\"./train.csv\")\n",
        "raw_test_data = pd.read_csv(\"./test.csv\")\n",
        "raw_test_label = pd.read_csv(\"./test_labels.csv\")\n",
        "raw_test = pd.merge(raw_test_data, raw_test_label, left_on='id', right_on='id', how='inner')\n",
        "\n",
        "if TEST_MODE:\n",
        "    raw_train = raw_train.head(256)\n",
        "    raw_test = raw_test.head(256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqRp3qdczPM5"
      },
      "source": [
        "## Raw data (train.csv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKAe7wXMzOMd"
      },
      "source": [
        "raw_train.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wfx5DKry2KjZ"
      },
      "source": [
        "raw_train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NaP9KvTzJuZ"
      },
      "source": [
        "raw_train[raw_train['toxic'] > 0].head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0jSqXxxQrUj"
      },
      "source": [
        "## Raw data (test.csv)\n",
        "Remove the rows where the label value is -1 as as the meaning is not clearly defined.\n",
        "\n",
        "> test_labels.csv - labels for the test data; value of -1 indicates it was not used for scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9kdtivUQqtW"
      },
      "source": [
        "# Removing rows where 'toxic' label >= 0 is sufficicent to eliminate all -1\n",
        "raw_test = raw_test[(raw_test['toxic'] >= 0)] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve713cdzWptk"
      },
      "source": [
        "raw_test.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVyle97CWy7A"
      },
      "source": [
        "raw_test.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmN1y1eitrz_"
      },
      "source": [
        "---\n",
        "# Cleaning Data\n",
        "\n",
        "### De-contraction\n",
        "\n",
        "Restore contraction (e.g. I'll) into de-contracted form (I will) to have the valid words in place.\n",
        "\n",
        "### Stop words removal\n",
        "\n",
        "Remove the stop words (e.g. EOL, an, the, us) that has neither positive nor negative influence on the sentence.\n",
        "\n",
        "* Remove EOL (End of line)\n",
        "* Remove numbers, digits\n",
        "* Remove email, URL, phone number, currency symbols\n",
        "* Remove stop words e.g. a, an, the, us\n",
        "\n",
        "### Lemmatization\n",
        "Reducing the inflectional forms (e.g. having, had, has) of a word into its base (have) to focus on the core meaning, not the form of the words.\n",
        "\n",
        "### Case consistency\n",
        "Lower the case to remove variations of the same word.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoBuLSZlGZCy"
      },
      "source": [
        "def decontracted(sentences):\n",
        "    \"\"\"Restore the contracted words\"\"\"\n",
        "    # specific\n",
        "    sentences = re.sub(r\"won\\'t\", \"will not\", sentences, flags=re.IGNORECASE)\n",
        "    sentences = re.sub(r\"can\\'t\", \"can not\", sentences, flags=re.IGNORECASE)\n",
        "    # general\n",
        "    sentences = re.sub(r\"n\\'t\", \" not\", sentences, flags=re.IGNORECASE)\n",
        "    sentences = re.sub(r\"\\'re\", \" are\", sentences, flags=re.IGNORECASE)\n",
        "    sentences = re.sub(r\"\\'s\", \" is\", sentences, flags=re.IGNORECASE)\n",
        "    sentences = re.sub(r\"\\'d\", \" would\", sentences, flags=re.IGNORECASE)\n",
        "    sentences = re.sub(r\"\\'ll\", \" will\", sentences, flags=re.IGNORECASE)\n",
        "    sentences = re.sub(r\"\\'t\", \" not\", sentences, flags=re.IGNORECASE)\n",
        "    sentences = re.sub(r\"\\'ve\", \" have\", sentences, flags=re.IGNORECASE)\n",
        "    sentences = re.sub(r\"\\'m\", \" am\", sentences, flags=re.IGNORECASE)\n",
        "    return sentences\n",
        "\n",
        "def remove_noises(sentences):\n",
        "    \"\"\"Clean up noises in the text\n",
        "    \"\"\"\n",
        "    sentences = re.sub(r'[~=+|<>.^]+', \"\", sentences)\n",
        "    sentences = clean(sentences,\n",
        "        fix_unicode=True,               # fix various unicode errors\n",
        "        to_ascii=True,                  # transliterate to closest ASCII representation\n",
        "        lower=True,                     # lowercase text\n",
        "        no_line_breaks=True,            # fully strip line breaks as opposed to only normalizing them\n",
        "        no_urls=True,                   # replace all URLs with a special token\n",
        "        no_emails=True,                 # replace all email addresses with a special token\n",
        "        no_phone_numbers=True,          # replace all phone numbers with a special token\n",
        "        no_numbers=True,                # replace all numbers with a special token\n",
        "        no_digits=True,                 # replace all digits with a special token\n",
        "        no_currency_symbols=True,       # replace all currency symbols with a special token\n",
        "        no_punct=True,                  # remove punctuations\n",
        "        replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
        "        replace_with_url=\"<URL>\",\n",
        "        replace_with_email=\"<EMAIL>\",\n",
        "        replace_with_phone_number=\"<PHONE>\",\n",
        "        replace_with_number=\"\",\n",
        "        replace_with_digit=\"\",\n",
        "        replace_with_currency_symbol=\"\",\n",
        "        lang=\"en\"                       # set to 'de' for German special handling\n",
        "    )\n",
        "    return sentences\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize(sentence):\n",
        "    return \" \".join([\n",
        "        lemmatizer.lemmatize(word, pos=\"v\") \n",
        "        for word in nltk.word_tokenize(sentence.lower()) \n",
        "        if word not in stopwords.words('english')\n",
        "    ])\n",
        "\n",
        "def clean_comment_text(sentences):\n",
        "    return lemmatize(remove_noises(decontracted(sentences)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g61Kn3ljtrOx"
      },
      "source": [
        "raw_train['comment_text'] = raw_train['comment_text'].apply(clean_comment_text)\n",
        "raw_train.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mnjMOM2uwar"
      },
      "source": [
        "raw_test['comment_text'] = raw_test['comment_text'].apply(clean_comment_text)\n",
        "raw_test.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOtHJ1edOFoS"
      },
      "source": [
        "---\n",
        "# Training the Models\n",
        "\n",
        "Run BERT Fine Tuning on the data. The warning below is expected for the HuggingFace fine tuning.\n",
        "\n",
        "```\n",
        "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_transform', 'vocab_projector', 'vocab_layer_norm']\n",
        "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
        "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_19']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "```\n",
        "\n",
        "* [Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification](https://github.com/huggingface/transformers/issues/5421#issuecomment-652582854)\n",
        "> This is expected, and tells you that you won't have good performance with your BertForSequenceClassification model before you fine-tune it slightly_smiling_face.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGFHYUT2gzL3"
      },
      "source": [
        "CATEGORIES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "\n",
        "# HuggingFace\n",
        "MAX_SEQUENTH_LENGTH = 256   # Max token length to accept\n",
        "\n",
        "# Model training\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Result output directory\n",
        "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%b%d%H%M\").upper()\n",
        "RESULT_DIRECTORY = f\"/content/drive/MyDrive/data/toxicity_classification_{TIMESTAMP}\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0xIlT_ugzL4"
      },
      "source": [
        "def run(category):\n",
        "    \"\"\"Wrapper to create the Runnler instances for the respective category\"\"\"\n",
        "    print(\"\\n--------------------------------------------------------------------------------\")\n",
        "    print(f\"Model training on [{category}]\")\n",
        "    print(\"--------------------------------------------------------------------------------\")\n",
        "\n",
        "    data = raw_train['comment_text'].tolist()\n",
        "    label = raw_train[category].tolist()\n",
        "\n",
        "    train_data, validation_data, train_label, validation_label = train_test_split(\n",
        "        data,\n",
        "        label,\n",
        "        test_size=.2,\n",
        "        shuffle=True\n",
        "    )\n",
        "    runner = Runner(\n",
        "        category=category,\n",
        "        training_data=train_data,\n",
        "        training_label=train_label,\n",
        "        validation_data=validation_data,\n",
        "        validation_label=validation_label,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        max_sequence_length=MAX_SEQUENTH_LENGTH,\n",
        "        num_epochs=NUM_EPOCHS,\n",
        "        output_directory=RESULT_DIRECTORY\n",
        "    )\n",
        "    \n",
        "    runner.train()\n",
        "    runner.save()\n",
        "\n",
        "    print(\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "    print(f\"Model evaluation on [{category}]\")\n",
        "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "    test_data = raw_test['comment_text'].tolist()\n",
        "    test_label = raw_test[category].tolist()\n",
        "    evaluation = runner.evaluate(test_data, test_label)\n",
        "    print(f\"Evaluation: {runner.model_metric_names}:{evaluation}\")\n",
        "\n",
        "    return runner, evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMjy-emngzL4"
      },
      "source": [
        "runners = {}      # To save the Runner instance for each category.\n",
        "evaluations = {}  # Evaluation results for each category\n",
        "\n",
        "for category in CATEGORIES:\n",
        "    runners[category], evaluations[category] = run(category)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mx_mMR6_d9_"
      },
      "source": [
        "---\n",
        "# TensorBoard\n",
        "\n",
        "Examine the model trainig history in the TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a90boOpP_wi2"
      },
      "source": [
        "category = 'toxic'\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir {RESULT_DIRECTORY}/log_C{category}_B{BATCH_SIZE}_L{MAX_SEQUENTH_LENGTH}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc-Os5eSvBXQ"
      },
      "source": [
        "category = 'threat'\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir {RESULT_DIRECTORY}/log_C{category}_B{BATCH_SIZE}_L{MAX_SEQUENTH_LENGTH}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z59L6LtXQU7"
      },
      "source": [
        "---\n",
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvZ2Z7JQ9jsS"
      },
      "source": [
        "### Instantiate predictors from the save models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BwOtYdWqgyL"
      },
      "source": [
        "test_data = raw_test['comment_text'].tolist()\n",
        "test_label = raw_test[category].tolist()\n",
        "\n",
        "dummy_data = test_data[:5]\n",
        "dummy_label = test_label[:5]\n",
        "id=\"\"\n",
        "predictors = {}\n",
        "\n",
        "for category in CATEGORIES:\n",
        "    predictor = Runner(\n",
        "        category=category,\n",
        "        training_data=dummy_data,\n",
        "        training_label=dummy_label,\n",
        "        validation_data=dummy_data,\n",
        "        validation_label=dummy_label,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        max_sequence_length=MAX_SEQUENTH_LENGTH,\n",
        "    )\n",
        "    path_to_dir = \"{parent}/model_C{category}_B{size}_L{length}\".format(\n",
        "        parent=RESULT_DIRECTORY,\n",
        "        category=category,\n",
        "        size=BATCH_SIZE,\n",
        "        length=MAX_SEQUENTH_LENGTH\n",
        "    )\n",
        "    predictor.load(path_to_dir)\n",
        "    predictors[category] = predictor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG21clJX9p6Q"
      },
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PaLPDVTpCcw"
      },
      "source": [
        "row = {}\n",
        "index = np.random.randint(0, len(test_data))\n",
        "data = test_data[index]\n",
        "\n",
        "row['data'] = data\n",
        "for category in CATEGORIES:\n",
        "    row[category] = np.argmax(predictors[category].predict(data).tolist()[0])\n",
        "\n",
        "pd.DataFrame([row])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij0hmoTM9xSq"
      },
      "source": [
        "### True Ratings\n",
        "\n",
        "True results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZKDPjOtdpRr"
      },
      "source": [
        "raw_test.iloc[[index]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOwh3MqN7zvZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}