{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3KFBkjcknCD"
   },
   "source": [
    "# Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brzFWR6YhqCF"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow transformers pandas scikit-learn spacy wordcloud gensim\n",
    "!pip install matplotlib seaborn\n",
    "!pip install line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jaR9e-xkqNc"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UPio2cmTcWO"
   },
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLeUHOaJ1bt5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXx4rCOeakbp"
   },
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Fct7lHwa1yg"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    DistilBertTokenizerFast,\n",
    "    TFDistilBertForSequenceClassification,\n",
    "    TFTrainer,\n",
    "    TFTrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fc67_HTf0zyO"
   },
   "outputs": [],
   "source": [
    "class SavePretrainedCallback(tf.keras.callbacks.Callback):\n",
    "    # Hugging Face models have a save_pretrained() method that saves both the weights and the necessary\n",
    "    # metadata to allow them to be loaded as a pretrained model in future. This is a simple Keras callback\n",
    "    # that saves the model with this method after each epoch.\n",
    "    def __init__(self, output_dir, **kwargs):\n",
    "        super().__init__()\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.model.save_pretrained(self.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXWNRYbes8V6"
   },
   "outputs": [],
   "source": [
    "class Runner:\n",
    "    # ================================================================================\n",
    "    # Class\n",
    "    # ================================================================================\n",
    "    USE_HF_TRAINER = False\n",
    "    _model_name = 'distilbert-base-cased'\n",
    "    _tokenizer = DistilBertTokenizerFast.from_pretrained(_model_name)\n",
    "\n",
    "    # ================================================================================\n",
    "    # Instance\n",
    "    # ================================================================================\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Instance properties\n",
    "    # --------------------------------------------------------------------------------\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        assert self._batch_size > 0\n",
    "        return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def X(self):\n",
    "        \"\"\"Training DataSet\"\"\"\n",
    "        return self._X\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        \"\"\"Validation DataSet\"\"\"\n",
    "        return self._V\n",
    "\n",
    "    @property\n",
    "    def model_name(self):\n",
    "        \"\"\"HuggingFace pretrained model name\"\"\"\n",
    "        return self._model_name\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        \"\"\"Model\"\"\"\n",
    "        return self._model\n",
    "\n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        return self._learning_rate\n",
    "\n",
    "    @property\n",
    "    def num_epochs(self):\n",
    "        return self._num_epochs\n",
    "\n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        \"\"\"\"\"\"\n",
    "        return self._tokenizer\n",
    "\n",
    "    @property\n",
    "    def trainer(self):\n",
    "        \"\"\"\"\"\"\n",
    "        return self._trainer\n",
    "\n",
    "    @property\n",
    "    def output_directory(self):\n",
    "        \"\"\"Directory to save models, etc\"\"\"\n",
    "        return self._output_directory\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Instance initialization\n",
    "    # --------------------------------------------------------------------------------\n",
    "    def __init__(\n",
    "            self,\n",
    "            training_data,\n",
    "            training_label,\n",
    "            validation_data,\n",
    "            validation_label,\n",
    "            batch_size=16,\n",
    "            learning_rate=5e-5,\n",
    "            num_epochs=3,\n",
    "            output_directory=\"./output\"\n",
    "    ):\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # TensorFlow DataSet\n",
    "        # --------------------------------------------------------------------------------\n",
    "        assert np.all(np.isin(training_label, [0, 1]))\n",
    "        assert np.all(np.isin(validation_label, [0, 1]))\n",
    "        self._X = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(self.tokenizer(training_data, truncation=True, padding=True)),\n",
    "            training_label\n",
    "        ))\n",
    "        self._V = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(self.tokenizer(validation_data, truncation=True, padding=True)),\n",
    "            validation_label\n",
    "        ))\n",
    "        assert batch_size > 0\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Keras Model\n",
    "        # --------------------------------------------------------------------------------\n",
    "        assert learning_rate > 0.0\n",
    "        self._learning_rate = learning_rate\n",
    "        self._model = None\n",
    "\n",
    "        assert num_epochs > 0\n",
    "        self._num_epochs = num_epochs\n",
    "\n",
    "        assert os.path.isdir(output_directory) and os.access(output_directory, os.W_OK)\n",
    "        self._output_directory = output_directory\n",
    "        Path(self.output_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # HuggingFace\n",
    "        # --------------------------------------------------------------------------------\n",
    "        self._trainer = None\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Instance methods\n",
    "    # --------------------------------------------------------------------------------\n",
    "    def _hf_train(self):\n",
    "        self._training_args = TFTrainingArguments(\n",
    "            output_dir='./results',             # output directory\n",
    "            num_train_epochs=3,                 # total number of training epochs\n",
    "            per_device_train_batch_size=self.batch_size,     # batch size per device during training\n",
    "            per_device_eval_batch_size=self.batch_size,      # batch size for evaluation\n",
    "            warmup_steps=500,                   # number of warmup steps for learning rate scheduler\n",
    "            weight_decay=0.01,                  # strength of weight decay\n",
    "            logging_dir='./logs',               # directory for storing logs\n",
    "            logging_steps=10,\n",
    "        )\n",
    "\n",
    "        with self._training_args.strategy.scope():\n",
    "            self._model = TFDistilBertForSequenceClassification.from_pretrained(self.model_name)\n",
    "\n",
    "        self._trainer = TFTrainer(\n",
    "            model=self._model,\n",
    "            args=self._training_args,   # training arguments\n",
    "            train_dataset=self.X,       # training dataset\n",
    "            eval_dataset=self.V         # evaluation dataset\n",
    "        )\n",
    "        self.trainer.train()\n",
    "\n",
    "    def _keras_train(self):\n",
    "        self._model = TFDistilBertForSequenceClassification.from_pretrained(self.model_name)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        self.model.compile(optimizer=optimizer, loss=self.model.compute_loss)\n",
    "        self.model.summary()\n",
    "        self.model.fit(\n",
    "            self.X.shuffle(1000).batch(self.batch_size).prefetch(1),\n",
    "            epochs=self.num_epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            validation_data=self.V.shuffle(1000).batch(self.batch_size).prefetch(1),\n",
    "            callbacks=[SavePretrainedCallback(output_dir=self.output_directory)]\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        if self.USE_HF_TRAINER:\n",
    "            self._hf_train()\n",
    "        else:\n",
    "            self._keras_train()\n",
    "\n",
    "    def evaluate(self, data, label):\n",
    "        assert np.all(np.isin(label, [0, 1]))\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(self.tokenizer(data, truncation=True, padding=True)),\n",
    "            label\n",
    "        ))\n",
    "        evaluation = self.model.evaluate(\n",
    "            test_dataset.shuffle(1000).batch(self.batch_size).prefetch(1)\n",
    "        )\n",
    "        print(f\"Evaluation: (loss, accuracy):{evaluation}\")\n",
    "\n",
    "    def save(self, path_to_dir):\n",
    "        if os.path.isdir(path_to_dir) and os.access(path_to_dir, os.W_OK):\n",
    "            Path(path_to_dir).mkdir(parents=True, exist_ok=True)\n",
    "            self.trainer.save_model(path_to_dir) if self.USE_HF_TRAINER else self.model.save(path_to_dir)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Cannot write to {path_to_dir} directory.\")\n",
    "\n",
    "    def load(self, path_to_dir):\n",
    "        if os.path.isdir(path_to_dir) and os.access(path_to_dir, os.R_OK):\n",
    "            self._model = PreTrainedModel.from_pretrained(path_to_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6fCHrZNTePV"
   },
   "source": [
    "## Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zP1Ln4gxTgio"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_GOOGLE_COLAB = True\n",
    "    DATA_PATH=\"/content/drive/MyDrive/data/jigsaw-toxic-comment-classification-challenge.zip\"\n",
    "    google.colab.drive.mount('/content/drive')\n",
    "except:\n",
    "    IN_GOOGLE_COLAB = False\n",
    "    DATA_PATH = input(\"Enter the data archive path\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PFau2osjvx-"
   },
   "source": [
    "---\n",
    "# Data\n",
    "\n",
    "First, upload data to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sz-5NlxtoB34"
   },
   "outputs": [],
   "source": [
    "!unzip -o $DATA_PATH\n",
    "!unzip -o train.csv.zip\n",
    "!unzip -o test.csv.zip\n",
    "!unzip -o test_labels.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7Id3UZspT7h"
   },
   "outputs": [],
   "source": [
    "raw_train = pd.read_csv(\"./train.csv\")\n",
    "raw_test_data = pd.read_csv(\"./test.csv\")\n",
    "raw_test_label = pd.read_csv(\"./test_labels.csv\")\n",
    "raw_test = pd.merge(raw_test_data, raw_test_label, left_on='id', right_on='id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqRp3qdczPM5"
   },
   "source": [
    "## Training (Raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKAe7wXMzOMd"
   },
   "outputs": [],
   "source": [
    "raw_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wfx5DKry2KjZ"
   },
   "outputs": [],
   "source": [
    "raw_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NaP9KvTzJuZ"
   },
   "outputs": [],
   "source": [
    "raw_train[raw_train['toxic'] > 0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0jSqXxxQrUj"
   },
   "source": [
    "## Test (Raw)\n",
    "The label value -1 is not clear. Remove the rows where value is -1.\n",
    "\n",
    "> test_labels.csv - labels for the test data; value of -1 indicates it was not used for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9kdtivUQqtW"
   },
   "outputs": [],
   "source": [
    "raw_test = raw_test[(raw_test['toxic'] > 0)]   # Removing rows where 'toxic' label > 0 is sufficicent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ve713cdzWptk"
   },
   "outputs": [],
   "source": [
    "raw_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVyle97CWy7A"
   },
   "outputs": [],
   "source": [
    "raw_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iW6NJrmxQSQa"
   },
   "source": [
    "## Trainig (Toxic) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSEJpPnDZepw"
   },
   "outputs": [],
   "source": [
    "toxic_data = raw_train['comment_text'].tolist()\n",
    "toxic_label = raw_train['toxic'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbYY-86DOaEI"
   },
   "outputs": [],
   "source": [
    "toxic_train_data, toxic_validation_data, toxic_train_label, toxic_validation_label = train_test_split(\n",
    "    toxic_data, \n",
    "    toxic_label, \n",
    "    test_size=.2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOtHJ1edOFoS"
   },
   "source": [
    "---\n",
    "# BERT Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijBH2069vJOf"
   },
   "outputs": [],
   "source": [
    "runner = Runner(\n",
    "  training_data=toxic_train_data,\n",
    "  training_label=toxic_train_label,\n",
    "  validation_data=toxic_validation_data,\n",
    "  validation_label=toxic_validation_label,\n",
    "  output_directory=\"/content/drive/MyDrive/data/model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zrda-UZGOFoT"
   },
   "outputs": [],
   "source": [
    "sentence = toxic_train_data[0]\n",
    "tokenizer = runner.tokenizer\n",
    "indices = tokenizer.encode(\n",
    "    sentence,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "for word, index in zip(sentence.split(), indices):\n",
    "    print(f\"word:[{word:16s}] index:[{index:8d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NeVRT5BkRqdj"
   },
   "outputs": [],
   "source": [
    "runner.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMM88OCBQTWk"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wx58z3O2SSZ9"
   },
   "outputs": [],
   "source": [
    "toxic_test_data = raw_test['comment_text'].tolist()\n",
    "toxic_test_label = raw_test['toxic'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTi239ykXwCQ"
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer(toxic_test_data, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6-UH4UTZPQ3"
   },
   "outputs": [],
   "source": [
    "print(len(toxic_test_label))\n",
    "print(len(tokens['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1zuJXUhTr5w"
   },
   "outputs": [],
   "source": [
    "runner.evaluate(toxic_test_data, toxic_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvZb-98PWXAi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "toxic_comment_classification.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
