{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "toxic_comment_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KVDkx-1pky8C",
        "Qpx-pzGWasel",
        "leheXoY8q8b_",
        "W6fCHrZNTePV",
        "7UPio2cmTcWO",
        "QRq43r5RSoCA",
        "CXx4rCOeakbp",
        "nUYPD0TQvEaI",
        "GOtHJ1edOFoS",
        "1Mx_mMR6_d9_",
        "SvZ2Z7JQ9jsS"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDHpPdvDZ0bG"
      },
      "source": [
        "# Toxic Comment Classification \n",
        "\n",
        "[Kaggle - Toxic Comment Classification Challenage](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview)\n",
        "\n",
        "> Build a multi-headed model to detect different types of of toxicity like threats, obscenity, insults, and identity-based. \n",
        "\n",
        "A multi-label classification where the data can belongs to multiple labels simultaneously.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d91HTzCYqPux"
      },
      "source": [
        "# Approach\n",
        "![toxic_comment_classification_flow.png](data:image/png;base64,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)\n",
        "\n",
        "## Assumptions\n",
        "\n",
        "1. Text comments are in English\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNrGF0zopimF"
      },
      "source": [
        "\n",
        "## Notes on Related Resources\n",
        "\n",
        "* [How AI Is Learning to Identify Toxic Online Content](https://www.scientificamerican.com/article/can-ai-identify-toxic-online-content/)\n",
        ">  This is how our team built Detoxify, an open-source, user-friendly comment detection library to identify inappropriate or harmful text online. Its intended use is to help researchers and practitioners identify potential toxic comments. ... Each model can be easily accessed in one line of code and all models and training code are [publicly available on GitHub (Detoxify)]((https://github.com/unitaryai/detoxify)).\n",
        "\n",
        "* [Detecting toxic comments with Keras and interpreting the model with ELI5](https://medium.com/@armandj.olivares/detecting-toxic-comments-with-keras-and-interpreting-the-model-with-eli5-dbe734f3e86b) - ([Github](https://github.com/ArmandDS/toxic_detection/blob/master/toxic_detection.ipynb))\n",
        "> Develop an estimator with a neural network model for a text classification problem and used ELI5 library for explain the predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3KFBkjcknCD"
      },
      "source": [
        "---\n",
        "# Setups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6by6Rbv4_Z9"
      },
      "source": [
        "# To reduce the data volumen to run through the training in short timeframe.\n",
        "TEST_MODE = False\n",
        "\n",
        "# Directory to manage the data\n",
        "DATA_DIR = \"/content/drive/MyDrive/data\"\n",
        "\n",
        "# Flag to overwrite the cleaned data\n",
        "FORCE_OVERWRITE = False\n",
        "\n",
        "# Labbels that classifies the type of the comment.\n",
        "CATEGORIES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVDkx-1pky8C"
      },
      "source": [
        "## Modules\n",
        "\n",
        "Install and load Python modules required for the task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brzFWR6YhqCF",
        "outputId": "e096a439-359a-404b-f16e-f9e42eb3c8c5"
      },
      "source": [
        "!pip install tensorflow transformers pandas scikit-learn spacy gensim h5py \n",
        "!pip install clean-text unidecode nltk wordcloud\n",
        "!pip install matplotlib seaborn\n",
        "!pip install line_profiler"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/d5/c6c23ad75491467a9a84e526ef2364e523d45e2b0fae28a7cbe8689e7e84/transformers-4.8.1-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.34.1)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 21.3MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 28.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.31.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n",
            "Installing collected packages: huggingface-hub, sacremoses, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.1\n",
            "Collecting clean-text\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/f5/f0db7d0185e26f9a85d425fd19d9bd891ff733f2ec0d1ee06791fd7f13b6/clean_text-0.4.0-py3-none-any.whl\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 6.8MB/s \n",
            "\u001b[?25hCollecting ftfy<7.0,>=6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/da/d215a091986e5f01b80f5145cff6f22e2dc57c6b048aab2e882a07018473/ftfy-6.0.3.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from wordcloud) (1.19.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud) (7.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.5)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-cp37-none-any.whl size=41935 sha256=d7e568e3df7355f7257b1fa0fce041bf943e9488c0d858cfd391e67b14b94546\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/2c/e6/109c8a28fef7a443f67ba58df21fe1d0067ac3322e75e6b0b7\n",
            "Successfully built ftfy\n",
            "Installing collected packages: emoji, ftfy, clean-text, unidecode\n",
            "Successfully installed clean-text-0.4.0 emoji-1.2.0 ftfy-6.0.3 unidecode-1.2.0\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.1.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2018.9)\n",
            "Collecting line_profiler\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/84/922000ff9798c58a95e701d602aa24f98b55bc52039cda4e452c2f879bdb/line_profiler-3.3.0-cp37-cp37m-manylinux2010_x86_64.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: IPython>=0.13; python_version >= \"3.7\" in /usr/local/lib/python3.7/dist-packages (from line_profiler) (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (57.0.0)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (5.0.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (2.6.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython>=0.13; python_version >= \"3.7\"->line_profiler) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython>=0.13; python_version >= \"3.7\"->line_profiler) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->IPython>=0.13; python_version >= \"3.7\"->line_profiler) (0.7.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->IPython>=0.13; python_version >= \"3.7\"->line_profiler) (0.2.0)\n",
            "Installing collected packages: line-profiler\n",
            "Successfully installed line-profiler-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jaR9e-xkqNc"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import datetime\n",
        "import re\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from wordcloud import (\n",
        "    WordCloud, \n",
        "    STOPWORDS\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-Oj3ltY_T8m"
      },
      "source": [
        "## Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz9YyZnU_ZdE"
      },
      "source": [
        "# Matplotlib utilities\n",
        "def plot_on_ax(ax, X, Y, label=None, color=None, title=None, xlabel=None, ylabel=None, legend=True, scale=None, limits=None):\n",
        "    ax.set_xlim(limits[0:2]) if limits is not None else None\n",
        "    ax.set_ylim(limits[2:4]) if limits is not None else None\n",
        "    ax.set_xlabel(xlabel) if xlabel else ...\n",
        "    ax.set_ylabel(ylabel) if ylabel else ...\n",
        "    ax.set_title(title) if title is not None else ...\n",
        "    ax.plot(X, Y, color=color, label=label)\n",
        "    ax.legend() if legend else None\n",
        "    ax.grid(which='major', b=False, linestyle='--')\n",
        "    ax.grid(which='minor', alpha=0.2, linestyle='--')\n",
        "    ax.set_xscale(scale) if scale else ...\n",
        "    ax.xaxis.get_ticklocs(minor=True)\n",
        "    ax.minorticks_on()\n",
        "\n",
        "def hist_on_ax(ax, X, Y, label=None, color=None, alpha=0.5, title=None, xlabel=None, ylabel=None, legend=True, scale=None, limits=None):\n",
        "    ax.set_xlim(limits[0:2]) if limits is not None else None\n",
        "    ax.set_ylim(limits[2:4]) if limits is not None else None\n",
        "    ax.set_xlabel(xlabel) if xlabel else ...\n",
        "    ax.set_ylabel(ylabel) if ylabel else ...\n",
        "    ax.set_title(title) if title is not None else ...\n",
        "    ax.hist(Y, bins=X, alpha=alpha, color=color,label=label)\n",
        "    ax.grid(which='both')\n",
        "    ax.grid(which='major', b=False, linestyle='--')\n",
        "    ax.grid(which='minor', alpha=0.2, linestyle='--')\n",
        "    ax.legend() if legend else None\n",
        "    ax.set_xscale(scale) if scale else ...\n",
        "\n",
        "def plotter(ax, x, hy, uy, hcolor, ucolor, hlabel, ulabel, xlabel, ylabel, title, limits=None, single=False):\n",
        "    plot_on_ax(ax, x, hy, color=hcolor, label=hlabel, xlabel=xlabel, ylabel=ylabel, title=title, limits=limits)\n",
        "    plot_on_ax(ax, x, uy, color=ucolor, label=ulabel) if not single else None\n",
        "    return\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)       \n",
        "    plt.title(title, fontsize=20)\n",
        "    plt.grid()\n",
        "    ax.legend() if legend else None\n",
        "    plt.axis(limits) if limits is not None else None"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZZbTtUGWx9S"
      },
      "source": [
        "## Jupyter Notebook\n",
        "\n",
        "Jupyter cell format configurations. Align the cell output to the left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "AFiq3iQjVDEL",
        "outputId": "bf378c78-3f62-43c6-c017-617c4a4bda8f"
      },
      "source": [
        "%%html\n",
        "<style>\n",
        "table {float:left}\n",
        "</style>\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "np.set_printoptions(linewidth=1000) "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "table {float:left}\n",
              "</style>\n",
              "\n",
              "np.set_printoptions(threshold=sys.maxsize)\n",
              "np.set_printoptions(linewidth=1000) "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qpx-pzGWasel"
      },
      "source": [
        "## Python logging\n",
        "\n",
        "Control the logging outputs to supress the warning and information to prevent the execution results from being cluttered. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofY8HgJPcu7K"
      },
      "source": [
        "logging.disable(logging.WARNING)\n",
        "logging.basicConfig(level=logging.ERROR)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leheXoY8q8b_"
      },
      "source": [
        "## Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CzdpqIDq7oA",
        "outputId": "fc41c6df-2e4b-464a-dd51-93964a45383d"
      },
      "source": [
        "from cleantext import clean\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6fCHrZNTePV"
      },
      "source": [
        "## Google Colab\n",
        "\n",
        "Google Colab specific operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzCt--KyGh8i"
      },
      "source": [
        "def google_colab_info():\n",
        "    \"\"\"Information on the Google Colab environment\n",
        "    \"\"\"\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # GPU\n",
        "    # --------------------------------------------------------------------------------\n",
        "    gpu_info = !nvidia-smi\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "        print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "        print('and then re-execute this cell.')\n",
        "    else:\n",
        "        print(gpu_info)\n",
        "\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # Memory\n",
        "    # --------------------------------------------------------------------------------\n",
        "    from psutil import virtual_memory\n",
        "    ram_gb = virtual_memory().total / 1e9\n",
        "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "    if ram_gb < 20:\n",
        "        print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "        print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "        print('re-execute this cell.')\n",
        "    else:\n",
        "        print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zP1Ln4gxTgio",
        "outputId": "a1c64c2d-73e6-4e25-e721-6616fcd4e11e"
      },
      "source": [
        "try:\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # Check if the environment is Google Colab.\n",
        "    # --------------------------------------------------------------------------------\n",
        "    import google.colab\n",
        "    IN_GOOGLE_COLAB = True\n",
        "    print(\"Using Google Colab environment.\")\n",
        "\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # Show Google Colab information\n",
        "    # --------------------------------------------------------------------------------\n",
        "    google_colab_info()\n",
        "    \n",
        "except:\n",
        "    IN_GOOGLE_COLAB = False"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Google Colab environment.\n",
            "Mon Jun 28 04:08:45 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UPio2cmTcWO"
      },
      "source": [
        "## Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLeUHOaJ1bt5"
      },
      "source": [
        "import pandas as pd\n",
        "pd.options.display.max_colwidth = 1000   # Allow long string content in a cell"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRq43r5RSoCA"
      },
      "source": [
        "## TensorFlow\n",
        "\n",
        "Control TensorFlow logging.\n",
        "\n",
        "| TF_CPP_MIN_LOG_LEVEL | Description|          \n",
        "| - |------------- | \n",
        "|0| Suppress all messages are logged (default behavior)|\n",
        "|1 |Suppress INFO messages are not printed|\n",
        "|2 |Suppress INFO and WARNING messages are not printed|\n",
        "|3 |Suppress INFO, WARNING, and ERROR messages are not printed|\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJfDGer2Sqkh"
      },
      "source": [
        "import tensorflow as tf\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXx4rCOeakbp"
      },
      "source": [
        "## Transformers\n",
        "\n",
        "[HuggingFace](https://huggingface.co/transformers/) offers the libarary for NLP (Natural Language Processing) based on the Transfoemer architecture introduced in the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) by Google.\n",
        "\n",
        "> Transformers provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between Jax, PyTorch and TensorFlow.\n",
        "\n",
        "### Transfer Learning (Fine-Tuning)\n",
        "\n",
        "Utilize the [Sequence Classification](https://huggingface.co/transformers/task_summary.html#sequence-classification) capabilty of BERT for the text classification by fine-tuing the pre-trained BERT model upon the data provided. \n",
        "\n",
        "* [Fine-tuning a pretrained model](https://huggingface.co/transformers/training.html)\n",
        "> How to fine-tune a pretrained model from the Transformers library. In TensorFlow, models can be directly trained using Keras and the fit method. \n",
        "\n",
        "* [Fine-tuning with custom datasets](https://huggingface.co/transformers/custom_datasets.html)\n",
        "> This tutorial will take you through several examples of using 🤗 Transformers models with your own datasets.\n",
        "\n",
        "* [HuggingFace Text classification examples](https://github.com/huggingface/transformers/tree/master/examples/tensorflow/text-classification)\n",
        "> This folder contains some scripts showing examples of text classification with the hugs Transformers library. \n",
        "\n",
        "The code in this notebook is based on the [run_text_classification.py](https://github.com/huggingface/transformers/blob/master/examples/tensorflow/text-classification/run_text_classification.py) example for TensorFlow and the code in the documentation [Fine-tuning with custom datasets](https://huggingface.co/transformers/custom_datasets.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Fct7lHwa1yg"
      },
      "source": [
        "from transformers import (\n",
        "    PreTrainedModel,\n",
        "    DistilBertTokenizerFast,\n",
        "    TFDistilBertForSequenceClassification,\n",
        "    TFTrainer,\n",
        "    TFTrainingArguments\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# Control log level (https://huggingface.co/transformers/main_classes/logging.html)\n",
        "# --------------------------------------------------------------------------------\n",
        "os.environ['TRANSFORMERS_VERBOSITY'] = \"error\"\n",
        "import transformers\n",
        "transformers.logging.set_verbosity(transformers.logging.ERROR)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PFau2osjvx-"
      },
      "source": [
        "---\n",
        "# Ingenstion\n",
        "\n",
        "### DATA_PATH\n",
        "**DATA_PATH** variable points to the location of the data package for [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data) in the Google Drive or in the local directory. Unzip the data package to extract the data for training and testing.\n",
        "\n",
        "* train.csv\n",
        "* test.csv\n",
        "* test_labels.csv - 0/1 binary labels to identify the comment is rated for each category (e.g. toxici)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXvCh3GqXtw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41b4ce12-216b-4f62-92f6-a3239b46b127"
      },
      "source": [
        "if IN_GOOGLE_COLAB:\n",
        "    google.colab.drive.mount('/content/drive')\n",
        "    DATA_PATH=f\"{DATA_DIR}/jigsaw-toxic-comment-classification-challenge.zip\"\n",
        "else:\n",
        "    DATA_PATH = input(\"Enter the data archive path\") "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz-5NlxtoB34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cec614b7-e1de-432d-c858-352c685d1e00"
      },
      "source": [
        "!unzip -o $DATA_PATH\n",
        "!unzip -o train.csv.zip\n",
        "!unzip -o test.csv.zip\n",
        "!unzip -o test_labels.csv.zip"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/data/jigsaw-toxic-comment-classification-challenge.zip\n",
            "  inflating: sample_submission.csv.zip  \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: test_labels.csv.zip     \n",
            "  inflating: train.csv.zip           \n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n",
            "Archive:  test_labels.csv.zip\n",
            "  inflating: test_labels.csv         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQievs3ZNwkz"
      },
      "source": [
        "## Raw Dataframes\n",
        "\n",
        "Load the original data from the CSV files into ```raw_``` dataframes.\n",
        "\n",
        "* raw_train is from train.csv\n",
        "* raw_test is merged from test.csv and test_labels.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7Id3UZspT7h"
      },
      "source": [
        "raw_train = pd.read_csv(\"./train.csv\")\n",
        "raw_test_data = pd.read_csv(\"./test.csv\")\n",
        "raw_test_label = pd.read_csv(\"./test_labels.csv\")\n",
        "raw_test = pd.merge(raw_test_data, raw_test_label, left_on='id', right_on='id', how='inner')\n",
        "\n",
        "if TEST_MODE:\n",
        "    raw_train = raw_train.head(256)\n",
        "    raw_test = raw_test.head(256)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqRp3qdczPM5"
      },
      "source": [
        "### Raw data (train.csv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKAe7wXMzOMd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "26c12cd0-a200-48e3-c82f-43820616a2a7"
      },
      "source": [
        "raw_train.head(3)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  0000997932d777bf  ...             0\n",
              "1  000103f0d9cfb60f  ...             0\n",
              "2  000113f07ec002fd  ...             0\n",
              "\n",
              "[3 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wfx5DKry2KjZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "c6a259bf-ca95-4219-aa02-e70ffc61ede1"
      },
      "source": [
        "raw_train.describe()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>159571.000000</td>\n",
              "      <td>159571.000000</td>\n",
              "      <td>159571.000000</td>\n",
              "      <td>159571.000000</td>\n",
              "      <td>159571.000000</td>\n",
              "      <td>159571.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.095844</td>\n",
              "      <td>0.009996</td>\n",
              "      <td>0.052948</td>\n",
              "      <td>0.002996</td>\n",
              "      <td>0.049364</td>\n",
              "      <td>0.008805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.294379</td>\n",
              "      <td>0.099477</td>\n",
              "      <td>0.223931</td>\n",
              "      <td>0.054650</td>\n",
              "      <td>0.216627</td>\n",
              "      <td>0.093420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               toxic   severe_toxic  ...         insult  identity_hate\n",
              "count  159571.000000  159571.000000  ...  159571.000000  159571.000000\n",
              "mean        0.095844       0.009996  ...       0.049364       0.008805\n",
              "std         0.294379       0.099477  ...       0.216627       0.093420\n",
              "min         0.000000       0.000000  ...       0.000000       0.000000\n",
              "25%         0.000000       0.000000  ...       0.000000       0.000000\n",
              "50%         0.000000       0.000000  ...       0.000000       0.000000\n",
              "75%         0.000000       0.000000  ...       0.000000       0.000000\n",
              "max         1.000000       1.000000  ...       1.000000       1.000000\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NaP9KvTzJuZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "e975cfc1-4ad6-481f-a3f1-91f67e21146b"
      },
      "source": [
        "raw_train[raw_train['toxic'] > 0].head(3)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0002bcb3da6cb337</td>\n",
              "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0005c987bdfc9d4b</td>\n",
              "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\\n\\nAsk Sityush to clean up his behavior than issue me nonsensical warnings...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0007e25b2121310b</td>\n",
              "      <td>Bye! \\n\\nDon't look, come or think of comming back! Tosser.</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  id  ... identity_hate\n",
              "6   0002bcb3da6cb337  ...             0\n",
              "12  0005c987bdfc9d4b  ...             0\n",
              "16  0007e25b2121310b  ...             0\n",
              "\n",
              "[3 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0jSqXxxQrUj"
      },
      "source": [
        "### Raw data (test.csv)\n",
        "Remove the rows where the label value is -1 as as the meaning is not clearly defined.\n",
        "\n",
        "> test_labels.csv - labels for the test data; value of -1 indicates it was not used for scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9kdtivUQqtW"
      },
      "source": [
        "# Removing rows where 'toxic' label >= 0 is sufficicent to eliminate all -1\n",
        "raw_test = raw_test[(raw_test['toxic'] >= 0)] "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve713cdzWptk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "f99bb1b1-d9ea-4c53-b4f1-2aaa1f6d7fb2"
      },
      "source": [
        "raw_test.describe()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>63978.000000</td>\n",
              "      <td>63978.000000</td>\n",
              "      <td>63978.000000</td>\n",
              "      <td>63978.000000</td>\n",
              "      <td>63978.000000</td>\n",
              "      <td>63978.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.095189</td>\n",
              "      <td>0.005736</td>\n",
              "      <td>0.057692</td>\n",
              "      <td>0.003298</td>\n",
              "      <td>0.053565</td>\n",
              "      <td>0.011129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.293478</td>\n",
              "      <td>0.075522</td>\n",
              "      <td>0.233161</td>\n",
              "      <td>0.057334</td>\n",
              "      <td>0.225160</td>\n",
              "      <td>0.104905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              toxic  severe_toxic  ...        insult  identity_hate\n",
              "count  63978.000000  63978.000000  ...  63978.000000   63978.000000\n",
              "mean       0.095189      0.005736  ...      0.053565       0.011129\n",
              "std        0.293478      0.075522  ...      0.225160       0.104905\n",
              "min        0.000000      0.000000  ...      0.000000       0.000000\n",
              "25%        0.000000      0.000000  ...      0.000000       0.000000\n",
              "50%        0.000000      0.000000  ...      0.000000       0.000000\n",
              "75%        0.000000      0.000000  ...      0.000000       0.000000\n",
              "max        1.000000      1.000000  ...      1.000000       1.000000\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVyle97CWy7A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "05ff795f-3bd2-4e85-bbb9-82b2fa26582e"
      },
      "source": [
        "raw_test.head(3)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0001ea8717f6de06</td>\n",
              "      <td>Thank you for understanding. I think very highly of you and would not revert without discussion.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>000247e83dcc1211</td>\n",
              "      <td>:Dear god this site is horrible.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0002f87b16116a7f</td>\n",
              "      <td>\"::: Somebody will invariably try to add Religion?  Really??  You mean, the way people have invariably kept adding \"\"Religion\"\" to the Samuel Beckett infobox?  And why do you bother bringing up the long-dead completely non-existent \"\"Influences\"\" issue?  You're just flailing, making up crap on the fly. \\n ::: For comparison, the only explicit acknowledgement in the entire Amos Oz article that he is personally Jewish is in the categories!    \\n\\n \"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  id  ... identity_hate\n",
              "5   0001ea8717f6de06  ...             0\n",
              "7   000247e83dcc1211  ...             0\n",
              "11  0002f87b16116a7f  ...             0\n",
              "\n",
              "[3 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wwZFMlD6pIJ"
      },
      "source": [
        "---\n",
        "# Transformation\n",
        "## Enrichment\n",
        "### Unhealthiness label \n",
        "\n",
        "Use the sum of labels as the unhealthiness level of the comment (0: healthy, 6: the most unhealthy). The ```unhealthiness``` is 2 for a comment labeled as ```toxic``` and ```insult```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZCFG5Bv_53c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c68ddaf-3902-4a91-bdd3-f9b2a968e638"
      },
      "source": [
        "train = raw_train.copy()\n",
        "test = raw_test.copy()\n",
        "\n",
        "del raw_train\n",
        "gc.collect()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "164"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPbVcApY0AIy"
      },
      "source": [
        "train['unhealthiness'] = train.iloc[:, 1:].sum(axis=1)\n",
        "train['unhealthy'] = train['unhealthiness'].apply(lambda x: int(x > 0))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVwVH7nm7Tjn"
      },
      "source": [
        "### Comment length\n",
        "Add the comment length to analyze the co-relation the content with its length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-qGfbsl7dJy"
      },
      "source": [
        "train['length'] = train['comment_text'].apply(lambda x: len(x))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmN1y1eitrz_"
      },
      "source": [
        "## Cleaning\n",
        "\n",
        "### De-contraction\n",
        "\n",
        "Restore contraction (e.g. I'll) into de-contracted form (I will) to have the valid words in place.\n",
        "\n",
        "### Stop words removal\n",
        "\n",
        "Remove the stop words (e.g. EOL, an, the, us) that has neither positive nor negative influence on the sentence.\n",
        "\n",
        "* EOL (End of line)\n",
        "* Numbers, digits\n",
        "* Email, URL, phone number, currency symbols\n",
        "* English stop words e.g. a, an, the, us\n",
        "\n",
        "### Lemmatization\n",
        "Reducing the inflectional forms (e.g. having, had, has) of a word into its base (have) to focus on the core meaning, not the form of the words.\n",
        "\n",
        "### Case consistency\n",
        "Lower the case to remove variations of the same word.\n",
        "\n",
        "### Data Argumentation (**Not implemented**)\n",
        "\n",
        "Permutate the data to provide additional data and address the skewness of the data representation.\n",
        "\n",
        "* [Data Augmentation in NLP\n",
        "](https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28) - nlpaug\n",
        "> We explore different authors how they leverage augmentation to tickle NLP tasks via generating more text data to boost up the models.  \n",
        "\n",
        "* Github [nlpaug](https://github.com/makcedward/nlpaug)\n",
        "> This python library helps you with augmenting nlp for your machine learning projects. Visit this introduction to understand about Data Augmentation in NLP. Augmenter is the basic element of augmentation while Flow is a pipeline to orchestra multi augmenter together.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoBuLSZlGZCy"
      },
      "source": [
        "def decontracted(sentences):\n",
        "    \"\"\"Restore the contracted words\"\"\"\n",
        "    # specific\n",
        "    sentences = re.sub(r\"won\\'t\", \"will not\", sentences, flags=re.IGNORECASE)\n",
        "    sentences = re.sub(r\"can\\'t\", \"can not\", sentences, flags=re.IGNORECASE)\n",
        "    # general\n",
        "    sentences = re.sub(r\"n\\'t\", \" not\", sentences, flags=re.IGNORECASE)\n",
        "    sentences = re.sub(r\"\\'re\", \" are\", sentences, flags=re.IGNORECASE)\n",
        "    sentences = re.sub(r\"\\'s\", \" is\", sentences, flags=re.IGNORECASE)\n",
        "    sentences = re.sub(r\"\\'d\", \" would\", sentences, flags=re.IGNORECASE)\n",
        "    sentences = re.sub(r\"\\'ll\", \" will\", sentences, flags=re.IGNORECASE)\n",
        "    sentences = re.sub(r\"\\'t\", \" not\", sentences, flags=re.IGNORECASE)\n",
        "    sentences = re.sub(r\"\\'ve\", \" have\", sentences, flags=re.IGNORECASE)\n",
        "    sentences = re.sub(r\"\\'m\", \" am\", sentences, flags=re.IGNORECASE)\n",
        "    return sentences\n",
        "\n",
        "def remove_noises(sentences):\n",
        "    \"\"\"Clean up noises in the text\n",
        "    \"\"\"\n",
        "    sentences = re.sub(r'[~=+|<>.^]+', \"\", sentences)\n",
        "    sentences = clean(sentences,\n",
        "        fix_unicode=True,               # fix various unicode errors\n",
        "        to_ascii=True,                  # transliterate to closest ASCII representation\n",
        "        lower=True,                     # lowercase text\n",
        "        no_line_breaks=True,            # fully strip line breaks as opposed to only normalizing them\n",
        "        no_urls=True,                   # replace all URLs with a special token\n",
        "        no_emails=True,                 # replace all email addresses with a special token\n",
        "        no_phone_numbers=True,          # replace all phone numbers with a special token\n",
        "        no_numbers=True,                # replace all numbers with a special token\n",
        "        no_digits=True,                 # replace all digits with a special token\n",
        "        no_currency_symbols=True,       # replace all currency symbols with a special token\n",
        "        no_punct=True,                  # remove punctuations\n",
        "        replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
        "        replace_with_url=\"<URL>\",\n",
        "        replace_with_email=\"<EMAIL>\",\n",
        "        replace_with_phone_number=\"<PHONE>\",\n",
        "        replace_with_number=\"\",\n",
        "        replace_with_digit=\"\",\n",
        "        replace_with_currency_symbol=\"\",\n",
        "        lang=\"en\"                       # set to 'de' for German special handling\n",
        "    )\n",
        "    return sentences\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize(sentence):\n",
        "    return \" \".join([\n",
        "        lemmatizer.lemmatize(word, pos=\"v\") \n",
        "        for word in nltk.word_tokenize(sentence.lower()) \n",
        "        if word not in stopwords.words('english')\n",
        "    ])\n",
        "\n",
        "def clean_comment_text_without_lemmatize(sentences):\n",
        "    return lemmatize(remove_noises(decontracted(sentences)))\n",
        "\n",
        "def clean_comment_text(sentences):\n",
        "    return lemmatize(remove_noises(decontracted(sentences)))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g61Kn3ljtrOx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "378a1c79-9c68-428b-97a3-d321ca09cce5"
      },
      "source": [
        "train_pickle_path = f\"{DATA_DIR}/train_lemmatized_{int(TEST_MODE)}.pkl\"\n",
        "if os.path.isfile(train_pickle_path) and (not FORCE_OVERWRITE):\n",
        "    del train\n",
        "    gc.collect()\n",
        "    train = pd.read_pickle(train_pickle_path)\n",
        "else:\n",
        "    train['comment_text'] = train['comment_text'].apply(clean_comment_text)\n",
        "    train.to_pickle(train_pickle_path)\n",
        "\n",
        "train[train['toxic'] > 0].head(3)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>unhealthiness</th>\n",
              "      <th>unhealthy</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0002bcb3da6cb337</td>\n",
              "      <td>cocksucker piss around work</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0005c987bdfc9d4b</td>\n",
              "      <td>hey talk exclusive group wp talibanswho good destroy selfappointed purist gang one ask question abt antisocial destructive noncontribution wp ask sityush clean behavior issue nonsensical warn</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0007e25b2121310b</td>\n",
              "      <td>bye look come think comming back tosser</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  id  ... length\n",
              "6   0002bcb3da6cb337  ...     44\n",
              "12  0005c987bdfc9d4b  ...    319\n",
              "16  0007e25b2121310b  ...     57\n",
              "\n",
              "[3 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJtWXfznhyrf"
      },
      "source": [
        "test_pickle_path = f\"{DATA_DIR}/test_lemmatized_{int(TEST_MODE)}.pkl\"\n",
        "if os.path.isfile(test_pickle_path) and (not FORCE_OVERWRITE):\n",
        "    del test\n",
        "    gc.collect()\n",
        "    test = pd.read_pickle(test_pickle_path)\n",
        "else:\n",
        "    test['comment_text'] = test['comment_text'].apply(clean_comment_text)\n",
        "    test.to_pickle(test_pickle_path)\n",
        "\n",
        "test.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2RhoIxURhtG"
      },
      "source": [
        "if test['id'].count() >= train['id'].count():\n",
        "    raise RuntimeError(\"Invalid counts. Verfity the train/test data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUYPD0TQvEaI"
      },
      "source": [
        "---\n",
        "# Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fb9CoVtbHHP"
      },
      "source": [
        "def unhealthy_volume_ration_per_length():\n",
        "    \"\"\"Ratio of unhealthy comments in the length ranges\"\"\"\n",
        "    ratios = []\n",
        "    lengths = []\n",
        "    max_length = 1000\n",
        "    interval = 50\n",
        "    steps = range(0, max_length + 1, interval)\n",
        "    for start in steps:\n",
        "        stop = start + interval\n",
        "        total_volume = train[\n",
        "            (start <= train['length']) & (train['length'] < stop)\n",
        "        ]['id'].count()\n",
        "        unhealthy_volume = train[\n",
        "            (start <= train['length']) & \n",
        "            (train['length'] < stop) & \n",
        "            (train['unhealthiness'] > 0)\n",
        "        ]['id'].count()\n",
        "\n",
        "        lengths.append(stop)\n",
        "        ratios.append(unhealthy_volume / total_volume)\n",
        "\n",
        "    return lengths, ratios\n",
        "\n",
        "def run_analysis():\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10)) \n",
        "    bins = [1,200,400,600,800,1000,1200]\n",
        "    limits=[0, 1200, 0, 90000]\n",
        "    H = train[train['unhealthiness'] == 0]['length']\n",
        "    U = train[train['unhealthiness'] > 0]['length']\n",
        "\n",
        "    # Volume / Length\n",
        "    ax_volume_length = axes[0][0]\n",
        "    hist_on_ax(ax_volume_length, bins, U, label=\"unhealthy comment\", color='r', alpha=0.9, title=\"Comment volume per length\", xlabel='Length of comments', ylabel='Number of comments', legend=True, limits=limits)\n",
        "    hist_on_ax(ax_volume_length, bins, H, label=\"healthy comment\", color='g', alpha=0.4, title=\"Comment volume per length\", xlabel='Length of comments', ylabel='Number of comments', legend=True, limits=limits)\n",
        "\n",
        "    # Ratio / Length\n",
        "    ax_ratio_length = axes[0][1]\n",
        "    X, Y = unhealthy_volume_ration_per_length()\n",
        "    plot_on_ax(ax_ratio_length, X, Y, label=\"unhealthy ratio\", color='orange', title=\"Unhealthy comment ratio\", xlabel='Length of comments', ylabel='Ratio of unhealthy comments', legend=True, limits=None)\n",
        "\n",
        "    # Length / Percentile\n",
        "    ax_legth_percentile = axes[1][0]\n",
        "    comment_length_percentil_U = U = (train[train['unhealthiness'] > 0]['length']).quantile(np.linspace(.1, 1, 9, 0), 'lower').values\n",
        "    comment_length_percentil_H = H = (train[train['unhealthiness'] == 0]['length']).quantile(np.linspace(.1, 1, 9, 0), 'lower').values\n",
        "    plotter(ax_legth_percentile, x, H, U, 'g', 'r', \"Healthy\", \"Unhealthy\", 'Percentil', 'Comment length', \"Comment length percentile\", [None, None, 0, 1000], False)\n",
        "\n",
        "    # Ratio / Percentile\n",
        "    ax_ratio_percentile = axes[1][1]\n",
        "    ratio = comment_length_percentil_U / comment_length_percentil_H\n",
        "    plotter(ax_ratio_percentile, x, ratio, None, 'k', 'r', \"length ratio (unhealthy/healthy)\", None, 'Percentil', 'Ratio', \"Comment length Ratio\", [None, None, 0, 1], True)\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# Comment length histram (bins = [1,200,400,600,800,1000,1200])\n",
        "# --------------------------------------------------------------------------------\n",
        "sep = (\"\".join(([\"-\"] * 80))) + \"\\n\"\n",
        "print(\n",
        "    f\"{sep}Histgram for Healthy comment length:\\n{sep}\"\n",
        "    f\"{healthy.groupby(pd.cut(healthy['length'], np.arange(0, 1200, 200)))['id'].count()}\"\n",
        ")\n",
        "print(\n",
        "    f\"\\n{sep}Histgram for Unhealthy comment length:\\n{sep}\"\n",
        "    f\"{unhealthy.groupby(pd.cut(unhealthy['length'], np.arange(0, 1200, 200)))['id'].count()}\"\n",
        ")\n",
        "\n",
        "healthy = train[train['unhealthiness'] == 0]\n",
        "unhealthy = train[train['unhealthiness'] > 0]\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# Quantiles of comment length\n",
        "# --------------------------------------------------------------------------------\n",
        "print(f\"{sep}Quantiles of comment length (healthy)\\n{sep[:-1]}\")\n",
        "print(healthy['length'].quantile(np.linspace(.1, 1, 9, 0)))\n",
        "\n",
        "print(f\"\\n{sep}Quantiles of comment length (unhealthy)\\n{sep[:-1]}\")\n",
        "print(unhealthy['length'].quantile(np.linspace(.1, 1, 9, 0)))\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# Median comment length\n",
        "# --------------------------------------------------------------------------------\n",
        "print(f\"{sep}Median comment length:\\n{sep}Healthy[{healthy['length'].median()}] \\nUnhealthy[{unhealthy['length'].median()}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLtKXQa1vpCH"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJThGLhKDyOS"
      },
      "source": [
        "# Comment length\n",
        "\n",
        "### Objective\n",
        "Co-relation between the length and *healthiness* of a comment. **Healthiness** is a binary classification either a comment is healthy or unhealthy where \"unhealthy\" comments are those labeled as toxic, insult, etc.\n",
        "\n",
        "### Findings\n",
        "\n",
        "> * Unhealthy comments tend to be **short**. \n",
        "> * Approx 20% of comments are unhealthy when shorter than **100**.\n",
        "> * Approx 70% of unhealthy comments are shorter than **300**. \n",
        "\n",
        "Most (80%) comments have length of \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzXPQQ3cWIDv"
      },
      "source": [
        "run_analysis()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vzC5vbNxZ9Z"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynvWaiihxXY2"
      },
      "source": [
        "# Wordcloud Utilities\n",
        "wc_stopwords=set(STOPWORDS)\n",
        "def generate_wordcloud_image(words):\n",
        "    \"\"\"Generate world could image\"\"\"\n",
        "    image = WordCloud(\n",
        "        max_words=500, \n",
        "        min_font_size=5,\n",
        "        width = 1000, \n",
        "        height = 500,\n",
        "        stopwords=wc_stopwords, \n",
        "        background_color=\"black\", \n",
        "        margin=5, \n",
        "        collocations=False,\n",
        "        random_state=10\n",
        "    ).generate(words)\n",
        "    return image\n",
        "\n",
        "def generate_healthy_comment_wordclouds():\n",
        "    words_healthy = \" \".join(train[train['unhealthiness'] == 0][\"comment_text\"].values)\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    plt.title(\"Frequent words in Healthy comments\", fontsize=25)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(generate_wordcloud_image(words_healthy), interpolation = 'bilinear')\n",
        "    del words_healthy\n",
        "    \n",
        "def generate_unhealthy_comment_wordclouds():\n",
        "    num_cols = 2\n",
        "    num_rows = len(CATEGORIES) // num_cols\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15,15))\n",
        "    plt.axis('off')\n",
        "    images = []\n",
        "    for index, category in enumerate(CATEGORIES):\n",
        "        image = generate_wordcloud_image(\n",
        "            \" \".join(train[train[category] == 1][\"comment_text\"].values)\n",
        "        )\n",
        "        images.append(image)\n",
        "\n",
        "    for index, category in enumerate(CATEGORIES):\n",
        "        row = index // num_cols\n",
        "        col = index % num_cols\n",
        "        ax = axes[row][col]\n",
        "        ax.imshow(images[index])\n",
        "        ax.set_title(\n",
        "            category, \n",
        "            fontdict={'fontsize': 25, 'fontweight': 'medium'}\n",
        "        )\n",
        "        for pos in ['top', 'bottom', 'right', 'left']:\n",
        "            ax.spines[pos].set_visible(False)\n",
        "            ax.set_xticklabels([])\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticklabels([])\n",
        "            ax.set_yticks([])\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "    del images\n",
        "    del fig, axes\n",
        "\n",
        "import collections\n",
        "class WordAnalysis:\n",
        "    def __init__(self, corpus):\n",
        "        \"\"\"Generate word indices from a text corpus\n",
        "        Args:\n",
        "            corpus: A string including sentences to process.\n",
        "        \"\"\"\n",
        "        words = corpus.lower().split()\n",
        "        self._total = len(words)\n",
        "        assert self._total > 0\n",
        "\n",
        "        self._counts = collections.Counter(words)\n",
        "        del words\n",
        "        self._word_to_probability = {\n",
        "            word: (count / self._total) for (word, count) in self._counts.items()\n",
        "        }\n",
        "\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # Vocabulary from the keys of probabilities preserving the same word order\n",
        "        # --------------------------------------------------------------------------------\n",
        "        self.vocabulary = list(self._word_to_probability.keys())\n",
        "\n",
        "    def probability(self, word):\n",
        "        return self._word_to_probability.get(word.lower(), 0.0)\n",
        "\n",
        "    def count(self, word):\n",
        "        \"\"\"Number of times when the word occured in the original corpus\"\"\"\n",
        "        return self._counts.get(word.lower(), 0)\n",
        "\n",
        "    def size(self):\n",
        "        \"\"\"Number of words in the vocabulary\"\"\"\n",
        "        return len(self.vocabulary)\n",
        "\n",
        "    def total(self):\n",
        "        \"\"\"Total words in the original corpus\"\"\"\n",
        "        return self._total\n",
        "\n",
        "    def top(self, n):\n",
        "        \"\"\"Top n most common words\"\"\"\n",
        "        assert 0 < n < self._total\n",
        "        return [\n",
        "            (word, count/self._total)\n",
        "            for (word, count) in self._counts.most_common(n)\n",
        "        ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwtaEz_ZKpXJ"
      },
      "source": [
        "# Dominant words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8i6dZrQdL5y7"
      },
      "source": [
        "## Healthy comment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PawP2uC_vIpt"
      },
      "source": [
        "generate_healthy_comment_wordclouds()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUwb-RI6O7Bf"
      },
      "source": [
        "## Unhealthy  Comment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy-xbuWmSWby"
      },
      "source": [
        "generate_unhealthy_comment_wordclouds()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4jNfuchQiPy"
      },
      "source": [
        "# Word Frequencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuHaN9Xq36fi"
      },
      "source": [
        "## Words only apperas on unhealthy comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HWPYmIFkuvp"
      },
      "source": [
        "clean = WordAnalysis(\" \".join(train[train['unhealthy'] == 0]['comment_text'].values))\n",
        "dirty = WordAnalysis(\" \".join(train[train['unhealthy'] > 0]['comment_text'].values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuW-W2fgtAFP"
      },
      "source": [
        "words_only_in_dirty = set(dirty.vocabulary) - set(clean.vocabulary)\n",
        "_words = []\n",
        "for word in words_only_in_dirty:\n",
        "    _words = _words + ([word] * dirty.count(word))\n",
        "filthy = WordAnalysis(\" \".join(_words))\n",
        "image_filthy_words = generate_wordcloud_image(\" \".join(_words))\n",
        "del _words\n",
        "\n",
        "n = 10\n",
        "print(\"--------------------------------------------------------------------------------\")\n",
        "print(f\"Top {n} most used words in unhealthy comments\")\n",
        "print(\"--------------------------------------------------------------------------------\")\n",
        "for word, ratio in filthy.top(n):\n",
        "    print(f\"{word:20s} {ratio:0.5f}\")\n",
        "\n",
        "plt.figure(figsize=(15, 12))\n",
        "plt.title(\"Words only in unhealthy comments\", fontsize=35)\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(image_filthy_words, interpolation = 'bilinear')\n",
        "del image_filthy_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcS2etAMtAJG"
      },
      "source": [
        "_ratio = 10.0\n",
        "words_common = set.intersection(set(dirty.vocabulary), set(clean.vocabulary))\n",
        "_words = [\n",
        "    word for word in words_common \n",
        "    if dirty.probability(word) / clean.probability(word) > ratio\n",
        "]\n",
        "_image_more_frequent_words = generate_wordcloud_image(\" \".join(_words))\n",
        "\n",
        "plt.figure(figsize=(15, 12))\n",
        "plt.title(\"Words more frequent in unhealthy comments\", fontsize=35)\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(_image_more_frequent_words, interpolation = 'bilinear')\n",
        "del _words\n",
        "del _image_more_frequent_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-RWZFJj8lJZ"
      },
      "source": [
        "# Vocabulary Size\n",
        "\n",
        "Variety of the vocabulary for healthy comments against unhealthy comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay6oVMRX8BDI"
      },
      "source": [
        "print(f\"Vocaburary size healthy[{clean.size()}] unhealthy[{dirty.size()}]\")\n",
        "plt.bar([\"healthy\", \"unhealthy\"], [clean.size(), dirty.size()], color = ['green','red'])\n",
        "plt.xlabel('Comment type', fontsize=15)\n",
        "plt.ylabel('Vocabulary size', fontsize=15)\n",
        "plt.title('Size of vocabuary for comment type', fontsize=15)    \n",
        "plt.grid()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w_3rDvyC3P2"
      },
      "source": [
        "x = 0.0\n",
        "for word in clean.vocabulary:\n",
        "    x += dirty.probability(word)\n",
        "print(f\"[{(1.0 - x) * 100:2.2f}] % of the words in unhealthy comments are only used in there.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve0RI4pWElUm"
      },
      "source": [
        "x = 0.0\n",
        "for word in dirty.vocabulary:\n",
        "    x += clean.probability(word)\n",
        "print(f\"[{(1.0 - x) * 100:2.2f}] % of the words in healthy comments are only used in there.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOtHJ1edOFoS"
      },
      "source": [
        "---\n",
        "# Training the Models\n",
        "\n",
        "Run BERT Fine Tuning on the data. The warning below is expected for the HuggingFace fine tuning.\n",
        "\n",
        "```\n",
        "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_transform', 'vocab_projector', 'vocab_layer_norm']\n",
        "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
        "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_19']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "```\n",
        "\n",
        "* [Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification](https://github.com/huggingface/transformers/issues/5421#issuecomment-652582854)\n",
        "> This is expected, and tells you that you won't have good performance with your BertForSequenceClassification model before you fine-tune it slightly_smiling_face.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB2cjqCZhcE9"
      },
      "source": [
        "## Fine Tuning Implementation\n",
        "\n",
        "Utilize TensorFlow 2.x Keras for training the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bp2VT2ohkKD"
      },
      "source": [
        "#### Keras Callbacks\n",
        "\n",
        "Utilize [Keras Callbacks API](https://keras.io/api/callbacks/) to apply Eary Stopping, Reduce Learning Rate, and TensorBoard during the model training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc67_HTf0zyO"
      },
      "source": [
        "class SavePretrainedCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Hugging Face models have a save_pretrained() method that saves both \n",
        "    the weights and the necessary metadata to allow them to be loaded as \n",
        "    a pretrained model in future. This is a simple Keras callback that \n",
        "    saves the model with this method after each epoch.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dir, **kwargs):\n",
        "        super().__init__()\n",
        "        self.output_dir = output_dir\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.model.save_pretrained(self.output_dir)\n",
        "\n",
        "class TensorBoardCallback(tf.keras.callbacks.TensorBoard):\n",
        "    \"\"\"TensorBoard visualization of the model training\n",
        "    See https://keras.io/api/callbacks/tensorboard/\n",
        "    \"\"\"\n",
        "    def __init__(self, output_directory):\n",
        "        super().__init__(\n",
        "            log_dir=output_directory,\n",
        "            write_graph=True,\n",
        "            write_images=True,\n",
        "            histogram_freq=1,     # log histogram visualizations every 1 epoch\n",
        "            embeddings_freq=1,    # log embedding visualizations every 1 epoch\n",
        "            update_freq=\"epoch\",      # every epoch\n",
        "        )\n",
        "\n",
        "class EaryStoppingCallback(tf.keras.callbacks.EarlyStopping):\n",
        "    \"\"\"Stop training when no progress on the metric to monitor\n",
        "    See: \n",
        "    https://keras.io/api/callbacks/early_stopping/\n",
        "    https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=3):\n",
        "        assert patience > 0\n",
        "        super().__init__(\n",
        "            monitor='val_loss', \n",
        "            mode='min', \n",
        "            verbose=1, \n",
        "            patience=patience,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "class ModelCheckpointCallback(tf.keras.callbacks.ModelCheckpoint):\n",
        "    \"\"\"Check point to save the model\n",
        "    See https://keras.io/api/callbacks/model_checkpoint/\n",
        "\n",
        "    NOTE: Did not work with HuggingFace with the error.\n",
        "        NotImplementedError: Saving the model to HDF5 format requires the model \n",
        "        to be a Functional model or a Sequential model. \n",
        "        It does not work for subclassed models, because such models are defined \n",
        "        via the body of a Python method, which isn't safely serializable. \n",
        "    \"\"\"\n",
        "    def __init__(self, path_to_file):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            path_to_file: path to the model file to save at check points\n",
        "        \"\"\"\n",
        "        super().__init__(\n",
        "            filepath=path_to_file, \n",
        "            monitor='val_loss', \n",
        "            mode='min', \n",
        "            save_best_only=True\n",
        "        )\n",
        "\n",
        "class ReduceLRCallback(tf.keras.callbacks.ReduceLROnPlateau):\n",
        "    \"\"\"Reduce learning rate when a metric has stopped improving.\n",
        "    See https://keras.io/api/callbacks/reduce_lr_on_plateau/\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=3):\n",
        "        assert patience > 0\n",
        "        super().__init__(\n",
        "            monitor=\"val_loss\",\n",
        "            factor=0.3,\n",
        "            patience=patience,\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0Rpsc7IjWFd"
      },
      "source": [
        "### Fine Tuning Runner\n",
        "\n",
        "The Runner class implements the fine-tuning based on the [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) pretrained model. Each classification category e.g. ```toxic``` will have a dedicated Runner class instance. The reason for using the ***Distilled*** BERT model is to run the training on the limited resources\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXWNRYbes8V6"
      },
      "source": [
        "class Runner:\n",
        "    \"\"\"Fine tuning implementation class\n",
        "    See https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
        "    \"\"\"\n",
        "    # ================================================================================\n",
        "    # Class\n",
        "    # ================================================================================\n",
        "    USE_HF_TRAINER = False\n",
        "    # _model_name = 'distilbert-base-cased'\n",
        "    _model_name = 'distilbert-base-uncased'\n",
        "    _tokenizer = DistilBertTokenizerFast.from_pretrained(_model_name)\n",
        "\n",
        "    # ================================================================================\n",
        "    # Instance\n",
        "    # ================================================================================\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # Instance properties\n",
        "    # --------------------------------------------------------------------------------\n",
        "    @property\n",
        "    def category(self):\n",
        "        \"\"\"Category of the text comment classification, e.g. toxic\"\"\"\n",
        "        return self._category\n",
        "\n",
        "    @property\n",
        "    def batch_size(self):\n",
        "        \"\"\"Mini batch size during the training\"\"\"\n",
        "        assert self._batch_size > 0\n",
        "        return self._batch_size\n",
        "\n",
        "    @property\n",
        "    def X(self):\n",
        "        \"\"\"Training TensorFlow DataSet\"\"\"\n",
        "        return self._X\n",
        "\n",
        "    @property\n",
        "    def V(self):\n",
        "        \"\"\"Validation TensorFlow DataSet\"\"\"\n",
        "        return self._V\n",
        "\n",
        "    @property\n",
        "    def model_name(self):\n",
        "        \"\"\"HuggingFace pretrained model name\"\"\"\n",
        "        return self._model_name\n",
        "\n",
        "    @property\n",
        "    def model(self):\n",
        "        \"\"\"TensorFlow/Keras Model instance\"\"\"\n",
        "        return self._model\n",
        "\n",
        "    @property\n",
        "    def model_metric_names(self):\n",
        "        \"\"\"Model mtrics\n",
        "        The attribute model.metrics_names gives labels for the scalar metrics\n",
        "        to be returned from model.evaluate().\n",
        "        \"\"\"\n",
        "        return self.model.metrics_names\n",
        "\n",
        "    @property\n",
        "    def history(self):\n",
        "        \"\"\"The history object returned from model.fit(). \n",
        "        The object holds a record of the loss and metric during training\n",
        "        \"\"\"\n",
        "        assert self._history is not None\n",
        "        return self._history\n",
        "\n",
        "    @property\n",
        "    def learning_rate(self):\n",
        "        \"\"\"Training learning rate\"\"\"\n",
        "        return self._learning_rate\n",
        "\n",
        "    @property\n",
        "    def num_epochs(self):\n",
        "        \"\"\"Number of maximum epochs to run for the training\"\"\"\n",
        "        return self._num_epochs\n",
        "\n",
        "    @property\n",
        "    def tokenizer(self):\n",
        "        \"\"\"BERT tokenizer. The Tokenzer must match the pretrained model\"\"\"\n",
        "        return self._tokenizer\n",
        "\n",
        "    @property\n",
        "    def max_sequence_length(self):\n",
        "        \"\"\"Maximum token length for the BERT tokenizer can accept. Max 512\n",
        "        \"\"\"\n",
        "        assert 128 <= self._max_sequence_length <= 512\n",
        "        return self._max_sequence_length\n",
        "\n",
        "    @property\n",
        "    def trainer(self):\n",
        "        \"\"\"HuggingFace trainer instance\n",
        "        HuggingFace offers an optimized Trainer because PyTorch does not have\n",
        "        the training loop as Keras/Model has. It is available for TensorFlow\n",
        "        as well, hence to be able to hold the instance in case using it.\n",
        "        \"\"\"\n",
        "        return self._trainer\n",
        "\n",
        "    @property\n",
        "    def output_directory(self):\n",
        "        \"\"\"Parent directory to manage training artefacts\"\"\"\n",
        "        return self._output_directory\n",
        "\n",
        "    @property\n",
        "    def model_directory(self):\n",
        "        \"\"\"Directory to save the trained models\"\"\"\n",
        "        return self._model_directory\n",
        "\n",
        "    @property\n",
        "    def log_directory(self):\n",
        "        \"\"\"Directory to save logs, e.g. TensorBoard logs\"\"\"\n",
        "        return self._log_directory\n",
        "\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # Instance initialization\n",
        "    # --------------------------------------------------------------------------------\n",
        "    def __init__(\n",
        "            self,\n",
        "            category,\n",
        "            training_data,\n",
        "            training_label,\n",
        "            validation_data,\n",
        "            validation_label,\n",
        "            max_sequence_length=256,\n",
        "            batch_size=16,\n",
        "            learning_rate=5e-5,\n",
        "            num_epochs=3,\n",
        "            output_directory=\"./output\"\n",
        "    ):\n",
        "        self._category = category\n",
        "\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # Keras Model\n",
        "        # --------------------------------------------------------------------------------\n",
        "        assert learning_rate > 0.0\n",
        "        self._learning_rate = learning_rate\n",
        "        self._model = None\n",
        "\n",
        "        assert num_epochs > 0\n",
        "        self._num_epochs = num_epochs\n",
        "\n",
        "        assert batch_size > 0\n",
        "        self._batch_size = batch_size\n",
        "        \n",
        "        # model.fit() result holder\n",
        "        self._history = None  \n",
        "\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # HuggingFace artefacts\n",
        "        # --------------------------------------------------------------------------------\n",
        "        assert 128 <= max_sequence_length <= 512\n",
        "        self._max_sequence_length = max_sequence_length\n",
        "        self._model = TFDistilBertForSequenceClassification.from_pretrained(self.model_name)\n",
        "        self._trainer = None\n",
        "\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # Output directories\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # Parent directory\n",
        "        self._output_directory = output_directory\n",
        "        Path(self.output_directory).mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Model directory\n",
        "        self._model_directory = \"{parent}/model_C{category}_B{size}_L{length}\".format(\n",
        "            parent=self.output_directory,\n",
        "            category=self.category,\n",
        "            size=self.batch_size,\n",
        "            length=self.max_sequence_length\n",
        "        )\n",
        "        Path(self.model_directory).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Log directory\n",
        "        self._log_directory = \"{parent}/log_C{category}_B{size}_L{length}\".format(\n",
        "            parent=self.output_directory,\n",
        "            category=self.category,\n",
        "            size=self.batch_size,\n",
        "            length=self.max_sequence_length\n",
        "        )\n",
        "        Path(self.log_directory).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # TensorFlow DataSet\n",
        "        # --------------------------------------------------------------------------------\n",
        "        assert np.all(np.isin(training_label, [0, 1]))\n",
        "        assert np.all(np.isin(validation_label, [0, 1]))\n",
        "        self._X = tf.data.Dataset.from_tensor_slices((\n",
        "            dict(self.tokenizer(\n",
        "                training_data,\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=self.max_sequence_length,\n",
        "                return_tensors=\"tf\"\n",
        "            )),\n",
        "            training_label\n",
        "        ))\n",
        "        self._V = tf.data.Dataset.from_tensor_slices((\n",
        "            dict(self.tokenizer(\n",
        "                validation_data,\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=self.max_sequence_length,\n",
        "                return_tensors=\"tf\"\n",
        "            )),\n",
        "            validation_label\n",
        "        ))\n",
        "\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # Instance methods\n",
        "    # --------------------------------------------------------------------------------\n",
        "    def _hf_train(self):\n",
        "        \"\"\"Train the model using HuggingFace Trainer\"\"\"\n",
        "        self._training_args = TFTrainingArguments(\n",
        "            output_dir='./results',             # output directory\n",
        "            num_train_epochs=3,                 # total number of training epochs\n",
        "            per_device_train_batch_size=self.batch_size,     # batch size per device during training\n",
        "            per_device_eval_batch_size=self.batch_size,      # batch size for evaluation\n",
        "            warmup_steps=500,                   # number of warmup steps for learning rate scheduler\n",
        "            weight_decay=0.01,                  # strength of weight decay\n",
        "            logging_dir='./logs',               # directory for storing logs\n",
        "            logging_steps=10,\n",
        "        )\n",
        "\n",
        "        # with self._training_args.strategy.scope():\n",
        "        #     self._model = TFDistilBertForSequenceClassification.from_pretrained(self.model_name)\n",
        "\n",
        "        self._trainer = TFTrainer(\n",
        "            model=self.model,\n",
        "            args=self._training_args,   # training arguments\n",
        "            train_dataset=self.X,       # training dataset\n",
        "            eval_dataset=self.V         # evaluation dataset\n",
        "        )\n",
        "        self.trainer.train()\n",
        "\n",
        "    def _keras_train(self):\n",
        "        \"\"\"Train the model using Keras\n",
        "        \"\"\"\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # Build the model\n",
        "        # --------------------------------------------------------------------------------\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "        self.model.compile(\n",
        "            optimizer=optimizer, \n",
        "            loss=self.model.compute_loss,\n",
        "            # loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            # loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "            # [\"accuracy\", \"AUC\"] causes an error:\n",
        "            # ValueError: Shapes (None, 1) and (None, 2) are incompatible\n",
        "            metrics = [\"accuracy\"]  \n",
        "        )\n",
        "        self.model.summary()\n",
        "\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # Train the model\n",
        "        # --------------------------------------------------------------------------------\n",
        "        self._history = self.model.fit(\n",
        "            self.X.shuffle(1000).batch(self.batch_size).prefetch(1),\n",
        "            epochs=self.num_epochs,\n",
        "            batch_size=self.batch_size,\n",
        "            validation_data=self.V.shuffle(1000).batch(self.batch_size).prefetch(1),\n",
        "            callbacks=[\n",
        "                EaryStoppingCallback(patience=3),\n",
        "                # SavePretrainedCallback(output_dir=self.output_directory),\n",
        "                ReduceLRCallback(patience=2),\n",
        "                TensorBoardCallback(self.log_directory)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Run the model trainig\"\"\"\n",
        "        if self.USE_HF_TRAINER:\n",
        "            self._hf_train()\n",
        "        else:\n",
        "            self._keras_train()\n",
        "\n",
        "    def evaluate(self, data, label):\n",
        "        \"\"\"Evaluate the model on the given data and label.\n",
        "        https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate\n",
        "        The attribute model.metrics_names gives labels for the scalar metrics\n",
        "        to be returned from model.evaluate().\n",
        "\n",
        "        Args:\n",
        "            data: data to run the prediction\n",
        "            label: label for the data\n",
        "        Returns: \n",
        "            scalar loss if the model has a single output and no metrics, OR \n",
        "            list of scalars (if the model has multiple outputs and/or metrics). \n",
        "        \"\"\"\n",
        "        assert np.all(np.isin(label, [0, 1]))\n",
        "        test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "            dict(self.tokenizer(\n",
        "                data,\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=self.max_sequence_length,\n",
        "                return_tensors=\"tf\"\n",
        "            )),\n",
        "            label\n",
        "        ))\n",
        "        evaluation = self.model.evaluate(\n",
        "            test_dataset.shuffle(1000).batch(self.batch_size).prefetch(1)\n",
        "        )\n",
        "        return evaluation\n",
        "\n",
        "    def predict(self, data):\n",
        "        \"\"\"Calcuate the prediction for the data\n",
        "        Args:\n",
        "            data: text data to classify\n",
        "        Returns: Probabilities for label value 0 and 1\n",
        "        \"\"\"\n",
        "        tokens = dict(self.tokenizer(\n",
        "            data,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=self.max_sequence_length,\n",
        "            return_tensors=\"tf\"\n",
        "        ))\n",
        "        logits = self.model.predict(tokens)[\"logits\"]\n",
        "        # return logits\n",
        "        return tf.nn.softmax(logits)\n",
        "\n",
        "    def save(self, path_to_dir=None):\n",
        "        \"\"\"Save the model from the HuggingFace. \n",
        "        - config.json \n",
        "        - tf_model.h5  \n",
        "\n",
        "        Args:\n",
        "            path_to_dir: directory path to save the HuggingFace model artefacts\n",
        "        \"\"\"\n",
        "\n",
        "        if path_to_dir is None or len(path_to_dir) == 0:\n",
        "            path_to_dir = self.model_directory\n",
        "        Path(path_to_dir).mkdir(parents=True, exist_ok=True)\n",
        "        if self.USE_HF_TRAINER:\n",
        "            self.trainer.save_model(path_to_dir)  \n",
        "        else:\n",
        "            self.model.save_pretrained(path_to_dir)\n",
        "\n",
        "    def load(self, path_to_dir):\n",
        "        \"\"\"Load the model as the HuggingFace format.\n",
        "        Args:\n",
        "            path_to_dir: Directory path from where to load config.json and .h5.\n",
        "        \"\"\"\n",
        "        if os.path.isdir(path_to_dir) and os.access(path_to_dir, os.R_OK):\n",
        "            self._model = TFDistilBertForSequenceClassification.from_pretrained(path_to_dir)\n",
        "        else:\n",
        "            raise RuntimeError(f\"{path_to_dir} does not exit\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV_uiVhBNM7n"
      },
      "source": [
        "## Run Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGFHYUT2gzL3"
      },
      "source": [
        "# HuggingFace\n",
        "MAX_SEQUENTH_LENGTH = 256   # Max token length to accept. 512 taks 1 hour/epoch on Google Colab\n",
        "\n",
        "# Model training\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Result output directory\n",
        "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%b%d%H%M\").upper()\n",
        "RESULT_DIRECTORY = f\"/content/drive/MyDrive/data/toxicity_classification_{TIMESTAMP}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0xIlT_ugzL4"
      },
      "source": [
        "def run(category):\n",
        "    \"\"\"Wrapper to create the Runnler instances for the respective category\"\"\"\n",
        "    print(\"\\n--------------------------------------------------------------------------------\")\n",
        "    print(f\"Model training on [{category}]\")\n",
        "    print(\"--------------------------------------------------------------------------------\")\n",
        "\n",
        "    data = train['comment_text'].tolist()\n",
        "    label = train[category].tolist()\n",
        "\n",
        "    train_data, validation_data, train_label, validation_label = train_test_split(\n",
        "        data,\n",
        "        label,\n",
        "        test_size=.2,\n",
        "        shuffle=True\n",
        "    )\n",
        "    runner = Runner(\n",
        "        category=category,\n",
        "        training_data=train_data,\n",
        "        training_label=train_label,\n",
        "        validation_data=validation_data,\n",
        "        validation_label=validation_label,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        max_sequence_length=MAX_SEQUENTH_LENGTH,\n",
        "        num_epochs=NUM_EPOCHS,\n",
        "        output_directory=RESULT_DIRECTORY\n",
        "    )\n",
        "    \n",
        "    runner.train()\n",
        "    runner.save()\n",
        "\n",
        "    print(\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "    print(f\"Model evaluation on [{category}]\")\n",
        "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "    test_data = test['comment_text'].tolist()\n",
        "    test_label = test[category].tolist()\n",
        "    evaluation = runner.evaluate(test_data, test_label)\n",
        "    print(f\"Evaluation: {runner.model_metric_names}:{evaluation}\")\n",
        "\n",
        "    return runner, evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMjy-emngzL4"
      },
      "source": [
        "runners = {}      # To save the Runner instance for each category.\n",
        "evaluations = {}  # Evaluation results for each category\n",
        "\n",
        "for category in CATEGORIES:\n",
        "    runners[category], evaluations[category] = run(category)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mx_mMR6_d9_"
      },
      "source": [
        "---\n",
        "# TensorBoard\n",
        "\n",
        "Examine the model trainig history in the TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a90boOpP_wi2"
      },
      "source": [
        "category = 'toxic'\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir {RESULT_DIRECTORY}/log_C{category}_B{BATCH_SIZE}_L{MAX_SEQUENTH_LENGTH}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc-Os5eSvBXQ"
      },
      "source": [
        "category = 'threat'\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir {RESULT_DIRECTORY}/log_C{category}_B{BATCH_SIZE}_L{MAX_SEQUENTH_LENGTH}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z59L6LtXQU7"
      },
      "source": [
        "---\n",
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvZ2Z7JQ9jsS"
      },
      "source": [
        "### Instantiate predictors from the save models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BwOtYdWqgyL"
      },
      "source": [
        "test_data = test['comment_text'].tolist()\n",
        "test_label = test[category].tolist()\n",
        "\n",
        "dummy_data = test_data[:5]\n",
        "dummy_label = test_label[:5]\n",
        "id=\"\"\n",
        "predictors = {}\n",
        "\n",
        "for category in CATEGORIES:\n",
        "    predictor = Runner(\n",
        "        category=category,\n",
        "        training_data=dummy_data,\n",
        "        training_label=dummy_label,\n",
        "        validation_data=dummy_data,\n",
        "        validation_label=dummy_label,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        max_sequence_length=MAX_SEQUENTH_LENGTH,\n",
        "    )\n",
        "    path_to_dir = \"{parent}/model_C{category}_B{size}_L{length}\".format(\n",
        "        parent=RESULT_DIRECTORY,\n",
        "        category=category,\n",
        "        size=BATCH_SIZE,\n",
        "        length=MAX_SEQUENTH_LENGTH\n",
        "    )\n",
        "    predictor.load(path_to_dir)\n",
        "    predictors[category] = predictor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG21clJX9p6Q"
      },
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UZMD-JBYIPj"
      },
      "source": [
        "row = {}\n",
        "index = np.random.randint(0, len(test_data))\n",
        "data = test_data[index]\n",
        "row['data'] = data\n",
        "for category in CATEGORIES:\n",
        "    row[category] = np.argmax(predictors[category].predict(data).numpy().tolist()[0])\n",
        "\n",
        "pd.DataFrame([row])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij0hmoTM9xSq"
      },
      "source": [
        "### True Ratings\n",
        "\n",
        "True results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZKDPjOtdpRr"
      },
      "source": [
        "raw_test.iloc[[index]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOwh3MqN7zvZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}