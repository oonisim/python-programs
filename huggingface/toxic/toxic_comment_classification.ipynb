{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "toxic_comment_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3KFBkjcknCD"
      },
      "source": [
        "# Setups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brzFWR6YhqCF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf65f5b-c40f-48ed-a44a-00b51c4695a6"
      },
      "source": [
        "!pip install tensorflow transformers pandas scikit-learn spacy wordcloud gensim\n",
        "!pip install matplotlib seaborn\n",
        "!pip install line_profiler"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/d5/c6c23ad75491467a9a84e526ef2364e523d45e2b0fae28a7cbe8689e7e84/transformers-4.8.1-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 14.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.34.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 52.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 59.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud) (7.1.2)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.1\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.1.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2018.9)\n",
            "Collecting line_profiler\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/84/922000ff9798c58a95e701d602aa24f98b55bc52039cda4e452c2f879bdb/line_profiler-3.3.0-cp37-cp37m-manylinux2010_x86_64.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: IPython>=0.13; python_version >= \"3.7\" in /usr/local/lib/python3.7/dist-packages (from line_profiler) (5.5.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (57.0.0)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (5.0.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython>=0.13; python_version >= \"3.7\"->line_profiler) (0.7.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->IPython>=0.13; python_version >= \"3.7\"->line_profiler) (0.7.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython>=0.13; python_version >= \"3.7\"->line_profiler) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython>=0.13; python_version >= \"3.7\"->line_profiler) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->IPython>=0.13; python_version >= \"3.7\"->line_profiler) (0.2.0)\n",
            "Installing collected packages: line-profiler\n",
            "Successfully installed line-profiler-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jaR9e-xkqNc"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AFiq3iQjVDEL",
        "outputId": "3edcaaf3-893b-454d-b10d-6c564268b77c"
      },
      "source": [
        "%%html\n",
        "<style>\n",
        "table {float:left}\n",
        "</style>"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "table {float:left}\n",
              "</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofY8HgJPcu7K"
      },
      "source": [
        "logging.disable(logging.WARNING)\n",
        "logging.basicConfig(level=logging.ERROR)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6fCHrZNTePV"
      },
      "source": [
        "## Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzCt--KyGh8i"
      },
      "source": [
        "def google_colab_info():\n",
        "    gpu_info = !nvidia-smi\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "        print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "        print('and then re-execute this cell.')\n",
        "    else:\n",
        "        print(gpu_info)\n",
        "\n",
        "    from psutil import virtual_memory\n",
        "    ram_gb = virtual_memory().total / 1e9\n",
        "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "    if ram_gb < 20:\n",
        "        print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "        print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "        print('re-execute this cell.')\n",
        "    else:\n",
        "        print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP1Ln4gxTgio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0474b3cc-3aec-4090-c567-111dda98778f"
      },
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_GOOGLE_COLAB = True\n",
        "    google_colab_info()\n",
        "    \n",
        "    DATA_PATH=\"/content/drive/MyDrive/data/jigsaw-toxic-comment-classification-challenge.zip\"\n",
        "    google.colab.drive.mount('/content/drive')\n",
        "except:\n",
        "    IN_GOOGLE_COLAB = False\n",
        "    DATA_PATH = input(\"Enter the data archive path\") "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jun 25 14:51:29 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UPio2cmTcWO"
      },
      "source": [
        "## Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLeUHOaJ1bt5"
      },
      "source": [
        "import pandas as pd\n",
        "pd.options.display.max_colwidth = 1000"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRq43r5RSoCA"
      },
      "source": [
        "## TensorFlow\n",
        "\n",
        "| TF_CPP_MIN_LOG_LEVEL | Description|          \n",
        "| - |------------- | \n",
        "|0| Suppress all messages are logged (default behavior)|\n",
        "|1 |Suppress INFO messages are not printed|\n",
        "|2 |Suppress INFO and WARNING messages are not printed|\n",
        "|3 |Suppress INFO, WARNING, and ERROR messages are not printed|\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJfDGer2Sqkh"
      },
      "source": [
        "import tensorflow as tf\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXx4rCOeakbp"
      },
      "source": [
        "## Transformers\n",
        "\n",
        "* [Logging](https://huggingface.co/transformers/main_classes/logging.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Fct7lHwa1yg"
      },
      "source": [
        "os.environ['TRANSFORMERS_VERBOSITY'] = \"error\"\n",
        "import transformers\n",
        "transformers.logging.set_verbosity(transformers.logging.ERROR)\n",
        "\n",
        "from transformers import (\n",
        "    PreTrainedModel,\n",
        "    DistilBertTokenizerFast,\n",
        "    TFDistilBertForSequenceClassification,\n",
        "    TFTrainer,\n",
        "    TFTrainingArguments\n",
        ")"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc67_HTf0zyO"
      },
      "source": [
        "class SavePretrainedCallback(tf.keras.callbacks.Callback):\n",
        "    # Hugging Face models have a save_pretrained() method that saves both the weights and the necessary\n",
        "    # metadata to allow them to be loaded as a pretrained model in future. This is a simple Keras callback\n",
        "    # that saves the model with this method after each epoch.\n",
        "    def __init__(self, output_dir, **kwargs):\n",
        "        super().__init__()\n",
        "        self.output_dir = output_dir\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.model.save_pretrained(self.output_dir)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXWNRYbes8V6"
      },
      "source": [
        "class Runner:\n",
        "    # ================================================================================\n",
        "    # Class\n",
        "    # ================================================================================\n",
        "    USE_HF_TRAINER = False\n",
        "    _model_name = 'distilbert-base-cased'\n",
        "    _tokenizer = DistilBertTokenizerFast.from_pretrained(_model_name)\n",
        "\n",
        "    # ================================================================================\n",
        "    # Instance\n",
        "    # ================================================================================\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # Instance properties\n",
        "    # --------------------------------------------------------------------------------\n",
        "    @property\n",
        "    def batch_size(self):\n",
        "        assert self._batch_size > 0\n",
        "        return self._batch_size\n",
        "\n",
        "    @property\n",
        "    def X(self):\n",
        "        \"\"\"Training DataSet\"\"\"\n",
        "        return self._X\n",
        "\n",
        "    @property\n",
        "    def V(self):\n",
        "        \"\"\"Validation DataSet\"\"\"\n",
        "        return self._V\n",
        "\n",
        "    @property\n",
        "    def model_name(self):\n",
        "        \"\"\"HuggingFace pretrained model name\"\"\"\n",
        "        return self._model_name\n",
        "\n",
        "    @property\n",
        "    def model(self):\n",
        "        \"\"\"Model\"\"\"\n",
        "        return self._model\n",
        "\n",
        "    @property\n",
        "    def learning_rate(self):\n",
        "        return self._learning_rate\n",
        "\n",
        "    @property\n",
        "    def num_epochs(self):\n",
        "        return self._num_epochs\n",
        "\n",
        "    @property\n",
        "    def tokenizer(self):\n",
        "        \"\"\"\"\"\"\n",
        "        return self._tokenizer\n",
        "\n",
        "    @property\n",
        "    def max_sequence_length(self):\n",
        "        \"\"\"\"\"\"\n",
        "        return self._max_sequence_length\n",
        "\n",
        "    @property\n",
        "    def trainer(self):\n",
        "        \"\"\"\"\"\"\n",
        "        return self._trainer\n",
        "\n",
        "    @property\n",
        "    def output_directory(self):\n",
        "        \"\"\"Directory to save models, etc\"\"\"\n",
        "        return self._output_directory\n",
        "\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # Instance initialization\n",
        "    # --------------------------------------------------------------------------------\n",
        "    def __init__(\n",
        "            self,\n",
        "            training_data,\n",
        "            training_label,\n",
        "            validation_data,\n",
        "            validation_label,\n",
        "            max_sequence_length=256,\n",
        "            batch_size=16,\n",
        "            learning_rate=5e-5,\n",
        "            num_epochs=3,\n",
        "            output_directory=\"./output\"\n",
        "    ):\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # Keras Model\n",
        "        # --------------------------------------------------------------------------------\n",
        "        assert learning_rate > 0.0\n",
        "        self._learning_rate = learning_rate\n",
        "        self._model = None\n",
        "\n",
        "        assert num_epochs > 0\n",
        "        self._num_epochs = num_epochs\n",
        "\n",
        "        assert batch_size > 0\n",
        "        self._batch_size = batch_size\n",
        "\n",
        "        self._output_directory = output_directory\n",
        "        Path(self.output_directory).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # HuggingFace\n",
        "        # --------------------------------------------------------------------------------\n",
        "        self._model = TFDistilBertForSequenceClassification.from_pretrained(self.model_name)\n",
        "        self._trainer = None\n",
        "        assert 128 <= max_sequence_length <= 512\n",
        "        self._max_sequence_length = max_sequence_length\n",
        "\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # TensorFlow DataSet\n",
        "        # --------------------------------------------------------------------------------\n",
        "        assert np.all(np.isin(training_label, [0, 1]))\n",
        "        assert np.all(np.isin(validation_label, [0, 1]))\n",
        "        self._X = tf.data.Dataset.from_tensor_slices((\n",
        "            dict(self.tokenizer(\n",
        "                training_data,\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=self.max_sequence_length,\n",
        "                return_tensors=\"tf\"\n",
        "            )),\n",
        "            training_label\n",
        "        ))\n",
        "        self._V = tf.data.Dataset.from_tensor_slices((\n",
        "            dict(self.tokenizer(\n",
        "                validation_data,\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=self.max_sequence_length,\n",
        "                return_tensors=\"tf\"\n",
        "            )),\n",
        "            validation_label\n",
        "        ))\n",
        "\n",
        "\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # Instance methods\n",
        "    # --------------------------------------------------------------------------------\n",
        "    def _hf_train(self):\n",
        "        self._training_args = TFTrainingArguments(\n",
        "            output_dir='./results',             # output directory\n",
        "            num_train_epochs=3,                 # total number of training epochs\n",
        "            per_device_train_batch_size=self.batch_size,     # batch size per device during training\n",
        "            per_device_eval_batch_size=self.batch_size,      # batch size for evaluation\n",
        "            warmup_steps=500,                   # number of warmup steps for learning rate scheduler\n",
        "            weight_decay=0.01,                  # strength of weight decay\n",
        "            logging_dir='./logs',               # directory for storing logs\n",
        "            logging_steps=10,\n",
        "        )\n",
        "\n",
        "        # with self._training_args.strategy.scope():\n",
        "        #     self._model = TFDistilBertForSequenceClassification.from_pretrained(self.model_name)\n",
        "\n",
        "        self._trainer = TFTrainer(\n",
        "            model=self.model,\n",
        "            args=self._training_args,   # training arguments\n",
        "            train_dataset=self.X,       # training dataset\n",
        "            eval_dataset=self.V         # evaluation dataset\n",
        "        )\n",
        "        self.trainer.train()\n",
        "\n",
        "    def _keras_train(self):\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "        self.model.compile(\n",
        "            optimizer=optimizer, \n",
        "            # loss=self.model.compute_loss,\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            metrics = [\"accuracy\"]\n",
        "        )\n",
        "        self.model.summary()\n",
        "        self.model.fit(\n",
        "            self.X.shuffle(1000).batch(self.batch_size).prefetch(1),\n",
        "            epochs=self.num_epochs,\n",
        "            batch_size=self.batch_size,\n",
        "            validation_data=self.V.shuffle(1000).batch(self.batch_size).prefetch(1),\n",
        "            # callbacks=[SavePretrainedCallback(output_dir=self.output_directory)],\n",
        "        )\n",
        "\n",
        "    def train(self):\n",
        "        if self.USE_HF_TRAINER:\n",
        "            self._hf_train()\n",
        "        else:\n",
        "            self._keras_train()\n",
        "\n",
        "    def evaluate(self, data, label):\n",
        "        assert np.all(np.isin(label, [0, 1]))\n",
        "        test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "            dict(self.tokenizer(\n",
        "                data,\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=self.max_sequence_length,\n",
        "                return_tensors=\"tf\"\n",
        "            )),\n",
        "            label\n",
        "        ))\n",
        "        evaluation = self.model.evaluate(\n",
        "            test_dataset.shuffle(1000).batch(self.batch_size).prefetch(1)\n",
        "        )\n",
        "        return evaluation\n",
        "\n",
        "    def predict(self, data):\n",
        "        tokens = dict(self.tokenizer(\n",
        "            data,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=self.max_sequence_length,\n",
        "            return_tensors=\"tf\"\n",
        "        ))\n",
        "        logits = self.model.predict(tokens)[\"logits\"]\n",
        "        return tf.nn.softmax(logits)\n",
        "\n",
        "    def save(self, path_to_dir=None):\n",
        "        if path_to_dir is None or len(path_to_dir) == 0:\n",
        "            path_to_dir = self.output_directory\n",
        "        Path(path_to_dir).mkdir(parents=True, exist_ok=True)\n",
        "        if self.USE_HF_TRAINER:\n",
        "            self.trainer.save_model(path_to_dir)  \n",
        "        else:\n",
        "            self.model.save_pretrained(path_to_dir)\n",
        "\n",
        "    def load(self, path_to_dir=None):\n",
        "        if path_to_dir is None or len(path_to_dir) == 0:\n",
        "            path_to_dir = self.output_directory\n",
        "        if os.path.isdir(path_to_dir) and os.access(path_to_dir, os.R_OK):\n",
        "            self._model = TFDistilBertForSequenceClassification.from_pretrained(path_to_dir)\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PFau2osjvx-"
      },
      "source": [
        "---\n",
        "# Data\n",
        "\n",
        "First, upload data to "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz-5NlxtoB34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "729ae95d-ba7a-474c-873c-fe7d62e6e0aa"
      },
      "source": [
        "!unzip -o $DATA_PATH\n",
        "!unzip -o train.csv.zip\n",
        "!unzip -o test.csv.zip\n",
        "!unzip -o test_labels.csv.zip"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/data/jigsaw-toxic-comment-classification-challenge.zip\n",
            "  inflating: sample_submission.csv.zip  \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: test_labels.csv.zip     \n",
            "  inflating: train.csv.zip           \n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n",
            "Archive:  test_labels.csv.zip\n",
            "  inflating: test_labels.csv         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7Id3UZspT7h"
      },
      "source": [
        "raw_train = pd.read_csv(\"./train.csv\")\n",
        "raw_test_data = pd.read_csv(\"./test.csv\")\n",
        "raw_test_label = pd.read_csv(\"./test_labels.csv\")\n",
        "raw_test = pd.merge(raw_test_data, raw_test_label, left_on='id', right_on='id', how='inner')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqRp3qdczPM5"
      },
      "source": [
        "## Training (Raw)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKAe7wXMzOMd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "79dacbb6-b701-474b-b53f-720579083959"
      },
      "source": [
        "raw_train.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  0000997932d777bf  ...             0\n",
              "1  000103f0d9cfb60f  ...             0\n",
              "2  000113f07ec002fd  ...             0\n",
              "3  0001b41b1c6bb37e  ...             0\n",
              "4  0001d958c54c6e35  ...             0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wfx5DKry2KjZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "53fe18cf-336d-4cd7-ccf8-e42bc80c3b4a"
      },
      "source": [
        "raw_train.describe()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>159571.000000</td>\n",
              "      <td>159571.000000</td>\n",
              "      <td>159571.000000</td>\n",
              "      <td>159571.000000</td>\n",
              "      <td>159571.000000</td>\n",
              "      <td>159571.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.095844</td>\n",
              "      <td>0.009996</td>\n",
              "      <td>0.052948</td>\n",
              "      <td>0.002996</td>\n",
              "      <td>0.049364</td>\n",
              "      <td>0.008805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.294379</td>\n",
              "      <td>0.099477</td>\n",
              "      <td>0.223931</td>\n",
              "      <td>0.054650</td>\n",
              "      <td>0.216627</td>\n",
              "      <td>0.093420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               toxic   severe_toxic  ...         insult  identity_hate\n",
              "count  159571.000000  159571.000000  ...  159571.000000  159571.000000\n",
              "mean        0.095844       0.009996  ...       0.049364       0.008805\n",
              "std         0.294379       0.099477  ...       0.216627       0.093420\n",
              "min         0.000000       0.000000  ...       0.000000       0.000000\n",
              "25%         0.000000       0.000000  ...       0.000000       0.000000\n",
              "50%         0.000000       0.000000  ...       0.000000       0.000000\n",
              "75%         0.000000       0.000000  ...       0.000000       0.000000\n",
              "max         1.000000       1.000000  ...       1.000000       1.000000\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NaP9KvTzJuZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "9927202c-73e0-47f1-d787-a3fa4b963b72"
      },
      "source": [
        "raw_train[raw_train['toxic'] > 0].head(5)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0002bcb3da6cb337</td>\n",
              "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0005c987bdfc9d4b</td>\n",
              "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\\n\\nAsk Sityush to clean up his behavior than issue me nonsensical warnings...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0007e25b2121310b</td>\n",
              "      <td>Bye! \\n\\nDon't look, come or think of comming back! Tosser.</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>001810bf8c45bf5f</td>\n",
              "      <td>You are gay or antisemmitian? \\n\\nArchangel WHite Tiger\\n\\nMeow! Greetingshhh!\\n\\nUh, there are two ways, why you do erased my comment about WW2, that holocaust was brutally slaying of Jews and not gays/Gypsys/Slavs/anyone...\\n\\n1 - If you are anti-semitian, than shave your head bald and go to the skinhead meetings!\\n\\n2 - If you doubt words of the Bible, that homosexuality is a deadly sin, make a pentagram tatoo on your forehead go to the satanistic masses with your gay pals!\\n\\n3 - First and last warning, you fucking gay - I won't appreciate if any more nazi shwain would write in my page! I don't wish to talk to you anymore!\\n\\nBeware of the Dark Side!</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>00190820581d90ce</td>\n",
              "      <td>FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  id  ... identity_hate\n",
              "6   0002bcb3da6cb337  ...             0\n",
              "12  0005c987bdfc9d4b  ...             0\n",
              "16  0007e25b2121310b  ...             0\n",
              "42  001810bf8c45bf5f  ...             1\n",
              "43  00190820581d90ce  ...             0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0jSqXxxQrUj"
      },
      "source": [
        "## Test (Raw)\n",
        "The label value -1 is not clear. Remove the rows where value is -1.\n",
        "\n",
        "> test_labels.csv - labels for the test data; value of -1 indicates it was not used for scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9kdtivUQqtW"
      },
      "source": [
        "raw_test = raw_test[(raw_test['toxic'] > 0)]   # Removing rows where 'toxic' label > 0 is sufficicent"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve713cdzWptk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "d0960228-d5f4-4cd0-e132-560b7e418e45"
      },
      "source": [
        "raw_test.describe()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6090.0</td>\n",
              "      <td>6090.000000</td>\n",
              "      <td>6090.000000</td>\n",
              "      <td>6090.000000</td>\n",
              "      <td>6090.000000</td>\n",
              "      <td>6090.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.060263</td>\n",
              "      <td>0.595402</td>\n",
              "      <td>0.033662</td>\n",
              "      <td>0.548768</td>\n",
              "      <td>0.113793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.237993</td>\n",
              "      <td>0.490854</td>\n",
              "      <td>0.180372</td>\n",
              "      <td>0.497657</td>\n",
              "      <td>0.317586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        toxic  severe_toxic  ...       insult  identity_hate\n",
              "count  6090.0   6090.000000  ...  6090.000000    6090.000000\n",
              "mean      1.0      0.060263  ...     0.548768       0.113793\n",
              "std       0.0      0.237993  ...     0.497657       0.317586\n",
              "min       1.0      0.000000  ...     0.000000       0.000000\n",
              "25%       1.0      0.000000  ...     0.000000       0.000000\n",
              "50%       1.0      0.000000  ...     1.000000       0.000000\n",
              "75%       1.0      0.000000  ...     1.000000       0.000000\n",
              "max       1.0      1.000000  ...     1.000000       1.000000\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVyle97CWy7A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a6378b75-1513-4063-ee40-83bf363df776"
      },
      "source": [
        "raw_test.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>00091c35fa9d0465</td>\n",
              "      <td>== Arabs are committing genocide in Iraq, but no protests in Europe. == \\n\\n May Europe also burn in hell.</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0013fed3aeae76b7</td>\n",
              "      <td>DJ Robinson is gay as hell! he sucks his dick so much!!!!!</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>0017d4d47894af05</td>\n",
              "      <td>:Fuck off, you anti-semitic cunt.  |</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>001d739c97bc2ae4</td>\n",
              "      <td>How dare you vandalize that page about the HMS Beagle! Don't vandalize again, demon!</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>001eff4007dbb65b</td>\n",
              "      <td>::No, he is an arrogant, self serving, immature idiot. Get it right.</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  id  ... identity_hate\n",
              "21  00091c35fa9d0465  ...             0\n",
              "48  0013fed3aeae76b7  ...             1\n",
              "59  0017d4d47894af05  ...             0\n",
              "76  001d739c97bc2ae4  ...             0\n",
              "81  001eff4007dbb65b  ...             0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOtHJ1edOFoS"
      },
      "source": [
        "---\n",
        "# BERT Fine Tuning\n",
        "\n",
        "* [Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification](https://github.com/huggingface/transformers/issues/5421#issuecomment-652582854)\n",
        "\n",
        "\n",
        "```\n",
        "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_transform', 'vocab_projector', 'vocab_layer_norm']\n",
        "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
        "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_19']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "```\n",
        "\n",
        "> This is expected, and tells you that you won't have good performance with your BertForSequenceClassification model before you fine-tune it slightly_smiling_face.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGFHYUT2gzL3"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "MAX_SEQUENTH_LENGTH = 256"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0xIlT_ugzL4"
      },
      "source": [
        "def run(category):\n",
        "    print(\"--------------------------------------------------------------------------------\")\n",
        "    print(f\"{category}\")\n",
        "    print(\"--------------------------------------------------------------------------------\")\n",
        "\n",
        "    data = raw_train['comment_text'].tolist()\n",
        "    label = raw_train[category].tolist()\n",
        "    \n",
        "    train_data, validation_data, train_label, validation_label = train_test_split(\n",
        "        data,\n",
        "        label,\n",
        "        test_size=.2,\n",
        "        shuffle=True\n",
        "    )\n",
        "    runner = Runner(\n",
        "        training_data=train_data,\n",
        "        training_label=train_label,\n",
        "        validation_data=validation_data,\n",
        "        validation_label=validation_label,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        max_sequence_length=MAX_SEQUENTH_LENGTH,\n",
        "        output_directory=\"/content/drive/MyDrive/data/model_C{}_B{}_L{}\".format(\n",
        "            category.upper(), BATCH_SIZE, MAX_SEQUENTH_LENGTH\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    runner.train()\n",
        "    runner.save()\n",
        "    runners[category] = runner\n",
        "\n",
        "    test_data = raw_test['comment_text'].tolist()\n",
        "    test_label = raw_test[category].tolist()\n",
        "    evaluation = runner.evaluate(test_data, test_label)\n",
        "    evaluations[category] = evaluation\n",
        "    print(f\"Evaluation: (accuracy):{evaluation}\")"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMjy-emngzL4",
        "outputId": "49cd0a35-1d13-4d43-bdf2-2a826b1bc6f2"
      },
      "source": [
        "categories = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "runners = {}\n",
        "evaluations = {}\n",
        "\n",
        "for category in categories:\n",
        "    run(category)\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "toxic\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_layer_norm', 'activation_13', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier', 'dropout_279', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "distilbert (TFDistilBertMain multiple                  65190912  \n",
            "_________________________________________________________________\n",
            "pre_classifier (Dense)       multiple                  590592    \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  1538      \n",
            "_________________________________________________________________\n",
            "dropout_279 (Dropout)        multiple                  0         \n",
            "=================================================================\n",
            "Total params: 65,783,042\n",
            "Trainable params: 65,783,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "3990/3990 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 0.9595WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "3990/3990 [==============================] - 1152s 287ms/step - loss: 0.1049 - accuracy: 0.9595 - val_loss: 0.0959 - val_accuracy: 0.9621\n",
            "Epoch 2/3\n",
            "3990/3990 [==============================] - 1143s 286ms/step - loss: 0.0682 - accuracy: 0.9730 - val_loss: 0.1145 - val_accuracy: 0.9635\n",
            "Epoch 3/3\n",
            "3990/3990 [==============================] - 1143s 287ms/step - loss: 0.0441 - accuracy: 0.9831 - val_loss: 0.1430 - val_accuracy: 0.9618\n",
            "191/191 [==============================] - 18s 92ms/step - loss: 0.4860 - accuracy: 0.8741\n",
            "Evaluation: (accuracy):[0.4860093891620636, 0.874055802822113]\n",
            "--------------------------------------------------------------------------------\n",
            "severe_toxic\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_layer_norm', 'activation_13', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier', 'dropout_299', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "distilbert (TFDistilBertMain multiple                  65190912  \n",
            "_________________________________________________________________\n",
            "pre_classifier (Dense)       multiple                  590592    \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  1538      \n",
            "_________________________________________________________________\n",
            "dropout_299 (Dropout)        multiple                  0         \n",
            "=================================================================\n",
            "Total params: 65,783,042\n",
            "Trainable params: 65,783,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "3990/3990 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9895WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "3990/3990 [==============================] - 1148s 286ms/step - loss: 0.0293 - accuracy: 0.9895 - val_loss: 0.0254 - val_accuracy: 0.9902\n",
            "Epoch 2/3\n",
            "3990/3990 [==============================] - 1139s 285ms/step - loss: 0.0324 - accuracy: 0.9898 - val_loss: 0.0567 - val_accuracy: 0.9902\n",
            "Epoch 3/3\n",
            "3990/3990 [==============================] - 1139s 285ms/step - loss: 0.0518 - accuracy: 0.9900 - val_loss: 0.0536 - val_accuracy: 0.9902\n",
            "191/191 [==============================] - 17s 91ms/step - loss: 0.3074 - accuracy: 0.9397\n",
            "Evaluation: (accuracy):[0.3074435889720917, 0.9397372603416443]\n",
            "--------------------------------------------------------------------------------\n",
            "obscene\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_layer_norm', 'activation_13', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_319']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "distilbert (TFDistilBertMain multiple                  65190912  \n",
            "_________________________________________________________________\n",
            "pre_classifier (Dense)       multiple                  590592    \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  1538      \n",
            "_________________________________________________________________\n",
            "dropout_319 (Dropout)        multiple                  0         \n",
            "=================================================================\n",
            "Total params: 65,783,042\n",
            "Trainable params: 65,783,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "3990/3990 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9785WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "3990/3990 [==============================] - 1148s 286ms/step - loss: 0.0597 - accuracy: 0.9785 - val_loss: 0.0528 - val_accuracy: 0.9811\n",
            "Epoch 2/3\n",
            "3990/3990 [==============================] - 1139s 286ms/step - loss: 0.0416 - accuracy: 0.9842 - val_loss: 0.0550 - val_accuracy: 0.9804\n",
            "Epoch 3/3\n",
            "3990/3990 [==============================] - 1139s 285ms/step - loss: 0.0338 - accuracy: 0.9876 - val_loss: 0.0585 - val_accuracy: 0.9778\n",
            "191/191 [==============================] - 17s 91ms/step - loss: 0.5614 - accuracy: 0.7722\n",
            "Evaluation: (accuracy):[0.5613734126091003, 0.7722495794296265]\n",
            "--------------------------------------------------------------------------------\n",
            "threat\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_layer_norm', 'activation_13', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier', 'dropout_339', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "distilbert (TFDistilBertMain multiple                  65190912  \n",
            "_________________________________________________________________\n",
            "pre_classifier (Dense)       multiple                  590592    \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  1538      \n",
            "_________________________________________________________________\n",
            "dropout_339 (Dropout)        multiple                  0         \n",
            "=================================================================\n",
            "Total params: 65,783,042\n",
            "Trainable params: 65,783,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "3990/3990 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9968WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "3990/3990 [==============================] - 1147s 286ms/step - loss: 0.0165 - accuracy: 0.9968 - val_loss: 0.0125 - val_accuracy: 0.9971\n",
            "Epoch 2/3\n",
            "3990/3990 [==============================] - 1138s 285ms/step - loss: 0.0148 - accuracy: 0.9970 - val_loss: 0.0186 - val_accuracy: 0.9969\n",
            "Epoch 3/3\n",
            "3990/3990 [==============================] - 1136s 285ms/step - loss: 0.0151 - accuracy: 0.9969 - val_loss: 0.0144 - val_accuracy: 0.9971\n",
            "191/191 [==============================] - 17s 90ms/step - loss: 0.1549 - accuracy: 0.9663\n",
            "Evaluation: (accuracy):[0.15491706132888794, 0.9663382768630981]\n",
            "--------------------------------------------------------------------------------\n",
            "insult\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_layer_norm', 'activation_13', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier', 'dropout_359', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "distilbert (TFDistilBertMain multiple                  65190912  \n",
            "_________________________________________________________________\n",
            "pre_classifier (Dense)       multiple                  590592    \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  1538      \n",
            "_________________________________________________________________\n",
            "dropout_359 (Dropout)        multiple                  0         \n",
            "=================================================================\n",
            "Total params: 65,783,042\n",
            "Trainable params: 65,783,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "3990/3990 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9703WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "3990/3990 [==============================] - 1144s 285ms/step - loss: 0.0768 - accuracy: 0.9703 - val_loss: 0.0667 - val_accuracy: 0.9725\n",
            "Epoch 2/3\n",
            "3990/3990 [==============================] - 1136s 285ms/step - loss: 0.0559 - accuracy: 0.9767 - val_loss: 0.0720 - val_accuracy: 0.9702\n",
            "Epoch 3/3\n",
            "3990/3990 [==============================] - 1136s 285ms/step - loss: 0.0456 - accuracy: 0.9816 - val_loss: 0.1042 - val_accuracy: 0.9499\n",
            "191/191 [==============================] - 17s 91ms/step - loss: 0.9879 - accuracy: 0.4511\n",
            "Evaluation: (accuracy):[0.9879341721534729, 0.45106732845306396]\n",
            "--------------------------------------------------------------------------------\n",
            "identity_hate\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_layer_norm', 'activation_13', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_379']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "distilbert (TFDistilBertMain multiple                  65190912  \n",
            "_________________________________________________________________\n",
            "pre_classifier (Dense)       multiple                  590592    \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  1538      \n",
            "_________________________________________________________________\n",
            "dropout_379 (Dropout)        multiple                  0         \n",
            "=================================================================\n",
            "Total params: 65,783,042\n",
            "Trainable params: 65,783,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "3990/3990 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9907WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "3990/3990 [==============================] - 1145s 285ms/step - loss: 0.0329 - accuracy: 0.9907 - val_loss: 0.0258 - val_accuracy: 0.9919\n",
            "Epoch 2/3\n",
            "3990/3990 [==============================] - 1136s 285ms/step - loss: 0.0255 - accuracy: 0.9911 - val_loss: 0.0317 - val_accuracy: 0.9917\n",
            "Epoch 3/3\n",
            "3990/3990 [==============================] - 1134s 284ms/step - loss: 0.0252 - accuracy: 0.9910 - val_loss: 0.0266 - val_accuracy: 0.9917\n",
            "191/191 [==============================] - 17s 90ms/step - loss: 0.2448 - accuracy: 0.8862\n",
            "Evaluation: (accuracy):[0.24476265907287598, 0.8862069249153137]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsIhlErchQP7",
        "outputId": "6dab02f6-bac7-472e-d24b-d83b60c883c4"
      },
      "source": [
        "test_data = raw_test['comment_text'].tolist()\n",
        "categories = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "\n",
        "for category in categories:\n",
        "    index = np.random.randint(0, len(test_data))\n",
        "    data = test_data[index]\n",
        "\n",
        "    prediction = runners[category].predict(data)\n",
        "    print(f\"category: {category} prediction: {prediction}\\ndata: {data}\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "category: toxic prediction: [[0.04939984 0.95060015]]\n",
            "data: IM SO GAY I WATCH GFAY PEOPLE LICK THEMSLVES\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "category: severe_toxic prediction: [[0.99324554 0.00675443]]\n",
            "data: If you don't know any better then shut up!!!\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3c34b9fd40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "category: obscene prediction: [[0.18318702 0.816813  ]]\n",
            "data: CAPTIAN UNDERPANTS SUCKS WHO WOULD THINK OF SUCH A DAMN STUPID IDEA\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3c32f2bdd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "category: threat prediction: [[9.9929214e-01 7.0781034e-04]]\n",
            "data: PLEASE ALL CALM DOWN !! LONDON WAS BOMBED.. AND I'll KILL THE PAKIES\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "category: insult prediction: [[0.7141192  0.28588083]]\n",
            "data: \" \n",
            "\n",
            " \"\"Duur duuur duur \"\"I don't negotiate with terrorists\"\"? What are you like five fckko? Watch a lil too much 24 did ya? Where's a terrorist? \n",
            " What are you supposedly \"\"negotiating\"\" with retard? \n",
            "\n",
            " Just so you all know there's too many ips and computers here to stop me from trashing wiki as I did over the last week. \n",
            " Over 200 tine minor edits to numbers dates and measurements. \n",
            "\n",
            " So keep pretending you're \"\"negotiating with terrorists\"\" you fckng simpleton retard. That silly crack just cost wiki ANOTHER 25 errors!\"\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "category: identity_hate prediction: [[0.9988601  0.00113985]]\n",
            "data: Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            "\n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            "\n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            "\n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            "\n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            "\n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            "\n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            "\n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            "\n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            "\n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            "\n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            "\n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            "\n",
            " Fuck you azzholes...eat shit. Fuck you in the man hole. \n",
            " Fuck you azzholes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z59L6LtXQU7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BwOtYdWqgyL"
      },
      "source": [
        "test_data = raw_test['comment_text'].tolist()\n",
        "test_label = raw_test[category].tolist()\n",
        "\n",
        "dummy_data = test_data[:5]\n",
        "dummy_label = test_label[:5]\n",
        "\n",
        "runner = Runner(\n",
        "    training_data=dummy_data,\n",
        "    training_label=dummy_label,\n",
        "    validation_data=dummy_data,\n",
        "    validation_label=dummy_label,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    max_sequence_length=MAX_SEQUENTH_LENGTH,\n",
        ")\n",
        "\n",
        "path_to_dir = \"/content/drive/MyDrive/data/model_CTOXIC_B16_L512\"\n",
        "runner.load(path_to_dir)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UZMD-JBYIPj",
        "outputId": "ead39068-f4e7-48b3-c032-2f9520df125d"
      },
      "source": [
        "data = test_data[np.random.randint(0, len(test_data))]\n",
        "print(f\"prediction {np.argmax(runner.predict(data).numpy().tolist()[0])} for data [{data}]\")\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction 1 for data [== You == \n",
            "\n",
            " Can I suck your cock?]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZKDPjOtdpRr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}