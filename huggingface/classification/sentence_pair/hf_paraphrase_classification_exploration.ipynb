{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentence Pair Classification \n",
    "\n",
    "Predict if two sentences are paraphrase, duplicate, or similar.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "220143300ac72b31"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Huggingface\n",
    "\n",
    "### Sentence Pair Tokenization (creating input to model)\n",
    "This will be model-dependent. \n",
    "\n",
    "* [Huggingface Youtube - Preprocessing sentence pairs (PyTorch)](https://www.youtube.com/watch?v=0u3ioSwev3s)\n",
    "* [Colab for Huggingface Youtube - Preprocessing sentence pairs (PyTorch)](https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/videos/sentence_pairs_pt.ipynb)\n",
    "> This notebook regroups the code sample of the video below, which is a part of the [Hugging Face course](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fhuggingface.co%2Fcourse).\n",
    "\n",
    "\n",
    "### Huggingface Forum Topic for Sentence Pair Classification\n",
    "\n",
    "* [Use two sentences as inputs for sentence classification](https://discuss.huggingface.co/t/use-two-sentences-as-inputs-for-sentence-classification/5444)\n",
    "\n",
    "> In BERT, two sentences are provided as follows to the model: ```[CLS] sentence1 [SEP] sentence2 [SEP] [PAD] [PAD] [PAD]```.\n",
    "> You can prepare them using BertTokenizer, simply by providing two sentences:\n",
    "> ```\n",
    "> from transformers import BertTokenizer\n",
    "> tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "> \n",
    "> sentence_a = \"this is a sentence\"\n",
    "> sentence_b = \"this is another sentence\"\n",
    "> \n",
    "> encoding = tokenizer(sentence_a, sentence_b, padding=\"max_length\", truncation=True)\n",
    "> ```\n",
    "\n",
    "* [Train a Bert Classifier with more than 2 Input Text Columns](https://discuss.huggingface.co/t/train-a-bert-classifier-with-more-than-2-input-text-columns/59895)\n",
    "\n",
    "> ```\n",
    "> def tokenize_function(examples):\n",
    ">     return tokenizer(examples[\"text1\"], examples[\"text2\"])\n",
    "> ```\n",
    "\n",
    "## SageMaker\n",
    "\n",
    "* [Sentence Pair Classification - HuggingFace](https://sagemaker.readthedocs.io/en/v2.143.0/algorithms/text/sentence_pair_classification_hugging_face.html)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf087f5fc5d3b8d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T23:17:54.962478298Z",
     "start_time": "2024-02-15T23:17:54.948946436Z"
    }
   },
   "id": "1714373c7047df56"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentence Transformer for Sentence Pair Classification \n",
    "\n",
    "* [Cross-Encoders](https://www.sbert.net/examples/applications/cross-encoder/README.html)\n",
    "\n",
    "> SentenceTransformers also supports to load Cross-Encoders for sentence pair scoring and sentence pair classification tasks. Cross-Encoders can be used whenever you have a pre-defined set of sentence pairs you want to score. For example, you have 100 sentence pairs and you want to get similarity scores for these 100 pairs. For a Cross-Encoder, we pass both sentences simultaneously to the Transformer network. It produces then an output value between 0 and 1 indicating the similarity of the input sentence pair:\n",
    ">   \n",
    "> <img src=\"image/Bi_vs_Cross-Encoder.png\" align=\"left\" width=500/>\n",
    ">   \n",
    "> As detailed in our [paper](https://arxiv.org/abs/1908.10084), Cross-Encoder achieve better performances than Bi-Encoders. However, for many application they are not practical as they do not produce embeddings we could e.g. index or efficiently compare using cosine similarity.\n",
    "> \n",
    "> ### Cross-Encoders Usage\n",
    "> \n",
    "> ```\n",
    "> from sentence_transformers.cross_encoder import CrossEncoder\n",
    "> \n",
    "> model = CrossEncoder(\"model_name_or_path\")\n",
    "> scores = model.predict([[\"My first\", \"sentence pair\"], [\"Second text\", \"pair\"]])\n",
    "> ```\n",
    "> You pass to ```model.predict``` a list of sentence pairs. Note, Cross-Encoder do not work on individual sentence, you have to pass sentence pairs. As model name, you can pass any model or path that is compatible with Huggingface [AutoModel](https://huggingface.co/transformers/model_doc/auto.html) class For a full example, to score a query with all possible sentences in a corpus see [cross-encoder_usage.py](https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/cross-encoder/cross-encoder_usage.py).\n",
    ">   \n",
    "> ### Combining Bi- and Cross-Encoders\n",
    "> \n",
    "> Cross-Encoder achieve higher performance than Bi-Encoders, however, they do not scale well for large datasets. Here, it can make sense to combine Cross- and Bi-Encoders, for example in Information Retrieval / Semantic Search scenarios: First, you use an efficient Bi-Encoder to retrieve e.g. the top-100 most similar sentences for a query. Then, you use a Cross-Encoder to re-rank these 100 hits by computing the score for every (query, hit) combination.\n",
    ">   \n",
    "> For more details on combing Bi- and Cross-Encoders, see [Application - Information Retrieval](https://www.sbert.net/examples/applications/retrieve_rerank/README.html).\n",
    ">\n",
    "> ### Training Cross-Encoders\n",
    "> \n",
    "> See [Cross-Encoder Training](https://www.sbert.net/examples/training/cross-encoder/README.html) how to train your own Cross-Encoder models.\n",
    "\n",
    "### Sentence Transformer - Cross Encoder Models\n",
    "\n",
    "* [Huggingface - Sentence Transformers - Cross-Encoders](https://huggingface.co/cross-encoder)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3a6c84111e76bb4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine Tuning Sentence Pair Classification Models\n",
    "\n",
    "* [Huggingface - Fine-Tuning BERT for Sentence-Pair Classification](https://github.com/sukhijapiyush/Fine-Tune-Bert-for-Sentence-Pair-Classification)\n",
    "* [Google Research - Fine_tune_ALBERT_sentence_pair_classification.ipynb](https://colab.research.google.com/github/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb)\n",
    "* [Kaggle - Quora Question Pairs Competition](https://www.kaggle.com/competitions/quora-question-pairs)\n",
    "> Can you identify question pairs that have the same intent?\n",
    "\n",
    "* [Kaggle - Fine tune BERT for Queation-pair classification](https://www.kaggle.com/code/sharanharsoor/fine-tune-bert-for-queation-pair-classification)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad3db78502a3d9a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T23:17:55.073414353Z",
     "start_time": "2024-02-15T23:17:54.963446151Z"
    }
   },
   "id": "e6437cfa5d880f25"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HuggingFace BERT Sentence Classification "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3dd1e6bb58bb651"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T23:17:56.086104503Z",
     "start_time": "2024-02-15T23:17:55.010277233Z"
    }
   },
   "id": "71b562c4-1757-4302-b86a-b1d3bc5b40d7"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "888c9d05-b37d-49e8-a348-c5b9059c0b34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T23:17:56.089632845Z",
     "start_time": "2024-02-15T23:17:56.087245316Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME: str = \"bert-base-cased-finetuned-mrpc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47051f0b-c118-41ed-836d-042a06e6ef36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T23:18:43.883408704Z",
     "start_time": "2024-02-15T23:17:56.089925212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bed8962b042c48dbbde0a5f44f23d1ae"
      },
      "application/json": {
       "n": 0,
       "total": 29,
       "elapsed": 0.006084442138671875,
       "ncols": null,
       "nrows": null,
       "prefix": "tokenizer_config.json",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/433 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ddfad280cfdb4a9db4ff43d8f9430ddb"
      },
      "application/json": {
       "n": 0,
       "total": 433,
       "elapsed": 0.0028867721557617188,
       "ncols": null,
       "nrows": null,
       "prefix": "config.json",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b7547c5745c43f991636300c89371cc"
      },
      "application/json": {
       "n": 0,
       "total": 213450,
       "elapsed": 0.008103370666503906,
       "ncols": null,
       "nrows": null,
       "prefix": "vocab.txt",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d47f5f9a7c19423abfdf2c4935a3be36"
      },
      "application/json": {
       "n": 0,
       "total": 435797,
       "elapsed": 0.005993366241455078,
       "ncols": null,
       "nrows": null,
       "prefix": "tokenizer.json",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46821e2e187a4a55b0b918805a362e94"
      },
      "application/json": {
       "n": 0,
       "total": 433270768,
       "elapsed": 0.002577066421508789,
       "ncols": null,
       "nrows": null,
       "prefix": "model.safetensors",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First phrase: Machine Learning (ML) makes predictions from data\n",
      "\n",
      "Second phrase: ML uses data to compute a prediction.\n",
      "\n",
      "Second phrase: \n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "phrase_0 = \"Machine Learning (ML) makes predictions from data\"\n",
    "phrase_1 = \"ML uses data to compute a prediction.\"\n",
    "\n",
    "print(f\"\\nFirst phrase: {phrase_0}\")\n",
    "print(f\"\\nSecond phrase: {phrase_1}\")\n",
    "print(\"\\nSecond phrase: \")\n",
    "phrase_tokenized = tokenizer(phrase_0, phrase_1, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af7ae50a-34e7-4ca7-8705-f987fa551152",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T23:18:43.884266798Z",
     "start_time": "2024-02-15T23:18:43.067644726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pseudo-probabilities of not-a-para, is-a-para: [[0.05828979 0.94171023]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**phrase_tokenized).logits\n",
    "    probabilities = torch.softmax(logits, dim=1).numpy()\n",
    "\n",
    "print(f\"\\nPseudo-probabilities of not-a-para, is-a-para: {probabilities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca3512a9-f52e-4498-923f-939f67b50c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T23:18:43.884585550Z",
     "start_time": "2024-02-15T23:18:43.257173462Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
