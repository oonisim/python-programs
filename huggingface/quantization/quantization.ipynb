{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57d240a-04f8-4fd6-a96b-84947e1af579",
   "metadata": {},
   "source": [
    "# Introduction and Hands on\n",
    "\n",
    "* [Deeplearning.ai - Quantization Fundamentals with Hugging Face](https://learn.deeplearning.ai/courses/quantization-fundamentals/lesson/1/introduction)\n",
    "* [Deeplearning.ai - Quantization in depth](https://learn.deeplearning.ai/courses/quantization-in-depth/lesson/2/overview) (MUST)\n",
    "\n",
    "# Articles\n",
    "* [Huggingface - Quantization](https://huggingface.co/docs/transformers/main/en/quantization/overview)\n",
    "* [Facebook AI - Quantization](https://www.llama.com/docs/how-to-guides/quantization/)\n",
    "\n",
    "# Libraries\n",
    "\n",
    "* [Huggingface - Quanto](https://huggingface.co/docs/transformers/main/en/quantization/quanto)\n",
    "\n",
    "> ðŸ¤— Quanto library is a versatile pytorch quantization toolkit. The quantization method used is the linear quantization.\n",
    "> ```\n",
    "> from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n",
    "> \n",
    "> model_id = \"facebook/opt-125m\"\n",
    "> tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "> quantization_config = QuantoConfig(weights=\"int8\")\n",
    "> quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda:0\", quantization_config=quantization_config)\n",
    "> ```\n",
    ">\n",
    "> ```\n",
    "> from quanto import quantize\n",
    ">\n",
    "> quantize(model, weights=torch.int8)\n",
    "> freeze(model)\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41c0afb-c3f2-4e67-9b0f-8b8ee5fbe622",
   "metadata": {},
   "source": [
    "# Algorithms\n",
    "\n",
    "<img src=\"image/quantization_methods_01.png\" align=\"left\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e232784f-b55c-4270-8bf0-76e3a3c60d8c",
   "metadata": {},
   "source": [
    "<img src=\"image/quantization_methods_02.png\" align=\"left\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e565fb18-04d2-4c73-a4a7-c9ec0d946e8e",
   "metadata": {},
   "source": [
    "# What is Calibration\n",
    "\n",
    "The range of the weight values in a layer is different from other layers. To map the value range into int8 etc, first need to identify the statistics of the weight values in a layer (mean, min, max) and use them for quantizations.\n",
    "\n",
    "## No calibration methods\n",
    "\n",
    "<img src=\"image/quantization_methods_03.png\" align=\"left\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcde071-1d08-4413-9dee-d6be33f11d20",
   "metadata": {},
   "source": [
    "# Pre Quantized Models\n",
    "\n",
    "There are pre-quantized models available in HF e.g. [TheBloke](https://huggingface.co/TheBloke).\n",
    "\n",
    "<img src=\"image/hf_pre_quantized_models.png\" align=\"left\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3fc6fe-e8bd-4ca0-951a-47698bf36ec6",
   "metadata": {},
   "source": [
    "# LLM Leaderboard\n",
    "\n",
    "Quantized models performances can be verified at [Huggingface - About the Leader Board](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard). See the [Leader Board documentaiton](https://huggingface.co/docs/leaderboards/open_llm_leaderboard/about)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae8ea6-a270-40ce-841d-b3e306888ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
