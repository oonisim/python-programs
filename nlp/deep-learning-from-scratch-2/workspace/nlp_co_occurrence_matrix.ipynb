{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PTB = True\n",
    "USE_NATIVE=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The fool doth think he is wise, but the wise man knows himself to be a fool.\"\n",
    "text = \"To be, or not to be, that is the question\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTB (Penn Treebank) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oonisim/dataset\n"
     ]
    }
   ],
   "source": [
    "#coding: utf-8\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "try:\n",
    "    import urllib.request\n",
    "except ImportError:\n",
    "    raise ImportError('Use Python3!')\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "url_base = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/'\n",
    "key_file = {\n",
    "    'train':'ptb.train.txt',\n",
    "    'test':'ptb.test.txt',\n",
    "    'valid':'ptb.valid.txt'\n",
    "}\n",
    "save_file = {\n",
    "    'train':'ptb.train.npy',\n",
    "    'test':'ptb.test.npy',\n",
    "    'valid':'ptb.valid.npy'\n",
    "}\n",
    "vocab_file = 'ptb.vocab.pkl'\n",
    "\n",
    "#dataset_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "dataset_dir = os.path.dirname(os.path.abspath(\"/home/oonisim/dataset/hoge\"))\n",
    "print(dataset_dir)\n",
    "\n",
    "def _download(file_name):\n",
    "    file_path = dataset_dir + '/' + file_name\n",
    "    if os.path.exists(file_path):\n",
    "        return\n",
    "\n",
    "    print('Downloading ' + file_name + ' ... ')\n",
    "\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "    except urllib.error.URLError:\n",
    "        import ssl\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "\n",
    "    print('Done')\n",
    "\n",
    "\n",
    "def load_text():\n",
    "    data_type = 'train'\n",
    "    file_name = key_file[data_type]\n",
    "    file_path = dataset_dir + '/' + file_name\n",
    "    vocab_path = dataset_dir + '/' + vocab_file\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        _download(file_name)\n",
    "\n",
    "    text = open(file_path).read().replace('\\n', '<eos>').strip()\n",
    "    return(text)\n",
    "    \n",
    "def load_vocab():\n",
    "    vocab_path = dataset_dir + '/' + vocab_file\n",
    "\n",
    "    if os.path.exists(vocab_path):\n",
    "        with open(vocab_path, 'rb') as f:\n",
    "            word_to_id, id_to_word = pickle.load(f)\n",
    "        return word_to_id, id_to_word\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    data_type = 'train'\n",
    "    file_name = key_file[data_type]\n",
    "    file_path = dataset_dir + '/' + file_name\n",
    "\n",
    "    _download(file_name)\n",
    "\n",
    "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in word_to_id:\n",
    "            tmp_id = len(word_to_id)\n",
    "            word_to_id[word] = tmp_id\n",
    "            id_to_word[tmp_id] = word\n",
    "\n",
    "    with open(vocab_path, 'wb') as f:\n",
    "        pickle.dump((word_to_id, id_to_word), f)\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "\n",
    "def load_data(data_type='train'):\n",
    "    '''\n",
    "        :param data_type: データの種類：'train' or 'test' or 'valid (val)'\n",
    "        :return:\n",
    "    '''\n",
    "    if data_type == 'val': data_type = 'valid'\n",
    "    save_path = dataset_dir + '/' + save_file[data_type]\n",
    "\n",
    "    word_to_id, id_to_word = load_vocab()\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        corpus = np.load(save_path)\n",
    "        return corpus, word_to_id, id_to_word\n",
    "\n",
    "    file_name = key_file[data_type]\n",
    "    file_path = dataset_dir + '/' + file_name\n",
    "    _download(file_name)\n",
    "\n",
    "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    np.save(save_path, corpus)\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word indexing\n",
    "Assign a numerical id to each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proprietary indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        text: A string including sentences to process.\n",
    "    Returns:\n",
    "        corpus: \n",
    "            A numpy array of word indices to every word in the originlal text as as they appear in the text.\n",
    "            The objective of corpus is to preserve the original text but as numerical indices.\n",
    "        word_to_id: A dictionary to map a word to a word index\n",
    "        id_to_word: A dictionary to map a word index to a word\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929589\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "if USE_NATIVE:\n",
    "    if USE_PTB:\n",
    "        (corpus, word_to_id, id_to_word)= load_data('train')\n",
    "    else:\n",
    "        (corpus, word_to_id, id_to_word) = preprocess(text)\n",
    "\n",
    "    print(len(corpus))\n",
    "    print(len(word_to_id))\n",
    "    vocab_size = max(word_to_id.values()) + 1\n",
    "    print(vocab_size)\n",
    "    \n",
    "if not USE_PTB:\n",
    "    print(word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Tokenizer indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "\n",
    "USE_TENSORFLOW = (not USE_NATIVE)\n",
    "if USE_TENSORFLOW:\n",
    "    if USE_PTB:\n",
    "        text = load_text()\n",
    "    else: \n",
    "        text = text\n",
    "\n",
    "    # Each text in \"texts\" is a complete document as one string, \n",
    "    # e.g \"To be or not to be, that is the question.\"\n",
    "    texts = [ text ]   \n",
    "\n",
    "    # fit_on_texts() processes multiple documents and handles all words in all the documents.\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    word_to_id = tokenizer.word_index\n",
    "\n",
    "    # texts_to_sequences() ruturns word index sequence of each document in \"texts\".\n",
    "    sequences = (tokenizer.texts_to_sequences(texts))\n",
    "\n",
    "    # corpus is word index sequence for a single document loaded by load_text().\n",
    "    corpus = sequences[0]\n",
    "\n",
    "    print(len(sequences))\n",
    "    print(len(corpus))\n",
    "    print(len(word_to_id))\n",
    "    \n",
    "    # Index of tokenizer.word_index starts at 1, NOT 0.\n",
    "    # e.g. {'<OOV>': 1, 'the': 2, 'fool': 3, 'wise': 4, 'doth': 5, ...}\n",
    "    vocab_size = max(word_to_id.values()) + 1\n",
    "    print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-occurrence vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    '''Generate co-occurreance matrix for the corpus.\n",
    "\n",
    "    :param corpus:\n",
    "    :param vocab_size:The number of unique words in the corpus. \n",
    "    :param window_size: \n",
    "        The number of words either left or right of the word to count co-occurreances, which is (context_ize / 2)\n",
    "    :return: co-occurrence matrix\n",
    "    '''\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 0 1 0 0 0 0]\n",
      " [2 0 1 0 1 0 0 0]\n",
      " [0 1 0 1 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 1 0]\n",
      " [0 0 0 0 0 1 0 1]\n",
      " [0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -n 1 \n",
    "com1 = create_co_matrix(corpus, vocab_size, 1)\n",
    "print(com1)\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -T create_co_matrix.log -f create_co_matrix create_co_matrix(corpus, vocab_size, 1)\n",
    "print(open('create_co_matrix.log', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy vectorized way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(corpus, index, stride, flag=False):\n",
    "    if not flag:\n",
    "        return\n",
    "    \n",
    "    n = len(corpus)\n",
    "    print(\"word is {} and context is {}\".format(\n",
    "        id_to_word[corpus[index]],\n",
    "        [ id_to_word[i] for i in corpus[max(0, (index-stride) +1) : min((index+stride) +1, n)]]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cooccurrence_matrix(sequence, vocabrary_size, context_size=3):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        sequence: word index sequence of the original corpus text\n",
    "        vocabrary_size: number of words in the vocabrary (same with co-occurrence vector size)\n",
    "        context_size: context (N-gram size N) within to check co-occurrences.\n",
    "    Returns:\n",
    "        co_occurrence matrix\n",
    "    \"\"\"\n",
    "    n = sequence_size = len(sequence)\n",
    "    co_occurrence_matrix = np.zeros((vocabrary_size, vocabrary_size), dtype=np.int32)\n",
    "\n",
    "    stride = int((context_size - 1)/2 )\n",
    "    assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
    "        n, stride\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    # Handle position=slice(0 : (stride-1) +1),       co-occurrences=slice(max(0, position-stride): min((position+stride),n-1) +1)\n",
    "    # Handle position=slice((n-1-stride) : (n-1) +1), co-occurrences=slice(max(0, position-stride): min((position+stride),n-1) +1)\n",
    "    indices = [*range(0, (stride-1) +1), *range((n-1)-stride +1, (n-1) +1)]\n",
    "    #print(indices)\n",
    "    \n",
    "    for position in indices:\n",
    "        debug(sequence, position, stride, False)\n",
    "        co_occurrence_matrix[\n",
    "            sequence[position],                                             # position to the word\n",
    "            sequence[max(0, position-stride) : min((position+stride),n-1) +1]  # indices to co-occurence words \n",
    "        ] += 1\n",
    "\n",
    "    \n",
    "    # Handle position=slice(stride, ((sequence_size-1) - stride) +1)\n",
    "    for position in range(stride, (sequence_size-1) - stride + 1):        \n",
    "        co_occurrence_matrix[\n",
    "            sequence[position],                                 # position to the word\n",
    "            sequence[(position-stride) : (position + stride + 1)]  # indices to co-occurence words \n",
    "        ] += 1\n",
    "    \"\"\"        \n",
    "    \n",
    "    for position in range(0, n):        \n",
    "        co_occurrence_matrix[\n",
    "            sequence[position],                                                # position  to the word\n",
    "            sequence[max(0, position-stride) : min((position+stride),n-1) +1]  # positions to co-occurence words \n",
    "        ] += 1\n",
    "\n",
    "    np.fill_diagonal(co_occurrence_matrix, 0)\n",
    "    \n",
    "    return co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 0 1 0 0 0 0]\n",
      " [2 0 1 0 1 0 0 0]\n",
      " [0 1 0 1 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 1 0]\n",
      " [0 0 0 0 0 1 0 1]\n",
      " [0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -n 1 \n",
    "com2 = create_cooccurrence_matrix(corpus, vocab_size, 3)\n",
    "#print((com2))\n",
    "assert (com1==com2).all()\n",
    "print(com2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'create_cooccurrence_matrix.log'. \n",
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 23.0015 s\n",
      "File: <ipython-input-8-27f5e530d4ff>\n",
      "Function: create_cooccurrence_matrix at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def create_cooccurrence_matrix(sequence, vocabrary_size, context_size=3):\n",
      "     2                                               \"\"\"\n",
      "     3                                               Args: \n",
      "     4                                                   sequence: word index sequence of the original corpus text\n",
      "     5                                                   vocabrary_size: number of words in the vocabrary (same with co-occurrence vector size)\n",
      "     6                                                   context_size: context (N-gram size N) within to check co-occurrences.\n",
      "     7                                               Returns:\n",
      "     8                                                   co_occurrence matrix\n",
      "     9                                               \"\"\"\n",
      "    10         1          4.0      4.0      0.0      n = sequence_size = len(sequence)\n",
      "    11         1         98.0     98.0      0.0      co_occurrence_matrix = np.zeros((vocabrary_size, vocabrary_size), dtype=np.int32)\n",
      "    12                                           \n",
      "    13         1          5.0      5.0      0.0      stride = int((context_size - 1)/2 )\n",
      "    14         1          1.0      1.0      0.0      assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
      "    15                                                   n, stride\n",
      "    16                                               )\n",
      "    17                                           \n",
      "    18                                               \"\"\"\n",
      "    19                                               # Handle position=slice(0 : (stride-1) +1),       co-occurrences=slice(max(0, position-stride): min((position+stride),n-1) +1)\n",
      "    20                                               # Handle position=slice((n-1-stride) : (n-1) +1), co-occurrences=slice(max(0, position-stride): min((position+stride),n-1) +1)\n",
      "    21                                               indices = [*range(0, (stride-1) +1), *range((n-1)-stride +1, (n-1) +1)]\n",
      "    22                                               #print(indices)\n",
      "    23                                               \n",
      "    24                                               for position in indices:\n",
      "    25                                                   debug(sequence, position, stride, False)\n",
      "    26                                                   co_occurrence_matrix[\n",
      "    27                                                       sequence[position],                                             # position to the word\n",
      "    28                                                       sequence[max(0, position-stride) : min((position+stride),n-1) +1]  # indices to co-occurance words \n",
      "    29                                                   ] += 1\n",
      "    30                                           \n",
      "    31                                               \n",
      "    32                                               # Handle position=slice(stride, ((sequence_size-1) - stride) +1)\n",
      "    33                                               for position in range(stride, (sequence_size-1) - stride + 1):        \n",
      "    34                                                   co_occurrence_matrix[\n",
      "    35                                                       sequence[position],                                 # position to the word\n",
      "    36                                                       sequence[(position-stride) : (position + stride + 1)]  # indices to co-occurance words \n",
      "    37                                                   ] += 1\n",
      "    38                                               \"\"\"        \n",
      "    39                                               \n",
      "    40    929590    1175326.0      1.3      5.1      for position in range(0, n):        \n",
      "    41   2788767   15304643.0      5.5     66.5          co_occurrence_matrix[\n",
      "    42   1859178    2176964.0      1.2      9.5              sequence[position],                                                # position  to the word\n",
      "    43    929589    3280181.0      3.5     14.3              sequence[max(0, position-stride) : min((position+stride),n-1) +1]  # positions to co-occurance words \n",
      "    44    929589    1062613.0      1.1      4.6          ] += 1\n",
      "    45                                           \n",
      "    46         1       1698.0   1698.0      0.0      np.fill_diagonal(co_occurrence_matrix, 0)\n",
      "    47                                               \n",
      "    48         1          2.0      2.0      0.0      return co_occurrence_matrix\n"
     ]
    }
   ],
   "source": [
    "%lprun -T create_cooccurrence_matrix.log -f create_cooccurrence_matrix create_cooccurrence_matrix(corpus, vocab_size, 3)\n",
    "print(open('create_cooccurrence_matrix.log', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cooccurrence_matrix_2(corpus, vector_size, context_size=3):\n",
    "    \"\"\"\n",
    "    Create a co-occurrence matrix for the copus with two paths.\n",
    "        1. Handle co-occurrences to the right of the word in the context.\n",
    "        2. Handle co-occurrences to the left  of the word in the context.\n",
    "    Args: \n",
    "        copus: Original text as word indices\n",
    "        vector_size: Co-occurrence vector size (including the word itself)\n",
    "        context_size: Context (N-gram) size N\n",
    "        \n",
    "    \"\"\"\n",
    "    corpus_size = len(corpus)\n",
    "    co_ccurrence_matrix = np.zeros((vector_size, vector_size), dtype=np.int32)\n",
    "\n",
    "    stride = int((context_size - 1)/2 )\n",
    "\n",
    "    # Handle right stride\n",
    "    for index in range(0, (corpus_size-1) - stride +1):\n",
    "        debug(corpus, index, stride)\n",
    "        co_ccurrence_matrix[\n",
    "            corpus[index],                       # index to the word\n",
    "            corpus[index+1 : (index+stride) +1]  # indices to right co-occurance words excluding word itself\n",
    "        ] += 1\n",
    "        \n",
    "    # Handle left stride\n",
    "    for index in range(stride, (corpus_size-1) +1):\n",
    "        debug(corpus, index, stride) \n",
    "        co_ccurrence_matrix[\n",
    "            corpus[index],                      # index to the word\n",
    "            corpus[(index-stride) : index]      # indices to left co-occurance words excluding word itself\n",
    "        ] += 1\n",
    "\n",
    "    return co_ccurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit -n 1 \n",
    "com3 = create_cooccurrence_matrix_2(corpus, vocab_size, 3)\n",
    "#print(com3)\n",
    "assert (com1==com3).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -T create_cooccurrence_matrix_2.log -f create_cooccurrence_matrix_2 create_cooccurrence_matrix_2(corpus, vocab_size, 3)\n",
    "print(open('create_cooccurrence_matrix_2.log', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text: A string including sentences to process.\n",
    "    Returns:\n",
    "        corpus:\n",
    "            A numpy array of word indices to every word in the originlal text as as they appear in the text.\n",
    "            The objective of corpus is to preserve the original text but as numerical indices.\n",
    "        word_to_id: A dictionary to map a word to a word index\n",
    "        id_to_word: A dictionary to map a word index to a word\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    " \n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    " \n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    " \n",
    "    return corpus, word_to_id, id_to_word\n",
    " \n",
    "\n",
    "def create_cooccurrence_matrix(sequence, vocabrary_size, context_size=3):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequence: word index sequence of the original corpus text\n",
    "        vocabrary_size: number of words in the vocabrary (same with co-occurrence vector size)\n",
    "        context_size: context (N-gram size N) within to check co-occurrences.\n",
    "\n",
    "    \"\"\"\n",
    "    n = sequence_size = len(sequence)\n",
    "    co_ccurrence_matrix = np.zeros((vocabrary_size, vocabrary_size), dtype=np.int32)\n",
    "\n",
    "    stride = int((context_size - 1)/2 )\n",
    "    assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
    "        n, stride\n",
    "    ) \n",
    "\n",
    "    # For each position, the co_occurrence_indices is (1, stride), ..., (1, 2 * stride)\n",
    "    position = 0\n",
    "    indices = np.array([\n",
    "        [\n",
    "            sequence[position],                                                # position  to the word\n",
    "            sequence[max(0, position-stride) : min((position+stride),n-1) +1]  # positions to co-occurance words\n",
    "        ]]\n",
    "    )\n",
    "    for position in range(0, n):\n",
    "        co_occurrence_indices = np.array([\n",
    "            [\n",
    "                sequence[position],                                                # position  to the word\n",
    "                sequence[max(0, position-stride) : min((position+stride),n-1) +1]  # positions to co-occurance words\n",
    "            ]]\n",
    "        )\n",
    "        print(\"indices \\n{}\".format(indices))\n",
    "        print(\"indices shape {}\\n\".format(indices.shape))\n",
    "        print(\"co_occurrence_indices \\n{}\".format(co_occurrence_indices))\n",
    "        print(\"co_occurrence_indices shape {}\\n\".format(co_occurrence_indices.shape))\n",
    "        indices = np.append(\n",
    "            indices,\n",
    "            co_occurrence_indices,\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        \n",
    "    print(\"-------------------------------\")\n",
    "    print(indices[0])\n",
    "    indices = np.delete(indices, 0)\n",
    "    print(\"-------------------------------\")\n",
    "    print(indices)\n",
    "    print(indices.shape)\n",
    "    print(\"Updating the co_occurrence_matrix: indices \\n{} \\nindices.dtype {}\".format(\n",
    "        indices,\n",
    "        indices.dtype\n",
    "    ))\n",
    "    co_ccurrence_matrix[\n",
    "        indices\n",
    "    ] += 1\n",
    " \n",
    "    np.fill_diagonal(co_ccurrence_matrix, 0)\n",
    "     \n",
    "    return co_ccurrence_matrix\n",
    "\n",
    "\n",
    "\n",
    "corpus = \"To be, or not to be, that is the question\"\n",
    " \n",
    "sequence, word_to_id, id_to_word = preprocess(corpus)\n",
    "vocabrary_size = max(word_to_id.values()) + 1\n",
    "create_cooccurrence_matrix(sequence, vocabrary_size , 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the co_occurrence_matrix: indices \n",
      "[[0 array([0, 1])]\n",
      " [1 array([0, 1, 2])]\n",
      " [2 array([1, 2, 3])]\n",
      " [3 array([2, 3, 0])]\n",
      " [0 array([3, 0, 1])]\n",
      " [1 array([0, 1, 4])]\n",
      " [4 array([1, 4, 5])]\n",
      " [5 array([4, 5, 6])]\n",
      " [6 array([5, 6, 7])]\n",
      " [7 array([6, 7])]] \n",
      "indices.dtype object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-88-d9b081bf2f1a>:48: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  indices = np.array([\n",
      "<ipython-input-88-d9b081bf2f1a>:56: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  co_occurrence_indices = np.array([\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-d9b081bf2f1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_to_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mvocabrary_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mcreate_cooccurrence_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabrary_size\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-d9b081bf2f1a>\u001b[0m in \u001b[0;36mcreate_cooccurrence_matrix\u001b[0;34m(sequence, vocabrary_size, context_size)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     ))\n\u001b[0;32m---> 72\u001b[0;31m     co_ccurrence_matrix[\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     ] += 1\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text: A string including sentences to process.\n",
    "    Returns:\n",
    "        corpus:\n",
    "            A numpy array of word indices to every word in the originlal text as as they appear in the text.\n",
    "            The objective of corpus is to preserve the original text but as numerical indices.\n",
    "        word_to_id: A dictionary to map a word to a word index\n",
    "        id_to_word: A dictionary to map a word index to a word\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    " \n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    " \n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    " \n",
    "    return corpus, word_to_id, id_to_word\n",
    " \n",
    "\n",
    "def create_cooccurrence_matrix(sequence, vocabrary_size, context_size=3):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequence: word index sequence of the original corpus text\n",
    "        vocabrary_size: number of words in the vocabrary (same with co-occurrence vector size)\n",
    "        context_size: context (N-gram size N) within to check co-occurrences.\n",
    "\n",
    "    \"\"\"\n",
    "    n = sequence_size = len(sequence)\n",
    "    co_ccurrence_matrix = np.zeros((vocabrary_size, vocabrary_size), dtype=np.int32)\n",
    "\n",
    "    stride = int((context_size - 1)/2 )\n",
    "    assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
    "        n, stride\n",
    "    ) \n",
    "\n",
    "    position = 0\n",
    "    indices = np.array([\n",
    "        [\n",
    "            sequence[0],                                         # position  to the word\n",
    "            sequence[max(0, 0-stride) : min((0+stride),n-1) +1]  # positions to co-occurrencewords\n",
    "        ]]\n",
    "    )\n",
    "    assert n > 1\n",
    "    for position in range(1, n):\n",
    "        co_occurrence_indices = np.array([\n",
    "            [\n",
    "                sequence[position],                                                # position  to the word\n",
    "                sequence[max(0, position-stride) : min((position+stride),n-1) +1]  # positions to co-occurrencewords\n",
    "            ]]\n",
    "        )\n",
    "        indices = np.append(\n",
    "            indices,\n",
    "            co_occurrence_indices,\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "    print(\"Updating the co_occurrence_matrix: indices \\n{} \\nindices.dtype {}\".format(\n",
    "        indices,\n",
    "        indices.dtype\n",
    "    ))\n",
    "    co_ccurrence_matrix[\n",
    "        indices\n",
    "    ] += 1\n",
    " \n",
    "    np.fill_diagonal(co_ccurrence_matrix, 0)\n",
    "     \n",
    "    return co_ccurrence_matrix\n",
    "\n",
    "\n",
    "\n",
    "corpus = \"To be, or not to be, that is the question\"\n",
    " \n",
    "sequence, word_to_id, id_to_word = preprocess(corpus)\n",
    "vocabrary_size = max(word_to_id.values()) + 1\n",
    "create_cooccurrence_matrix(sequence, vocabrary_size , 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
