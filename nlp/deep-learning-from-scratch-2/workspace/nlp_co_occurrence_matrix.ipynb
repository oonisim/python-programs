{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=80) \n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PTB = False\n",
    "USE_NATIVE=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"The fool doth think he is wise, but the wise man knows himself to be a fool.\"\n",
    "#text = \"To be, or not to be, that is the question\"\n",
    "text = \"I know how to build an attention in neural networks. But I don’t understand how attention layers learn the weights that pay attention to some specific embedding.I have this question because I’m tackling a NLP task using attention layer. I believe it should be very easy to learn (the most important part is to learn alignments). However, my neural networks only achieve 50% test set accuracy. And the attention matrix is weird. I don’t know how to improve my networks.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTB (Penn Treebank) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oonisim/dataset\n"
     ]
    }
   ],
   "source": [
    "#coding: utf-8\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "try:\n",
    "    import urllib.request\n",
    "except ImportError:\n",
    "    raise ImportError('Use Python3!')\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "url_base = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/'\n",
    "key_file = {\n",
    "    'train':'ptb.train.txt',\n",
    "    'test':'ptb.test.txt',\n",
    "    'valid':'ptb.valid.txt'\n",
    "}\n",
    "save_file = {\n",
    "    'train':'ptb.train.npy',\n",
    "    'test':'ptb.test.npy',\n",
    "    'valid':'ptb.valid.npy'\n",
    "}\n",
    "vocab_file = 'ptb.vocab.pkl'\n",
    "\n",
    "#dataset_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "dataset_dir = os.path.dirname(os.path.abspath(\"/home/oonisim/dataset/hoge\"))\n",
    "print(dataset_dir)\n",
    "\n",
    "def _download(file_name):\n",
    "    file_path = dataset_dir + '/' + file_name\n",
    "    if os.path.exists(file_path):\n",
    "        return\n",
    "\n",
    "    print('Downloading ' + file_name + ' ... ')\n",
    "\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "    except urllib.error.URLError:\n",
    "        import ssl\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "\n",
    "    print('Done')\n",
    "\n",
    "\n",
    "def load_text():\n",
    "    data_type = 'train'\n",
    "    file_name = key_file[data_type]\n",
    "    file_path = dataset_dir + '/' + file_name\n",
    "    vocab_path = dataset_dir + '/' + vocab_file\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        _download(file_name)\n",
    "\n",
    "    text = open(file_path).read().replace('\\n', '<eos>').strip()\n",
    "    return(text)\n",
    "    \n",
    "def load_vocab():\n",
    "    vocab_path = dataset_dir + '/' + vocab_file\n",
    "\n",
    "    if os.path.exists(vocab_path):\n",
    "        with open(vocab_path, 'rb') as f:\n",
    "            word_to_id, id_to_word = pickle.load(f)\n",
    "        return word_to_id, id_to_word\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    data_type = 'train'\n",
    "    file_name = key_file[data_type]\n",
    "    file_path = dataset_dir + '/' + file_name\n",
    "\n",
    "    _download(file_name)\n",
    "\n",
    "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in word_to_id:\n",
    "            tmp_id = len(word_to_id)\n",
    "            word_to_id[word] = tmp_id\n",
    "            id_to_word[tmp_id] = word\n",
    "\n",
    "    with open(vocab_path, 'wb') as f:\n",
    "        pickle.dump((word_to_id, id_to_word), f)\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "\n",
    "def load_data(data_type='train'):\n",
    "    '''\n",
    "        :param data_type: データの種類：'train' or 'test' or 'valid (val)'\n",
    "        :return:\n",
    "    '''\n",
    "    if data_type == 'val': data_type = 'valid'\n",
    "    save_path = dataset_dir + '/' + save_file[data_type]\n",
    "\n",
    "    word_to_id, id_to_word = load_vocab()\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        corpus = np.load(save_path)\n",
    "        return corpus, word_to_id, id_to_word\n",
    "\n",
    "    file_name = key_file[data_type]\n",
    "    file_path = dataset_dir + '/' + file_name\n",
    "    _download(file_name)\n",
    "\n",
    "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    np.save(save_path, corpus)\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word indexing\n",
    "Assign a numerical id to each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proprietary indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        text: A string including sentences to process.\n",
    "    Returns:\n",
    "        corpus: \n",
    "            A numpy array of word indices to every word in the originlal text as as they appear in the text.\n",
    "            The objective of corpus is to preserve the original text but as numerical indices.\n",
    "        word_to_id: A dictionary to map a word to a word index\n",
    "        id_to_word: A dictionary to map a word index to a word\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "59\n",
      "59\n",
      "{'i': 0, 'know': 1, 'how': 2, 'to': 3, 'build': 4, 'an': 5, 'attention': 6, 'in': 7, 'neural': 8, 'networks': 9, '.': 10, 'but': 11, 'don’t': 12, 'understand': 13, 'layers': 14, 'learn': 15, 'the': 16, 'weights': 17, 'that': 18, 'pay': 19, 'some': 20, 'specific': 21, 'embedding': 22, '.i': 23, 'have': 24, 'this': 25, 'question': 26, 'because': 27, 'i’m': 28, 'tackling': 29, 'a': 30, 'nlp': 31, 'task': 32, 'using': 33, 'layer': 34, 'believe': 35, 'it': 36, 'should': 37, 'be': 38, 'very': 39, 'easy': 40, '(the': 41, 'most': 42, 'important': 43, 'part': 44, 'is': 45, 'alignments)': 46, 'however,': 47, 'my': 48, 'only': 49, 'achieve': 50, '50%': 51, 'test': 52, 'set': 53, 'accuracy': 54, 'and': 55, 'matrix': 56, 'weird': 57, 'improve': 58}\n"
     ]
    }
   ],
   "source": [
    "if USE_NATIVE:\n",
    "    if USE_PTB:\n",
    "        (corpus, word_to_id, id_to_word)= load_data('train')\n",
    "    else:\n",
    "        (corpus, word_to_id, id_to_word) = preprocess(text)\n",
    "\n",
    "    print(len(corpus))\n",
    "    print(len(word_to_id))\n",
    "    vocab_size = max(word_to_id.values()) + 1\n",
    "    print(vocab_size)\n",
    "    \n",
    "if not USE_PTB:\n",
    "    print(word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Tokenizer indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "\n",
    "USE_TENSORFLOW = (not USE_NATIVE)\n",
    "if USE_TENSORFLOW:\n",
    "    if USE_PTB:\n",
    "        text = load_text()\n",
    "    else: \n",
    "        text = text\n",
    "\n",
    "    # Each text in \"texts\" is a complete document as one string, \n",
    "    # e.g \"To be or not to be, that is the question.\"\n",
    "    texts = [ text ]   \n",
    "\n",
    "    # fit_on_texts() processes multiple documents and handles all words in all the documents.\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    word_to_id = tokenizer.word_index\n",
    "\n",
    "    # texts_to_sequences() ruturns word index sequence of each document in \"texts\".\n",
    "    sequences = (tokenizer.texts_to_sequences(texts))\n",
    "\n",
    "    # corpus is word index sequence for a single document loaded by load_text().\n",
    "    corpus = sequences[0]\n",
    "\n",
    "    print(len(sequences))\n",
    "    print(len(corpus))\n",
    "    print(len(word_to_id))\n",
    "    \n",
    "    # Index of tokenizer.word_index starts at 1, NOT 0.\n",
    "    # e.g. {'<OOV>': 1, 'the': 2, 'fool': 3, 'wise': 4, 'doth': 5, ...}\n",
    "    vocab_size = max(word_to_id.values()) + 1\n",
    "    print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-occurrence vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    '''Generate co-occurreance matrix for the corpus.\n",
    "\n",
    "    :param corpus:\n",
    "    :param vocab_size:The number of unique words in the corpus. \n",
    "    :param window_size: \n",
    "        The number of words either left or right of the word to count co-occurreances, which is (context_ize / 2)\n",
    "    :return: co-occurrence matrix\n",
    "    '''\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0 0 0 0 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 2 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 2 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 2 0 1 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [2 0 0 0 0 0 0 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 2 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -n 1 \n",
    "com1 = create_co_matrix(corpus, vocab_size, 1)\n",
    "#print(com1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'create_co_matrix.log'. \n",
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 26.9073 s\n",
      "File: <ipython-input-70-571efcd7c8c3>\n",
      "Function: create_co_matrix at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def create_co_matrix(corpus, vocab_size, window_size=1):\n",
      "     2                                               '''Generate co-occurreance matrix for the corpus.\n",
      "     3                                           \n",
      "     4                                               :param corpus:\n",
      "     5                                               :param vocab_size:The number of unique words in the corpus. \n",
      "     6                                               :param window_size: \n",
      "     7                                                   The number of words either left or right of the word to count co-occurreances, which is (context_ize / 2)\n",
      "     8                                               :return: co-occurrence matrix\n",
      "     9                                               '''\n",
      "    10         1          8.0      8.0      0.0      corpus_size = len(corpus)\n",
      "    11         1        404.0    404.0      0.0      co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
      "    12                                           \n",
      "    13    929590    1251801.0      1.3      4.7      for idx, word_id in enumerate(corpus):\n",
      "    14   1859178    2860523.0      1.5     10.6          for i in range(1, window_size + 1):\n",
      "    15    929589     923639.0      1.0      3.4              left_idx = idx - i\n",
      "    16    929589     886373.0      1.0      3.3              right_idx = idx + i\n",
      "    17                                           \n",
      "    18    929589     874136.0      0.9      3.2              if left_idx >= 0:\n",
      "    19    929588    1320196.0      1.4      4.9                  left_word_id = corpus[left_idx]\n",
      "    20    929588    8553651.0      9.2     31.8                  co_matrix[word_id, left_word_id] += 1\n",
      "    21                                           \n",
      "    22    929589    1008843.0      1.1      3.7              if right_idx < corpus_size:\n",
      "    23    929588    1273044.0      1.4      4.7                  right_word_id = corpus[right_idx]\n",
      "    24    929588    7954640.0      8.6     29.6                  co_matrix[word_id, right_word_id] += 1\n",
      "    25                                           \n",
      "    26         1          1.0      1.0      0.0      return co_matrix\n"
     ]
    }
   ],
   "source": [
    "%lprun -T create_co_matrix.log -f create_co_matrix create_co_matrix(corpus, vocab_size, 1)\n",
    "print(open('create_co_matrix.log', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy vectorized way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(corpus, index, stride, flag=False):\n",
    "    if not flag:\n",
    "        return\n",
    "    \n",
    "    n = len(corpus)\n",
    "    print(\"word is {} and context is {}\".format(\n",
    "        id_to_word[corpus[index]],\n",
    "        [ id_to_word[i] for i in corpus[max(0, (index-stride)) : min((index+stride) +1, n)]]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cooccurrence_matrix_01(sequence, vocabrary_size, context_size=3):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        sequence: word index sequence of the original corpus text\n",
    "        vocabrary_size: number of words in the vocabrary (same with co-occurrence vector size)\n",
    "        context_size: context (N-gram size N) within to check co-occurrences.\n",
    "    Returns:\n",
    "        co_occurrence matrix\n",
    "    \"\"\"\n",
    "    n = sequence_size = len(sequence)\n",
    "    co_occurrence_matrix = np.zeros((vocabrary_size, vocabrary_size), dtype=np.int32)\n",
    "\n",
    "    stride = int((context_size - 1)/2 )\n",
    "    assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
    "        n, stride\n",
    "    )\n",
    "\n",
    "    # Handle position=slice(0 : (stride-1) +1),       co-occurrences=slice(max(0, position-stride): min((position+stride),n-1) +1)\n",
    "    # Handle position=slice((n-1-stride) : (n-1) +1), co-occurrences=slice(max(0, position-stride): min((position+stride),n-1) +1)\n",
    "    indices = [*range(0, (stride-1) +1), *range((n-1)-stride +1, (n-1) +1)]\n",
    "    #print(indices)\n",
    "    \n",
    "    for position in indices:\n",
    "        debug(sequence, position, stride, False)\n",
    "        co_occurrence_matrix[\n",
    "            sequence[position],                                                # position to the word\n",
    "            sequence[max(0, position-stride) : min((position+stride),n-1) +1]  # indices to co-occurence words \n",
    "        ] += 1\n",
    "\n",
    "    \n",
    "    # Handle position=slice(stride, ((sequence_size-1) - stride) +1)\n",
    "    for position in range(stride, (sequence_size-1) - stride + 1):        \n",
    "        co_occurrence_matrix[\n",
    "            sequence[position],                                    # position to the word\n",
    "            sequence[(position-stride) : (position + stride + 1)]  # indices to co-occurence words \n",
    "        ] += 1\n",
    "    \n",
    "    np.fill_diagonal(co_occurrence_matrix, 0)\n",
    "    \n",
    "    return co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit -n 1 \n",
    "#com2 = create_cooccurrence_matrix_01(corpus, vocab_size, 3)\n",
    "#print(com2)\n",
    "#assert (com1==com2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%lprun -T create_cooccurrence_matrix_01.log -f create_cooccurrence_matrix_01 create_cooccurrence_matrix_01(corpus, vocab_size, 3)\n",
    "#print(open('create_cooccurrence_matrix_01.log', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cooccurrence_matrix_03(corpus, vector_size, context_size=3):\n",
    "    \"\"\"\n",
    "    Create a co-occurrence matrix for the copus with two paths.\n",
    "        1. Handle co-occurrences to the right of the word in the context.\n",
    "        2. Handle co-occurrences to the left  of the word in the context.\n",
    "    Args: \n",
    "        copus: Original text as word indices\n",
    "        vector_size: Co-occurrence vector size (including the word itself)\n",
    "        context_size: Context (N-gram) size N\n",
    "        \n",
    "    \"\"\"\n",
    "    corpus_size = len(corpus)\n",
    "    co_ccurrence_matrix = np.zeros((vector_size, vector_size), dtype=np.int32)\n",
    "\n",
    "    stride = int((context_size - 1)/2 )\n",
    "\n",
    "    # Handle right stride\n",
    "    for index in range(0, (corpus_size-1) - stride +1):\n",
    "        debug(corpus, index, stride)\n",
    "        co_ccurrence_matrix[\n",
    "            corpus[index],                       # index to the word\n",
    "            corpus[index+1 : (index+stride) +1]  # indices to right co-occurance words excluding word itself\n",
    "        ] += 1\n",
    "        \n",
    "    # Handle left stride\n",
    "    for index in range(stride, (corpus_size-1) +1):\n",
    "        debug(corpus, index, stride) \n",
    "        co_ccurrence_matrix[\n",
    "            corpus[index],                      # index to the word\n",
    "            corpus[(index-stride) : index]      # indices to left co-occurance words excluding word itself\n",
    "        ] += 1\n",
    "\n",
    "    return co_ccurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit -n 1 \n",
    "com3 = create_cooccurrence_matrix_03(corpus, vocab_size, 3)\n",
    "#print(com3)\n",
    "assert (com1==com3).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'create_cooccurrence_matrix_03.log'. \n",
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.016627 s\n",
      "File: <ipython-input-15-2561e2b057b6>\n",
      "Function: create_cooccurrence_matrix_03 at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def create_cooccurrence_matrix_03(corpus, vector_size, context_size=3):\n",
      "     2                                               \"\"\"\n",
      "     3                                               Create a co-occurrence matrix for the copus with two paths.\n",
      "     4                                                   1. Handle co-occurrences to the right of the word in the context.\n",
      "     5                                                   2. Handle co-occurrences to the left  of the word in the context.\n",
      "     6                                               Args: \n",
      "     7                                                   copus: Original text as word indices\n",
      "     8                                                   vector_size: Co-occurrence vector size (including the word itself)\n",
      "     9                                                   context_size: Context (N-gram) size N\n",
      "    10                                                   \n",
      "    11                                               \"\"\"\n",
      "    12         1          6.0      6.0      0.0      corpus_size = len(corpus)\n",
      "    13         1        113.0    113.0      0.7      co_ccurrence_matrix = np.zeros((vector_size, vector_size), dtype=np.int32)\n",
      "    14                                           \n",
      "    15         1         10.0     10.0      0.1      stride = int((context_size - 1)/2 )\n",
      "    16                                           \n",
      "    17                                               # Handle right stride\n",
      "    18        87        188.0      2.2      1.1      for index in range(0, (corpus_size-1) - stride +1):\n",
      "    19        86        529.0      6.2      3.2          debug(corpus, index, stride)\n",
      "    20       258       5708.0     22.1     34.3          co_ccurrence_matrix[\n",
      "    21       172        372.0      2.2      2.2              corpus[index],                       # index to the word\n",
      "    22        86        291.0      3.4      1.8              corpus[index+1 : (index+stride) +1]  # indices to right co-occurance words excluding word itself\n",
      "    23        86        159.0      1.8      1.0          ] += 1\n",
      "    24                                                   \n",
      "    25                                               # Handle left stride\n",
      "    26        87        214.0      2.5      1.3      for index in range(stride, (corpus_size-1) +1):\n",
      "    27        86        574.0      6.7      3.5          debug(corpus, index, stride) \n",
      "    28       258       4785.0     18.5     28.8          co_ccurrence_matrix[\n",
      "    29       172       3047.0     17.7     18.3              corpus[index],                      # index to the word\n",
      "    30        86        454.0      5.3      2.7              corpus[(index-stride) : index]      # indices to left co-occurance words excluding word itself\n",
      "    31        86        175.0      2.0      1.1          ] += 1\n",
      "    32                                           \n",
      "    33         1          2.0      2.0      0.0      return co_ccurrence_matrix\n"
     ]
    }
   ],
   "source": [
    "%lprun -T create_cooccurrence_matrix_03.log -f create_cooccurrence_matrix_03 create_cooccurrence_matrix_03(corpus, vocab_size, 3)\n",
    "print(open('create_cooccurrence_matrix_03.log', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cooccurrence_matrix(sequence, vocabrary_size, context_size=3):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        sequence: word index sequence of the original corpus text\n",
    "        vocabrary_size: number of words in the vocabrary (same with co-occurrence vector size)\n",
    "        context_size: context (N-gram size N) within to check co-occurrences.\n",
    "    Returns:\n",
    "        co_occurrence matrix\n",
    "    \"\"\"\n",
    "    n = sequence_size = len(sequence)\n",
    "    co_occurrence_matrix = np.zeros((vocabrary_size, vocabrary_size), dtype=np.int32)\n",
    "\n",
    "    stride = int((context_size - 1)/2 )\n",
    "    assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
    "        n, stride\n",
    "    )\n",
    "    for position in range(0, n):        \n",
    "        co_occurrence_matrix[\n",
    "            sequence[position],                                                # position  to the word\n",
    "            sequence[max(0, position-stride) : min((position+stride),n-1) +1]  # positions to co-occurence words \n",
    "        ] += 1\n",
    "\n",
    "    np.fill_diagonal(co_occurrence_matrix, 0)\n",
    "    \n",
    "    return co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 ... 0 0 0]\n",
      " [1 0 2 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -n 1 \n",
    "com5 = create_cooccurrence_matrix(corpus, vocab_size, 3)\n",
    "print(com5)\n",
    "assert (com1==com5).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attemp 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def xslice(x, slices):\n",
    "    \"\"\"Extract slices from array-like\n",
    "    Args:\n",
    "        x: array-like\n",
    "        slices: slice or tuple of slice objects\n",
    "    \"\"\"\n",
    "    if isinstance(slices, tuple):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            return np.concatenate([x[_slice] for _slice in slices])\n",
    "        else:\n",
    "            return sum((x[s] if isinstance(s, slice) else [x[s]] for s in slices), [])        \n",
    "    elif isinstance(slices, slice):\n",
    "        return x[slices]\n",
    "    else:\n",
    "        return [x[slices]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cooccurrence_matrix(sequence, vocabrary_size, context_size=3):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        sequence: word index sequence of the original corpus text\n",
    "        vocabrary_size: number of words in the vocabrary (same with co-occurrence vector size)\n",
    "        context_size: context (N-gram size N) within to check co-occurrences.\n",
    "    Returns:\n",
    "        co_occurrence matrix\n",
    "    \"\"\"\n",
    "    n = sequence_size = len(sequence)\n",
    "    co_occurrence_matrix = np.zeros((vocabrary_size, vocabrary_size), dtype=np.int32)\n",
    "\n",
    "    stride = int((context_size - 1)/2 )\n",
    "    assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
    "        n, stride\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Bug: This does not work as not considering. the duplicates.\n",
    "    # e.g. stride=2\n",
    "    # |W|W|W|W|W| If co-occurences are all same word W at the position, need +4 for W\n",
    "    # |X|X|W|X|X| If co-occurances are all same word X, need +4 for X\n",
    "    # |X|X|W|Y|Y| If co-occurances X x 2, Y x 2, then need +2 for X and Y respectively.\n",
    "    # ...\n",
    "    # Should go through \n",
    "    # --------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    for position in range(0, n):        \n",
    "        co_occurrence_matrix[\n",
    "            sequence[position],   # position  to the word \n",
    "            # positions to co-occurence words, the word itself can co-occurr.\n",
    "            xslice(\n",
    "                sequence,\n",
    "                np.s_[max(0, position-stride): position, position+1 : min((position+stride),n-1) +1]\n",
    "            )  \n",
    "        ] += 1\n",
    "    \"\"\"\n",
    "    for position in range(0, n):        \n",
    "        context = co_occurrence_matrix[\n",
    "            sequence[position],   # position  to the word \n",
    "            # positions to co-occurence words, the word itself can co-occurr.\n",
    "            xslice(\n",
    "                sequence,\n",
    "                np.s_[max(0, position-stride): position, position+1 : min((position+stride),n-1) +1]\n",
    "            )  \n",
    "        ]\n",
    "    \n",
    "\n",
    "    np.fill_diagonal(co_occurrence_matrix, 0)\n",
    "    return co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15 18 21 24 27 30 33 36 39 42 45 48]\n",
      "[1 2 3 1 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], dtype=int32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.arange(15, 50, 3).astype(np.int32)\n",
    "print(a)\n",
    "\n",
    "# %%timeit -n 10000 -> 7.17 µs ± 1.17 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
    "slices = np.r_[1:4, 1:3] \n",
    "print(slices)\n",
    "np.add(a[slices], [1])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare for debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cooccurrence_matrix(sequence, vocabrary_size, context_size=3):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        sequence: word index sequence of the original corpus text\n",
    "        vocabrary_size: number of words in the vocabrary (same with co-occurrence vector size)\n",
    "        context_size: context (N-gram size N) within to check co-occurrences.\n",
    "    Returns:\n",
    "        co_occurrence matrix\n",
    "    \"\"\"\n",
    "    n = sequence_size = len(sequence)\n",
    "    co_occurrence_matrix = np.zeros((vocabrary_size, vocabrary_size), dtype=np.int32)\n",
    "    test_matrix = np.zeros((vocabrary_size, vocabrary_size), dtype=np.int32)\n",
    "\n",
    "    stride = int((context_size - 1)/2 )\n",
    "    assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
    "        n, stride\n",
    "    )\n",
    "\n",
    "\n",
    "    # Handle initial right\n",
    "    for index in range(0, stride):\n",
    "        debug(corpus, index, stride)\n",
    "        test_matrix[\n",
    "            corpus[index],                       # index to the word\n",
    "            corpus[index+1 : (index+stride) +1]  # indices to right co-occurance words excluding word itself\n",
    "        ] += 1\n",
    "        \n",
    "    for position in range(0, stride):        \n",
    "        co_occurrence_matrix[\n",
    "            sequence[position],   # position  to the word \n",
    "            # positions to co-occurence words, the word itself can co-occurr.\n",
    "            xslice(\n",
    "                sequence,\n",
    "                np.s_[max(0, position-stride): position, position+1 : min((position+stride),n-1) +1]\n",
    "            )  \n",
    "        ] += 1\n",
    "\n",
    "    # Handle right stride\n",
    "    for index in range(stride, (n-1) - stride +1):\n",
    "        test_matrix[\n",
    "            sequence[index],                       # index to the word\n",
    "            sequence[index+1 : (index+stride) +1]  # indices to right co-occurance words excluding word itself\n",
    "        ] += 1\n",
    "\n",
    "        test_matrix[\n",
    "            sequence[index],                      # index to the word\n",
    "            sequence[(index-stride) : index]      # indices to left co-occurance words excluding word itself\n",
    "        ] += 1\n",
    "        \n",
    "        co_occurrence_matrix[\n",
    "            sequence[index],   # position  to the word \n",
    "            sequence[max(0, index-stride) : min((index+stride),n-1) +1]  # positions to co-occurence words \n",
    "        ] \n",
    "        np.fill_diagonal(co_occurrence_matrix, 0)\n",
    "            \n",
    "        if(np.array_equal(test_matrix, co_occurrence_matrix)) is not True:\n",
    "            print(\"index is {} test index {} matrix {}\\n\".format(\n",
    "                index, \n",
    "                [\n",
    "                    sequence[index],                       # index to the word\n",
    "                    sequence[(index-stride) : (index+stride) +1]  # indices to right co-occurance words excluding word itself\n",
    "                ],\n",
    "                test_matrix[\n",
    "                    [\n",
    "                        sequence[index],                       # index to the word\n",
    "                        sequence[(index-stride) : (index+stride) +1]  # indices to right co-occurance words excluding word itself\n",
    "                    ]\n",
    "                ]\n",
    "            ))\n",
    "            print(\"co_occurrence_matrix index is {} matrix is {}\\n\".format(\n",
    "                [\n",
    "                    sequence[index],   # position  to the word \n",
    "                    sequence[max(0, index-stride) : min((index+stride),n-1) +1]  # positions to co-occurence words \n",
    "                ],\n",
    "                co_occurrence_matrix[          \n",
    "                    [\n",
    "                        sequence[index],   # position  to the word \n",
    "                        sequence[max(0, index-stride) : min((index+stride),n-1) +1]  # positions to co-occurence words \n",
    "                        # positions to co-occurence words, the word itself can co-occurr.\n",
    "#                        xslice(\n",
    "#                            sequence,\n",
    "#                            np.s_[max(0, index-stride): index, index+1 : min((index+stride),n-1) +1]\n",
    "#                        )  \n",
    "                    ]\n",
    "                ]\n",
    "            ))\n",
    "            debug(sequence, index, stride, True) \n",
    "            print(\"diff \\n{}\".format(test_matrix - co_occurrence_matrix))\n",
    "#            print(\"co_occurrence_matrix \\n{}\".format(co_occurrence_matrix))\n",
    "\n",
    "            assert False\n",
    "\n",
    "    # Handle left stride\n",
    "    for index in range((n-1) - stride, (n-1) +1):\n",
    "        debug(sequence, index, stride) \n",
    "        test_matrix[\n",
    "            sequence[index],                      # index to the word\n",
    "            sequence[(index-stride) : index]      # indices to left co-occurance words excluding word itself\n",
    "        ] += 1\n",
    "    \n",
    "    for position in range((n-1) - stride +1, n):        \n",
    "        co_occurrence_matrix[\n",
    "            sequence[index],   # position  to the word \n",
    "            sequence[max(0, index-stride) : min((index+stride),n-1) +1]  # positions to co-occurence words \n",
    "        ] += 1\n",
    "\n",
    "\n",
    "    np.fill_diagonal(co_occurrence_matrix, 0)\n",
    "    return co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-96-fd31b4199c87>:68: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  test_matrix[\n",
      "<ipython-input-96-fd31b4199c87>:80: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  co_occurrence_matrix[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index is 198 test index [26, array([35, 26, 26])] matrix [1 1 1]\n",
      "\n",
      "co_occurrence_matrix index is [26, array([35, 26, 26])] matrix is [1 0 0]\n",
      "\n",
      "word is <unk> and context is ['a', '<unk>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -n 1 \n",
    "com4 = create_cooccurrence_matrix(corpus, vocab_size, 3)\n",
    "#print(com4)\n",
    "assert np.array_equal(com1, com4)\n",
    "print(com1-com4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'create_cooccurrence_matrix.log'. \n",
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.009004 s\n",
      "File: <ipython-input-21-c1947eddb03c>\n",
      "Function: create_cooccurrence_matrix at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def create_cooccurrence_matrix(sequence, vocabrary_size, context_size=3):\n",
      "     2                                               \"\"\"\n",
      "     3                                               Args: \n",
      "     4                                                   sequence: word index sequence of the original corpus text\n",
      "     5                                                   vocabrary_size: number of words in the vocabrary (same with co-occurrence vector size)\n",
      "     6                                                   context_size: context (N-gram size N) within to check co-occurrences.\n",
      "     7                                               Returns:\n",
      "     8                                                   co_occurrence matrix\n",
      "     9                                               \"\"\"\n",
      "    10         1          6.0      6.0      0.1      n = sequence_size = len(sequence)\n",
      "    11         1         84.0     84.0      0.9      co_occurrence_matrix = np.zeros((vocabrary_size, vocabrary_size), dtype=np.int32)\n",
      "    12                                           \n",
      "    13         1          6.0      6.0      0.1      stride = int((context_size - 1)/2 )\n",
      "    14         1          1.0      1.0      0.0      assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
      "    15                                                   n, stride\n",
      "    16                                               )\n",
      "    17                                               \n",
      "    18        88        119.0      1.4      1.3      for position in range(0, n):        \n",
      "    19                                                   # Must be view indexing to update the original, not a copy.\n",
      "    20       261       3440.0     13.2     38.2          co_occurrence_matrix[\n",
      "    21       174        242.0      1.4      2.7              sequence[position],   # position  to the word \n",
      "    22                                                       # positions to co-occurence words, the word itself can co-occurr.\n",
      "    23       174       4433.0     25.5     49.2              xslice(\n",
      "    24        87         67.0      0.8      0.7                  sequence,\n",
      "    25        87        505.0      5.8      5.6                  np.s_[max(0, position-stride): position, position+1 : min((position+stride),n-1) +1]\n",
      "    26                                                       )  \n",
      "    27        87         80.0      0.9      0.9          ] += 1\n",
      "    28                                           \n",
      "    29         1         20.0     20.0      0.2      np.fill_diagonal(co_occurrence_matrix, 0)\n",
      "    30         1          1.0      1.0      0.0      return co_occurrence_matrix\n"
     ]
    }
   ],
   "source": [
    "%lprun -T create_cooccurrence_matrix.log -f create_cooccurrence_matrix create_cooccurrence_matrix(corpus, vocab_size, 3)\n",
    "print(open('create_cooccurrence_matrix.log', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = com1 - com4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
