{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=80) \n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRIDE size?2\n",
      "USE_PTB? Just enter for False or any string for Trueyes\n"
     ]
    }
   ],
   "source": [
    "NIL = \"<nil>\"  # Lower letter as lower() will be applied.\n",
    "STRIDE = int(input(\"STRIDE size?\"))\n",
    "CONTEXT_SIZE = 1 + (STRIDE * 2)\n",
    "\n",
    "SPACE = ' '\n",
    "\n",
    "USE_PTB = bool(input(\"USE_PTB? Just enter for False or any string for True\"))\n",
    "USE_NATIVE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "VALIDATION = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = \"The fool doth think he is wise, but the wise man knows himself to be a fool.\"\n",
    "#corpus = \"To be, or not to be, that is the question\"\n",
    "#corpus = \"To to be be, or not not not not not to be, that is that the question that matters\"\n",
    "corpus = \"To be, or not to be, that is the question that matters\"\n",
    "\n",
    "#corpus = \"I know how to build an attention in neural networks. But I don’t understand how attention layers learn the weights that pay attention to some specific embedding. I have this question because I’m tackling a NLP task using attention layer. I believe it should be very easy to learn (the most important part is to learn alignments). However, my neural networks only achieve 50% test set accuracy. And the attention matrix is weird. I don’t know how to improve my networks.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTB (Penn Treebank) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oonisim/dataset\n"
     ]
    }
   ],
   "source": [
    "#coding: utf-8\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "try:\n",
    "    import urllib.request\n",
    "except ImportError:\n",
    "    raise ImportError('Use Python3!')\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "url_base = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/'\n",
    "key_file = {\n",
    "    'train':'ptb.train.txt',\n",
    "    'test':'ptb.test.txt',\n",
    "    'valid':'ptb.valid.txt'\n",
    "}\n",
    "save_file = {\n",
    "    'train':'ptb.train.npy',\n",
    "    'test':'ptb.test.npy',\n",
    "    'valid':'ptb.valid.npy'\n",
    "}\n",
    "vocab_file = 'ptb.vocab.pkl'\n",
    "\n",
    "#dataset_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "dataset_dir = os.path.dirname(os.path.abspath(\"/home/oonisim/dataset/hoge\"))\n",
    "print(dataset_dir)\n",
    "\n",
    "def _download(file_name):\n",
    "    file_path = dataset_dir + '/' + file_name\n",
    "    if os.path.exists(file_path):\n",
    "        return\n",
    "\n",
    "    print('Downloading ' + file_name + ' ... ')\n",
    "\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "    except urllib.error.URLError:\n",
    "        import ssl\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "\n",
    "    print('Done')\n",
    "\n",
    "\n",
    "def load_text(data_type):\n",
    "#    data_type = 'train'\n",
    "    file_name = key_file[data_type]\n",
    "    file_path = dataset_dir + '/' + file_name\n",
    "    vocab_path = dataset_dir + '/' + vocab_file\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        _download(file_name)\n",
    "\n",
    "    text = open(file_path).read().replace('\\n', '<eos>').strip()\n",
    "    return(text)\n",
    "    \n",
    "def load_vocab():\n",
    "    vocab_path = dataset_dir + '/' + vocab_file\n",
    "\n",
    "    if os.path.exists(vocab_path):\n",
    "        with open(vocab_path, 'rb') as f:\n",
    "            word_to_id, id_to_word = pickle.load(f)\n",
    "        return word_to_id, id_to_word\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    data_type = 'train'\n",
    "    file_name = key_file[data_type]\n",
    "    file_path = dataset_dir + '/' + file_name\n",
    "\n",
    "    _download(file_name)\n",
    "\n",
    "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in word_to_id:\n",
    "            tmp_id = len(word_to_id)\n",
    "            word_to_id[word] = tmp_id\n",
    "            id_to_word[tmp_id] = word\n",
    "\n",
    "    with open(vocab_path, 'wb') as f:\n",
    "        pickle.dump((word_to_id, id_to_word), f)\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "\n",
    "def load_data(data_type='train'):\n",
    "    '''\n",
    "        :param data_type: データの種類：'train' or 'test' or 'valid (val)'\n",
    "        :return:\n",
    "    '''\n",
    "    if data_type == 'val': data_type = 'valid'\n",
    "    save_path = dataset_dir + '/' + save_file[data_type]\n",
    "\n",
    "    word_to_id, id_to_word = load_vocab()\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        sequence = np.load(save_path)\n",
    "        return sequence, word_to_id, id_to_word\n",
    "\n",
    "    file_name = key_file[data_type]\n",
    "    file_path = dataset_dir + '/' + file_name\n",
    "    _download(file_name)\n",
    "\n",
    "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
    "    sequence = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    np.save(save_path, sequence)\n",
    "    return sequence, word_to_id, id_to_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid boundary checking when iterate through the sequenced corpus, pad the source text with '<nil>'.\n",
    "e.g. (when context is of size 5):    \n",
    "From:\n",
    "```\n",
    "|B|X|Y|Z|...|P|Q|R|E|\n",
    "```\n",
    "\n",
    "To:\n",
    "```\n",
    "|<nil>|<nil>|B|X|Y|Z|...|P|Q|R|E|<nil>|<nil>| \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELIMITER = \" \"\n",
    "def pad(corpus:str) -> str:\n",
    "    \"\"\"Prepand and appeend STRIDE times of the NIL word to the corpus\"\"\"\n",
    "    assert corpus and len(corpus) > 0 and isinstance(corpus, str)\n",
    "    \n",
    "    padded = DELIMITER.join(\n",
    "        [ NIL ] * STRIDE + [ corpus ] + [ NIL ] * STRIDE\n",
    "    )\n",
    "    \"\"\"\n",
    "    padded = sum(\n",
    "        [ \n",
    "            [ NIL ] * STRIDE, \n",
    "            corpus.split(' '),\n",
    "            [ NIL ] * STRIDE\n",
    "        ],\n",
    "        start=[]\n",
    "    )\n",
    "    \"\"\"\n",
    "    return padded\n",
    "\n",
    "#print(\"[{}]\".format(pad(\"tako ika bin\")))\n",
    "assert pad(\"tako ika bin\") == NIL + DELIMITER + NIL + DELIMITER + \"tako ika bin \" + NIL + DELIMITER + NIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### co-occurrence check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurrence_words(co_occurrence_matrix, word, word_to_id, id_to_word):\n",
    "    \"\"\"Provide the co-occurred words for the word\"\"\"\n",
    "    return [(id_to_word[i], count) for i, count in enumerate(co_occurrence_matrix[word_to_id[word]])]\n",
    "\n",
    "def word_frequency(co_occurrence_matrix, word, word_to_id):\n",
    "    \"\"\"Number of times when the word occurred in the sequene\"\"\"\n",
    "    # Each time the word occurrs in the sequence, it will see (CONTEXT_SIZE -1) words. \n",
    "    co_occurrence_matrix[\n",
    "        word_to_id[word]\n",
    "    ].sum() / (CONTEXT_SIZE -1)\n",
    "    \n",
    "def total_frequencies(co_occurrence_matrix, word_to_id):\n",
    "    \"\"\"Sum of all word occurrence except NIL (same with vocabrary size excluding NIL)\"\"\"\n",
    "    return (co_occurrence_matrix.sum() - co_occurrence_matrix[word_to_id[NIL]].sum()) / (CONTEXT_SIZE -1)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract gapped slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def xslice(x, slices):\n",
    "    \"\"\"Extract multiple slices from an array-like and concatenate them.\n",
    "    Args:\n",
    "        x: array-like\n",
    "        slices: slice or tuple of slice objects\n",
    "    Return:\n",
    "        Combined slices\n",
    "    \"\"\"\n",
    "    if isinstance(slices, tuple):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            return np.concatenate([x[_slice] for _slice in slices])\n",
    "        else:\n",
    "            return sum((x[s] if isinstance(s, slice) else [x[s]] for s in slices), [])        \n",
    "    elif isinstance(slices, slice):\n",
    "        return x[slices]\n",
    "    else:\n",
    "        return [x[slices]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word indexing\n",
    "Assign a numerical id to each word.\n",
    "\n",
    "The row index of co-occurrence matrix is a word index. The number of words in the corpus can be less than the number of word indices because additional meta-word such as OOV, UNK, NIL can be added to the original corpus.\n",
    "\n",
    "Make sure **the co-occurrence matrix row index matches with the word index**, unless explicitly adjust when row-index and word-index do not match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the corpus text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PTB:\n",
    "    corpus = pad(load_text('train'))\n",
    "else:\n",
    "    print(\"Original corpus: \\n[{}]\".format(corpus))\n",
    "    corpus = pad(corpus)\n",
    "    print(\"Padded corpus: \\n[{}]\".format(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native word indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def native_word_indexing(corpus):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        corpus: A string including sentences to process.\n",
    "    Returns:\n",
    "        sequence: \n",
    "            A numpy array of word indices to every word in the originlal corpus as as they appear in it.\n",
    "            The objective of sequence is to preserve the original corpus but as numerical indices.\n",
    "        word_to_id: A dictionary to map a word to a word index\n",
    "        id_to_word: A dictionary to map a word index to a word\n",
    "        vocabulary_size: Number of words identiifed in the corpus\n",
    "    \"\"\"\n",
    "    words = re.compile('[\\s\\t]+').split(corpus)\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    \n",
    "    min_id = len(word_to_id)\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "            \n",
    "    # Word index starts with 0. Total words = max(word index) + 1\n",
    "    vocabulary_size = new_id + 1\n",
    "    assert vocabulary_size == (max(word_to_id.values()) + 1)\n",
    "\n",
    "    sequence = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return sequence, word_to_id, id_to_word, vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = re.sub('[.,:;]+', SPACE, corpus.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9927\n"
     ]
    }
   ],
   "source": [
    "if USE_NATIVE:\n",
    "    (sequence, word_to_id, id_to_word, vocabulary_size) = native_word_indexing(corpus)\n",
    "\n",
    "print(vocabulary_size)\n",
    "if not USE_PTB:\n",
    "    print(word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Tokenizer indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "\n",
    "USE_TENSORFLOW = (not USE_NATIVE)\n",
    "if USE_TENSORFLOW:\n",
    "    # Each text in \"texts\" is a complete document as one string, \n",
    "    # e.g \"To be or not to be, that is the question.\"\n",
    "    texts = [ corpus ]   \n",
    "\n",
    "    # fit_on_texts() processes multiple documents and handles all words in all the documents.\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    word_to_id = tokenizer.word_index\n",
    "\n",
    "    # texts_to_sequences() ruturns sequences, one sequence for each text in \"texts\".\n",
    "    sequences = (tokenizer.texts_to_sequences(texts))\n",
    "    sequence = sequences[0]\n",
    "\n",
    "    print(len(sequences))\n",
    "    print(len(word_to_id))\n",
    "    \n",
    "    # Index of tokenizer.word_index starts at 1, NOT 0.\n",
    "    # e.g. {'<OOV>': 1, 'the': 2, 'fool': 3, 'wise': 4, 'doth': 5, ...}\n",
    "    vocabulary_size = max(word_to_id.values()) + 1\n",
    "    print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_PTB:\n",
    "    print(\"word to id \\n{}\".format(word_to_id))\n",
    "    print(\"id to word \\n{}\".format(id_to_word))\n",
    "    print()\n",
    "    print(\"corpus is \\n[{}]\".format(corpus))\n",
    "    print(\"sequence is \\n{}\".format(sequence))\n",
    "    print(\"corpus size is {} sequence size is {} expected sum is {}\".format(\n",
    "        len(re.compile('[\\t\\s]+').split(corpus)), \n",
    "        len(sequence), \n",
    "        (len(sequence) - (2*STRIDE)) * (2*STRIDE)  # Exclude NIL from the sequence\n",
    "    ))\n",
    "    #print([id_to_word[index] for index in sequence])\n",
    "    print(np.array([id_to_word[index] for index in sequence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-occurrence vector(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLFS2 iterative approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlfs2_create_co_matrix(sequence, vocabulary_size, context_size=3):\n",
    "    '''Generate co-occurreance matrix for the sequence.\n",
    "    :param sequence: word index sequence of the sequence\n",
    "    :param vocabulary_size:The number of unique words in the sequence. \n",
    "    :param window_size: \n",
    "        The number of words either left or right of the word to count co-occurreances, which is (context_ize / 2)\n",
    "    :return: co-occurrence matrix\n",
    "    '''\n",
    "    assert (context_size % 2) == 1\n",
    "    \n",
    "    n = sequence_size = len(sequence)\n",
    "    co_matrix = np.zeros((vocabulary_size, vocabulary_size), dtype=np.int32)\n",
    "\n",
    "    window_size = int((context_size -1) / 2)\n",
    "    for idx, word_id in enumerate(sequence):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = sequence[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < sequence_size:\n",
    "                right_word_id = sequence[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "                \n",
    "    # Zero clear the co-occurrence words of NIL because NIL should not see other words.\n",
    "    co_matrix[\n",
    "        word_to_id[NIL.lower()]\n",
    "    ] = 0\n",
    "\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9927, 9927)\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -n 1 \n",
    "if VALIDATION:\n",
    "    com0 = dlfs2_create_co_matrix(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "    print(com0.shape)\n",
    "    \n",
    "    if not USE_PTB:\n",
    "        print(com0)\n",
    "        print(\"com0.sum() {}\".format(com0.sum()))\n",
    "\n",
    "    # Total sum of all word occurrences except NIL must matches with the original corpus size.\n",
    "    assert total_frequencies(com0, word_to_id) == len(sequence) - (CONTEXT_SIZE -1)\n",
    "    assert com0.sum() == (len(sequence) - (2*STRIDE)) * (2*STRIDE)  # Exclude NIL from the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(id_to_word[4])\n",
    "#cooccurrence_words(com0, id_to_word[4], word_to_id, id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'dlfs2_create_co_matrix.log'. \n",
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 32.2272 s\n",
      "File: <ipython-input-15-fc0829b4013d>\n",
      "Function: dlfs2_create_co_matrix at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def dlfs2_create_co_matrix(sequence, vocabulary_size, context_size=3):\n",
      "     2                                               '''Generate co-occurreance matrix for the sequence.\n",
      "     3                                               :param sequence: word index sequence of the sequence\n",
      "     4                                               :param vocabulary_size:The number of unique words in the sequence. \n",
      "     5                                               :param window_size: \n",
      "     6                                                   The number of words either left or right of the word to count co-occurreances, which is (context_ize / 2)\n",
      "     7                                               :return: co-occurrence matrix\n",
      "     8                                               '''\n",
      "     9         1          3.0      3.0      0.0      assert (context_size % 2) == 1\n",
      "    10                                               \n",
      "    11         1          3.0      3.0      0.0      n = sequence_size = len(sequence)\n",
      "    12         1         76.0     76.0      0.0      co_matrix = np.zeros((vocabulary_size, vocabulary_size), dtype=np.int32)\n",
      "    13                                           \n",
      "    14         1          4.0      4.0      0.0      window_size = int((context_size -1) / 2)\n",
      "    15    932236     926152.0      1.0      2.9      for idx, word_id in enumerate(sequence):\n",
      "    16   2796705    2659703.0      1.0      8.3          for i in range(1, window_size + 1):\n",
      "    17   1864470    1411500.0      0.8      4.4              left_idx = idx - i\n",
      "    18   1864470    1328415.0      0.7      4.1              right_idx = idx + i\n",
      "    19                                           \n",
      "    20   1864470    1332756.0      0.7      4.1              if left_idx >= 0:\n",
      "    21   1864467    1801136.0      1.0      5.6                  left_word_id = sequence[left_idx]\n",
      "    22   1864467    9945764.0      5.3     30.9                  co_matrix[word_id, left_word_id] += 1\n",
      "    23                                           \n",
      "    24   1864470    1506428.0      0.8      4.7              if right_idx < sequence_size:\n",
      "    25   1864467    1789225.0      1.0      5.6                  right_word_id = sequence[right_idx]\n",
      "    26   1864467    9526028.0      5.1     29.6                  co_matrix[word_id, right_word_id] += 1\n",
      "    27                                                           \n",
      "    28                                               # Zero clear the co-occurrence words of NIL because NIL should not see other words.\n",
      "    29         2         31.0     15.5      0.0      co_matrix[\n",
      "    30         1          6.0      6.0      0.0          word_to_id[NIL.lower()]\n",
      "    31         1          1.0      1.0      0.0      ] = 0\n",
      "    32                                           \n",
      "    33         1          1.0      1.0      0.0      return co_matrix\n"
     ]
    }
   ],
   "source": [
    "%lprun \\\n",
    "    -T dlfs2_create_co_matrix.log \\\n",
    "    -f dlfs2_create_co_matrix \\\n",
    "    dlfs2_create_co_matrix(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "    \n",
    "print(open('dlfs2_create_co_matrix.log', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved version\n",
    "* With the NIL padded sequence, no need for the boundary checks e.g. left_idx >= 0.\n",
    "* By limiting the position range to ```sequence[stride : ((n-1)-stride) +1]```, no need to zero-clear the co-occurrence words of NIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(sequence, vocabulary_size, context_size=3):\n",
    "    '''Generate co-occurreance matrix for the sequence.\n",
    "    :param sequence: word index sequence of the sequence\n",
    "    :param vocabulary_size:The number of unique words in the sequence. \n",
    "    :param stride: \n",
    "        The number of words either left or right of the word to count co-occurreances, which is (context_ize / 2)\n",
    "    :return: co-occurrence matrix\n",
    "    '''\n",
    "    assert (context_size % 2) == 1\n",
    "    \n",
    "    n = sequence_size = len(sequence)\n",
    "    co_matrix = np.zeros((vocabulary_size, vocabulary_size), dtype=np.int32)\n",
    "\n",
    "    stride = int((context_size -1) / 2)\n",
    "    for position, word_id in enumerate(sequence[stride : ((n-1)-stride) +1], stride):\n",
    "        for i in range(1, stride + 1):\n",
    "            left_idx =position - i\n",
    "            right_idx =position + i\n",
    "\n",
    "            left_word_id = sequence[left_idx]\n",
    "            co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            right_word_id = sequence[right_idx]\n",
    "            co_matrix[word_id, right_word_id] += 1\n",
    "                \n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9927, 9927)\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -n 1 \n",
    "if VALIDATION:\n",
    "    com1 = create_co_matrix(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "    print(com1.shape)\n",
    "\n",
    "    if not USE_PTB:\n",
    "        print(com1)\n",
    "        print(\"com1.sum() {}\".format(com1.sum()))\n",
    "\n",
    "    # Total sum of all word occurrences except NIL must matches with the original corpus size.\n",
    "    assert total_frequencies(com1, word_to_id) == len(sequence) - (CONTEXT_SIZE -1)\n",
    "    assert np.array_equal(com1, com0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'create_co_matrix.log'. \n",
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 28.6727 s\n",
      "File: <ipython-input-20-ea50491739c4>\n",
      "Function: create_co_matrix at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def create_co_matrix(sequence, vocabulary_size, context_size=3):\n",
      "     2                                               '''Generate co-occurreance matrix for the sequence.\n",
      "     3                                               :param sequence: word index sequence of the sequence\n",
      "     4                                               :param vocabulary_size:The number of unique words in the sequence. \n",
      "     5                                               :param stride: \n",
      "     6                                                   The number of words either left or right of the word to count co-occurreances, which is (context_ize / 2)\n",
      "     7                                               :return: co-occurrence matrix\n",
      "     8                                               '''\n",
      "     9         1          2.0      2.0      0.0      assert (context_size % 2) == 1\n",
      "    10                                               \n",
      "    11         1          3.0      3.0      0.0      n = sequence_size = len(sequence)\n",
      "    12         1        633.0    633.0      0.0      co_matrix = np.zeros((vocabulary_size, vocabulary_size), dtype=np.int32)\n",
      "    13                                           \n",
      "    14         1          9.0      9.0      0.0      stride = int((context_size -1) / 2)\n",
      "    15    932232     883857.0      0.9      3.1      for position, word_id in enumerate(sequence[stride : ((n-1)-stride) +1], stride):\n",
      "    16   2796693    2500380.0      0.9      8.7          for i in range(1, stride + 1):\n",
      "    17   1864462    1319956.0      0.7      4.6              left_idx =position - i\n",
      "    18   1864462    1237564.0      0.7      4.3              right_idx =position + i\n",
      "    19                                           \n",
      "    20   1864462    1667561.0      0.9      5.8              left_word_id = sequence[left_idx]\n",
      "    21   1864462    9898895.0      5.3     34.5              co_matrix[word_id, left_word_id] += 1\n",
      "    22                                           \n",
      "    23   1864462    1818053.0      1.0      6.3              right_word_id = sequence[right_idx]\n",
      "    24   1864462    9345794.0      5.0     32.6              co_matrix[word_id, right_word_id] += 1\n",
      "    25                                                           \n",
      "    26         1          2.0      2.0      0.0      return co_matrix\n"
     ]
    }
   ],
   "source": [
    "%lprun \\\n",
    "    -T create_co_matrix.log \\\n",
    "    -f create_co_matrix \\\n",
    "    create_co_matrix(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "\n",
    "print(open('create_co_matrix.log', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-impementation of the DLFS2 improved version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_create_co_matrix(sequence, co_occurrence_vector_size, context_size=3):\n",
    "    \"\"\"Implement the same logic with dlfs2 create_co_matrix.\n",
    "    Args: \n",
    "        sequence: word index sequence of the original corpus text\n",
    "        co_occurrence_vector_size: \n",
    "        context_size: context (N-gram size N) within to check co-occurrences.\n",
    "    Returns:\n",
    "        co_occurrence matrix\n",
    "    \"\"\"\n",
    "    assert int(context_size %2) == 1\n",
    "    \n",
    "    n = sequence_size = len(sequence)\n",
    "    co_occurrence_matrix = np.zeros((co_occurrence_vector_size, co_occurrence_vector_size), dtype=np.int32)\n",
    "\n",
    "    stride = int((context_size - 1)/2 )\n",
    "    assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
    "        n, stride\n",
    "    )\n",
    "\n",
    "    for position in range(stride, (n-1) - stride +1):        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Super slow spending approx 75% of execution time 35 secs\n",
    "        # Line #   Hits    Time       Per Hit   % Time \n",
    "        # 36    932231     805098.0      0.9      2.3          word_id = sequence[position]\n",
    "        # 37   2796693    2240226.0      0.8      6.4          for offset in range(1, stride+1):\n",
    "        # 38   5593386   11444886.0      2.0     32.6              co_occurrence_matrix[\n",
    "        # 39   3728924    1921460.0      0.5      5.5                  word_id,\n",
    "        # 40   1864462    1427308.0      0.8      4.1                  sequence[position - offset]\n",
    "        # 41   1864462    1022951.0      0.5      2.9              ] +=1\n",
    "        # 42   5593386   11203813.0      2.0     32.0              co_occurrence_matrix[\n",
    "        # 43   3728924    1972667.0      0.5      5.6                  word_id,\n",
    "        # 44   1864462    1473145.0      0.8      4.2                  sequence[position + offset]\n",
    "        # 45   1864462    1029893.0      0.6      2.9              ] +=1\n",
    "        # --------------------------------------------------------------------------------\n",
    "        word_id = sequence[position]\n",
    "        for offset in range(1, stride+1):\n",
    "            co_occurrence_matrix[\n",
    "                word_id,\n",
    "                sequence[position - offset]\n",
    "            ] +=1\n",
    "            co_occurrence_matrix[\n",
    "                word_id,\n",
    "                sequence[position + offset]\n",
    "            ] +=1\n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "    return co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9927, 9927)\n"
     ]
    }
   ],
   "source": [
    "if VALIDATION:\n",
    "    com2 = simulate_create_co_matrix(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "    print(com2.shape)\n",
    "\n",
    "    if not USE_PTB:\n",
    "        print(com2)\n",
    "        print(\"com2.sum() {}\".format(com2.sum()))\n",
    "\n",
    "    # Total sum of all word occurrences except NIL must matches with the original corpus size.\n",
    "    assert total_frequencies(com2, word_to_id) == len(sequence) - (CONTEXT_SIZE -1)\n",
    "    assert np.array_equal(com2, com0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'simulate_create_co_matrix.log'. \n",
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 38.3196 s\n",
      "File: <ipython-input-23-df6df2aa169a>\n",
      "Function: simulate_create_co_matrix at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def simulate_create_co_matrix(sequence, co_occurrence_vector_size, context_size=3):\n",
      "     2                                               \"\"\"Implement the same logic with dlfs2 create_co_matrix.\n",
      "     3                                               Args: \n",
      "     4                                                   sequence: word index sequence of the original corpus text\n",
      "     5                                                   co_occurrence_vector_size: \n",
      "     6                                                   context_size: context (N-gram size N) within to check co-occurrences.\n",
      "     7                                               Returns:\n",
      "     8                                                   co_occurrence matrix\n",
      "     9                                               \"\"\"\n",
      "    10         1          2.0      2.0      0.0      assert int(context_size %2) == 1\n",
      "    11                                               \n",
      "    12         1          2.0      2.0      0.0      n = sequence_size = len(sequence)\n",
      "    13         1         84.0     84.0      0.0      co_occurrence_matrix = np.zeros((co_occurrence_vector_size, co_occurrence_vector_size), dtype=np.int32)\n",
      "    14                                           \n",
      "    15         1          5.0      5.0      0.0      stride = int((context_size - 1)/2 )\n",
      "    16         1          1.0      1.0      0.0      assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
      "    17                                                   n, stride\n",
      "    18                                               )\n",
      "    19                                           \n",
      "    20    932232     608108.0      0.7      1.6      for position in range(stride, (n-1) - stride +1):        \n",
      "    21                                                   # --------------------------------------------------------------------------------\n",
      "    22                                                   # Super slow spending approx 75% of execution time 35 secs\n",
      "    23                                                   # Line #   Hits    Time       Per Hit   % Time \n",
      "    24                                                   # 36    932231     805098.0      0.9      2.3          word_id = sequence[position]\n",
      "    25                                                   # 37   2796693    2240226.0      0.8      6.4          for offset in range(1, stride+1):\n",
      "    26                                                   # 38   5593386   11444886.0      2.0     32.6              co_occurrence_matrix[\n",
      "    27                                                   # 39   3728924    1921460.0      0.5      5.5                  word_id,\n",
      "    28                                                   # 40   1864462    1427308.0      0.8      4.1                  sequence[position - offset]\n",
      "    29                                                   # 41   1864462    1022951.0      0.5      2.9              ] +=1\n",
      "    30                                                   # 42   5593386   11203813.0      2.0     32.0              co_occurrence_matrix[\n",
      "    31                                                   # 43   3728924    1972667.0      0.5      5.6                  word_id,\n",
      "    32                                                   # 44   1864462    1473145.0      0.8      4.2                  sequence[position + offset]\n",
      "    33                                                   # 45   1864462    1029893.0      0.6      2.9              ] +=1\n",
      "    34                                                   # --------------------------------------------------------------------------------\n",
      "    35    932231     860116.0      0.9      2.2          word_id = sequence[position]\n",
      "    36   2796693    2527602.0      0.9      6.6          for offset in range(1, stride+1):\n",
      "    37   5593386   12513944.0      2.2     32.7              co_occurrence_matrix[\n",
      "    38   3728924    2102095.0      0.6      5.5                  word_id,\n",
      "    39   1864462    1526376.0      0.8      4.0                  sequence[position - offset]\n",
      "    40   1864462    1172249.0      0.6      3.1              ] +=1\n",
      "    41   5593386   12140676.0      2.2     31.7              co_occurrence_matrix[\n",
      "    42   3728924    2134719.0      0.6      5.6                  word_id,\n",
      "    43   1864462    1555087.0      0.8      4.1                  sequence[position + offset]\n",
      "    44   1864462    1178495.0      0.6      3.1              ] +=1\n",
      "    45                                                   # --------------------------------------------------------------------------------\n",
      "    46                                           \n",
      "    47         1          2.0      2.0      0.0      return co_occurrence_matrix\n"
     ]
    }
   ],
   "source": [
    "%lprun \\\n",
    "    -T simulate_create_co_matrix.log \\\n",
    "    -f simulate_create_co_matrix \\\n",
    "    simulate_create_co_matrix(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "\n",
    "print(open('simulate_create_co_matrix.log', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(sequence, index, stride, flag=False):\n",
    "    if not flag:\n",
    "        return\n",
    "    \n",
    "    n = len(sequence)\n",
    "    print(\"word is {} and context is {}\".format(\n",
    "        id_to_word[sequence[index]],\n",
    "        [ id_to_word[i] for i in sequence[max(0, (index-stride)) : min((index+stride) +1, n)]]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cooccurrence_matrix(sequence, co_occurrence_vector_size, context_size=3):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        sequence: word index sequence of the original corpus text\n",
    "        co_occurrence_vector_size: \n",
    "        context_size: context (N-gram size N) within to check co-occurrences.\n",
    "    Returns:\n",
    "        co_occurrence matrix\n",
    "    \"\"\"\n",
    "    assert int(context_size %2) == 1\n",
    "    \n",
    "    n = sequence_size = len(sequence)\n",
    "    co_occurrence_matrix = np.zeros((co_occurrence_vector_size, co_occurrence_vector_size), dtype=np.int32)\n",
    "\n",
    "    stride = int((context_size - 1)/2 )\n",
    "    assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
    "        n, stride\n",
    "    )\n",
    "\n",
    "    for position in range(stride, (n-1) - stride +1):        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Consider counting a word multiple time, and the word itself for performance.\n",
    "        # e.g. stride=2\n",
    "        # |W|W|W|W|W| If co-occurrences are all same word W at the position, need +4 for W\n",
    "        # |X|X|W|X|X| If co-occurrences are all same word X, need +4 for X\n",
    "        # |X|X|W|Y|Y| If co-occurrences X x 2, Y x 2, then need +2 for X and Y respectively.\n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Line #   Hits    Time       Per Hit   % Time \n",
    "        # 32   1864462    5358433.0      2.9     61.5          np.add.at(\n",
    "        # 33    932231     446858.0      0.5      5.1             co_occurrence_matrix,\n",
    "        # 34    932231     463579.0      0.5      5.3             (\n",
    "        # 35    932231     609110.0      0.7      7.0                 sequence[position],\n",
    "        # 36    932231     862299.0      0.9      9.9                 sequence[position-stride : (position+stride) +1]    \n",
    "        # 37                                                      ),\n",
    "        # 38    932231     437542.0      0.5      5.0             1\n",
    "        # 39                                                   )\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # word_id = sequence[position]\n",
    "        np.add.at(\n",
    "           co_occurrence_matrix,\n",
    "           (\n",
    "               sequence[position],                               # word_id\n",
    "               sequence[position-stride : (position+stride) +1]  # indices to co-occurrence words \n",
    "           ),\n",
    "           1\n",
    "        )\n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Compensate the +1 self count of a word at each occurrence.\n",
    "    # F(w) (frequency/occurrences of a word in the sequence) has been extra added besides \n",
    "    # the expected (2 * stride) * F(w) times, resulting in (context_size) * F(w).\n",
    "    # --------------------------------------------------------------------------------\n",
    "    np.fill_diagonal(\n",
    "        co_occurrence_matrix,\n",
    "        (co_occurrence_matrix.diagonal() - co_occurrence_matrix.sum(axis=1) / context_size)\n",
    "    )\n",
    "\n",
    "    return co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug version for trouble shooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_cooccurrence_matrix(sequence, co_occurrence_vector_size, context_size=3):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        sequence: word position sequence of the original corpus text\n",
    "        co_occurrence_vector_size: \n",
    "        context_size: context (N-gram size N) within to check co-occurrences.\n",
    "    Returns:\n",
    "        co_occurrence matrix\n",
    "    \"\"\"\n",
    "    assert int(context_size %2) == 1\n",
    "\n",
    "    n = sequence_size = len(sequence)\n",
    "    co_occurrence_matrix = np.zeros((co_occurrence_vector_size, co_occurrence_vector_size), dtype=np.int32)\n",
    "    co_matrix = np.zeros((co_occurrence_vector_size, co_occurrence_vector_size), dtype=np.int32)\n",
    "\n",
    "    stride = int((context_size - 1)/2 )\n",
    "    assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
    "        n, stride\n",
    "    )\n",
    "\n",
    "    print(\"Starting comparison\")\n",
    "    for position in range(stride, (n-1) - stride +1):\n",
    "        print(position)\n",
    "        word_id = sequence[position]\n",
    "\n",
    "        for i in range(1, stride + 1):\n",
    "            left_idx = position - i\n",
    "            right_idx = position + i\n",
    "\n",
    "            left_word_id = sequence[left_idx]\n",
    "            co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            right_word_id = sequence[right_idx]\n",
    "            co_matrix[word_id, right_word_id] += 1\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Consider counting the word itself. \n",
    "        # e.g. stride=2\n",
    "        # |W|W|W|W|W| If co-occurrences are all same word W at the position, need +4 for W\n",
    "        # |X|X|W|X|X| If co-occurrences are all same word X, need +4 for X\n",
    "        # |X|X|W|Y|Y| If co-occurrences X x 2, Y x 2, then need +2 for X and Y respectively.\n",
    "        # --------------------------------------------------------------------------------\n",
    "        np.add.at(\n",
    "            co_occurrence_matrix,\n",
    "            (\n",
    "                word_id,\n",
    "                sequence[position-stride : (position+stride) +1]    # positions to co-occurence words \n",
    "            ),\n",
    "            1\n",
    "        )\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Remove the +1 self count of the word itself.\n",
    "        # To avoid the cost of creating a gapped indices np.r_[sequence, [i:j, j+1:k]].\n",
    "        # --------------------------------------------------------------------------------\n",
    "        co_occurrence_matrix[word_id, word_id] -=1\n",
    "\n",
    "        \n",
    "        if(np.array_equal(co_matrix, co_occurrence_matrix)) is not True:\n",
    "            print(\"sequence position is {}\".format(position))\n",
    "            print(\"Test matrix index \\n{}\\ matrix {}\\n\".format(\n",
    "                [\n",
    "                    word_id,                       # position to the word\n",
    "                    sequence[(position-stride) : (position+stride) +1]  # indices to right co-occurrence words excluding word itself\n",
    "                ],\n",
    "                co_matrix[\n",
    "                    [\n",
    "                        word_id,                       # position to the word\n",
    "                        sequence[(position-stride) : (position+stride) +1]  # indices to right co-occurrence words excluding word itself\n",
    "                    ]\n",
    "                ]\n",
    "            ))\n",
    "            print(\"co_occurrence_matrix index is \\n{}\\nmatrix is \\n{}\\n\".format(\n",
    "                [\n",
    "                    word_id,   # position  to the word \n",
    "                    sequence[position-stride : (position+stride) +1]  # positions to co-occurence words \n",
    "                ],\n",
    "                co_occurrence_matrix[          \n",
    "                    [\n",
    "                        word_id,   # position  to the word \n",
    "                        sequence[position-stride : (position+stride) +1]  # positions to co-occurence words \n",
    "                    ]\n",
    "                ]\n",
    "            ))\n",
    "            debug(sequence, position, stride, True) \n",
    "            print(\"diff \\n{}\".format(co_matrix - co_occurrence_matrix))\n",
    "\n",
    "            assert False\n",
    "        \n",
    "    return co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9927, 9927)\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -n 1 \n",
    "if DEBUG:\n",
    "    f = debug_cooccurrence_matrix\n",
    "else:\n",
    "    f = create_cooccurrence_matrix\n",
    "\n",
    "if VALIDATION:\n",
    "    com = f(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "    print(com.shape)\n",
    "\n",
    "    if not USE_PTB:\n",
    "        print(com)\n",
    "        print(\"com.sum() {}\".format(com.sum()))\n",
    "\n",
    "    assert total_frequencies(com, word_to_id) == len(sequence) - (CONTEXT_SIZE -1)\n",
    "    assert np.array_equal(com0, com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'create_cooccurrence_matrix.log'. \n",
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 11.4642 s\n",
      "File: <ipython-input-27-e09f9d94eab8>\n",
      "Function: create_cooccurrence_matrix at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def create_cooccurrence_matrix(sequence, co_occurrence_vector_size, context_size=3):\n",
      "     2                                               \"\"\"\n",
      "     3                                               Args: \n",
      "     4                                                   sequence: word index sequence of the original corpus text\n",
      "     5                                                   co_occurrence_vector_size: \n",
      "     6                                                   context_size: context (N-gram size N) within to check co-occurrences.\n",
      "     7                                               Returns:\n",
      "     8                                                   co_occurrence matrix\n",
      "     9                                               \"\"\"\n",
      "    10         1          5.0      5.0      0.0      assert int(context_size %2) == 1\n",
      "    11                                               \n",
      "    12         1          3.0      3.0      0.0      n = sequence_size = len(sequence)\n",
      "    13         1         85.0     85.0      0.0      co_occurrence_matrix = np.zeros((co_occurrence_vector_size, co_occurrence_vector_size), dtype=np.int32)\n",
      "    14                                           \n",
      "    15         1          5.0      5.0      0.0      stride = int((context_size - 1)/2 )\n",
      "    16         1          1.0      1.0      0.0      assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
      "    17                                                   n, stride\n",
      "    18                                               )\n",
      "    19                                           \n",
      "    20    932232     733468.0      0.8      6.4      for position in range(stride, (n-1) - stride +1):        \n",
      "    21                                                   # --------------------------------------------------------------------------------\n",
      "    22                                                   # Consider counting a word multiple time, and the word itself for performance.\n",
      "    23                                                   # e.g. stride=2\n",
      "    24                                                   # |W|W|W|W|W| If co-occurrences are all same word W at the position, need +4 for W\n",
      "    25                                                   # |X|X|W|X|X| If co-occurrences are all same word X, need +4 for X\n",
      "    26                                                   # |X|X|W|Y|Y| If co-occurrences X x 2, Y x 2, then need +2 for X and Y respectively.\n",
      "    27                                                   # --------------------------------------------------------------------------------\n",
      "    28                                           \n",
      "    29                                                   # --------------------------------------------------------------------------------\n",
      "    30                                                   # Line #   Hits    Time       Per Hit   % Time \n",
      "    31                                                   # 32   1864462    5358433.0      2.9     61.5          np.add.at(\n",
      "    32                                                   # 33    932231     446858.0      0.5      5.1             co_occurrence_matrix,\n",
      "    33                                                   # 34    932231     463579.0      0.5      5.3             (\n",
      "    34                                                   # 35    932231     609110.0      0.7      7.0                 sequence[position],\n",
      "    35                                                   # 36    932231     862299.0      0.9      9.9                 sequence[position-stride : (position+stride) +1]    \n",
      "    36                                                   # 37                                                      ),\n",
      "    37                                                   # 38    932231     437542.0      0.5      5.0             1\n",
      "    38                                                   # 39                                                   )\n",
      "    39                                                   # --------------------------------------------------------------------------------\n",
      "    40                                                   # word_id = sequence[position]\n",
      "    41   1864462    6980025.0      3.7     60.9          np.add.at(\n",
      "    42    932231     588965.0      0.6      5.1             co_occurrence_matrix,\n",
      "    43    932231     592987.0      0.6      5.2             (\n",
      "    44    932231     794504.0      0.9      6.9                 sequence[position],                               # word_id\n",
      "    45    932231    1108583.0      1.2      9.7                 sequence[position-stride : (position+stride) +1]  # indices to co-occurrence words \n",
      "    46                                                      ),\n",
      "    47    932231     558445.0      0.6      4.9             1\n",
      "    48                                                   )\n",
      "    49                                                   # --------------------------------------------------------------------------------\n",
      "    50                                           \n",
      "    51                                               # --------------------------------------------------------------------------------\n",
      "    52                                               # Compensate the +1 self count of a word at each occurrence.\n",
      "    53                                               # F(w) (frequency/occurrences of a word in the sequence) has been extra added besides \n",
      "    54                                               # the expected (2 * stride) * F(w) times, resulting in (context_size) * F(w).\n",
      "    55                                               # --------------------------------------------------------------------------------\n",
      "    56         2        698.0    349.0      0.0      np.fill_diagonal(\n",
      "    57         1          1.0      1.0      0.0          co_occurrence_matrix,\n",
      "    58         1     106441.0 106441.0      0.9          (co_occurrence_matrix.diagonal() - co_occurrence_matrix.sum(axis=1) / context_size)\n",
      "    59                                               )\n",
      "    60                                           \n",
      "    61         1          2.0      2.0      0.0      return co_occurrence_matrix\n"
     ]
    }
   ],
   "source": [
    "%lprun \\\n",
    "    -T create_cooccurrence_matrix.log \\\n",
    "    -f create_cooccurrence_matrix \\\n",
    "    create_cooccurrence_matrix(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "\n",
    "print(open('create_cooccurrence_matrix.log', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_create_co_occurrence_matrix(sequence, co_occurrence_vector_size, context_size=3):\n",
    "    \"\"\"Implement the same logic with dlfs2 create_co_matrix.\n",
    "    Args: \n",
    "        sequence: word index sequence of the original corpus text\n",
    "        co_occurrence_vector_size: \n",
    "        context_size: context (N-gram size N) within to check co-occurrences.\n",
    "    Returns:\n",
    "        co_occurrence matrix\n",
    "    \"\"\"\n",
    "    assert int(context_size %2) == 1\n",
    "    \n",
    "    n = sequence_size = len(sequence)\n",
    "    co_occurrence_matrix = np.zeros((co_occurrence_vector_size, co_occurrence_vector_size), dtype=np.int32)\n",
    "\n",
    "    stride = int((context_size - 1)/2 )\n",
    "    assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
    "        n, stride\n",
    "    )\n",
    "\n",
    "    for position in range(stride, (n-1) - stride +1):        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Consider counting the word itself. \n",
    "        # e.g. stride=2\n",
    "        # |W|W|W|W|W| If co-occurrences are all same word W at the position, need +4 for W\n",
    "        # |X|X|W|X|X| If co-occurrences are all same word X, need +4 for X\n",
    "        # |X|X|W|Y|Y| If co-occurrences X x 2, Y x 2, then need +2 for X and Y respectively.\n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Super slow spending approx 75% of execution time\n",
    "        # --------------------------------------------------------------------------------\n",
    "        #co_occurrence_vector = co_occurrence_matrix[sequence[position]]\n",
    "        #for index in range(context_size):\n",
    "        #    co_occurrence_vector[index] += 1\n",
    "        #REQUIRE_REMOVE_SELF_COUNTING = True\n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Super slow spending approx 75% of execution time 35 secs\n",
    "        # Line #   Hits    Time       Per Hit   % Time \n",
    "        # 36    932231     805098.0      0.9      2.3          word_id = sequence[position]\n",
    "        # 37   2796693    2240226.0      0.8      6.4          for offset in range(1, stride+1):\n",
    "        # 38   5593386   11444886.0      2.0     32.6              co_occurrence_matrix[\n",
    "        # 39   3728924    1921460.0      0.5      5.5                  word_id,\n",
    "        # 40   1864462    1427308.0      0.8      4.1                  sequence[position - offset]\n",
    "        # 41   1864462    1022951.0      0.5      2.9              ] +=1\n",
    "        # 42   5593386   11203813.0      2.0     32.0              co_occurrence_matrix[\n",
    "        # 43   3728924    1972667.0      0.5      5.6                  word_id,\n",
    "        # 44   1864462    1473145.0      0.8      4.2                  sequence[position + offset]\n",
    "        # 45   1864462    1029893.0      0.6      2.9              ] +=1\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # word_id = sequence[position]\n",
    "        # for offset in range(1, stride+1):\n",
    "        #     co_occurrence_matrix[\n",
    "        #         word_id,\n",
    "        #         sequence[position - offset]\n",
    "        #     ] +=1\n",
    "        #     co_occurrence_matrix[\n",
    "        #         word_id,\n",
    "        #         sequence[position + offset]\n",
    "        #     ] +=1\n",
    "        # REQUIRE_REMOVE_SELF_COUNTING = False\n",
    "        # --------------------------------------------------------------------------------\n",
    "            \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Consider counting the word itself. \n",
    "        # e.g. stride=2\n",
    "        # |W|W|W|W|W| If co-occurrences are all same word W at the position, need +4 for W\n",
    "        # |X|X|W|X|X| If co-occurrences are all same word X, need +4 for X\n",
    "        # |X|X|W|Y|Y| If co-occurrences X x 2, Y x 2, then need +2 for X and Y respectively.\n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # xslice() to create combined slices costs a lot.\n",
    "        # Total time: 22.0234 s\n",
    "        # Line #   Hits    Time       Per Hit   % Time \n",
    "        # 72    932231     945544.0      1.0      4.3          word_id = sequence[position]\n",
    "        # 73   1864462    6838467.0      3.7     31.1          np.add.at(\n",
    "        # 74    932231     613173.0      0.7      2.8              co_occurrence_matrix,\n",
    "        # 75    932231     671443.0      0.7      3.0              (\n",
    "        # 76    932231     583880.0      0.6      2.7                  word_id,\n",
    "        # 77   1864462    7954610.0      4.3     36.1                  xslice(\n",
    "        # 78    932231     600371.0      0.6      2.7                      sequence,\n",
    "        # 79    932231    1868599.0      2.0      8.5                      np.s_[position-stride: position, position+1 : position+stride +1]\n",
    "        # 80                                                           )\n",
    "        # 81                                                       ),\n",
    "        # 82    932231     615451.0      0.7      2.8              1\n",
    "        # 83                                                   )\n",
    "        # 84    932231     692304.0      0.7      3.1          REQUIRE_REMOVE_SELF_COUNTING = False        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        word_id = sequence[position]\n",
    "        np.add.at(\n",
    "            co_occurrence_matrix,\n",
    "            (\n",
    "                word_id,\n",
    "                xslice(\n",
    "                    sequence,\n",
    "                    np.s_[position-stride: position, position+1 : position+stride +1]\n",
    "                )\n",
    "            ),\n",
    "            1\n",
    "        )\n",
    "        REQUIRE_REMOVE_SELF_COUNTING = False\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Line #   Hits    Time       Per Hit   % Time \n",
    "        # 32   1864462    5358433.0      2.9     61.5          np.add.at(\n",
    "        # 33    932231     446858.0      0.5      5.1             co_occurrence_matrix,\n",
    "        # 34    932231     463579.0      0.5      5.3             (\n",
    "        # 35    932231     609110.0      0.7      7.0                 sequence[position],\n",
    "        # 36    932231     862299.0      0.9      9.9                 sequence[position-stride : (position+stride) +1]    \n",
    "        # 37                                                      ),\n",
    "        # 38    932231     437542.0      0.5      5.0             1\n",
    "        # 39                                                   )\n",
    "        # --------------------------------------------------------------------------------\n",
    "        #np.add.at(\n",
    "        #   co_occurrence_matrix,\n",
    "        #   (\n",
    "        #       sequence[position],\n",
    "        #       sequence[position-stride : (position+stride) +1]    # positions to co-occurence words \n",
    "        #   ),\n",
    "        #   1\n",
    "        #)\n",
    "        #REQUIRE_REMOVE_SELF_COUNTING = True\n",
    "        # --------------------------------------------------------------------------------\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Remove self counting of the word at position itself.\n",
    "        # To avoid the cost of creating a gapped indices np.r_[sequence, [i:j, j+1:k]].\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Very slow:  \n",
    "        # Line #   Hits    Time       Per Hit   % Time \n",
    "        # ==============================================================\n",
    "        # X        932231  4903853.0  5.3       33.5          \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # co_occurrence_matrix[sequence[position],sequence[position]] -=1\n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Remove self counting of the word at position itself.\n",
    "    # Line #   Hits   Time       Per Hit   % Time \n",
    "    # ==============================================================\n",
    "    # X        1      86069.0    86069.0   0.9 \n",
    "    # --------------------------------------------------------------------------------\n",
    "    if REQUIRE_REMOVE_SELF_COUNTING:\n",
    "        np.fill_diagonal(\n",
    "            co_occurrence_matrix,\n",
    "            (co_occurrence_matrix.diagonal() - co_occurrence_matrix.sum(axis=1) / context_size)\n",
    "        )\n",
    "\n",
    "    return co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9927, 9927)\n"
     ]
    }
   ],
   "source": [
    "if VALIDATION:\n",
    "    _matrix = research_create_co_occurrence_matrix(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "    print(_matrix.shape)\n",
    "\n",
    "    if not USE_PTB:\n",
    "        print(_matrix)\n",
    "        print(\"_matrix.sum() {}\".format(_matrix.sum()))\n",
    "\n",
    "    # Total sum of all word occurrences except NIL must matches with the original corpus size.\n",
    "    assert total_frequencies(_matrix, word_to_id) == len(sequence) - (CONTEXT_SIZE -1)\n",
    "    assert np.array_equal(_matrix, com0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'research_create_co_occurrence_matrix.log'. \n",
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 26.3222 s\n",
      "File: <ipython-input-31-cf484afbe5f5>\n",
      "Function: research_create_co_occurrence_matrix at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def research_create_co_occurrence_matrix(sequence, co_occurrence_vector_size, context_size=3):\n",
      "     2                                               \"\"\"Implement the same logic with dlfs2 create_co_matrix.\n",
      "     3                                               Args: \n",
      "     4                                                   sequence: word index sequence of the original corpus text\n",
      "     5                                                   co_occurrence_vector_size: \n",
      "     6                                                   context_size: context (N-gram size N) within to check co-occurrences.\n",
      "     7                                               Returns:\n",
      "     8                                                   co_occurrence matrix\n",
      "     9                                               \"\"\"\n",
      "    10         1          3.0      3.0      0.0      assert int(context_size %2) == 1\n",
      "    11                                               \n",
      "    12         1          3.0      3.0      0.0      n = sequence_size = len(sequence)\n",
      "    13         1        234.0    234.0      0.0      co_occurrence_matrix = np.zeros((co_occurrence_vector_size, co_occurrence_vector_size), dtype=np.int32)\n",
      "    14                                           \n",
      "    15         1          6.0      6.0      0.0      stride = int((context_size - 1)/2 )\n",
      "    16         1          1.0      1.0      0.0      assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
      "    17                                                   n, stride\n",
      "    18                                               )\n",
      "    19                                           \n",
      "    20    932232     728812.0      0.8      2.8      for position in range(stride, (n-1) - stride +1):        \n",
      "    21                                                   # --------------------------------------------------------------------------------\n",
      "    22                                                   # Consider counting the word itself. \n",
      "    23                                                   # e.g. stride=2\n",
      "    24                                                   # |W|W|W|W|W| If co-occurrences are all same word W at the position, need +4 for W\n",
      "    25                                                   # |X|X|W|X|X| If co-occurrences are all same word X, need +4 for X\n",
      "    26                                                   # |X|X|W|Y|Y| If co-occurrences X x 2, Y x 2, then need +2 for X and Y respectively.\n",
      "    27                                                   # --------------------------------------------------------------------------------\n",
      "    28                                           \n",
      "    29                                                   # --------------------------------------------------------------------------------\n",
      "    30                                                   # Super slow spending approx 75% of execution time\n",
      "    31                                                   # --------------------------------------------------------------------------------\n",
      "    32                                                   #co_occurrence_vector = co_occurrence_matrix[sequence[position]]\n",
      "    33                                                   #for index in range(context_size):\n",
      "    34                                                   #    co_occurrence_vector[index] += 1\n",
      "    35                                                   #REQUIRE_REMOVE_SELF_COUNTING = True\n",
      "    36                                                   # --------------------------------------------------------------------------------\n",
      "    37                                           \n",
      "    38                                                   # --------------------------------------------------------------------------------\n",
      "    39                                                   # Super slow spending approx 75% of execution time 35 secs\n",
      "    40                                                   # Line #   Hits    Time       Per Hit   % Time \n",
      "    41                                                   # 36    932231     805098.0      0.9      2.3          word_id = sequence[position]\n",
      "    42                                                   # 37   2796693    2240226.0      0.8      6.4          for offset in range(1, stride+1):\n",
      "    43                                                   # 38   5593386   11444886.0      2.0     32.6              co_occurrence_matrix[\n",
      "    44                                                   # 39   3728924    1921460.0      0.5      5.5                  word_id,\n",
      "    45                                                   # 40   1864462    1427308.0      0.8      4.1                  sequence[position - offset]\n",
      "    46                                                   # 41   1864462    1022951.0      0.5      2.9              ] +=1\n",
      "    47                                                   # 42   5593386   11203813.0      2.0     32.0              co_occurrence_matrix[\n",
      "    48                                                   # 43   3728924    1972667.0      0.5      5.6                  word_id,\n",
      "    49                                                   # 44   1864462    1473145.0      0.8      4.2                  sequence[position + offset]\n",
      "    50                                                   # 45   1864462    1029893.0      0.6      2.9              ] +=1\n",
      "    51                                                   # --------------------------------------------------------------------------------\n",
      "    52                                                   # word_id = sequence[position]\n",
      "    53                                                   # for offset in range(1, stride+1):\n",
      "    54                                                   #     co_occurrence_matrix[\n",
      "    55                                                   #         word_id,\n",
      "    56                                                   #         sequence[position - offset]\n",
      "    57                                                   #     ] +=1\n",
      "    58                                                   #     co_occurrence_matrix[\n",
      "    59                                                   #         word_id,\n",
      "    60                                                   #         sequence[position + offset]\n",
      "    61                                                   #     ] +=1\n",
      "    62                                                   # REQUIRE_REMOVE_SELF_COUNTING = False\n",
      "    63                                                   # --------------------------------------------------------------------------------\n",
      "    64                                                       \n",
      "    65                                                   # --------------------------------------------------------------------------------\n",
      "    66                                                   # Consider counting the word itself. \n",
      "    67                                                   # e.g. stride=2\n",
      "    68                                                   # |W|W|W|W|W| If co-occurrences are all same word W at the position, need +4 for W\n",
      "    69                                                   # |X|X|W|X|X| If co-occurrences are all same word X, need +4 for X\n",
      "    70                                                   # |X|X|W|Y|Y| If co-occurrences X x 2, Y x 2, then need +2 for X and Y respectively.\n",
      "    71                                                   # --------------------------------------------------------------------------------\n",
      "    72                                           \n",
      "    73                                                   # --------------------------------------------------------------------------------\n",
      "    74                                                   # xslice() to create combined slices costs a lot.\n",
      "    75                                                   # Total time: 22.0234 s\n",
      "    76                                                   # Line #   Hits    Time       Per Hit   % Time \n",
      "    77                                                   # 72    932231     945544.0      1.0      4.3          word_id = sequence[position]\n",
      "    78                                                   # 73   1864462    6838467.0      3.7     31.1          np.add.at(\n",
      "    79                                                   # 74    932231     613173.0      0.7      2.8              co_occurrence_matrix,\n",
      "    80                                                   # 75    932231     671443.0      0.7      3.0              (\n",
      "    81                                                   # 76    932231     583880.0      0.6      2.7                  word_id,\n",
      "    82                                                   # 77   1864462    7954610.0      4.3     36.1                  xslice(\n",
      "    83                                                   # 78    932231     600371.0      0.6      2.7                      sequence,\n",
      "    84                                                   # 79    932231    1868599.0      2.0      8.5                      np.s_[position-stride: position, position+1 : position+stride +1]\n",
      "    85                                                   # 80                                                           )\n",
      "    86                                                   # 81                                                       ),\n",
      "    87                                                   # 82    932231     615451.0      0.7      2.8              1\n",
      "    88                                                   # 83                                                   )\n",
      "    89                                                   # 84    932231     692304.0      0.7      3.1          REQUIRE_REMOVE_SELF_COUNTING = False        \n",
      "    90                                                   # --------------------------------------------------------------------------------\n",
      "    91    932231    1087168.0      1.2      4.1          word_id = sequence[position]\n",
      "    92   1864462    9195697.0      4.9     34.9          np.add.at(\n",
      "    93    932231     664741.0      0.7      2.5              co_occurrence_matrix,\n",
      "    94    932231     744129.0      0.8      2.8              (\n",
      "    95    932231     630459.0      0.7      2.4                  word_id,\n",
      "    96   1864462    9009496.0      4.8     34.2                  xslice(\n",
      "    97    932231     662829.0      0.7      2.5                      sequence,\n",
      "    98    932231    2131089.0      2.3      8.1                      np.s_[position-stride: position, position+1 : position+stride +1]\n",
      "    99                                                           )\n",
      "   100                                                       ),\n",
      "   101    932231     653503.0      0.7      2.5              1\n",
      "   102                                                   )\n",
      "   103    932231     813987.0      0.9      3.1          REQUIRE_REMOVE_SELF_COUNTING = False\n",
      "   104                                           \n",
      "   105                                                   # --------------------------------------------------------------------------------\n",
      "   106                                                   # Line #   Hits    Time       Per Hit   % Time \n",
      "   107                                                   # 32   1864462    5358433.0      2.9     61.5          np.add.at(\n",
      "   108                                                   # 33    932231     446858.0      0.5      5.1             co_occurrence_matrix,\n",
      "   109                                                   # 34    932231     463579.0      0.5      5.3             (\n",
      "   110                                                   # 35    932231     609110.0      0.7      7.0                 sequence[position],\n",
      "   111                                                   # 36    932231     862299.0      0.9      9.9                 sequence[position-stride : (position+stride) +1]    \n",
      "   112                                                   # 37                                                      ),\n",
      "   113                                                   # 38    932231     437542.0      0.5      5.0             1\n",
      "   114                                                   # 39                                                   )\n",
      "   115                                                   # --------------------------------------------------------------------------------\n",
      "   116                                                   #np.add.at(\n",
      "   117                                                   #   co_occurrence_matrix,\n",
      "   118                                                   #   (\n",
      "   119                                                   #       sequence[position],\n",
      "   120                                                   #       sequence[position-stride : (position+stride) +1]    # positions to co-occurence words \n",
      "   121                                                   #   ),\n",
      "   122                                                   #   1\n",
      "   123                                                   #)\n",
      "   124                                                   #REQUIRE_REMOVE_SELF_COUNTING = True\n",
      "   125                                                   # --------------------------------------------------------------------------------\n",
      "   126                                                   \n",
      "   127                                                   # --------------------------------------------------------------------------------\n",
      "   128                                                   # Remove self counting of the word at position itself.\n",
      "   129                                                   # To avoid the cost of creating a gapped indices np.r_[sequence, [i:j, j+1:k]].\n",
      "   130                                                   # --------------------------------------------------------------------------------\n",
      "   131                                                   # Very slow:  \n",
      "   132                                                   # Line #   Hits    Time       Per Hit   % Time \n",
      "   133                                                   # ==============================================================\n",
      "   134                                                   # X        932231  4903853.0  5.3       33.5          \n",
      "   135                                                   # --------------------------------------------------------------------------------\n",
      "   136                                                   # co_occurrence_matrix[sequence[position],sequence[position]] -=1\n",
      "   137                                                   # --------------------------------------------------------------------------------\n",
      "   138                                           \n",
      "   139                                               # --------------------------------------------------------------------------------\n",
      "   140                                               # Remove self counting of the word at position itself.\n",
      "   141                                               # Line #   Hits   Time       Per Hit   % Time \n",
      "   142                                               # ==============================================================\n",
      "   143                                               # X        1      86069.0    86069.0   0.9 \n",
      "   144                                               # --------------------------------------------------------------------------------\n",
      "   145         1          0.0      0.0      0.0      if REQUIRE_REMOVE_SELF_COUNTING:\n",
      "   146                                                   np.fill_diagonal(\n",
      "   147                                                       co_occurrence_matrix,\n",
      "   148                                                       (co_occurrence_matrix.diagonal() - co_occurrence_matrix.sum(axis=1) / context_size)\n",
      "   149                                                   )\n",
      "   150                                           \n",
      "   151         1          0.0      0.0      0.0      return co_occurrence_matrix\n"
     ]
    }
   ],
   "source": [
    "%lprun \\\n",
    "    -T research_create_co_occurrence_matrix.log \\\n",
    "    -f research_create_co_occurrence_matrix \\\n",
    "    research_create_co_occurrence_matrix(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "\n",
    "print(open('research_create_co_occurrence_matrix.log', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
