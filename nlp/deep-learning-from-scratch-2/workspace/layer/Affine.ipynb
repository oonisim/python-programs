{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    List, \n",
    "    Dict,\n",
    "    Optional,\n",
    "    Final\n",
    ")\n",
    "import logging \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLogLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # softmaxの出力\n",
    "        self.t = None # 教師データ\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t \n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 教師データがone-hot-vectorの場合\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coroutine(func):\n",
    "    def start(*args,**kwargs):\n",
    "        cr = func(*args,**kwargs)\n",
    "        next(cr)\n",
    "        return cr\n",
    "    \n",
    "    return start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine(object):\n",
    "    \"\"\"Affine (MatMul) Layer\"\"\"\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Class initialization\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Instance initialization\n",
    "    # --------------------------------------------------------------------------------    \n",
    "    def __init__(\n",
    "        self, \n",
    "        W: np.ndarray, \n",
    "        num_nodes: int, \n",
    "        optimizer: object, \n",
    "        layers: List[Generator[np.ndarray, None, None]],\n",
    "        log_level: int = logging.ERROR\n",
    "    ):\n",
    "        \"\"\"Initialize an affine layer that has 'num_nodes' nodes\n",
    "        Args:\n",
    "            W: An array of weight-vectors for each node.\n",
    "            num_nodes: Number of nodes in the layer\n",
    "            optmizer: Gradient descent implementation e.g SGD, Adam.\n",
    "            layers: Layers to which forward the output\n",
    "        \n",
    "        Batch X: shape(m, n) \n",
    "        --------------------\n",
    "        'x' is an individual row with n features where n=0 is a bias. A batch X has 'm' rows. \n",
    "        X[j] is [x(j)(0), x(j)(1), ... x(j)(n)] where bias 'x(j)(0)' is 1 as a bias input.\n",
    "        \"input\" is not limited to the 1st input data layer e.g. image pixels but to any layer.\n",
    "\n",
    "        Weights W: shape(u, n) where u=num_nodes\n",
    "        --------------------\n",
    "        k-th node (k:0, 1, .. u-1) has a weight vector W(k):[w(k)(0), w(k)(1), ... w(k)(n)].\n",
    "        w(k)(0) is its bias weight. Each w(k)(i) amplifies i-th feature in the input x.  \n",
    "                \n",
    "        \"\"\"\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Validate the expected dimensions.\n",
    "        # `W` has `u` nodes (nodes)\n",
    "        # --------------------------------------------------------------------------------\n",
    "        assert W.shape[0] == num_nodes, \\\n",
    "            f\"W has {W.shape[0]} weight vectors that should have matched with num_nodes {num_nodes}\"\n",
    "\n",
    "        # W: node W\n",
    "        self.u: int = num_nodes          # number of nodes in the layer\n",
    "        self.W: np.ndarray = W           # node weight vectors\n",
    "        self.dW: np.ndarray = np.empty(0, num_nodes)  # gradient of W\n",
    "\n",
    "        # X: batch input\n",
    "        self.m: int = -1                 # batch size: X.shape[0]\n",
    "        self.n: int = W.shape[1]         # number of features in x\n",
    "        self.X: np.ndarray = np.empty(0, W.shape[1])  # Batch input\n",
    "        self.dX: np.ndarray\n",
    "        \n",
    "        logging.basicConfig()\n",
    "        self._log_level = log_level\n",
    "        self._logger = logging.getLogger(__name__)\n",
    "        self._logger.setLevel(log_level)\n",
    "        \n",
    "        \n",
    "\n",
    "    def _forward(Y: np.ndarray, layer: Generator[np.ndarray, None, None]) -> np.ndarray:\n",
    "        \"\"\"Send the affine output Y to the next layer\n",
    "        Args:\n",
    "            Y: Affine output\n",
    "            layer: Layer where to propagete Y.\n",
    "        Returns:\n",
    "            Layer return\n",
    "        \"\"\"\n",
    "            loss: int = layer.send(Y)\n",
    "                \n",
    "    \n",
    "    @coroutine\n",
    "    def forward(self):\n",
    "        \"\"\"Foward propagate of the affine layer output Y = X@W\"\"\"\n",
    "        Y: Optional[np.ndarray] = None\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # \n",
    "        # --------------------------------------------------------------------------------\n",
    "        target.send(Y)\n",
    "        self.X = (yield )\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # X:shape(m, n) W.T:shape(n, u) -> Y:shape(m, u)\n",
    "        # --------------------------------------------------------------------------------\n",
    "        Y = np.matmul(self.X, self.W.T) \n",
    "        self.m = self.X.shape[0] if self.X is not None else -1\n",
    "\n",
    "        # X@W, needs shapes X(m, n) @ W(n, num_nodes) to generate output Y(m, num_nodes)\n",
    "        assert X.shape[1] == self.w.shape[0], \\\n",
    "            f\"numbef of input x features {X.shape[1]} must be that of weight vector {}\".format(\n",
    "                , self.W.shape[0]\n",
    "        )\n",
    "        \n",
    "         \n",
    "    @coroutine\n",
    "    def backward(self):\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Back propgation dy from the posterior layer. dy shape must match that of Y(m, num_nodes)\n",
    "        # --------------------------------------------------------------------------------\n",
    "        dy = next(target.backward)    # gradient back-propagated from the posterior \n",
    "        assert(dy.shape[0] == self.m), \\\n",
    "        \"gradient dy shape {} must match output Y shape ({}, {})\".format(\n",
    "            dy.shape, self.m, self.n\n",
    "        )\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Gradient descent on W\n",
    "        # --------------------------------------------------------------------------------\n",
    "        dw = np.dot(self.X, dy.T)\n",
    "        self.w = self.optimizer(self.w, dw)\n",
    "\n",
    "        dx = np.dot(dy, self.w)\n",
    "        \n",
    "        yield dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Hoge(object):\n",
    "    def __init__(self, units, weights, optimizer):\n",
    "        # neuron weight vectors\n",
    "        self.w: numpy.ndarray = weights  # weight vector per neuron\n",
    "        self.n: int = weights.shape[0]   # number of features expected\n",
    "        self.dw: numpy.ndarray = None    # gradient of W\n",
    "        \n",
    "        self.X: numpy.ndarray = np.empty((0, self.n))     # Batch input\n",
    "        self.m: int  = -1                # batch size: X.shape[0]\n",
    "            \n",
    "        self.forward = self.forward()\n",
    "\n",
    "    @coroutine\n",
    "    def forward(self):\n",
    "        \"\"\"Foward propagation of the affine layer X@W\"\"\"\n",
    "        while True:\n",
    "            self.X = yield\n",
    "\n",
    "            print(\"foward got \\n{}\".format(self.X))\n",
    "            self.m = self.X.shape[0] if self.X is not None else -1\n",
    "        \n",
    "            \n",
    "hoge = Hoge(4, np.arange(12).reshape((3, 4)), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foward got [[0 1 2]\n",
      " [3 4 5]]\n"
     ]
    }
   ],
   "source": [
    "hoge.forward.send(np.arange(6).reshape((2, 3)))\n",
    "#hoge.forward.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(5, 0), dtype=float64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.empty((5, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 38.  56.]\n",
      " [ 92. 137.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
