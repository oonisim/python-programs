{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing imoprt List, Dict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coroutine(func):\n",
    "    def start(*args,**kwargs):\n",
    "        cr = func(*args,**kwargs)\n",
    "        next(cr)\n",
    "        return cr\n",
    "    \n",
    "    return start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-c95fe576aca0>, line 65)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-c95fe576aca0>\"\u001b[0;36m, line \u001b[0;32m65\u001b[0m\n\u001b[0;31m    self.w = self.optimizer.(self.w, dw)\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Affine(object):\n",
    "    \"\"\"Affine (MatMul) Layer\"\"\"\n",
    "    def __init__(self, units, weights, optimizer, posteriors: List[object]):\n",
    "        \"\"\"\n",
    "        Initialize the affine layer.\n",
    "        \n",
    "        [X] shape(size, n)\n",
    "        Aka Batch. An array of input data x with n features (n: 0, 1, ..., n). n=0 is a bias.\n",
    "        j-th input X[j] is [x(j)(0), x(j)(1), ... x(j)(n)] where bias 'x(j)(0)' is 1.\n",
    "        Use capital X for batch and x for its individual input.\n",
    "        \n",
    "        NOTE: \"input\" is not limited to the first input data layer e.g. image pixels, \n",
    "              but \"input\" at any layer.\n",
    "\n",
    "        [weights] shape(n, units)\n",
    "        k-th neuron (k:0, 1, .. size-1) has its weight vector W(k):[w(k)(0), w(k)(1), ... w(k)(n)].\n",
    "        w(k)(0) is its bias weight. Each w(k)(i) amplifies i-th feature in the input x.  \n",
    "                \n",
    "        Args:\n",
    "            units: number of neurons in the layer\n",
    "            weights: array of weight-vectors of each neuron. shape(n, size)\n",
    "            optmizer: gradient descent implementation e.g SGD, Adam.\n",
    "            posteriors: next layers\n",
    "        \"\"\"\n",
    "        assert weights.shape[1] == units, \"number of weights {} must be that of neurons {}\".format(\n",
    "            self.w.shape[1], units\n",
    "        )\n",
    "        # neuron weight vectors\n",
    "        self.w: numpy.ndarray = weights  # weight vector per neuron\n",
    "        self.n: int = weights.shape[0]   # number of features expected\n",
    "        self.dw: numpy.ndarray = None    # gradient of W\n",
    "        \n",
    "        self.X: numpy.ndarray = np.empty(0, self.n)     # Batch input\n",
    "        self.m: int  = -1                # batch size: X.shape[0]\n",
    "        \n",
    "        \n",
    "    @coroutine\n",
    "    def forward(self):\n",
    "        \"\"\"Foward propagation of the affine layer X@W\"\"\"\n",
    "        self.X = (yield target.forward.send(np.dot(self.X, self.w)))\n",
    "        self.m = self.X.shape[0] if self.X is not None else -1\n",
    "\n",
    "        # X@W, needs shapes X(m, n) @ W(n, units) to generate output Y(m, units)\n",
    "        assert X.shape[1] == self.w.shape[0], \\\n",
    "        \"numbef of input x features {} must be that of weight vector {}\".format(\n",
    "            X.shape[1], self.w.shape[0]\n",
    "        )\n",
    "        \n",
    "         \n",
    "    @coroutine\n",
    "    def backward(self):\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Back propgation dy from the posterior layer. dy shape must match that of Y(m, units)\n",
    "        # --------------------------------------------------------------------------------\n",
    "        dy = next(target.backward)    # gradient back-propagated from the posterior \n",
    "        assert(dy.shape[0] == self.m), \\\n",
    "        \"gradient dy shape {} must match output Y shape ({}, {})\".format(\n",
    "            dy.shape, self.m, self.n\n",
    "        )\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Gradient descent on W\n",
    "        # --------------------------------------------------------------------------------\n",
    "        dw = np.dot(self.X, dy.T)\n",
    "        self.w = self.optimizer.(self.w, dw)\n",
    "\n",
    "        dx = np.dot(dy, self.w)\n",
    "        \n",
    "        yield dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLogLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # softmaxの出力\n",
    "        self.t = None # 教師データ\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t \n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 教師データがone-hot-vectorの場合\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Hoge(object):\n",
    "    def __init__(self, units, weights, optimizer):\n",
    "        # neuron weight vectors\n",
    "        self.w: numpy.ndarray = weights  # weight vector per neuron\n",
    "        self.n: int = weights.shape[0]   # number of features expected\n",
    "        self.dw: numpy.ndarray = None    # gradient of W\n",
    "        \n",
    "        self.X: numpy.ndarray = np.empty((0, self.n))     # Batch input\n",
    "        self.m: int  = -1                # batch size: X.shape[0]\n",
    "            \n",
    "        self.forward = self.forward()\n",
    "\n",
    "    @coroutine\n",
    "    def forward(self):\n",
    "        \"\"\"Foward propagation of the affine layer X@W\"\"\"\n",
    "        while True:\n",
    "            self.X = yield\n",
    "\n",
    "            print(\"foward got \\n{}\".format(self.X))\n",
    "            self.m = self.X.shape[0] if self.X is not None else -1\n",
    "        \n",
    "            \n",
    "hoge = Hoge(4, np.arange(12).reshape((3, 4)), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foward got [[0 1 2]\n",
      " [3 4 5]]\n"
     ]
    }
   ],
   "source": [
    "hoge.forward.send(np.arange(6).reshape((2, 3)))\n",
    "#hoge.forward.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "[1 2]\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2])\n",
    "print(a.shape)\n",
    "print(a.T)\n",
    "print(a.T.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
