{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec implementation\n",
    "\n",
    "## Original papers\n",
    "* [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "* [word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method](https://arxiv.org/abs/1402.3722)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The original paper proposed using:\n",
    "1. Matmul to extract word vectors from ```Win```.\n",
    "2. Softmax to calculate scores with all the word vectors from ```Wout```.\n",
    "\n",
    "<img src=\"image/word2vec_cbow_mechanism.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "### Motivation\n",
    "\n",
    "The original implementation was computationally expensive.\n",
    "\n",
    "1. Matmal to extract word vectors from ```Win```.\n",
    "2. Softmax can happen vocabulary size times with ```Wout```.\n",
    "\n",
    "<img src=\"image/word2vec_negative_sampling_motivation.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1. Use index to extract word vectors from Win.\n",
    "2. Instead of calculating softmax with all the word vectors from ```Wout```, sample small number (SL) of negative/false word vectors from ```Wout``` and calculate logistic log loss with each sample. \n",
    "\n",
    "<img src=\"image/wors2vec_neg_sample_backprop.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using only one Word vector space W\n",
    "\n",
    "There is no reasoning nor proof why two word vector space are required. In the end, we only use one word vector space, which appears to be ```Win```. \n",
    "\n",
    "However, if we use one vector space ```W``` for ```event```, ```context``` and ```negative samples```,then an event vector ```event=W[i]``` in a sentence can be used as a negative sample in another setence. Then the weight ```W[i]``` is updated for both positive and negative labels in the same gradient descent on ```W```. The actual [experiment of using only one vector space](./layer/embedding_single_vector_space.py) ```W``` did not work well.\n",
    "\n",
    "* [Why do we need 2 matrices for word2vec or GloVe](https://datascience.stackexchange.com/a/94422/68313)\n",
    "\n",
    "\n",
    "<img src=\"image/word2vec_why_not_one_W.png\" align=\"left\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from itertools import islice\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=400) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Google Colab environment\n",
    "\n",
    "Colab gets disconnected within approx 20 min. Hence not suitable for training (or need to upgrade to the pro version)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone github to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_GOOGLE_COLAB = True\n",
    "except:\n",
    "    IN_GOOGLE_COLAB = False\n",
    "    \n",
    "if IN_GOOGLE_COLAB:\n",
    "    !pip install line_profiler\n",
    "    !google.colab.drive.mount('/content/gdrive')\n",
    "    !rm -rf /content/drive/MyDrive/github\n",
    "    !mkdir -p /content/drive/MyDrive/github\n",
    "    !git clone https://github.com/oonisim/python-programs.git /content/drive/MyDrive/github\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone github to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_GOOGLE_COLAB = True\n",
    "except:\n",
    "    IN_GOOGLE_COLAB = False\n",
    "    \n",
    "if IN_GOOGLE_COLAB:\n",
    "    !pip install line_profiler\n",
    "    !google.colab.drive.mount('/content/gdrive')\n",
    "    !rm -rf /content/github\n",
    "    !mkdir -p /content/github\n",
    "    !git clone https://github.com/oonisim/python-programs.git /content/github\n",
    "        \n",
    "    import sys\n",
    "    sys.path.append('/content/github/nlp/src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook setups\n",
    "\n",
    "Auto reolaod causes an error in Jupyter notebooks. Restart the Jupyter kernel for the error:\n",
    "```TypeError: super(type, obj): obj must be an instance or subtype of type```\n",
    "See\n",
    "- https://stackoverflow.com/a/52927102/4281353\n",
    "- http://thomas-cokelaer.info/blog/2011/09/382/\n",
    "\n",
    "> The problem resides in the mechanism of reloading modules.\n",
    "> Reloading a module often changes the internal object in memory which\n",
    "> makes the isinstance test of super return False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "import function.fileio as fileio\n",
    "import function.text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.constant import (\n",
    "    TYPE_INT,\n",
    "    TYPE_FLOAT,\n",
    "    TYPE_LABEL,\n",
    "    TYPE_TENSOR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants\n",
    "\n",
    "Too large LR generates unusable event vector space.\n",
    "\n",
    "Uniform weight distribution does not work (Why?)\n",
    "Weights from the normal distribution sampling with small std (0.01) works (Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TEXT8 = False\n",
    "USE_PTB = not USE_TEXT8\n",
    "USE_CBOW = False\n",
    "USE_SGRAM = not USE_CBOW\n",
    "\n",
    "CORPUS_FILE = \"text8_256\" if USE_TEXT8 else \"ptb_train\"\n",
    "CORPUS_URL = \"https://data.deepai.org/text8.zip\" \\\n",
    "    if USE_TEXT8 else f'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt' \\\n",
    "\n",
    "TARGET_SIZE = TYPE_INT(1)       # Size of the target event (word)\n",
    "CONTEXT_SIZE = TYPE_INT(10)     # Size of the context in which the target event occurs.\n",
    "WINDOW_SIZE = TARGET_SIZE + CONTEXT_SIZE\n",
    "SAMPLE_SIZE = TYPE_INT(5)       # Size of the negative samples\n",
    "\n",
    "VECTOR_SIZE = TYPE_INT(100)     # Number of features in the event vector.\n",
    "WEIGHT_SCHEME = \"normal\"\n",
    "WEIGHT_PARAMS = {\n",
    "    \"std\": 0.01\n",
    "}\n",
    "\n",
    "LR = TYPE_FLOAT(20)\n",
    "NUM_SENTENCES = 10\n",
    "\n",
    "STATE_FILE = \"../models/word2vec_sgram_%s_E%s_C%s_S%s_W%s_%s_%s_V%s_LR%s_N%s.pkl\" % (\n",
    "    CORPUS_FILE,\n",
    "    TARGET_SIZE,\n",
    "    CONTEXT_SIZE,\n",
    "    SAMPLE_SIZE,\n",
    "    WEIGHT_SCHEME,\n",
    "    \"std\",\n",
    "    WEIGHT_PARAMS[\"std\"],\n",
    "    VECTOR_SIZE,\n",
    "    LR,\n",
    "    NUM_SENTENCES,\n",
    ")\n",
    "\n",
    "MAX_ITERATIONS = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oonisim/.keras/datasets/ptb_train\n"
     ]
    }
   ],
   "source": [
    "path_to_corpus = f\"~/.keras/datasets/{CORPUS_FILE}\"\n",
    "if fileio.Function.is_file(path_to_corpus):\n",
    "    pass\n",
    "else:\n",
    "    # text8, run \"cat text8 | xargs -n 512 > text8_512\" after download\n",
    "    path_to_corpus = tf.keras.utils.get_file(\n",
    "        fname=CORPUS_FILE,\n",
    "        origin=CORPUS_URL,\n",
    "        extract=True\n",
    "    )\n",
    "corpus = fileio.Function.read_file(path_to_corpus)\n",
    "print(path_to_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n"
     ]
    }
   ],
   "source": [
    "examples = corpus.split('\\n')[:1]\n",
    "for line in examples:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Event (word) indexing\n",
    "Index the events that have occurred in the event sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oonisim/conda/envs/python_programs/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "from layer.preprocessing import (\n",
    "    EventIndexing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexing = EventIndexing(\n",
    "    name=\"word_indexing\",\n",
    "    corpus=corpus,\n",
    "    min_sequence_length=WINDOW_SIZE\n",
    ")\n",
    "del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EventIndexing  for the corpus\n",
    "\n",
    "Adapt to the ```corpus``` and provides:\n",
    "* event_to_index dictionary\n",
    "* vocaburary of the corpus\n",
    "* word occurrence probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EventIndexing.vocabulary[10]:\n",
      "['<nil>' '<unk>' 'aer' 'banknote' 'berlitz' 'calloway' 'centrust' 'cluett' 'fromstein' 'gitano']\n",
      "\n",
      "EventIndexing.event_to_index[10]:\n",
      "('<nil>', 0)\n",
      "('<unk>', 1)\n",
      "('aer', 2)\n",
      "('banknote', 3)\n",
      "('berlitz', 4)\n",
      "('calloway', 5)\n",
      "('centrust', 6)\n",
      "('cluett', 7)\n",
      "('fromstein', 8)\n",
      "('gitano', 9)\n",
      "\n",
      "EventIndexing.probabilities[10]:\n",
      "<nil>                : 0.00000e+00\n",
      "<unk>                : 1.65308e-02\n",
      "aer                  : 5.34860e-06\n",
      "banknote             : 5.34860e-06\n",
      "berlitz              : 5.34860e-06\n",
      "calloway             : 5.34860e-06\n",
      "centrust             : 5.34860e-06\n",
      "cluett               : 5.34860e-06\n",
      "fromstein            : 5.34860e-06\n",
      "gitano               : 5.34860e-06\n"
     ]
    }
   ],
   "source": [
    "words = word_indexing.list_events(range(10))\n",
    "print(f\"EventIndexing.vocabulary[10]:\\n{words}\\n\")\n",
    "\n",
    "indices = word_indexing.list_indices(words)\n",
    "print(f\"EventIndexing.event_to_index[10]:\")\n",
    "for item in zip(words, indices):\n",
    "    print(item)\n",
    "\n",
    "probabilities = word_indexing.list_probabilities(words)\n",
    "print(f\"\\nEventIndexing.probabilities[10]:\")\n",
    "for word, p in zip(words, probabilities):\n",
    "    print(f\"{word:20s} : {p:.5e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling using the probability\n",
    "\n",
    "Sample events according to their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['addressing', 'abroad', 'carefully', 'maryland', 'co']\n"
     ]
    }
   ],
   "source": [
    "sample = word_indexing.sample(size=5)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "Sample events not including those events already sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_indices=[992, 36, 3110, 44, 237] \n",
      "events=['addresses' 'as' 'tissue' 'of' 'argue']\n"
     ]
    }
   ],
   "source": [
    "negative_indices = word_indexing.negative_sample_indices(\n",
    "    size=5, excludes=word_indexing.list_indices(sample)\n",
    ")\n",
    "print(f\"negative_indices={negative_indices} \\nevents={word_indexing.list_events(negative_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the             :    34\n",
      "asbestos        :    63\n",
      "fiber           :    86\n",
      "<unk>           :     1\n",
      "is              :    42\n",
      "unusually       :    87\n",
      "<unk>           :     1\n",
      "once            :    64\n",
      "it              :    80\n",
      "enters          :    88\n",
      "the             :    34\n",
      "<unk>           :     1\n"
     ]
    }
   ],
   "source": [
    "# sentences = \"\\n\".join(corpus.split('\\n')[5:6])\n",
    "sentences = \"\"\"\n",
    "the asbestos fiber <unk> is unusually <unk> once it enters the <unk> \n",
    "\"\"\"\n",
    "sequences = word_indexing.function(sentences)\n",
    "for pair in zip(sentences.strip().split(\" \"), sequences[0]):\n",
    "    print(f\"{pair[0]:15} : {pair[1]:5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EventContext\n",
    "\n",
    "EventContext layer generates ```(event, context)``` pairs from sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer.preprocessing import (\n",
    "    EventContext\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_context = EventContext(\n",
    "    name=\"ev\",\n",
    "    window_size=WINDOW_SIZE,\n",
    "    event_size=TARGET_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context of an event (word) in a sentence\n",
    "\n",
    "In the sentence ```\"a form of asbestos once used to make kent cigarette filters\"```, one of the context windows ```a form of asbestos once``` of size 5 and event size 1 has.\n",
    "* ```of``` as a target word.\n",
    "* ```(a, form) and (asbestos, once)``` as its context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence of the word indices for the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[37, 62, 44, 63, 64, 65, 66, 67, 68, 69, 70],\n",
       "       [29, 30, 31, 50, 51, 43, 44, 52, 53,  0,  0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \"\"\"\n",
    "a form of asbestos once used to make kent cigarette filters\n",
    "\n",
    "N years old and former chairman of consolidated gold \n",
    "\"\"\"\n",
    "\n",
    "sequence = word_indexing.function(sentences)\n",
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (event, context) pairs\n",
    "\n",
    "For each word (event) in the setence ```(of, asbestos, ... , kent)``` excludnig the ends of the sentence, create ```(target, context)``` as:\n",
    "\n",
    "```\n",
    "[\n",
    "  [of, a, form, asbestos, once],              # target is 'of', context is (a, form, asbestos, once)\n",
    "  ['asbestos', 'form', 'of', 'once', 'used'],\n",
    "  ['once', 'of', 'asbestos', 'used', 'to'],\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "### Format of the (event, context) pairs\n",
    "\n",
    "* **E** is the target event indices\n",
    "* **C** is the context indices\n",
    "\n",
    "<img src=\"image/event_context_format.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event context pairs. Shape (2, 11), Target event size 1, Window size 11.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[65, 37, 62, 44, 63, 64, 66, 67, 68, 69, 70],\n",
       "       [43, 29, 30, 31, 50, 51, 44, 52, 53,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_context_pairs = event_context.function(sequence)\n",
    "print(\n",
    "    f\"Event context pairs. Shape %s, Target event size %s, Window size %s.\" % \n",
    "    (event_context_pairs.shape, event_context.event_size, event_context.window_size)\n",
    ")\n",
    "event_context_pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (event, context) pairs in textual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['used',\n",
       "  'a',\n",
       "  'form',\n",
       "  'of',\n",
       "  'asbestos',\n",
       "  'once',\n",
       "  'to',\n",
       "  'make',\n",
       "  'kent',\n",
       "  'cigarette',\n",
       "  'filters'],\n",
       " ['chairman',\n",
       "  'n',\n",
       "  'years',\n",
       "  'old',\n",
       "  'and',\n",
       "  'former',\n",
       "  'of',\n",
       "  'consolidated',\n",
       "  'gold',\n",
       "  '<nil>',\n",
       "  '<nil>']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indexing.sequence_to_sentence(event_context_pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Word Embedding\n",
    "\n",
    "Embedding is to train the model to group similar events in a close proximity in the event vector space. If two events e.g. 'pencil' and 'pen' are similar concepts, then their event vectors resides in a close distance in the event space. \n",
    "\n",
    "* [Thought Vectors](https://wiki.pathmind.com/thought-vectors)\n",
    "\n",
    "## Training process\n",
    "\n",
    "1. Calculate ```Bc```, the BoW (Bag of Words) from context event vectors.\n",
    "2. Calculate ```Be```,  the BoW (Bag of Words) from target event vectors.\n",
    "3. The dot product ```Ye = dot(Bc, Be)``` is given the label 1 to get them closer.\n",
    "4. For each negative sample ```Ws(s)```, the dot product with ```Ys = dot(Be, Ws(s)``` is given the label 0 to get them apart. \n",
    "5. ```np.c_[Ye, Ys]``` is fowarded to the logistic log loss layer.\n",
    "\n",
    "<img src=\"image/word2vec_backprop_Be.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'USE_CBOW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-019e76d4283e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mUSE_CBOW\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     from layer.embedding_cbow_dual_vector_spaces.py import (\n\u001b[1;32m      4\u001b[0m         \u001b[0mEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'USE_CBOW' is not defined"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "if USE_CBOW:\n",
    "    from layer.embedding_cbow_dual_vector_spaces.py import (\n",
    "        Embedding\n",
    "    )\n",
    "else:\n",
    "    from layer.embedding_sgram import (\n",
    "        Embedding\n",
    "    )\n",
    "\n",
    "from optimizer import (\n",
    "    SGD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding: Embedding = Embedding(\n",
    "    name=\"embedding\",\n",
    "    num_nodes=WINDOW_SIZE,\n",
    "    target_size=TARGET_SIZE,\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    negative_sample_size=SAMPLE_SIZE,\n",
    "    event_vector_size=VECTOR_SIZE,\n",
    "    optimizer=SGD(lr=LR),\n",
    "    dictionary=word_indexing,\n",
    "    weight_initialization_scheme=WEIGHT_SCHEME,\n",
    "    weight_initialization_parameters=WEIGHT_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores ```np.c_[Ye, Ys]``` from the Embedding\n",
    "\n",
    "The 0th column is the scores for ```dot(Bc, Be``` for positive labels. The rest are the scores for ```dot(Bc, Ws)``` for negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "[[-9.1185665e-04 -2.4321437e-04  1.0157871e-04 -1.0421169e-03  5.6947256e-04  1.0440625e-03 -1.3973997e-03  1.0924268e-03  7.8901229e-04 -3.1144917e-04 -8.0015633e-04 -2.2692217e-04 -4.3101949e-04 -3.8034411e-04  1.9083207e-04]\n",
      " [ 5.4251461e-04  7.0913730e-04  2.2273404e-03  4.3803710e-04 -4.3168294e-04  1.2829594e-05  2.6357165e-04 -3.9755483e-05  0.0000000e+00  0.0000000e+00  4.5001745e-04 -6.9760508e-04  6.7859812e-04 -6.6164788e-04 -3.2094374e-04]]\n",
      "\n",
      "Scores for dot(Bc, Be): \n",
      "[[-0.00091186]\n",
      " [ 0.00054251]]\n"
     ]
    }
   ],
   "source": [
    "scores = embedding.function(event_context_pairs)\n",
    "print(f\"Scores:\\n{scores[:5]}\\n\")\n",
    "print(f\"Scores for dot(Bc, Be): \\n{scores[:5, :1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer.normalization import (\n",
    "    BatchNormalization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = BatchNormalization(\n",
    "    name=\"bn\",\n",
    "    num_nodes=1+SAMPLE_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Logistic Log Loss\n",
    "\n",
    "Train the model to get:\n",
    "1. BoW of the context event vectors close to the target event vector. Label 1\n",
    "2. BoW of the context event vectors away from each of the negative sample event vectors Label 0.\n",
    "\n",
    "This is a binary logistic classification, hence use Logistic Log Loss as the network objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.function import (\n",
    "    sigmoid_cross_entropy_log_loss,\n",
    "    sigmoid\n",
    ")\n",
    "from layer.objective import (\n",
    "    CrossEntropyLogLoss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CrossEntropyLogLoss(\n",
    "    name=\"loss\",\n",
    "    num_nodes=1,  # Logistic log loss\n",
    "    log_loss_function=sigmoid_cross_entropy_log_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Adapter\n",
    "\n",
    "The logistic log loss layer expects the input of shape ```(N,M=1)```, however Embedding outputs ```(N,(1+SL)``` where ```SL``` is SAMPLE_SIZE. The ```Adapter``` layer bridges between Embedding and the Loss layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer.adapter import (\n",
    "    Adapter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_function = embedding.adapt_function_to_logistic_log_loss(loss=loss)\n",
    "adapter_gradient = embedding.adapt_gradient_to_logistic_log_loss()\n",
    "\n",
    "adapter: Adapter = Adapter(\n",
    "    name=\"adapter\",\n",
    "    num_nodes=1,    # Number of output M=1 \n",
    "    function=adapter_function,\n",
    "    gradient=adapter_gradient\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Word2vec Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a sequential network\n",
    "\n",
    "$ \\text {Sentences} \\rightarrow EventIndexing \\rightarrow EventContext \\rightarrow  Embedding \\rightarrow Adapter \\rightarrow LogisticLogLoss$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.sequential import (\n",
    "    SequentialNetwork\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = SequentialNetwork(\n",
    "    name=\"word2vec\",\n",
    "    num_nodes=1,\n",
    "    inference_layers=[\n",
    "        word_indexing,\n",
    "        event_context,\n",
    "        embedding,\n",
    "        adapter\n",
    "    ],\n",
    "    objective_layers=[\n",
    "        loss\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved model file if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State file does not exist. Saving the initial model to ../models/word2vec_sgram_ptb_train_E1_C10_S5_Wnormal_std_0.01_V100_LR20.0_N10.pkl.\n"
     ]
    }
   ],
   "source": [
    "if fileio.Function.is_file(STATE_FILE):\n",
    "    print(\"Loading model...\\nSTATE_FILE: %s\" % STATE_FILE)\n",
    "    state = embedding.load(STATE_FILE)\n",
    "\n",
    "    fmt=\"\"\"Model loaded.\n",
    "    event_size %s\n",
    "    context_size: %s\n",
    "    event_vector_size: %s\n",
    "    \"\"\"\n",
    "    print(fmt % (\n",
    "        state[\"target_size\"], \n",
    "        state[\"context_size\"], \n",
    "        state[\"event_vector_size\"]\n",
    "    ))\n",
    "else:\n",
    "    print(\"State file does not exist. Saving the initial model to %s.\" % STATE_FILE)\n",
    "    embedding.save(STATE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 00000 of 10 sentences: Average Loss:   0.693137 Duration 0.211343\n",
      "Batch 00100 of 10 sentences: Average Loss:   0.668239 Duration 0.283479\n",
      "Batch 00200 of 10 sentences: Average Loss:   0.636050 Duration 0.192648\n",
      "Batch 00300 of 10 sentences: Average Loss:   0.612916 Duration 0.420230\n",
      "Batch 00400 of 10 sentences: Average Loss:   0.594639 Duration 0.183814\n",
      "Batch 00500 of 10 sentences: Average Loss:   0.581592 Duration 0.376362\n",
      "Batch 00600 of 10 sentences: Average Loss:   0.572798 Duration 0.172752\n",
      "Batch 00700 of 10 sentences: Average Loss:   0.563167 Duration 0.281529\n",
      "Batch 00800 of 10 sentences: Average Loss:   0.555399 Duration 0.259492\n",
      "Batch 00900 of 10 sentences: Average Loss:   0.549399 Duration 0.272705\n",
      "Batch 01000 of 10 sentences: Average Loss:   0.543852 Duration 0.407641\n",
      "Batch 01100 of 10 sentences: Average Loss:   0.539172 Duration 0.289123\n",
      "Batch 01200 of 10 sentences: Average Loss:   0.534918 Duration 0.254536\n",
      "Batch 01300 of 10 sentences: Average Loss:   0.530460 Duration 0.401049\n",
      "Batch 01400 of 10 sentences: Average Loss:   0.525776 Duration 0.244975\n",
      "Batch 01500 of 10 sentences: Average Loss:   0.522750 Duration 0.191103\n",
      "Batch 01600 of 10 sentences: Average Loss:   0.519390 Duration 0.249695\n",
      "Batch 01700 of 10 sentences: Average Loss:   0.516304 Duration 0.221259\n",
      "Batch 01800 of 10 sentences: Average Loss:   0.513565 Duration 0.265930\n",
      "Batch 01900 of 10 sentences: Average Loss:   0.510889 Duration 0.303209\n",
      "Batch 02000 of 10 sentences: Average Loss:   0.508037 Duration 0.273830\n",
      "Batch 02100 of 10 sentences: Average Loss:   0.505686 Duration 0.261772\n",
      "Batch 02200 of 10 sentences: Average Loss:   0.503053 Duration 0.365279\n",
      "Batch 02300 of 10 sentences: Average Loss:   0.500322 Duration 0.328828\n",
      "Batch 02400 of 10 sentences: Average Loss:   0.498484 Duration 0.244415\n",
      "Batch 02500 of 10 sentences: Average Loss:   0.496072 Duration 0.206160\n",
      "Batch 02600 of 10 sentences: Average Loss:   0.494158 Duration 0.238613\n",
      "Batch 02700 of 10 sentences: Average Loss:   0.492750 Duration 0.321376\n",
      "Batch 02800 of 10 sentences: Average Loss:   0.491098 Duration 0.434486\n",
      "Batch 02900 of 10 sentences: Average Loss:   0.489554 Duration 0.112507\n",
      "Batch 03000 of 10 sentences: Average Loss:   0.487946 Duration 0.195573\n",
      "Batch 03100 of 10 sentences: Average Loss:   0.486685 Duration 0.387284\n",
      "Batch 03200 of 10 sentences: Average Loss:   0.485237 Duration 0.302431\n",
      "Batch 03300 of 10 sentences: Average Loss:   0.483815 Duration 0.238338\n",
      "Batch 03400 of 10 sentences: Average Loss:   0.482549 Duration 0.195041\n",
      "Batch 03500 of 10 sentences: Average Loss:   0.481300 Duration 0.308671\n",
      "Batch 03600 of 10 sentences: Average Loss:   0.480020 Duration 0.358923\n",
      "Batch 03700 of 10 sentences: Average Loss:   0.479072 Duration 0.253361\n",
      "Batch 03800 of 10 sentences: Average Loss:   0.477959 Duration 0.244448\n",
      "Batch 03900 of 10 sentences: Average Loss:   0.476715 Duration 0.296314\n",
      "Batch 04000 of 10 sentences: Average Loss:   0.475446 Duration 0.198472\n",
      "Batch 04100 of 10 sentences: Average Loss:   0.474380 Duration 0.332412\n",
      "Batch 04200 of 10 sentences: Average Loss:   0.473499 Duration 0.169840\n",
      "epoch 0 batches 04207 done\n",
      "Batch 04300 of 10 sentences: Average Loss:   0.472741 Duration 0.379812\n",
      "Batch 04400 of 10 sentences: Average Loss:   0.471944 Duration 0.201968\n",
      "Batch 04500 of 10 sentences: Average Loss:   0.471023 Duration 0.248511\n",
      "Batch 04600 of 10 sentences: Average Loss:   0.469965 Duration 0.249342\n",
      "Batch 04700 of 10 sentences: Average Loss:   0.469188 Duration 0.312931\n",
      "Batch 04800 of 10 sentences: Average Loss:   0.468548 Duration 0.246416\n",
      "Batch 04900 of 10 sentences: Average Loss:   0.467699 Duration 0.205543\n",
      "Batch 05000 of 10 sentences: Average Loss:   0.466856 Duration 0.225995\n",
      "Batch 05100 of 10 sentences: Average Loss:   0.466225 Duration 0.409141\n",
      "Batch 05200 of 10 sentences: Average Loss:   0.465561 Duration 0.206477\n",
      "Batch 05300 of 10 sentences: Average Loss:   0.465026 Duration 0.165252\n",
      "Batch 05400 of 10 sentences: Average Loss:   0.464455 Duration 0.342313\n",
      "Batch 05500 of 10 sentences: Average Loss:   0.463829 Duration 0.241821\n",
      "Batch 05600 of 10 sentences: Average Loss:   0.462993 Duration 0.400646\n",
      "Batch 05700 of 10 sentences: Average Loss:   0.462433 Duration 0.169159\n",
      "Batch 05800 of 10 sentences: Average Loss:   0.461849 Duration 0.308530\n",
      "Batch 05900 of 10 sentences: Average Loss:   0.461257 Duration 0.291807\n",
      "Batch 06000 of 10 sentences: Average Loss:   0.460685 Duration 0.370427\n",
      "Batch 06100 of 10 sentences: Average Loss:   0.460139 Duration 0.229261\n",
      "Batch 06200 of 10 sentences: Average Loss:   0.459531 Duration 0.334804\n",
      "Batch 06300 of 10 sentences: Average Loss:   0.458971 Duration 0.259912\n",
      "Batch 06400 of 10 sentences: Average Loss:   0.458313 Duration 0.333048\n",
      "Batch 06500 of 10 sentences: Average Loss:   0.457542 Duration 0.186094\n",
      "Batch 06600 of 10 sentences: Average Loss:   0.457072 Duration 0.188459\n",
      "Batch 06700 of 10 sentences: Average Loss:   0.456468 Duration 0.245571\n",
      "Batch 06800 of 10 sentences: Average Loss:   0.455854 Duration 0.353215\n",
      "Batch 06900 of 10 sentences: Average Loss:   0.455441 Duration 0.269163\n",
      "Batch 07000 of 10 sentences: Average Loss:   0.454985 Duration 0.180767\n",
      "Batch 07100 of 10 sentences: Average Loss:   0.454530 Duration 0.393786\n",
      "Batch 07200 of 10 sentences: Average Loss:   0.453966 Duration 0.238593\n",
      "Batch 07300 of 10 sentences: Average Loss:   0.453600 Duration 0.276603\n",
      "Batch 07400 of 10 sentences: Average Loss:   0.453105 Duration 0.171775\n",
      "Batch 07500 of 10 sentences: Average Loss:   0.452631 Duration 0.237983\n",
      "Batch 07600 of 10 sentences: Average Loss:   0.452192 Duration 0.223228\n",
      "Batch 07700 of 10 sentences: Average Loss:   0.451723 Duration 0.376202\n",
      "Batch 07800 of 10 sentences: Average Loss:   0.451251 Duration 0.581702\n",
      "Batch 07900 of 10 sentences: Average Loss:   0.450935 Duration 0.352354\n",
      "Batch 08000 of 10 sentences: Average Loss:   0.450500 Duration 0.212490\n",
      "Batch 08100 of 10 sentences: Average Loss:   0.449998 Duration 0.195034\n",
      "Batch 08200 of 10 sentences: Average Loss:   0.449498 Duration 0.431189\n",
      "Batch 08300 of 10 sentences: Average Loss:   0.449050 Duration 0.347273\n",
      "Batch 08400 of 10 sentences: Average Loss:   0.448703 Duration 0.213637\n",
      "epoch 1 batches 08415 done\n",
      "Batch 08500 of 10 sentences: Average Loss:   0.448385 Duration 0.293060\n",
      "Batch 08600 of 10 sentences: Average Loss:   0.448070 Duration 0.453586\n",
      "Batch 08700 of 10 sentences: Average Loss:   0.447710 Duration 0.206971\n",
      "Batch 08800 of 10 sentences: Average Loss:   0.447251 Duration 0.177241\n",
      "Batch 08900 of 10 sentences: Average Loss:   0.446909 Duration 0.274546\n",
      "Batch 09000 of 10 sentences: Average Loss:   0.446652 Duration 0.291930\n",
      "Batch 09100 of 10 sentences: Average Loss:   0.446314 Duration 0.487090\n",
      "Batch 09200 of 10 sentences: Average Loss:   0.445913 Duration 0.471708\n",
      "Batch 09300 of 10 sentences: Average Loss:   0.445664 Duration 0.399354\n",
      "Batch 09400 of 10 sentences: Average Loss:   0.445365 Duration 0.263810\n",
      "Batch 09500 of 10 sentences: Average Loss:   0.445133 Duration 0.398913\n",
      "Batch 09600 of 10 sentences: Average Loss:   0.444835 Duration 0.244507\n",
      "Batch 09700 of 10 sentences: Average Loss:   0.444597 Duration 0.218755\n",
      "Batch 09800 of 10 sentences: Average Loss:   0.444178 Duration 0.258556\n",
      "Batch 09900 of 10 sentences: Average Loss:   0.443918 Duration 0.383352\n",
      "Batch 10000 of 10 sentences: Average Loss:   0.443673 Duration 0.186464\n",
      "Batch 10100 of 10 sentences: Average Loss:   0.443392 Duration 0.309949\n",
      "Batch 10200 of 10 sentences: Average Loss:   0.443138 Duration 0.232402\n",
      "Batch 10300 of 10 sentences: Average Loss:   0.442858 Duration 0.443259\n",
      "Batch 10400 of 10 sentences: Average Loss:   0.442566 Duration 0.321504\n",
      "Batch 10500 of 10 sentences: Average Loss:   0.442241 Duration 0.301989\n",
      "Batch 10600 of 10 sentences: Average Loss:   0.441876 Duration 0.232773\n",
      "Batch 10700 of 10 sentences: Average Loss:   0.441512 Duration 0.290576\n",
      "Batch 10800 of 10 sentences: Average Loss:   0.441230 Duration 0.255927\n",
      "Batch 10900 of 10 sentences: Average Loss:   0.440959 Duration 0.312921\n",
      "Batch 11000 of 10 sentences: Average Loss:   0.440609 Duration 0.174783\n",
      "Batch 11100 of 10 sentences: Average Loss:   0.440386 Duration 0.340395\n",
      "Batch 11200 of 10 sentences: Average Loss:   0.440152 Duration 0.300082\n",
      "Batch 11300 of 10 sentences: Average Loss:   0.439892 Duration 0.355455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11400 of 10 sentences: Average Loss:   0.439596 Duration 0.480985\n",
      "Batch 11500 of 10 sentences: Average Loss:   0.439370 Duration 0.218554\n",
      "Batch 11600 of 10 sentences: Average Loss:   0.439104 Duration 0.298814\n",
      "Batch 11700 of 10 sentences: Average Loss:   0.438841 Duration 0.171770\n",
      "Batch 11800 of 10 sentences: Average Loss:   0.438570 Duration 0.321132\n",
      "Batch 11900 of 10 sentences: Average Loss:   0.438357 Duration 0.389374\n",
      "Batch 12000 of 10 sentences: Average Loss:   0.438090 Duration 0.420473\n",
      "Batch 12100 of 10 sentences: Average Loss:   0.437911 Duration 0.277806\n",
      "Batch 12200 of 10 sentences: Average Loss:   0.437687 Duration 0.134432\n",
      "Batch 12300 of 10 sentences: Average Loss:   0.437359 Duration 0.167764\n",
      "Batch 12400 of 10 sentences: Average Loss:   0.437063 Duration 0.261726\n",
      "Batch 12500 of 10 sentences: Average Loss:   0.436828 Duration 0.289682\n",
      "Batch 12600 of 10 sentences: Average Loss:   0.436605 Duration 0.437866\n",
      "epoch 2 batches 12623 done\n",
      "Batch 12700 of 10 sentences: Average Loss:   0.436424 Duration 0.353098\n",
      "Batch 12800 of 10 sentences: Average Loss:   0.436243 Duration 0.339074\n",
      "Batch 12900 of 10 sentences: Average Loss:   0.436057 Duration 0.370731\n",
      "Batch 13000 of 10 sentences: Average Loss:   0.435763 Duration 0.284835\n",
      "Batch 13100 of 10 sentences: Average Loss:   0.435537 Duration 0.324223\n",
      "Batch 13200 of 10 sentences: Average Loss:   0.435398 Duration 0.250982\n",
      "Batch 13300 of 10 sentences: Average Loss:   0.435196 Duration 0.491314\n",
      "Batch 13400 of 10 sentences: Average Loss:   0.434924 Duration 0.213145\n",
      "Batch 13500 of 10 sentences: Average Loss:   0.434773 Duration 0.247119\n",
      "Batch 13600 of 10 sentences: Average Loss:   0.434603 Duration 0.451632\n",
      "Batch 13700 of 10 sentences: Average Loss:   0.434457 Duration 0.645679\n",
      "Batch 13800 of 10 sentences: Average Loss:   0.434306 Duration 0.223521\n",
      "Batch 13900 of 10 sentences: Average Loss:   0.434162 Duration 0.217689\n",
      "Batch 14000 of 10 sentences: Average Loss:   0.433896 Duration 0.322949\n",
      "Batch 14100 of 10 sentences: Average Loss:   0.433734 Duration 0.268606\n",
      "Batch 14200 of 10 sentences: Average Loss:   0.433605 Duration 0.290929\n",
      "Batch 14300 of 10 sentences: Average Loss:   0.433422 Duration 0.315752\n",
      "Batch 14400 of 10 sentences: Average Loss:   0.433262 Duration 0.256396\n",
      "Batch 14500 of 10 sentences: Average Loss:   0.433076 Duration 0.261133\n",
      "Batch 14600 of 10 sentences: Average Loss:   0.432896 Duration 0.331844\n",
      "Batch 14700 of 10 sentences: Average Loss:   0.432691 Duration 0.169292\n",
      "Batch 14800 of 10 sentences: Average Loss:   0.432499 Duration 0.375960\n",
      "Batch 14900 of 10 sentences: Average Loss:   0.432260 Duration 0.353755\n",
      "Batch 15000 of 10 sentences: Average Loss:   0.432029 Duration 0.246869\n",
      "Batch 15100 of 10 sentences: Average Loss:   0.431862 Duration 0.196038\n",
      "Batch 15200 of 10 sentences: Average Loss:   0.431655 Duration 0.271515\n",
      "Batch 15300 of 10 sentences: Average Loss:   0.431477 Duration 0.322824\n",
      "Batch 15400 of 10 sentences: Average Loss:   0.431324 Duration 0.265615\n",
      "Batch 15500 of 10 sentences: Average Loss:   0.431143 Duration 0.262270\n",
      "Batch 15600 of 10 sentences: Average Loss:   0.430959 Duration 0.316991\n",
      "Batch 15700 of 10 sentences: Average Loss:   0.430811 Duration 0.350712\n",
      "Batch 15800 of 10 sentences: Average Loss:   0.430631 Duration 0.297117\n",
      "Batch 15900 of 10 sentences: Average Loss:   0.430478 Duration 0.364999\n",
      "Batch 16000 of 10 sentences: Average Loss:   0.430270 Duration 0.344086\n",
      "Batch 16100 of 10 sentences: Average Loss:   0.430140 Duration 0.173605\n",
      "Batch 16200 of 10 sentences: Average Loss:   0.429954 Duration 0.236706\n",
      "Batch 16300 of 10 sentences: Average Loss:   0.429830 Duration 0.182566\n",
      "Batch 16400 of 10 sentences: Average Loss:   0.429656 Duration 0.263869\n",
      "Batch 16500 of 10 sentences: Average Loss:   0.429440 Duration 0.232830\n",
      "Batch 16600 of 10 sentences: Average Loss:   0.429225 Duration 0.326340\n",
      "Batch 16700 of 10 sentences: Average Loss:   0.429053 Duration 0.267708\n",
      "Batch 16800 of 10 sentences: Average Loss:   0.428904 Duration 0.248625\n",
      "epoch 3 batches 16831 done\n",
      "Batch 16900 of 10 sentences: Average Loss:   0.428760 Duration 0.175737\n",
      "Batch 17000 of 10 sentences: Average Loss:   0.428641 Duration 0.268104\n",
      "Batch 17100 of 10 sentences: Average Loss:   0.428526 Duration 0.329290\n",
      "Batch 17200 of 10 sentences: Average Loss:   0.428313 Duration 0.258742\n",
      "Batch 17300 of 10 sentences: Average Loss:   0.428149 Duration 0.258643\n",
      "Batch 17400 of 10 sentences: Average Loss:   0.428067 Duration 0.186368\n",
      "Batch 17500 of 10 sentences: Average Loss:   0.427947 Duration 0.184186\n",
      "Batch 17600 of 10 sentences: Average Loss:   0.427760 Duration 0.227871\n",
      "Batch 17700 of 10 sentences: Average Loss:   0.427666 Duration 0.355803\n",
      "Batch 17800 of 10 sentences: Average Loss:   0.427541 Duration 0.303504\n",
      "Batch 17900 of 10 sentences: Average Loss:   0.427434 Duration 0.357272\n",
      "Batch 18000 of 10 sentences: Average Loss:   0.427336 Duration 0.345440\n",
      "Batch 18100 of 10 sentences: Average Loss:   0.427237 Duration 0.414586\n",
      "Batch 18200 of 10 sentences: Average Loss:   0.427058 Duration 0.276645\n",
      "Batch 18300 of 10 sentences: Average Loss:   0.426910 Duration 0.321920\n",
      "Batch 18400 of 10 sentences: Average Loss:   0.426824 Duration 0.279194\n",
      "Batch 18500 of 10 sentences: Average Loss:   0.426678 Duration 0.268667\n",
      "Batch 18600 of 10 sentences: Average Loss:   0.426576 Duration 0.330466\n",
      "Batch 18700 of 10 sentences: Average Loss:   0.426434 Duration 0.372476\n",
      "Batch 18800 of 10 sentences: Average Loss:   0.426294 Duration 0.196198\n",
      "Batch 18900 of 10 sentences: Average Loss:   0.426163 Duration 0.162729\n",
      "Batch 19000 of 10 sentences: Average Loss:   0.426013 Duration 0.207481\n",
      "Batch 19100 of 10 sentences: Average Loss:   0.425840 Duration 0.398012\n",
      "Batch 19200 of 10 sentences: Average Loss:   0.425651 Duration 0.223932\n",
      "Batch 19300 of 10 sentences: Average Loss:   0.425533 Duration 0.164783\n",
      "Batch 19400 of 10 sentences: Average Loss:   0.425394 Duration 0.364202\n",
      "Batch 19500 of 10 sentences: Average Loss:   0.425252 Duration 0.253236\n",
      "Batch 19600 of 10 sentences: Average Loss:   0.425147 Duration 0.316031\n",
      "Batch 19700 of 10 sentences: Average Loss:   0.425025 Duration 0.453282\n",
      "Batch 19800 of 10 sentences: Average Loss:   0.424878 Duration 0.309481\n",
      "Batch 19900 of 10 sentences: Average Loss:   0.424769 Duration 0.222090\n",
      "Batch 20000 of 10 sentences: Average Loss:   0.424626 Duration 0.383839\n",
      "Batch 20100 of 10 sentences: Average Loss:   0.424530 Duration 0.327384\n",
      "Batch 20200 of 10 sentences: Average Loss:   0.424360 Duration 0.261170\n",
      "Batch 20300 of 10 sentences: Average Loss:   0.424277 Duration 0.449739\n",
      "Batch 20400 of 10 sentences: Average Loss:   0.424116 Duration 0.310145\n",
      "Batch 20500 of 10 sentences: Average Loss:   0.424027 Duration 0.258528\n",
      "Batch 20600 of 10 sentences: Average Loss:   0.423914 Duration 0.235014\n",
      "Batch 20700 of 10 sentences: Average Loss:   0.423737 Duration 0.302166\n",
      "Batch 20800 of 10 sentences: Average Loss:   0.423584 Duration 0.248026\n",
      "Batch 20900 of 10 sentences: Average Loss:   0.423435 Duration 0.254568\n",
      "Batch 21000 of 10 sentences: Average Loss:   0.423304 Duration 0.231882\n",
      "epoch 4 batches 21039 done\n",
      "Batch 21100 of 10 sentences: Average Loss:   0.423207 Duration 0.396387\n",
      "Batch 21200 of 10 sentences: Average Loss:   0.423127 Duration 0.351001\n",
      "Batch 21300 of 10 sentences: Average Loss:   0.423042 Duration 0.307743\n",
      "Batch 21400 of 10 sentences: Average Loss:   0.422871 Duration 0.270903\n",
      "Batch 21500 of 10 sentences: Average Loss:   0.422758 Duration 0.420431\n",
      "Batch 21600 of 10 sentences: Average Loss:   0.422692 Duration 0.358144\n",
      "Batch 21700 of 10 sentences: Average Loss:   0.422583 Duration 0.261984\n",
      "Batch 21800 of 10 sentences: Average Loss:   0.422438 Duration 0.241427\n",
      "Batch 21900 of 10 sentences: Average Loss:   0.422352 Duration 0.462462\n",
      "Batch 22000 of 10 sentences: Average Loss:   0.422276 Duration 0.242040\n",
      "Batch 22100 of 10 sentences: Average Loss:   0.422182 Duration 0.382664\n",
      "Batch 22200 of 10 sentences: Average Loss:   0.422114 Duration 0.330384\n",
      "Batch 22300 of 10 sentences: Average Loss:   0.422052 Duration 0.300099\n",
      "Batch 22400 of 10 sentences: Average Loss:   0.421928 Duration 0.403371\n",
      "Batch 22500 of 10 sentences: Average Loss:   0.421808 Duration 0.085313\n",
      "Batch 22600 of 10 sentences: Average Loss:   0.421744 Duration 0.221875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 22700 of 10 sentences: Average Loss:   0.421626 Duration 0.217629\n",
      "Batch 22800 of 10 sentences: Average Loss:   0.421561 Duration 0.479828\n",
      "Batch 22900 of 10 sentences: Average Loss:   0.421454 Duration 0.273519\n",
      "Batch 23000 of 10 sentences: Average Loss:   0.421329 Duration 0.290141\n",
      "Batch 23100 of 10 sentences: Average Loss:   0.421221 Duration 0.180151\n",
      "Batch 23200 of 10 sentences: Average Loss:   0.421106 Duration 0.665356\n",
      "Batch 23300 of 10 sentences: Average Loss:   0.420968 Duration 0.357016\n",
      "Batch 23400 of 10 sentences: Average Loss:   0.420815 Duration 0.172357\n",
      "Batch 23500 of 10 sentences: Average Loss:   0.420732 Duration 0.251593\n",
      "Batch 23600 of 10 sentences: Average Loss:   0.420615 Duration 0.288549\n",
      "Batch 23700 of 10 sentences: Average Loss:   0.420503 Duration 0.397693\n",
      "Batch 23800 of 10 sentences: Average Loss:   0.420421 Duration 0.261572\n",
      "Batch 23900 of 10 sentences: Average Loss:   0.420318 Duration 0.304776\n",
      "Batch 24000 of 10 sentences: Average Loss:   0.420210 Duration 0.336619\n",
      "Batch 24100 of 10 sentences: Average Loss:   0.420117 Duration 0.351768\n",
      "Batch 24200 of 10 sentences: Average Loss:   0.420011 Duration 0.331347\n",
      "Batch 24300 of 10 sentences: Average Loss:   0.419927 Duration 0.459817\n",
      "Batch 24400 of 10 sentences: Average Loss:   0.419794 Duration 0.280574\n",
      "Batch 24500 of 10 sentences: Average Loss:   0.419737 Duration 0.254047\n",
      "Batch 24600 of 10 sentences: Average Loss:   0.419581 Duration 0.443058\n",
      "Batch 24700 of 10 sentences: Average Loss:   0.419506 Duration 0.487797\n",
      "Batch 24800 of 10 sentences: Average Loss:   0.419426 Duration 0.293920\n",
      "Batch 24900 of 10 sentences: Average Loss:   0.419290 Duration 0.307979\n",
      "Batch 25000 of 10 sentences: Average Loss:   0.419191 Duration 0.362619\n",
      "Batch 25100 of 10 sentences: Average Loss:   0.419051 Duration 0.294655\n",
      "Batch 25200 of 10 sentences: Average Loss:   0.418965 Duration 0.316108\n",
      "epoch 5 batches 25247 done\n",
      "Batch 25300 of 10 sentences: Average Loss:   0.418881 Duration 0.312817\n",
      "Batch 25400 of 10 sentences: Average Loss:   0.418806 Duration 0.369367\n",
      "Batch 25500 of 10 sentences: Average Loss:   0.418746 Duration 0.350190\n",
      "Batch 25600 of 10 sentences: Average Loss:   0.418605 Duration 0.509605\n",
      "Batch 25700 of 10 sentences: Average Loss:   0.418509 Duration 0.257579\n",
      "Batch 25800 of 10 sentences: Average Loss:   0.418455 Duration 0.240569\n",
      "Batch 25900 of 10 sentences: Average Loss:   0.418371 Duration 0.404359\n",
      "Batch 26000 of 10 sentences: Average Loss:   0.418240 Duration 0.161827\n",
      "Batch 26100 of 10 sentences: Average Loss:   0.418168 Duration 0.261764\n",
      "Batch 26200 of 10 sentences: Average Loss:   0.418101 Duration 0.279984\n",
      "Batch 26300 of 10 sentences: Average Loss:   0.418026 Duration 0.304750\n",
      "Batch 26400 of 10 sentences: Average Loss:   0.417972 Duration 0.421345\n",
      "Batch 26500 of 10 sentences: Average Loss:   0.417917 Duration 0.303289\n",
      "Batch 26600 of 10 sentences: Average Loss:   0.417814 Duration 0.360124\n",
      "Batch 26700 of 10 sentences: Average Loss:   0.417710 Duration 0.215599\n",
      "Batch 26800 of 10 sentences: Average Loss:   0.417660 Duration 0.269225\n",
      "Batch 26900 of 10 sentences: Average Loss:   0.417561 Duration 0.273911\n",
      "Batch 27000 of 10 sentences: Average Loss:   0.417500 Duration 0.271042\n",
      "Batch 27100 of 10 sentences: Average Loss:   0.417414 Duration 0.294094\n",
      "Batch 27200 of 10 sentences: Average Loss:   0.417318 Duration 0.130673\n",
      "Batch 27300 of 10 sentences: Average Loss:   0.417225 Duration 0.314300\n",
      "Batch 27400 of 10 sentences: Average Loss:   0.417139 Duration 0.314925\n",
      "Batch 27500 of 10 sentences: Average Loss:   0.417036 Duration 0.348475\n",
      "Batch 27600 of 10 sentences: Average Loss:   0.416905 Duration 0.387579\n",
      "Batch 27700 of 10 sentences: Average Loss:   0.416840 Duration 0.381188\n",
      "Batch 27800 of 10 sentences: Average Loss:   0.416736 Duration 0.284829\n",
      "Batch 27900 of 10 sentences: Average Loss:   0.416645 Duration 0.409493\n",
      "Batch 28000 of 10 sentences: Average Loss:   0.416571 Duration 0.380115\n",
      "Batch 28100 of 10 sentences: Average Loss:   0.416489 Duration 0.370929\n",
      "Batch 28200 of 10 sentences: Average Loss:   0.416398 Duration 0.351206\n",
      "Batch 28300 of 10 sentences: Average Loss:   0.416316 Duration 0.208780\n",
      "Batch 28400 of 10 sentences: Average Loss:   0.416231 Duration 0.125571\n",
      "Batch 28500 of 10 sentences: Average Loss:   0.416159 Duration 0.171673\n",
      "Batch 28600 of 10 sentences: Average Loss:   0.416064 Duration 0.418858\n",
      "Batch 28700 of 10 sentences: Average Loss:   0.416017 Duration 0.173111\n",
      "Batch 28800 of 10 sentences: Average Loss:   0.415898 Duration 0.241063\n",
      "Batch 28900 of 10 sentences: Average Loss:   0.415836 Duration 0.152658\n",
      "Batch 29000 of 10 sentences: Average Loss:   0.415762 Duration 0.424482\n",
      "Batch 29100 of 10 sentences: Average Loss:   0.415646 Duration 0.227160\n",
      "Batch 29200 of 10 sentences: Average Loss:   0.415550 Duration 0.281050\n",
      "Batch 29300 of 10 sentences: Average Loss:   0.415431 Duration 0.236839\n",
      "Batch 29400 of 10 sentences: Average Loss:   0.415349 Duration 0.337782\n",
      "epoch 6 batches 29455 done\n",
      "Batch 29500 of 10 sentences: Average Loss:   0.415273 Duration 0.173292\n",
      "Batch 29600 of 10 sentences: Average Loss:   0.415211 Duration 0.136747\n",
      "Batch 29700 of 10 sentences: Average Loss:   0.415158 Duration 0.209019\n",
      "Batch 29800 of 10 sentences: Average Loss:   0.415045 Duration 0.197929\n",
      "Batch 29900 of 10 sentences: Average Loss:   0.414948 Duration 0.254488\n",
      "Batch 30000 of 10 sentences: Average Loss:   0.414900 Duration 0.245868\n",
      "Batch 30100 of 10 sentences: Average Loss:   0.414838 Duration 0.177978\n",
      "Batch 30200 of 10 sentences: Average Loss:   0.414735 Duration 0.336648\n",
      "Batch 30300 of 10 sentences: Average Loss:   0.414664 Duration 0.215074\n",
      "Batch 30400 of 10 sentences: Average Loss:   0.414609 Duration 0.268651\n",
      "Batch 30500 of 10 sentences: Average Loss:   0.414540 Duration 0.166873\n",
      "Batch 30600 of 10 sentences: Average Loss:   0.414500 Duration 0.172803\n",
      "Batch 30700 of 10 sentences: Average Loss:   0.414455 Duration 0.132920\n",
      "Batch 30800 of 10 sentences: Average Loss:   0.414368 Duration 0.208095\n",
      "Batch 30900 of 10 sentences: Average Loss:   0.414281 Duration 0.344134\n",
      "Batch 31000 of 10 sentences: Average Loss:   0.414233 Duration 0.254278\n",
      "Batch 31100 of 10 sentences: Average Loss:   0.414156 Duration 0.214707\n",
      "Batch 31200 of 10 sentences: Average Loss:   0.414105 Duration 0.164656\n",
      "Batch 31300 of 10 sentences: Average Loss:   0.414033 Duration 0.164355\n",
      "Batch 31400 of 10 sentences: Average Loss:   0.413957 Duration 0.316981\n",
      "Batch 31500 of 10 sentences: Average Loss:   0.413875 Duration 0.198810\n",
      "Batch 31600 of 10 sentences: Average Loss:   0.413794 Duration 0.159014\n",
      "Batch 31700 of 10 sentences: Average Loss:   0.413707 Duration 0.176036\n",
      "Batch 31800 of 10 sentences: Average Loss:   0.413582 Duration 0.224286\n",
      "Batch 31900 of 10 sentences: Average Loss:   0.413525 Duration 0.151234\n",
      "Batch 32000 of 10 sentences: Average Loss:   0.413441 Duration 0.274235\n",
      "Batch 32100 of 10 sentences: Average Loss:   0.413360 Duration 0.270045\n",
      "Batch 32200 of 10 sentences: Average Loss:   0.413305 Duration 0.289168\n",
      "Batch 32300 of 10 sentences: Average Loss:   0.413231 Duration 0.235460\n",
      "Batch 32400 of 10 sentences: Average Loss:   0.413153 Duration 0.247747\n",
      "Batch 32500 of 10 sentences: Average Loss:   0.413086 Duration 0.219853\n",
      "Batch 32600 of 10 sentences: Average Loss:   0.413017 Duration 0.178631\n",
      "Batch 32700 of 10 sentences: Average Loss:   0.412947 Duration 0.187430\n",
      "Batch 32800 of 10 sentences: Average Loss:   0.412866 Duration 0.162193\n",
      "Batch 32900 of 10 sentences: Average Loss:   0.412816 Duration 0.240204\n",
      "Batch 33000 of 10 sentences: Average Loss:   0.412718 Duration 0.185059\n",
      "Batch 33100 of 10 sentences: Average Loss:   0.412665 Duration 0.425472\n",
      "Batch 33200 of 10 sentences: Average Loss:   0.412608 Duration 0.173614\n",
      "Batch 33300 of 10 sentences: Average Loss:   0.412499 Duration 0.193204\n",
      "Batch 33400 of 10 sentences: Average Loss:   0.412418 Duration 0.219466\n",
      "Batch 33500 of 10 sentences: Average Loss:   0.412318 Duration 0.131088\n",
      "Batch 33600 of 10 sentences: Average Loss:   0.412242 Duration 0.290217\n",
      "epoch 7 batches 33663 done\n",
      "Batch 33700 of 10 sentences: Average Loss:   0.412188 Duration 0.368502\n",
      "Batch 33800 of 10 sentences: Average Loss:   0.412133 Duration 0.177492\n",
      "Batch 33900 of 10 sentences: Average Loss:   0.412081 Duration 0.138785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 34000 of 10 sentences: Average Loss:   0.411990 Duration 0.232713\n",
      "Batch 34100 of 10 sentences: Average Loss:   0.411904 Duration 0.231471\n",
      "Batch 34200 of 10 sentences: Average Loss:   0.411857 Duration 0.372552\n",
      "Batch 34300 of 10 sentences: Average Loss:   0.411814 Duration 0.211706\n",
      "Batch 34400 of 10 sentences: Average Loss:   0.411731 Duration 0.145076\n",
      "Batch 34500 of 10 sentences: Average Loss:   0.411666 Duration 0.336111\n",
      "Batch 34600 of 10 sentences: Average Loss:   0.411614 Duration 0.272889\n",
      "Batch 34700 of 10 sentences: Average Loss:   0.411558 Duration 0.234218\n",
      "Batch 34800 of 10 sentences: Average Loss:   0.411529 Duration 0.163997\n",
      "Batch 34900 of 10 sentences: Average Loss:   0.411485 Duration 0.169789\n",
      "Batch 35000 of 10 sentences: Average Loss:   0.411413 Duration 0.199089\n",
      "Batch 35100 of 10 sentences: Average Loss:   0.411337 Duration 0.256553\n",
      "Batch 35200 of 10 sentences: Average Loss:   0.411293 Duration 0.152184\n",
      "Batch 35300 of 10 sentences: Average Loss:   0.411230 Duration 0.153118\n",
      "Batch 35400 of 10 sentences: Average Loss:   0.411189 Duration 0.250784\n",
      "Batch 35500 of 10 sentences: Average Loss:   0.411125 Duration 0.200458\n",
      "Batch 35600 of 10 sentences: Average Loss:   0.411053 Duration 0.148759\n",
      "Batch 35700 of 10 sentences: Average Loss:   0.410982 Duration 0.199674\n",
      "Batch 35800 of 10 sentences: Average Loss:   0.410903 Duration 0.077965\n",
      "Batch 35900 of 10 sentences: Average Loss:   0.410829 Duration 0.220070\n",
      "Batch 36000 of 10 sentences: Average Loss:   0.410720 Duration 0.124969\n",
      "Batch 36100 of 10 sentences: Average Loss:   0.410677 Duration 0.169588\n",
      "Batch 36200 of 10 sentences: Average Loss:   0.410603 Duration 0.183789\n",
      "Batch 36300 of 10 sentences: Average Loss:   0.410522 Duration 0.160912\n",
      "Batch 36400 of 10 sentences: Average Loss:   0.410474 Duration 0.212546\n",
      "Batch 36500 of 10 sentences: Average Loss:   0.410411 Duration 0.221913\n",
      "Batch 36600 of 10 sentences: Average Loss:   0.410350 Duration 0.167230\n",
      "Batch 36700 of 10 sentences: Average Loss:   0.410295 Duration 0.188459\n",
      "Batch 36800 of 10 sentences: Average Loss:   0.410242 Duration 0.185610\n",
      "Batch 36900 of 10 sentences: Average Loss:   0.410180 Duration 0.203705\n",
      "Batch 37000 of 10 sentences: Average Loss:   0.410113 Duration 0.243279\n",
      "Batch 37100 of 10 sentences: Average Loss:   0.410065 Duration 0.235855\n",
      "Batch 37200 of 10 sentences: Average Loss:   0.409978 Duration 0.065647\n",
      "Batch 37300 of 10 sentences: Average Loss:   0.409920 Duration 0.172145\n",
      "Batch 37400 of 10 sentences: Average Loss:   0.409875 Duration 0.170219\n",
      "Batch 37500 of 10 sentences: Average Loss:   0.409780 Duration 0.199295\n",
      "Batch 37600 of 10 sentences: Average Loss:   0.409700 Duration 0.160885\n",
      "Batch 37700 of 10 sentences: Average Loss:   0.409609 Duration 0.121414\n",
      "Batch 37800 of 10 sentences: Average Loss:   0.409540 Duration 0.160867\n",
      "epoch 8 batches 37871 done\n",
      "Batch 37900 of 10 sentences: Average Loss:   0.409488 Duration 0.212317\n",
      "Batch 38000 of 10 sentences: Average Loss:   0.409447 Duration 0.188601\n",
      "Batch 38100 of 10 sentences: Average Loss:   0.409400 Duration 0.316155\n",
      "Batch 38200 of 10 sentences: Average Loss:   0.409318 Duration 0.185943\n",
      "Batch 38300 of 10 sentences: Average Loss:   0.409244 Duration 0.351670\n",
      "Batch 38400 of 10 sentences: Average Loss:   0.409195 Duration 0.147873\n",
      "Batch 38500 of 10 sentences: Average Loss:   0.409157 Duration 0.204007\n",
      "Batch 38600 of 10 sentences: Average Loss:   0.409089 Duration 0.199505\n",
      "Batch 38700 of 10 sentences: Average Loss:   0.409017 Duration 0.180654\n",
      "Batch 38800 of 10 sentences: Average Loss:   0.408977 Duration 0.232033\n",
      "Batch 38900 of 10 sentences: Average Loss:   0.408924 Duration 0.230428\n",
      "Batch 39000 of 10 sentences: Average Loss:   0.408901 Duration 0.217670\n",
      "Batch 39100 of 10 sentences: Average Loss:   0.408858 Duration 0.148431\n",
      "Batch 39200 of 10 sentences: Average Loss:   0.408792 Duration 0.204815\n",
      "Batch 39300 of 10 sentences: Average Loss:   0.408724 Duration 0.168644\n",
      "Batch 39400 of 10 sentences: Average Loss:   0.408691 Duration 0.205545\n",
      "Batch 39500 of 10 sentences: Average Loss:   0.408633 Duration 0.177476\n",
      "Batch 39600 of 10 sentences: Average Loss:   0.408588 Duration 0.167150\n",
      "Batch 39700 of 10 sentences: Average Loss:   0.408538 Duration 0.141181\n",
      "Batch 39800 of 10 sentences: Average Loss:   0.408471 Duration 0.106747\n",
      "Batch 39900 of 10 sentences: Average Loss:   0.408407 Duration 0.220306\n",
      "Batch 40000 of 10 sentences: Average Loss:   0.408338 Duration 0.063635\n",
      "Batch 40100 of 10 sentences: Average Loss:   0.408270 Duration 0.192704\n",
      "Batch 40200 of 10 sentences: Average Loss:   0.408177 Duration 0.196902\n",
      "Batch 40300 of 10 sentences: Average Loss:   0.408140 Duration 0.159810\n",
      "Batch 40400 of 10 sentences: Average Loss:   0.408071 Duration 0.167927\n",
      "Batch 40500 of 10 sentences: Average Loss:   0.408001 Duration 0.133949\n",
      "Batch 40600 of 10 sentences: Average Loss:   0.407956 Duration 0.179524\n",
      "Batch 40700 of 10 sentences: Average Loss:   0.407909 Duration 0.217240\n",
      "Batch 40800 of 10 sentences: Average Loss:   0.407851 Duration 0.219485\n",
      "Batch 40900 of 10 sentences: Average Loss:   0.407796 Duration 0.165023\n",
      "Batch 41000 of 10 sentences: Average Loss:   0.407754 Duration 0.184698\n",
      "Batch 41100 of 10 sentences: Average Loss:   0.407696 Duration 0.162444\n",
      "Batch 41200 of 10 sentences: Average Loss:   0.407639 Duration 0.142140\n",
      "Batch 41300 of 10 sentences: Average Loss:   0.407592 Duration 0.139743\n",
      "Batch 41400 of 10 sentences: Average Loss:   0.407523 Duration 0.154620\n",
      "Batch 41500 of 10 sentences: Average Loss:   0.407458 Duration 0.246761\n",
      "Batch 41600 of 10 sentences: Average Loss:   0.407423 Duration 0.189086\n",
      "Batch 41700 of 10 sentences: Average Loss:   0.407345 Duration 0.229595\n",
      "Batch 41800 of 10 sentences: Average Loss:   0.407277 Duration 0.232005\n",
      "Batch 41900 of 10 sentences: Average Loss:   0.407194 Duration 0.149840\n",
      "Batch 42000 of 10 sentences: Average Loss:   0.407130 Duration 0.187060\n",
      "epoch 9 batches 42079 done\n",
      "Batch 42100 of 10 sentences: Average Loss:   0.407078 Duration 0.133631\n",
      "Batch 42200 of 10 sentences: Average Loss:   0.407042 Duration 0.160405\n",
      "Batch 42300 of 10 sentences: Average Loss:   0.407002 Duration 0.177758\n",
      "Batch 42400 of 10 sentences: Average Loss:   0.406928 Duration 0.186796\n",
      "Batch 42500 of 10 sentences: Average Loss:   0.406865 Duration 0.154454\n",
      "Batch 42600 of 10 sentences: Average Loss:   0.406814 Duration 0.138002\n",
      "Batch 42700 of 10 sentences: Average Loss:   0.406783 Duration 0.184362\n",
      "Batch 42800 of 10 sentences: Average Loss:   0.406718 Duration 0.174518\n",
      "Batch 42900 of 10 sentences: Average Loss:   0.406650 Duration 0.195182\n",
      "Batch 43000 of 10 sentences: Average Loss:   0.406613 Duration 0.102122\n",
      "Batch 43100 of 10 sentences: Average Loss:   0.406574 Duration 0.189029\n",
      "Batch 43200 of 10 sentences: Average Loss:   0.406549 Duration 0.232934\n",
      "Batch 43300 of 10 sentences: Average Loss:   0.406511 Duration 0.193102\n",
      "Batch 43400 of 10 sentences: Average Loss:   0.406461 Duration 0.198300\n",
      "Batch 43500 of 10 sentences: Average Loss:   0.406395 Duration 0.168914\n",
      "Batch 43600 of 10 sentences: Average Loss:   0.406362 Duration 0.186597\n",
      "Batch 43700 of 10 sentences: Average Loss:   0.406316 Duration 0.171672\n",
      "Batch 43800 of 10 sentences: Average Loss:   0.406273 Duration 0.254367\n",
      "Batch 43900 of 10 sentences: Average Loss:   0.406233 Duration 0.245008\n",
      "Batch 44000 of 10 sentences: Average Loss:   0.406174 Duration 0.164803\n",
      "Batch 44100 of 10 sentences: Average Loss:   0.406112 Duration 0.212235\n",
      "Batch 44200 of 10 sentences: Average Loss:   0.406059 Duration 0.204234\n",
      "Batch 44300 of 10 sentences: Average Loss:   0.405985 Duration 0.215178\n",
      "Batch 44400 of 10 sentences: Average Loss:   0.405903 Duration 0.129381\n",
      "Batch 44500 of 10 sentences: Average Loss:   0.405865 Duration 0.177215\n",
      "Batch 44600 of 10 sentences: Average Loss:   0.405796 Duration 0.279383\n",
      "Batch 44700 of 10 sentences: Average Loss:   0.405729 Duration 0.122113\n",
      "Batch 44800 of 10 sentences: Average Loss:   0.405690 Duration 0.259768\n",
      "Batch 44900 of 10 sentences: Average Loss:   0.405641 Duration 0.156797\n",
      "Batch 45000 of 10 sentences: Average Loss:   0.405588 Duration 0.246334\n",
      "Batch 45100 of 10 sentences: Average Loss:   0.405541 Duration 0.241993\n",
      "Batch 45200 of 10 sentences: Average Loss:   0.405498 Duration 0.168687\n",
      "Batch 45300 of 10 sentences: Average Loss:   0.405442 Duration 0.209086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 45400 of 10 sentences: Average Loss:   0.405388 Duration 0.227461\n",
      "Batch 45500 of 10 sentences: Average Loss:   0.405346 Duration 0.197183\n",
      "Batch 45600 of 10 sentences: Average Loss:   0.405281 Duration 0.231802\n",
      "Batch 45700 of 10 sentences: Average Loss:   0.405220 Duration 0.231430\n",
      "Batch 45800 of 10 sentences: Average Loss:   0.405185 Duration 0.198672\n",
      "Batch 45900 of 10 sentences: Average Loss:   0.405112 Duration 0.209959\n",
      "Batch 46000 of 10 sentences: Average Loss:   0.405050 Duration 0.171040\n",
      "Batch 46100 of 10 sentences: Average Loss:   0.404975 Duration 0.201131\n",
      "Batch 46200 of 10 sentences: Average Loss:   0.404918 Duration 0.166828\n",
      "epoch 10 batches 46287 done\n",
      "Batch 46300 of 10 sentences: Average Loss:   0.404874 Duration 0.181416\n",
      "Batch 46400 of 10 sentences: Average Loss:   0.404840 Duration 0.195611\n",
      "Batch 46500 of 10 sentences: Average Loss:   0.404803 Duration 0.124634\n",
      "Batch 46600 of 10 sentences: Average Loss:   0.404744 Duration 0.251456\n",
      "Batch 46700 of 10 sentences: Average Loss:   0.404686 Duration 0.204163\n",
      "Batch 46800 of 10 sentences: Average Loss:   0.404633 Duration 0.253350\n",
      "Batch 46900 of 10 sentences: Average Loss:   0.404605 Duration 0.188618\n",
      "Batch 47000 of 10 sentences: Average Loss:   0.404543 Duration 0.218926\n",
      "Batch 47100 of 10 sentences: Average Loss:   0.404487 Duration 0.136214\n",
      "Batch 47200 of 10 sentences: Average Loss:   0.404449 Duration 0.109002\n",
      "Batch 47300 of 10 sentences: Average Loss:   0.404411 Duration 0.264505\n",
      "Batch 47400 of 10 sentences: Average Loss:   0.404392 Duration 0.229535\n",
      "Batch 47500 of 10 sentences: Average Loss:   0.404352 Duration 0.236313\n",
      "Batch 47600 of 10 sentences: Average Loss:   0.404314 Duration 0.123178\n",
      "Batch 47700 of 10 sentences: Average Loss:   0.404251 Duration 0.198565\n",
      "Batch 47800 of 10 sentences: Average Loss:   0.404217 Duration 0.195512\n",
      "Batch 47900 of 10 sentences: Average Loss:   0.404179 Duration 0.301190\n",
      "Batch 48000 of 10 sentences: Average Loss:   0.404138 Duration 0.179718\n",
      "Batch 48100 of 10 sentences: Average Loss:   0.404101 Duration 0.145605\n",
      "Batch 48200 of 10 sentences: Average Loss:   0.404048 Duration 0.300759\n",
      "Batch 48300 of 10 sentences: Average Loss:   0.403997 Duration 0.219570\n",
      "Batch 48400 of 10 sentences: Average Loss:   0.403946 Duration 0.372850\n",
      "Batch 48500 of 10 sentences: Average Loss:   0.403877 Duration 0.165100\n",
      "Batch 48600 of 10 sentences: Average Loss:   0.403804 Duration 0.171986\n",
      "Batch 48700 of 10 sentences: Average Loss:   0.403762 Duration 0.189335\n",
      "Batch 48800 of 10 sentences: Average Loss:   0.403695 Duration 0.208512\n",
      "Batch 48900 of 10 sentences: Average Loss:   0.403632 Duration 0.202754\n",
      "Batch 49000 of 10 sentences: Average Loss:   0.403590 Duration 0.215350\n",
      "Batch 49100 of 10 sentences: Average Loss:   0.403554 Duration 0.186237\n",
      "Batch 49200 of 10 sentences: Average Loss:   0.403507 Duration 0.185347\n",
      "Batch 49300 of 10 sentences: Average Loss:   0.403461 Duration 0.332006\n",
      "Batch 49400 of 10 sentences: Average Loss:   0.403422 Duration 0.203778\n",
      "Batch 49500 of 10 sentences: Average Loss:   0.403372 Duration 0.141513\n",
      "Batch 49600 of 10 sentences: Average Loss:   0.403322 Duration 0.278301\n",
      "Batch 49700 of 10 sentences: Average Loss:   0.403282 Duration 0.138294\n",
      "Batch 49800 of 10 sentences: Average Loss:   0.403223 Duration 0.170940\n",
      "Batch 49900 of 10 sentences: Average Loss:   0.403167 Duration 0.187320\n",
      "Batch 50000 of 10 sentences: Average Loss:   0.403135 Duration 0.241568\n",
      "Batch 50100 of 10 sentences: Average Loss:   0.403072 Duration 0.154139\n",
      "Batch 50200 of 10 sentences: Average Loss:   0.403012 Duration 0.250215\n",
      "Batch 50300 of 10 sentences: Average Loss:   0.402938 Duration 0.172023\n",
      "Batch 50400 of 10 sentences: Average Loss:   0.402888 Duration 0.253046\n",
      "epoch 11 batches 50495 done\n",
      "Batch 50500 of 10 sentences: Average Loss:   0.402849 Duration 0.201961\n",
      "Batch 50600 of 10 sentences: Average Loss:   0.402816 Duration 0.129593\n",
      "Batch 50700 of 10 sentences: Average Loss:   0.402780 Duration 0.175459\n",
      "Batch 50800 of 10 sentences: Average Loss:   0.402728 Duration 0.123078\n",
      "Batch 50900 of 10 sentences: Average Loss:   0.402668 Duration 0.190635\n",
      "Batch 51000 of 10 sentences: Average Loss:   0.402618 Duration 0.284022\n",
      "Batch 51100 of 10 sentences: Average Loss:   0.402594 Duration 0.227961\n",
      "Batch 51200 of 10 sentences: Average Loss:   0.402543 Duration 0.250490\n",
      "Batch 51300 of 10 sentences: Average Loss:   0.402487 Duration 0.177326\n",
      "Batch 51400 of 10 sentences: Average Loss:   0.402451 Duration 0.139061\n",
      "Batch 51500 of 10 sentences: Average Loss:   0.402414 Duration 0.188802\n",
      "Batch 51600 of 10 sentences: Average Loss:   0.402390 Duration 0.204556\n",
      "Batch 51700 of 10 sentences: Average Loss:   0.402353 Duration 0.207830\n",
      "Batch 51800 of 10 sentences: Average Loss:   0.402318 Duration 0.203913\n",
      "Batch 51900 of 10 sentences: Average Loss:   0.402260 Duration 0.140043\n",
      "Batch 52000 of 10 sentences: Average Loss:   0.402234 Duration 0.171975\n",
      "Batch 52100 of 10 sentences: Average Loss:   0.402195 Duration 0.213181\n",
      "Batch 52200 of 10 sentences: Average Loss:   0.402155 Duration 0.268693\n",
      "Batch 52300 of 10 sentences: Average Loss:   0.402118 Duration 0.201494\n",
      "Batch 52400 of 10 sentences: Average Loss:   0.402073 Duration 0.253495\n",
      "Batch 52500 of 10 sentences: Average Loss:   0.402021 Duration 0.154794\n",
      "Batch 52600 of 10 sentences: Average Loss:   0.401977 Duration 0.159760\n",
      "Batch 52700 of 10 sentences: Average Loss:   0.401915 Duration 0.207055\n",
      "Batch 52800 of 10 sentences: Average Loss:   0.401851 Duration 0.252791\n",
      "Batch 52900 of 10 sentences: Average Loss:   0.401814 Duration 0.401966\n",
      "Batch 53000 of 10 sentences: Average Loss:   0.401755 Duration 0.196023\n",
      "Batch 53100 of 10 sentences: Average Loss:   0.401695 Duration 0.110776\n",
      "Batch 53200 of 10 sentences: Average Loss:   0.401658 Duration 0.156007\n",
      "Batch 53300 of 10 sentences: Average Loss:   0.401614 Duration 0.144884\n",
      "Batch 53400 of 10 sentences: Average Loss:   0.401570 Duration 0.086673\n",
      "Batch 53500 of 10 sentences: Average Loss:   0.401523 Duration 0.156114\n",
      "Batch 53600 of 10 sentences: Average Loss:   0.401489 Duration 0.238965\n",
      "Batch 53700 of 10 sentences: Average Loss:   0.401446 Duration 0.114158\n",
      "Batch 53800 of 10 sentences: Average Loss:   0.401404 Duration 0.231467\n",
      "Batch 53900 of 10 sentences: Average Loss:   0.401357 Duration 0.480226\n",
      "Batch 54000 of 10 sentences: Average Loss:   0.401305 Duration 0.178697\n",
      "Batch 54100 of 10 sentences: Average Loss:   0.401256 Duration 0.296951\n",
      "Batch 54200 of 10 sentences: Average Loss:   0.401224 Duration 0.189878\n",
      "Batch 54300 of 10 sentences: Average Loss:   0.401172 Duration 0.318540\n",
      "Batch 54400 of 10 sentences: Average Loss:   0.401114 Duration 0.255338\n",
      "Batch 54500 of 10 sentences: Average Loss:   0.401051 Duration 0.259252\n",
      "Batch 54600 of 10 sentences: Average Loss:   0.400997 Duration 0.330638\n",
      "Batch 54700 of 10 sentences: Average Loss:   0.400957 Duration 0.243951\n",
      "epoch 12 batches 54703 done\n",
      "Batch 54800 of 10 sentences: Average Loss:   0.400928 Duration 0.180791\n",
      "Batch 54900 of 10 sentences: Average Loss:   0.400893 Duration 0.367115\n",
      "Batch 55000 of 10 sentences: Average Loss:   0.400849 Duration 0.273046\n",
      "Batch 55100 of 10 sentences: Average Loss:   0.400786 Duration 0.226476\n",
      "Batch 55200 of 10 sentences: Average Loss:   0.400745 Duration 0.249923\n",
      "Batch 55300 of 10 sentences: Average Loss:   0.400719 Duration 0.252022\n",
      "Batch 55400 of 10 sentences: Average Loss:   0.400675 Duration 0.332933\n",
      "Batch 55500 of 10 sentences: Average Loss:   0.400626 Duration 0.136991\n",
      "Batch 55600 of 10 sentences: Average Loss:   0.400593 Duration 0.193554\n",
      "Batch 55700 of 10 sentences: Average Loss:   0.400556 Duration 0.199622\n",
      "Batch 55800 of 10 sentences: Average Loss:   0.400534 Duration 0.333866\n",
      "Batch 55900 of 10 sentences: Average Loss:   0.400503 Duration 0.422608\n",
      "Batch 56000 of 10 sentences: Average Loss:   0.400472 Duration 0.239085\n",
      "Batch 56100 of 10 sentences: Average Loss:   0.400415 Duration 0.216191\n",
      "Batch 56200 of 10 sentences: Average Loss:   0.400391 Duration 0.151515\n",
      "Batch 56300 of 10 sentences: Average Loss:   0.400357 Duration 0.364451\n",
      "Batch 56400 of 10 sentences: Average Loss:   0.400320 Duration 0.206520\n",
      "Batch 56500 of 10 sentences: Average Loss:   0.400281 Duration 0.271152\n",
      "Batch 56600 of 10 sentences: Average Loss:   0.400243 Duration 0.274988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 56700 of 10 sentences: Average Loss:   0.400193 Duration 0.189291\n",
      "Batch 56800 of 10 sentences: Average Loss:   0.400154 Duration 0.288986\n",
      "Batch 56900 of 10 sentences: Average Loss:   0.400092 Duration 0.252168\n",
      "Batch 57000 of 10 sentences: Average Loss:   0.400029 Duration 0.158776\n",
      "Batch 57100 of 10 sentences: Average Loss:   0.399990 Duration 0.201000\n",
      "Batch 57200 of 10 sentences: Average Loss:   0.399939 Duration 0.235571\n",
      "Batch 57300 of 10 sentences: Average Loss:   0.399881 Duration 0.209062\n",
      "Batch 57400 of 10 sentences: Average Loss:   0.399843 Duration 0.198005\n",
      "Batch 57500 of 10 sentences: Average Loss:   0.399803 Duration 0.321909\n",
      "Batch 57600 of 10 sentences: Average Loss:   0.399773 Duration 0.228950\n",
      "Batch 57700 of 10 sentences: Average Loss:   0.399718 Duration 0.221080\n",
      "Batch 57800 of 10 sentences: Average Loss:   0.399691 Duration 0.219162\n",
      "Batch 57900 of 10 sentences: Average Loss:   0.399649 Duration 0.320042\n",
      "Batch 58000 of 10 sentences: Average Loss:   0.399611 Duration 0.300613\n",
      "Batch 58100 of 10 sentences: Average Loss:   0.399571 Duration 0.257860\n",
      "Batch 58200 of 10 sentences: Average Loss:   0.399525 Duration 0.283135\n",
      "Batch 58300 of 10 sentences: Average Loss:   0.399478 Duration 0.364279\n",
      "Batch 58400 of 10 sentences: Average Loss:   0.399451 Duration 0.177494\n",
      "Batch 58500 of 10 sentences: Average Loss:   0.399402 Duration 0.316706\n",
      "Batch 58600 of 10 sentences: Average Loss:   0.399348 Duration 0.177595\n",
      "Batch 58700 of 10 sentences: Average Loss:   0.399288 Duration 0.297760\n",
      "Batch 58800 of 10 sentences: Average Loss:   0.399240 Duration 0.223268\n",
      "Batch 58900 of 10 sentences: Average Loss:   0.399201 Duration 0.232761\n",
      "epoch 13 batches 58911 done\n",
      "Batch 59000 of 10 sentences: Average Loss:   0.399167 Duration 0.184535\n",
      "Batch 59100 of 10 sentences: Average Loss:   0.399136 Duration 0.243943\n",
      "Batch 59200 of 10 sentences: Average Loss:   0.399093 Duration 0.269414\n",
      "Batch 59300 of 10 sentences: Average Loss:   0.399038 Duration 0.254372\n",
      "Batch 59400 of 10 sentences: Average Loss:   0.398999 Duration 0.260979\n",
      "Batch 59500 of 10 sentences: Average Loss:   0.398972 Duration 0.243512\n",
      "Batch 59600 of 10 sentences: Average Loss:   0.398933 Duration 0.241043\n",
      "Batch 59700 of 10 sentences: Average Loss:   0.398883 Duration 0.299105\n",
      "Batch 59800 of 10 sentences: Average Loss:   0.398853 Duration 0.176228\n",
      "Batch 59900 of 10 sentences: Average Loss:   0.398816 Duration 0.305472\n",
      "Batch 60000 of 10 sentences: Average Loss:   0.398792 Duration 0.265949\n",
      "Batch 60100 of 10 sentences: Average Loss:   0.398760 Duration 0.279715\n",
      "Batch 60200 of 10 sentences: Average Loss:   0.398735 Duration 0.238121\n",
      "Batch 60300 of 10 sentences: Average Loss:   0.398682 Duration 0.182467\n",
      "Batch 60400 of 10 sentences: Average Loss:   0.398655 Duration 0.217178\n",
      "Batch 60500 of 10 sentences: Average Loss:   0.398625 Duration 0.143877\n",
      "Batch 60600 of 10 sentences: Average Loss:   0.398590 Duration 0.252426\n",
      "Batch 60700 of 10 sentences: Average Loss:   0.398555 Duration 0.296486\n",
      "Batch 60800 of 10 sentences: Average Loss:   0.398518 Duration 0.256218\n",
      "Batch 60900 of 10 sentences: Average Loss:   0.398479 Duration 0.222350\n",
      "Batch 61000 of 10 sentences: Average Loss:   0.398436 Duration 0.209505\n",
      "Batch 61100 of 10 sentences: Average Loss:   0.398380 Duration 0.292731\n",
      "Batch 61200 of 10 sentences: Average Loss:   0.398325 Duration 0.186948\n",
      "Batch 61300 of 10 sentences: Average Loss:   0.398283 Duration 0.269740\n",
      "Batch 61400 of 10 sentences: Average Loss:   0.398248 Duration 0.259539\n",
      "Batch 61500 of 10 sentences: Average Loss:   0.398187 Duration 0.203630\n",
      "Batch 61600 of 10 sentences: Average Loss:   0.398158 Duration 0.202387\n",
      "Batch 61700 of 10 sentences: Average Loss:   0.398128 Duration 0.232971\n",
      "Batch 61800 of 10 sentences: Average Loss:   0.398095 Duration 0.250978\n",
      "Batch 61900 of 10 sentences: Average Loss:   0.398050 Duration 0.231912\n",
      "Batch 62000 of 10 sentences: Average Loss:   0.398022 Duration 0.359076\n",
      "Batch 62100 of 10 sentences: Average Loss:   0.397981 Duration 0.414937\n",
      "Batch 62200 of 10 sentences: Average Loss:   0.397944 Duration 0.221705\n",
      "Batch 62300 of 10 sentences: Average Loss:   0.397903 Duration 0.292681\n",
      "Batch 62400 of 10 sentences: Average Loss:   0.397861 Duration 0.235337\n",
      "Batch 62500 of 10 sentences: Average Loss:   0.397813 Duration 0.273799\n",
      "Batch 62600 of 10 sentences: Average Loss:   0.397784 Duration 0.149015\n",
      "Batch 62700 of 10 sentences: Average Loss:   0.397747 Duration 0.341881\n",
      "Batch 62800 of 10 sentences: Average Loss:   0.397690 Duration 0.197582\n",
      "Batch 62900 of 10 sentences: Average Loss:   0.397635 Duration 0.198070\n",
      "Batch 63000 of 10 sentences: Average Loss:   0.397590 Duration 0.300058\n",
      "Batch 63100 of 10 sentences: Average Loss:   0.397554 Duration 0.345840\n",
      "epoch 14 batches 63119 done\n",
      "Batch 63200 of 10 sentences: Average Loss:   0.397518 Duration 0.183012\n",
      "Batch 63300 of 10 sentences: Average Loss:   0.397487 Duration 0.261314\n",
      "Batch 63400 of 10 sentences: Average Loss:   0.397453 Duration 0.233123\n",
      "Batch 63500 of 10 sentences: Average Loss:   0.397398 Duration 0.239787\n",
      "Batch 63600 of 10 sentences: Average Loss:   0.397358 Duration 0.292075\n",
      "Batch 63700 of 10 sentences: Average Loss:   0.397333 Duration 0.136149\n",
      "Batch 63800 of 10 sentences: Average Loss:   0.397298 Duration 0.263938\n",
      "Batch 63900 of 10 sentences: Average Loss:   0.397249 Duration 0.247460\n",
      "Batch 64000 of 10 sentences: Average Loss:   0.397224 Duration 0.325988\n",
      "Batch 64100 of 10 sentences: Average Loss:   0.397190 Duration 0.157686\n",
      "Batch 64200 of 10 sentences: Average Loss:   0.397167 Duration 0.295358\n",
      "Batch 64300 of 10 sentences: Average Loss:   0.397138 Duration 0.293427\n",
      "Batch 64400 of 10 sentences: Average Loss:   0.397114 Duration 0.292434\n",
      "Batch 64500 of 10 sentences: Average Loss:   0.397064 Duration 0.190124\n",
      "Batch 64600 of 10 sentences: Average Loss:   0.397035 Duration 0.257807\n",
      "Batch 64700 of 10 sentences: Average Loss:   0.397010 Duration 0.136428\n",
      "Batch 64800 of 10 sentences: Average Loss:   0.396975 Duration 0.229946\n",
      "Batch 64900 of 10 sentences: Average Loss:   0.396946 Duration 0.229873\n",
      "Batch 65000 of 10 sentences: Average Loss:   0.396907 Duration 0.317345\n",
      "Batch 65100 of 10 sentences: Average Loss:   0.396873 Duration 0.251714\n",
      "Batch 65200 of 10 sentences: Average Loss:   0.396828 Duration 0.157775\n",
      "Batch 65300 of 10 sentences: Average Loss:   0.396783 Duration 0.201758\n",
      "Batch 65400 of 10 sentences: Average Loss:   0.396734 Duration 0.290755\n",
      "Batch 65500 of 10 sentences: Average Loss:   0.396690 Duration 0.226404\n",
      "Batch 65600 of 10 sentences: Average Loss:   0.396657 Duration 0.343219\n",
      "Batch 65700 of 10 sentences: Average Loss:   0.396610 Duration 0.222577\n",
      "Batch 65800 of 10 sentences: Average Loss:   0.396573 Duration 0.205114\n",
      "Batch 65900 of 10 sentences: Average Loss:   0.396544 Duration 0.274202\n",
      "Batch 66000 of 10 sentences: Average Loss:   0.396513 Duration 0.258689\n",
      "Batch 66100 of 10 sentences: Average Loss:   0.396474 Duration 0.301945\n",
      "Batch 66200 of 10 sentences: Average Loss:   0.396441 Duration 0.277493\n",
      "Batch 66300 of 10 sentences: Average Loss:   0.396404 Duration 0.246832\n",
      "Batch 66400 of 10 sentences: Average Loss:   0.396372 Duration 0.202076\n",
      "Batch 66500 of 10 sentences: Average Loss:   0.396325 Duration 0.267778\n",
      "Batch 66600 of 10 sentences: Average Loss:   0.396292 Duration 0.334848\n",
      "Batch 66700 of 10 sentences: Average Loss:   0.396249 Duration 0.324614\n",
      "Batch 66800 of 10 sentences: Average Loss:   0.396222 Duration 0.262648\n",
      "Batch 66900 of 10 sentences: Average Loss:   0.396181 Duration 0.201247\n",
      "Batch 67000 of 10 sentences: Average Loss:   0.396130 Duration 0.368493\n",
      "Batch 67100 of 10 sentences: Average Loss:   0.396078 Duration 0.222161\n",
      "Batch 67200 of 10 sentences: Average Loss:   0.396039 Duration 0.215386\n",
      "Batch 67300 of 10 sentences: Average Loss:   0.396000 Duration 0.282352\n",
      "epoch 15 batches 67327 done\n",
      "Batch 67400 of 10 sentences: Average Loss:   0.395969 Duration 0.231789\n",
      "Batch 67500 of 10 sentences: Average Loss:   0.395939 Duration 0.358124\n",
      "Batch 67600 of 10 sentences: Average Loss:   0.395912 Duration 0.236381\n",
      "Batch 67700 of 10 sentences: Average Loss:   0.395859 Duration 0.297766\n",
      "Batch 67800 of 10 sentences: Average Loss:   0.395823 Duration 0.289812\n",
      "Batch 67900 of 10 sentences: Average Loss:   0.395800 Duration 0.201242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 68000 of 10 sentences: Average Loss:   0.395770 Duration 0.345976\n",
      "Batch 68100 of 10 sentences: Average Loss:   0.395722 Duration 0.177583\n",
      "Batch 68200 of 10 sentences: Average Loss:   0.395700 Duration 0.340410\n",
      "Batch 68300 of 10 sentences: Average Loss:   0.395668 Duration 0.268842\n",
      "Batch 68400 of 10 sentences: Average Loss:   0.395644 Duration 0.265176\n",
      "Batch 68500 of 10 sentences: Average Loss:   0.395619 Duration 0.261589\n",
      "Batch 68600 of 10 sentences: Average Loss:   0.395597 Duration 0.346136\n",
      "Batch 68700 of 10 sentences: Average Loss:   0.395548 Duration 0.243470\n",
      "Batch 68800 of 10 sentences: Average Loss:   0.395518 Duration 0.273837\n",
      "Batch 68900 of 10 sentences: Average Loss:   0.395497 Duration 0.229344\n",
      "Batch 69000 of 10 sentences: Average Loss:   0.395465 Duration 0.257639\n",
      "Batch 69100 of 10 sentences: Average Loss:   0.395438 Duration 0.194525\n",
      "Batch 69200 of 10 sentences: Average Loss:   0.395401 Duration 0.167110\n",
      "Batch 69300 of 10 sentences: Average Loss:   0.395364 Duration 0.298382\n",
      "Batch 69400 of 10 sentences: Average Loss:   0.395326 Duration 0.336445\n",
      "Batch 69500 of 10 sentences: Average Loss:   0.395285 Duration 0.348065\n",
      "Batch 69600 of 10 sentences: Average Loss:   0.395241 Duration 0.229196\n",
      "Batch 69700 of 10 sentences: Average Loss:   0.395194 Duration 0.193157\n",
      "Batch 69800 of 10 sentences: Average Loss:   0.395162 Duration 0.181577\n",
      "Batch 69900 of 10 sentences: Average Loss:   0.395118 Duration 0.347250\n",
      "Batch 70000 of 10 sentences: Average Loss:   0.395080 Duration 0.303565\n",
      "Batch 70100 of 10 sentences: Average Loss:   0.395052 Duration 0.269569\n",
      "Batch 70200 of 10 sentences: Average Loss:   0.395017 Duration 0.203513\n",
      "Batch 70300 of 10 sentences: Average Loss:   0.394976 Duration 0.585116\n",
      "Batch 70400 of 10 sentences: Average Loss:   0.394951 Duration 0.417418\n",
      "Batch 70500 of 10 sentences: Average Loss:   0.394914 Duration 0.430364\n",
      "Batch 70600 of 10 sentences: Average Loss:   0.394883 Duration 0.264754\n",
      "Batch 70700 of 10 sentences: Average Loss:   0.394840 Duration 0.366129\n",
      "Batch 70800 of 10 sentences: Average Loss:   0.394812 Duration 0.336438\n",
      "Batch 70900 of 10 sentences: Average Loss:   0.394762 Duration 0.254261\n",
      "Batch 71000 of 10 sentences: Average Loss:   0.394738 Duration 0.352209\n",
      "Batch 71100 of 10 sentences: Average Loss:   0.394699 Duration 0.196396\n",
      "Batch 71200 of 10 sentences: Average Loss:   0.394650 Duration 0.353878\n",
      "Batch 71300 of 10 sentences: Average Loss:   0.394601 Duration 0.340772\n",
      "Batch 71400 of 10 sentences: Average Loss:   0.394565 Duration 0.444787\n",
      "Batch 71500 of 10 sentences: Average Loss:   0.394531 Duration 0.283352\n",
      "epoch 16 batches 71535 done\n",
      "Batch 71600 of 10 sentences: Average Loss:   0.394503 Duration 0.187116\n",
      "Batch 71700 of 10 sentences: Average Loss:   0.394476 Duration 0.302255\n",
      "Batch 71800 of 10 sentences: Average Loss:   0.394452 Duration 0.316758\n",
      "Batch 71900 of 10 sentences: Average Loss:   0.394403 Duration 0.347934\n",
      "Batch 72000 of 10 sentences: Average Loss:   0.394366 Duration 0.281980\n",
      "Batch 72100 of 10 sentences: Average Loss:   0.394344 Duration 0.158856\n",
      "Batch 72200 of 10 sentences: Average Loss:   0.394318 Duration 0.326506\n",
      "Batch 72300 of 10 sentences: Average Loss:   0.394273 Duration 0.327734\n",
      "Batch 72400 of 10 sentences: Average Loss:   0.394247 Duration 0.366402\n",
      "Batch 72500 of 10 sentences: Average Loss:   0.394219 Duration 0.342377\n",
      "Batch 72600 of 10 sentences: Average Loss:   0.394195 Duration 0.312302\n",
      "Batch 72700 of 10 sentences: Average Loss:   0.394174 Duration 0.122161\n",
      "Batch 72800 of 10 sentences: Average Loss:   0.394154 Duration 0.300442\n",
      "Batch 72900 of 10 sentences: Average Loss:   0.394118 Duration 0.304417\n",
      "Batch 73000 of 10 sentences: Average Loss:   0.394081 Duration 0.508142\n",
      "Batch 73100 of 10 sentences: Average Loss:   0.394060 Duration 0.342076\n",
      "Batch 73200 of 10 sentences: Average Loss:   0.394028 Duration 0.299047\n",
      "Batch 73300 of 10 sentences: Average Loss:   0.394007 Duration 0.310902\n",
      "Batch 73400 of 10 sentences: Average Loss:   0.393971 Duration 0.136342\n",
      "Batch 73500 of 10 sentences: Average Loss:   0.393936 Duration 0.193246\n",
      "Batch 73600 of 10 sentences: Average Loss:   0.393899 Duration 0.218466\n",
      "Batch 73700 of 10 sentences: Average Loss:   0.393857 Duration 0.235903\n",
      "Batch 73800 of 10 sentences: Average Loss:   0.393817 Duration 0.467073\n",
      "Batch 73900 of 10 sentences: Average Loss:   0.393767 Duration 0.323269\n",
      "Batch 74000 of 10 sentences: Average Loss:   0.393740 Duration 0.193155\n",
      "Batch 74100 of 10 sentences: Average Loss:   0.393702 Duration 0.301915\n",
      "Batch 74200 of 10 sentences: Average Loss:   0.393662 Duration 0.286288\n",
      "Batch 74300 of 10 sentences: Average Loss:   0.393638 Duration 0.236572\n",
      "Batch 74400 of 10 sentences: Average Loss:   0.393603 Duration 0.448244\n",
      "Batch 74500 of 10 sentences: Average Loss:   0.393568 Duration 0.387015\n",
      "Batch 74600 of 10 sentences: Average Loss:   0.393538 Duration 0.224256\n",
      "Batch 74700 of 10 sentences: Average Loss:   0.393507 Duration 0.250625\n",
      "Batch 74800 of 10 sentences: Average Loss:   0.393478 Duration 0.337713\n",
      "Batch 74900 of 10 sentences: Average Loss:   0.393433 Duration 0.429216\n",
      "Batch 75000 of 10 sentences: Average Loss:   0.393412 Duration 0.241192\n",
      "Batch 75100 of 10 sentences: Average Loss:   0.393362 Duration 0.354174\n",
      "Batch 75200 of 10 sentences: Average Loss:   0.393344 Duration 0.207388\n",
      "Batch 75300 of 10 sentences: Average Loss:   0.393313 Duration 0.290611\n",
      "Batch 75400 of 10 sentences: Average Loss:   0.393263 Duration 0.386926\n",
      "Batch 75500 of 10 sentences: Average Loss:   0.393226 Duration 0.188210\n",
      "Batch 75600 of 10 sentences: Average Loss:   0.393180 Duration 0.409057\n",
      "Batch 75700 of 10 sentences: Average Loss:   0.393146 Duration 0.238478\n",
      "epoch 17 batches 75743 done\n",
      "Batch 75800 of 10 sentences: Average Loss:   0.393118 Duration 0.220120\n",
      "Batch 75900 of 10 sentences: Average Loss:   0.393091 Duration 0.161072\n",
      "Batch 76000 of 10 sentences: Average Loss:   0.393067 Duration 0.471217\n",
      "Batch 76100 of 10 sentences: Average Loss:   0.393021 Duration 0.386882\n",
      "Batch 76200 of 10 sentences: Average Loss:   0.392987 Duration 0.682629\n",
      "Batch 76300 of 10 sentences: Average Loss:   0.392969 Duration 0.253243\n",
      "Batch 76400 of 10 sentences: Average Loss:   0.392942 Duration 0.370928\n",
      "Batch 76500 of 10 sentences: Average Loss:   0.392899 Duration 0.254896\n",
      "Batch 76600 of 10 sentences: Average Loss:   0.392873 Duration 0.186623\n",
      "Batch 76700 of 10 sentences: Average Loss:   0.392852 Duration 0.309154\n",
      "Batch 76800 of 10 sentences: Average Loss:   0.392824 Duration 0.232492\n",
      "Batch 76900 of 10 sentences: Average Loss:   0.392807 Duration 0.387769\n",
      "Batch 77000 of 10 sentences: Average Loss:   0.392789 Duration 0.215575\n",
      "Batch 77100 of 10 sentences: Average Loss:   0.392758 Duration 0.342884\n",
      "Batch 77200 of 10 sentences: Average Loss:   0.392720 Duration 0.390593\n",
      "Batch 77300 of 10 sentences: Average Loss:   0.392700 Duration 0.303290\n",
      "Batch 77400 of 10 sentences: Average Loss:   0.392669 Duration 0.266258\n",
      "Batch 77500 of 10 sentences: Average Loss:   0.392646 Duration 0.258837\n",
      "Batch 77600 of 10 sentences: Average Loss:   0.392610 Duration 0.364701\n",
      "Batch 77700 of 10 sentences: Average Loss:   0.392576 Duration 0.362423\n",
      "Batch 77800 of 10 sentences: Average Loss:   0.392543 Duration 0.318331\n",
      "Batch 77900 of 10 sentences: Average Loss:   0.392506 Duration 0.283632\n",
      "Batch 78000 of 10 sentences: Average Loss:   0.392471 Duration 0.400867\n",
      "Batch 78100 of 10 sentences: Average Loss:   0.392425 Duration 0.389186\n",
      "Batch 78200 of 10 sentences: Average Loss:   0.392399 Duration 0.494982\n",
      "Batch 78300 of 10 sentences: Average Loss:   0.392359 Duration 0.269873\n",
      "Batch 78400 of 10 sentences: Average Loss:   0.392325 Duration 0.300560\n",
      "Batch 78500 of 10 sentences: Average Loss:   0.392297 Duration 0.374662\n",
      "Batch 78600 of 10 sentences: Average Loss:   0.392266 Duration 0.408563\n",
      "Batch 78700 of 10 sentences: Average Loss:   0.392235 Duration 0.495991\n",
      "Batch 78800 of 10 sentences: Average Loss:   0.392206 Duration 0.159298\n",
      "Batch 78900 of 10 sentences: Average Loss:   0.392176 Duration 0.134295\n",
      "Batch 79000 of 10 sentences: Average Loss:   0.392152 Duration 0.307143\n",
      "Batch 79100 of 10 sentences: Average Loss:   0.392107 Duration 0.278278\n",
      "Batch 79200 of 10 sentences: Average Loss:   0.392090 Duration 0.292609\n",
      "Batch 79300 of 10 sentences: Average Loss:   0.392041 Duration 0.190482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 79400 of 10 sentences: Average Loss:   0.392020 Duration 0.293583\n",
      "Batch 79500 of 10 sentences: Average Loss:   0.391991 Duration 0.262933\n",
      "Batch 79600 of 10 sentences: Average Loss:   0.391948 Duration 0.248520\n",
      "Batch 79700 of 10 sentences: Average Loss:   0.391910 Duration 0.511306\n",
      "Batch 79800 of 10 sentences: Average Loss:   0.391865 Duration 0.358510\n",
      "Batch 79900 of 10 sentences: Average Loss:   0.391838 Duration 0.372678\n",
      "epoch 18 batches 79951 done\n",
      "Batch 80000 of 10 sentences: Average Loss:   0.391807 Duration 0.433802\n",
      "Batch 80100 of 10 sentences: Average Loss:   0.391781 Duration 0.374878\n",
      "Batch 80200 of 10 sentences: Average Loss:   0.391762 Duration 0.314393\n",
      "Batch 80300 of 10 sentences: Average Loss:   0.391718 Duration 0.362284\n",
      "Batch 80400 of 10 sentences: Average Loss:   0.391681 Duration 0.211045\n",
      "Batch 80500 of 10 sentences: Average Loss:   0.391663 Duration 0.229786\n",
      "Batch 80600 of 10 sentences: Average Loss:   0.391638 Duration 0.215662\n",
      "Batch 80700 of 10 sentences: Average Loss:   0.391597 Duration 0.154662\n",
      "Batch 80800 of 10 sentences: Average Loss:   0.391573 Duration 0.411968\n",
      "Batch 80900 of 10 sentences: Average Loss:   0.391551 Duration 0.213778\n",
      "Batch 81000 of 10 sentences: Average Loss:   0.391524 Duration 0.274284\n",
      "Batch 81100 of 10 sentences: Average Loss:   0.391506 Duration 0.189548\n",
      "Batch 81200 of 10 sentences: Average Loss:   0.391488 Duration 0.207390\n",
      "Batch 81300 of 10 sentences: Average Loss:   0.391459 Duration 0.360803\n",
      "Batch 81400 of 10 sentences: Average Loss:   0.391424 Duration 0.212961\n",
      "Batch 81500 of 10 sentences: Average Loss:   0.391403 Duration 0.412122\n",
      "Batch 81600 of 10 sentences: Average Loss:   0.391374 Duration 0.207279\n",
      "Batch 81700 of 10 sentences: Average Loss:   0.391351 Duration 0.274402\n",
      "Batch 81800 of 10 sentences: Average Loss:   0.391320 Duration 0.288605\n",
      "Batch 81900 of 10 sentences: Average Loss:   0.391288 Duration 0.214207\n",
      "Batch 82000 of 10 sentences: Average Loss:   0.391253 Duration 0.141007\n",
      "Batch 82100 of 10 sentences: Average Loss:   0.391217 Duration 0.229856\n",
      "Batch 82200 of 10 sentences: Average Loss:   0.391182 Duration 0.398945\n",
      "Batch 82300 of 10 sentences: Average Loss:   0.391135 Duration 0.281295\n",
      "Batch 82400 of 10 sentences: Average Loss:   0.391109 Duration 0.267064\n",
      "Batch 82500 of 10 sentences: Average Loss:   0.391074 Duration 0.266218\n",
      "Batch 82600 of 10 sentences: Average Loss:   0.391038 Duration 0.369039\n",
      "Batch 82700 of 10 sentences: Average Loss:   0.391013 Duration 0.256387\n",
      "Batch 82800 of 10 sentences: Average Loss:   0.390982 Duration 0.299738\n",
      "Batch 82900 of 10 sentences: Average Loss:   0.390952 Duration 0.281436\n",
      "Batch 83000 of 10 sentences: Average Loss:   0.390925 Duration 0.270001\n",
      "Batch 83100 of 10 sentences: Average Loss:   0.390897 Duration 0.231025\n",
      "Batch 83200 of 10 sentences: Average Loss:   0.390871 Duration 0.271649\n",
      "Batch 83300 of 10 sentences: Average Loss:   0.390837 Duration 0.147893\n",
      "Batch 83400 of 10 sentences: Average Loss:   0.390813 Duration 0.204129\n",
      "Batch 83500 of 10 sentences: Average Loss:   0.390767 Duration 0.230981\n",
      "Batch 83600 of 10 sentences: Average Loss:   0.390747 Duration 0.268184\n",
      "Batch 83700 of 10 sentences: Average Loss:   0.390719 Duration 0.238081\n",
      "Batch 83800 of 10 sentences: Average Loss:   0.390676 Duration 0.320697\n",
      "Batch 83900 of 10 sentences: Average Loss:   0.390642 Duration 0.416750\n",
      "Batch 84000 of 10 sentences: Average Loss:   0.390599 Duration 0.162286\n",
      "Batch 84100 of 10 sentences: Average Loss:   0.390568 Duration 0.273499\n",
      "epoch 19 batches 84159 done\n",
      "Batch 84200 of 10 sentences: Average Loss:   0.390542 Duration 0.252202\n",
      "Batch 84300 of 10 sentences: Average Loss:   0.390516 Duration 0.289425\n",
      "Batch 84400 of 10 sentences: Average Loss:   0.390495 Duration 0.295164\n",
      "Batch 84500 of 10 sentences: Average Loss:   0.390457 Duration 0.328577\n",
      "Batch 84600 of 10 sentences: Average Loss:   0.390419 Duration 0.233057\n",
      "Batch 84700 of 10 sentences: Average Loss:   0.390399 Duration 0.271891\n",
      "Batch 84800 of 10 sentences: Average Loss:   0.390378 Duration 0.338736\n",
      "Batch 84900 of 10 sentences: Average Loss:   0.390341 Duration 0.274762\n",
      "Batch 85000 of 10 sentences: Average Loss:   0.390312 Duration 0.327328\n",
      "Batch 85100 of 10 sentences: Average Loss:   0.390289 Duration 0.268071\n",
      "Batch 85200 of 10 sentences: Average Loss:   0.390263 Duration 0.305000\n",
      "Batch 85300 of 10 sentences: Average Loss:   0.390249 Duration 0.269458\n",
      "Batch 85400 of 10 sentences: Average Loss:   0.390229 Duration 0.305643\n",
      "Batch 85500 of 10 sentences: Average Loss:   0.390198 Duration 0.303768\n",
      "Batch 85600 of 10 sentences: Average Loss:   0.390167 Duration 0.199425\n",
      "Batch 85700 of 10 sentences: Average Loss:   0.390149 Duration 0.330611\n",
      "Batch 85800 of 10 sentences: Average Loss:   0.390119 Duration 0.450197\n",
      "Batch 85900 of 10 sentences: Average Loss:   0.390099 Duration 0.210799\n",
      "Batch 86000 of 10 sentences: Average Loss:   0.390068 Duration 0.318061\n",
      "Batch 86100 of 10 sentences: Average Loss:   0.390041 Duration 0.251638\n",
      "Batch 86200 of 10 sentences: Average Loss:   0.390008 Duration 0.247643\n",
      "Batch 86300 of 10 sentences: Average Loss:   0.389971 Duration 0.335567\n",
      "Batch 86400 of 10 sentences: Average Loss:   0.389940 Duration 0.210701\n",
      "Batch 86500 of 10 sentences: Average Loss:   0.389895 Duration 0.276545\n",
      "Batch 86600 of 10 sentences: Average Loss:   0.389869 Duration 0.181318\n",
      "Batch 86700 of 10 sentences: Average Loss:   0.389838 Duration 0.212372\n",
      "Batch 86800 of 10 sentences: Average Loss:   0.389804 Duration 0.384643\n",
      "Batch 86900 of 10 sentences: Average Loss:   0.389780 Duration 0.300385\n",
      "Batch 87000 of 10 sentences: Average Loss:   0.389752 Duration 0.204461\n",
      "Batch 87100 of 10 sentences: Average Loss:   0.389725 Duration 0.173646\n",
      "Batch 87200 of 10 sentences: Average Loss:   0.389697 Duration 0.189575\n",
      "Batch 87300 of 10 sentences: Average Loss:   0.389673 Duration 0.332003\n",
      "Batch 87400 of 10 sentences: Average Loss:   0.389647 Duration 0.225587\n",
      "Batch 87500 of 10 sentences: Average Loss:   0.389615 Duration 0.517721\n",
      "Batch 87600 of 10 sentences: Average Loss:   0.389590 Duration 0.354204\n",
      "Batch 87700 of 10 sentences: Average Loss:   0.389548 Duration 0.079272\n",
      "Batch 87800 of 10 sentences: Average Loss:   0.389522 Duration 0.266790\n",
      "Batch 87900 of 10 sentences: Average Loss:   0.389501 Duration 0.274139\n",
      "Batch 88000 of 10 sentences: Average Loss:   0.389461 Duration 0.320074\n",
      "Batch 88100 of 10 sentences: Average Loss:   0.389426 Duration 0.235830\n",
      "Batch 88200 of 10 sentences: Average Loss:   0.389387 Duration 0.334754\n",
      "Batch 88300 of 10 sentences: Average Loss:   0.389355 Duration 0.354866\n",
      "epoch 20 batches 88367 done\n",
      "Batch 88400 of 10 sentences: Average Loss:   0.389328 Duration 0.236929\n",
      "Batch 88500 of 10 sentences: Average Loss:   0.389305 Duration 0.229709\n",
      "Batch 88600 of 10 sentences: Average Loss:   0.389281 Duration 0.277037\n",
      "Batch 88700 of 10 sentences: Average Loss:   0.389247 Duration 0.410374\n",
      "Batch 88800 of 10 sentences: Average Loss:   0.389212 Duration 0.225935\n",
      "Batch 88900 of 10 sentences: Average Loss:   0.389191 Duration 0.311670\n",
      "Batch 89000 of 10 sentences: Average Loss:   0.389172 Duration 0.427595\n",
      "Batch 89100 of 10 sentences: Average Loss:   0.389138 Duration 0.221180\n",
      "Batch 89200 of 10 sentences: Average Loss:   0.389106 Duration 0.370524\n",
      "Batch 89300 of 10 sentences: Average Loss:   0.389088 Duration 0.395776\n",
      "Batch 89400 of 10 sentences: Average Loss:   0.389061 Duration 0.341520\n",
      "Batch 89500 of 10 sentences: Average Loss:   0.389050 Duration 0.331480\n",
      "Batch 89600 of 10 sentences: Average Loss:   0.389028 Duration 0.283408\n",
      "Batch 89700 of 10 sentences: Average Loss:   0.389000 Duration 0.228329\n",
      "Batch 89800 of 10 sentences: Average Loss:   0.388967 Duration 0.430997\n",
      "Batch 89900 of 10 sentences: Average Loss:   0.388948 Duration 0.241586\n",
      "Batch 90000 of 10 sentences: Average Loss:   0.388923 Duration 0.258603\n",
      "Batch 90100 of 10 sentences: Average Loss:   0.388902 Duration 0.345456\n",
      "Batch 90200 of 10 sentences: Average Loss:   0.388877 Duration 0.367494\n",
      "Batch 90300 of 10 sentences: Average Loss:   0.388846 Duration 0.282947\n",
      "Batch 90400 of 10 sentences: Average Loss:   0.388815 Duration 0.481899\n",
      "Batch 90500 of 10 sentences: Average Loss:   0.388780 Duration 0.308777\n",
      "Batch 90600 of 10 sentences: Average Loss:   0.388748 Duration 0.375085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 90700 of 10 sentences: Average Loss:   0.388704 Duration 0.357444\n",
      "Batch 90800 of 10 sentences: Average Loss:   0.388683 Duration 0.206968\n",
      "Batch 90900 of 10 sentences: Average Loss:   0.388653 Duration 0.334456\n",
      "Batch 91000 of 10 sentences: Average Loss:   0.388618 Duration 0.285307\n",
      "Batch 91100 of 10 sentences: Average Loss:   0.388595 Duration 0.308721\n",
      "Batch 91200 of 10 sentences: Average Loss:   0.388571 Duration 0.222652\n",
      "Batch 91300 of 10 sentences: Average Loss:   0.388544 Duration 0.336539\n",
      "Batch 91400 of 10 sentences: Average Loss:   0.388519 Duration 0.346643\n",
      "Batch 91500 of 10 sentences: Average Loss:   0.388495 Duration 0.301454\n",
      "Batch 91600 of 10 sentences: Average Loss:   0.388469 Duration 0.423123\n",
      "Batch 91700 of 10 sentences: Average Loss:   0.388441 Duration 0.389730\n",
      "Batch 91800 of 10 sentences: Average Loss:   0.388419 Duration 0.323457\n",
      "Batch 91900 of 10 sentences: Average Loss:   0.388386 Duration 0.341766\n",
      "Batch 92000 of 10 sentences: Average Loss:   0.388354 Duration 0.438911\n",
      "Batch 92100 of 10 sentences: Average Loss:   0.388334 Duration 0.401296\n",
      "Batch 92200 of 10 sentences: Average Loss:   0.388296 Duration 0.399425\n",
      "Batch 92300 of 10 sentences: Average Loss:   0.388260 Duration 0.423586\n",
      "Batch 92400 of 10 sentences: Average Loss:   0.388223 Duration 0.336366\n",
      "Batch 92500 of 10 sentences: Average Loss:   0.388191 Duration 0.452163\n",
      "epoch 21 batches 92575 done\n",
      "Batch 92600 of 10 sentences: Average Loss:   0.388167 Duration 0.477241\n",
      "Batch 92700 of 10 sentences: Average Loss:   0.388145 Duration 0.436778\n",
      "Batch 92800 of 10 sentences: Average Loss:   0.388122 Duration 0.368308\n",
      "Batch 92900 of 10 sentences: Average Loss:   0.388092 Duration 0.260371\n",
      "Batch 93000 of 10 sentences: Average Loss:   0.388059 Duration 0.340013\n",
      "Batch 93100 of 10 sentences: Average Loss:   0.388034 Duration 0.315628\n",
      "Batch 93200 of 10 sentences: Average Loss:   0.388017 Duration 0.398853\n",
      "Batch 93300 of 10 sentences: Average Loss:   0.387988 Duration 0.350186\n",
      "Batch 93400 of 10 sentences: Average Loss:   0.387958 Duration 0.211462\n",
      "Batch 93500 of 10 sentences: Average Loss:   0.387939 Duration 0.388384\n",
      "Batch 93600 of 10 sentences: Average Loss:   0.387917 Duration 0.371660\n",
      "Batch 93700 of 10 sentences: Average Loss:   0.387904 Duration 0.425559\n",
      "Batch 93800 of 10 sentences: Average Loss:   0.387885 Duration 0.370312\n",
      "Batch 93900 of 10 sentences: Average Loss:   0.387858 Duration 0.144851\n",
      "Batch 94000 of 10 sentences: Average Loss:   0.387829 Duration 0.259565\n",
      "Batch 94100 of 10 sentences: Average Loss:   0.387811 Duration 0.237444\n",
      "Batch 94200 of 10 sentences: Average Loss:   0.387790 Duration 0.308406\n",
      "Batch 94300 of 10 sentences: Average Loss:   0.387769 Duration 0.307037\n",
      "Batch 94400 of 10 sentences: Average Loss:   0.387747 Duration 0.323775\n",
      "Batch 94500 of 10 sentences: Average Loss:   0.387721 Duration 0.283853\n",
      "Batch 94600 of 10 sentences: Average Loss:   0.387689 Duration 0.446209\n",
      "Batch 94700 of 10 sentences: Average Loss:   0.387659 Duration 0.148668\n",
      "Batch 94800 of 10 sentences: Average Loss:   0.387622 Duration 0.309437\n",
      "Batch 94900 of 10 sentences: Average Loss:   0.387580 Duration 0.425704\n",
      "Batch 95000 of 10 sentences: Average Loss:   0.387561 Duration 0.282294\n",
      "Batch 95100 of 10 sentences: Average Loss:   0.387530 Duration 0.283993\n",
      "Batch 95200 of 10 sentences: Average Loss:   0.387495 Duration 0.355708\n",
      "Batch 95300 of 10 sentences: Average Loss:   0.387472 Duration 0.326312\n",
      "Batch 95400 of 10 sentences: Average Loss:   0.387448 Duration 0.388988\n",
      "Batch 95500 of 10 sentences: Average Loss:   0.387421 Duration 0.318212\n",
      "Batch 95600 of 10 sentences: Average Loss:   0.387396 Duration 0.381889\n",
      "Batch 95700 of 10 sentences: Average Loss:   0.387375 Duration 0.570585\n",
      "Batch 95800 of 10 sentences: Average Loss:   0.387346 Duration 0.466714\n",
      "Batch 95900 of 10 sentences: Average Loss:   0.387320 Duration 0.367730\n",
      "Batch 96000 of 10 sentences: Average Loss:   0.387300 Duration 0.388286\n",
      "Batch 96100 of 10 sentences: Average Loss:   0.387267 Duration 0.329982\n",
      "Batch 96200 of 10 sentences: Average Loss:   0.387235 Duration 0.313117\n",
      "Batch 96300 of 10 sentences: Average Loss:   0.387218 Duration 0.299441\n",
      "Batch 96400 of 10 sentences: Average Loss:   0.387181 Duration 0.286416\n",
      "Batch 96500 of 10 sentences: Average Loss:   0.387147 Duration 0.211931\n",
      "Batch 96600 of 10 sentences: Average Loss:   0.387110 Duration 0.405924\n",
      "Batch 96700 of 10 sentences: Average Loss:   0.387083 Duration 0.206087\n",
      "epoch 22 batches 96783 done\n",
      "Batch 96800 of 10 sentences: Average Loss:   0.387058 Duration 0.401677\n",
      "Batch 96900 of 10 sentences: Average Loss:   0.387039 Duration 0.190873\n",
      "Batch 97000 of 10 sentences: Average Loss:   0.387017 Duration 0.259643\n",
      "Batch 97100 of 10 sentences: Average Loss:   0.386986 Duration 0.272742\n",
      "Batch 97200 of 10 sentences: Average Loss:   0.386957 Duration 0.204325\n",
      "Batch 97300 of 10 sentences: Average Loss:   0.386932 Duration 0.365802\n",
      "Batch 97400 of 10 sentences: Average Loss:   0.386914 Duration 0.176707\n",
      "Batch 97500 of 10 sentences: Average Loss:   0.386887 Duration 0.499310\n",
      "Batch 97600 of 10 sentences: Average Loss:   0.386857 Duration 0.414291\n",
      "Batch 97700 of 10 sentences: Average Loss:   0.386837 Duration 0.332405\n",
      "Batch 97800 of 10 sentences: Average Loss:   0.386816 Duration 0.361302\n",
      "Batch 97900 of 10 sentences: Average Loss:   0.386803 Duration 0.309581\n",
      "Batch 98000 of 10 sentences: Average Loss:   0.386784 Duration 0.323101\n",
      "Batch 98100 of 10 sentences: Average Loss:   0.386759 Duration 0.101263\n",
      "Batch 98200 of 10 sentences: Average Loss:   0.386730 Duration 0.420918\n",
      "Batch 98300 of 10 sentences: Average Loss:   0.386712 Duration 0.295124\n",
      "Batch 98400 of 10 sentences: Average Loss:   0.386687 Duration 0.318924\n",
      "Batch 98500 of 10 sentences: Average Loss:   0.386668 Duration 0.297707\n",
      "Batch 98600 of 10 sentences: Average Loss:   0.386646 Duration 0.464243\n",
      "Batch 98700 of 10 sentences: Average Loss:   0.386622 Duration 0.301180\n",
      "Batch 98800 of 10 sentences: Average Loss:   0.386591 Duration 0.302939\n",
      "Batch 98900 of 10 sentences: Average Loss:   0.386566 Duration 0.441724\n",
      "Batch 99000 of 10 sentences: Average Loss:   0.386530 Duration 0.471302\n",
      "Batch 99100 of 10 sentences: Average Loss:   0.386493 Duration 0.349352\n",
      "Batch 99200 of 10 sentences: Average Loss:   0.386472 Duration 0.276204\n",
      "Batch 99300 of 10 sentences: Average Loss:   0.386440 Duration 0.299970\n",
      "Batch 99400 of 10 sentences: Average Loss:   0.386408 Duration 0.310791\n",
      "Batch 99500 of 10 sentences: Average Loss:   0.386386 Duration 0.290933\n",
      "Batch 99600 of 10 sentences: Average Loss:   0.386365 Duration 0.351987\n",
      "Batch 99700 of 10 sentences: Average Loss:   0.386342 Duration 0.303648\n",
      "Batch 99800 of 10 sentences: Average Loss:   0.386319 Duration 0.427831\n",
      "Batch 99900 of 10 sentences: Average Loss:   0.386298 Duration 0.279313\n",
      "         12367156179 function calls (11910003608 primitive calls) in 24872.232 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        5    0.000    0.000 24872.231 4974.446 interactiveshell.py:3377(run_code)\n",
      "        5    0.000    0.000 24872.231 4974.446 {built-in method builtins.exec}\n",
      "        1    1.669    1.669 24872.164 24872.164 <ipython-input-33-0580c1f4e1b7>:25(<module>)\n",
      "    99977    6.287    0.000 24839.262    0.248 base.py:258(train)\n",
      "299931/199954    4.493    0.000 21771.770    0.109 function.py:1111(_)\n",
      "    99977    1.009    0.000 12753.989    0.128 base.py:230(gradient)\n",
      "    99977   47.028    0.000 11784.227    0.118 embedding_sgram.py:811(gradient)\n",
      "    99977    0.934    0.000 9046.947    0.090 base.py:211(function)\n",
      "    99977   21.323    0.000 8426.232    0.084 embedding_sgram.py:636(function)\n",
      "   499885    3.316    0.000 8240.961    0.016 arrayprint.py:1500(_array_str_implementation)\n",
      "   499885    5.281    0.000 8237.645    0.016 arrayprint.py:516(array2string)\n",
      "   499885    5.424    0.000 8223.428    0.016 arrayprint.py:461(wrapper)\n",
      "   499885    9.862    0.000 8216.239    0.016 arrayprint.py:478(_array2string)\n",
      "    99977  152.005    0.002 7995.163    0.080 embedding_sgram.py:760(<listcomp>)\n",
      " 14715076  673.271    0.000 7649.736    0.001 preprocess.py:283(negative_sample_indices)\n",
      " 14715076  553.026    0.000 6276.696    0.000 preprocess.py:254(sample)\n",
      " 14715076 3090.044    0.000 5675.279    0.000 {method 'choice' of 'numpy.random.mtrand.RandomState' objects}\n",
      "   499885    2.246    0.000 5365.775    0.011 arrayprint.py:736(_formatArray)\n",
      "452953341/499885 1352.719    0.000 5363.528    0.011 arrayprint.py:745(recurser)\n",
      "8398068/5998620   23.582    0.000 4659.789    0.001 dispatch.py:198(wrapper)\n",
      "  8398068 4148.015    0.000 4148.015    0.000 {built-in method tensorflow.python._pywrap_tfe.TFE_Py_FastPathExecute}\n",
      "   499885   25.274    0.000 4036.089    0.008 base.py:239(dX)\n",
      "   699839    5.765    0.000 3480.781    0.005 tf.py:206(einsum)\n",
      "   699839    4.369    0.000 3471.892    0.005 special_math_ops.py:606(einsum)\n",
      "   699839   25.665    0.000 3467.522    0.005 special_math_ops.py:1152(_einsum_v2)\n",
      "   699839    7.259    0.000 3373.415    0.005 gen_linalg_ops.py:979(einsum)\n",
      "   499885    6.928    0.000 2838.686    0.006 arrayprint.py:409(_get_format_function)\n",
      "   499885    2.643    0.000 2827.284    0.006 arrayprint.py:366(<lambda>)\n",
      "   499885   27.562    0.000 2824.641    0.006 arrayprint.py:863(__init__)\n",
      "   499885  591.206    0.001 2796.080    0.006 arrayprint.py:890(fillFormat)\n",
      "    99977    2.003    0.000 2744.340    0.027 base.py:239(update)\n",
      "    99977    0.844    0.000 2741.826    0.027 composite.py:207(update)\n",
      "    99977    1.199    0.000 2740.847    0.027 composite.py:214(<listcomp>)\n",
      "    99977   11.851    0.000 2736.610    0.027 embedding_sgram.py:1115(update)\n",
      "422023580 1735.330    0.000 2694.485    0.000 arrayprint.py:974(__call__)\n",
      "   299931   10.332    0.000 2677.489    0.009 embedding_sgram.py:1101(_gradient_descent)\n",
      "75674161/73474759  180.840    0.000 2449.495    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "   299931 2392.348    0.008 2392.348    0.008 {method 'at' of 'numpy.ufunc' objects}\n",
      "422023580  425.561    0.000 1216.977    0.000 arrayprint.py:709(_extendLine_pretty)\n",
      " 17672919   30.173    0.000 1058.147    0.000 <__array_function__ internals>:2(cumsum)\n",
      " 17672919   34.751    0.000  997.807    0.000 fromnumeric.py:2446(cumsum)\n",
      " 20072321   28.235    0.000  979.152    0.000 fromnumeric.py:52(_wrapfunc)\n",
      " 17672919  931.736    0.000  931.736    0.000 {method 'cumsum' of 'numpy.ndarray' objects}\n",
      "441263010  909.946    0.000  909.946    0.000 {built-in method numpy.core._multiarray_umath.dragon4_scientific}\n",
      "402784150  788.340    0.000  788.340    0.000 {built-in method numpy.core._multiarray_umath.dragon4_positional}\n",
      "201792027  120.059    0.000  739.969    0.000 arrayprint.py:945(<genexpr>)\n",
      "    99977    1.089    0.000  735.501    0.007 preprocess.py:655(gradient)\n",
      "220731438  118.456    0.000  733.333    0.000 arrayprint.py:918(<genexpr>)\n",
      " 17672919   36.379    0.000  726.287    0.000 <__array_function__ internals>:2(unique)\n",
      " 17672919   70.429    0.000  644.763    0.000 arraysetops.py:138(unique)\n",
      "  2299471   21.215    0.000  640.946    0.000 tf.py:54(is_finite)\n",
      "422023580  461.150    0.000  635.349    0.000 arrayprint.py:695(_extendLine)\n",
      " 29430152   35.059    0.000  628.301    0.000 preprocess.py:233(list_indices)\n",
      " 29430152  244.147    0.000  593.242    0.000 preprocess.py:238(<listcomp>)\n",
      " 33129165  186.119    0.000  560.366    0.000 {built-in method builtins.max}\n",
      "220731438  130.457    0.000  530.861    0.000 arrayprint.py:915(<genexpr>)\n",
      " 15015007   41.662    0.000  524.618    0.000 <__array_function__ internals>:2(prod)\n",
      " 17672919  338.035    0.000  523.303    0.000 arraysetops.py:310(_unique1d)\n",
      "201792027  135.241    0.000  473.968    0.000 arrayprint.py:940(<genexpr>)\n",
      "3854196646  436.342    0.000  436.342    0.000 {built-in method builtins.len}\n",
      " 15015007   58.766    0.000  403.053    0.000 fromnumeric.py:2912(prod)\n",
      "  2299471   18.207    0.000  381.399    0.000 math_ops.py:2815(reduce_all)\n",
      " 16614639  113.884    0.000  375.238    0.000 fromnumeric.py:70(_wrapreduction)\n",
      "  1399678   13.218    0.000  363.846    0.000 base.py:271(Y)\n",
      "    99977    6.224    0.000  331.451    0.003 preprocess.py:550(function)\n",
      "490543988  297.711    0.000  297.711    0.000 {method 'get' of 'dict' objects}\n",
      "    99977    6.240    0.000  292.795    0.003 preprocess.py:601(<listcomp>)\n",
      "   999724   57.173    0.000  285.500    0.000 utility.py:43(event_context_pairs)\n",
      "423823168  273.988    0.000  273.988    0.000 {method 'split' of 'str' objects}\n",
      "   999770    7.321    0.000  272.838    0.000 tf.py:180(multiply)\n",
      "   999770    4.174    0.000  262.417    0.000 math_ops.py:472(multiply)\n",
      "   999770    9.651    0.000  258.243    0.000 gen_math_ops.py:6046(mul)\n",
      "  1999540   10.679    0.000  237.210    0.000 base.py:232(reshape)\n",
      "  2299471   25.840    0.000  232.764    0.000 gen_math_ops.py:4546(is_finite)\n",
      "   599862    2.570    0.000  225.335    0.000 tf.py:35(ones)\n",
      "   599862   22.085    0.000  218.884    0.000 array_ops.py:3081(ones)\n",
      "  2299471   20.573    0.000  211.430    0.000 math_ops.py:1891(_ReductionDims)\n",
      "    99977    2.990    0.000  194.961    0.002 preprocess.py:341(function)\n",
      "   599862    3.531    0.000  194.218    0.000 array_ops.py:59(reshape)\n",
      " 14715076   52.700    0.000  193.551    0.000 {method 'prod' of 'numpy.generic' objects}\n",
      "220731438   62.412    0.000  187.597    0.000 arrayprint.py:919(<genexpr>)\n",
      "   599862   11.792    0.000  186.719    0.000 gen_array_ops.py:8288(reshape)\n",
      " 14715076   72.489    0.000  174.410    0.000 base.py:217(to_flat_list)\n",
      "    99977    1.499    0.000  170.784    0.002 preprocess.py:304(sentence_to_sequence)\n",
      " 21613489   76.763    0.000  169.662    0.000 numerictypes.py:359(issubdtype)\n",
      "    99977    7.972    0.000  168.410    0.002 text.py:218(sentence_to_sequence)\n",
      "    99977    5.086    0.000  163.380    0.002 objective.py:175(function)\n",
      " 16615639  159.019    0.000  159.019    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "  5198804    7.701    0.000  153.353    0.000 constant_op.py:166(constant)\n",
      "  2299471    9.615    0.000  146.507    0.000 gen_math_ops.py:509(_all)\n",
      "  5198804   18.985    0.000  145.652    0.000 constant_op.py:268(_constant_impl)\n",
      " 14715076   12.098    0.000  140.852    0.000 _methods.py:49(_prod)\n",
      " 45244193  136.676    0.000  136.676    0.000 {built-in method numpy.array}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   699839    4.925    0.000  136.636    0.000 array_ops.py:200(fill)\n",
      "   599862   10.162    0.000  136.342    0.000 gen_array_ops.py:8392(reshape_eager_fallback)\n",
      "   999724  136.293    0.000  136.293    0.000 utility.py:84(<listcomp>)\n",
      "   699839    4.511    0.000  126.065    0.000 gen_array_ops.py:3304(fill)\n",
      "   299931    1.939    0.000  117.002    0.000 base.py:308(dY)\n",
      "  7591829   36.568    0.000  111.235    0.000 base.py:119(tensor_shape)\n",
      "  5198804    6.765    0.000  110.978    0.000 constant_op.py:299(_constant_eager_impl)\n",
      " 14715076   29.113    0.000  110.223    0.000 <__array_function__ internals>:2(count_nonzero)\n",
      "  2199494    4.519    0.000  107.603    0.000 trace.py:158(wrapped)\n",
      "  5198804   84.920    0.000  104.213    0.000 constant_op.py:70(convert_to_eager_tensor)\n",
      " 29430152   85.339    0.000  103.495    0.000 preprocess.py:114(vocabulary_size)\n",
      "  2199494   17.387    0.000  103.084    0.000 ops.py:1481(convert_to_tensor)\n",
      " 35403846   64.024    0.000  101.723    0.000 tensor_util.py:992(is_tensor)\n",
      "422023580   95.828    0.000   95.828    0.000 {method 'splitlines' of 'str' objects}\n",
      "  1399678   16.510    0.000   95.247    0.000 execute.py:236(args_to_matching_eager)\n",
      " 17672919   93.031    0.000   93.031    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
      "   699839   16.126    0.000   91.524    0.000 base.py:205(X)\n",
      "    99977    4.741    0.000   90.333    0.001 text.py:266(<listcomp>)\n",
      "   299931    7.593    0.000   84.714    0.000 sgd.py:78(differential)\n",
      "220631505   84.016    0.000   84.016    0.000 {method 'partition' of 'str' objects}\n",
      "    99977    1.095    0.000   83.605    0.001 preprocess.py:380(gradient)\n",
      "   999724    2.032    0.000   81.440    0.000 <__array_function__ internals>:2(pad)\n",
      "220731438   58.382    0.000   80.811    0.000 arrayprint.py:920(<genexpr>)\n",
      "220731438   58.513    0.000   80.644    0.000 arrayprint.py:923(<genexpr>)\n",
      "    99977    0.409    0.000   80.401    0.001 embedding_sgram.py:621(_bagging)\n",
      " 43226978   50.410    0.000   79.981    0.000 numerictypes.py:285(issubclass_)\n",
      "    99977    0.989    0.000   78.127    0.001 adapter.py:69(gradient)\n",
      "438352673   76.609    0.000   76.609    0.000 preprocess.py:129(event_to_index)\n",
      " 29430152   63.784    0.000   76.445    0.000 getlimits.py:382(__new__)\n",
      "220731438   55.308    0.000   76.101    0.000 arrayprint.py:930(<genexpr>)\n",
      "   999724   14.762    0.000   75.717    0.000 arraypad.py:529(pad)\n",
      "  2499425    7.954    0.000   73.767    0.000 base.py:142(tensor_size)\n",
      " 35346838   22.509    0.000   70.462    0.000 _asarray.py:110(asanyarray)\n",
      "201792027   48.892    0.000   68.422    0.000 arrayprint.py:950(<genexpr>)\n",
      "201792027   48.389    0.000   68.269    0.000 arrayprint.py:949(<genexpr>)\n",
      "    99977    1.089    0.000   67.685    0.001 adapter.py:64(function)\n",
      "    99977   10.485    0.000   65.488    0.001 objective.py:251(gradient)\n",
      "   999724   19.493    0.000   64.519    0.000 index_tricks.py:317(__getitem__)\n",
      "  7198344   22.700    0.000   61.430    0.000 ops.py:1168(shape)\n",
      "161802272   58.243    0.000   60.942    0.000 {built-in method builtins.isinstance}\n",
      "   299931    0.642    0.000   56.049    0.000 array_ops.py:700(size_v2)\n",
      " 14715076   20.955    0.000   55.424    0.000 numeric.py:424(count_nonzero)\n",
      "   299931    0.652    0.000   55.017    0.000 array_ops.py:734(size)\n",
      "   299931    4.375    0.000   54.365    0.000 array_ops.py:766(size_internal)\n",
      " 19166105   50.369    0.000   50.369    0.000 {built-in method numpy.empty}\n",
      "   299931    5.725    0.000   48.179    0.000 math_ops.py:1796(range)\n",
      "  2299471    4.977    0.000   48.127    0.000 base.py:149(tensor_dtype)\n",
      "   299931    0.598    0.000   45.656    0.000 base.py:165(to_tensor)\n",
      "   299931   42.117    0.000   44.427    0.000 embedding_sgram.py:597(_extract_event_vectors)\n",
      "   699839    3.288    0.000   44.384    0.000 execute.py:33(quick_execute)\n",
      "  2299471    6.150    0.000   41.615    0.000 base.py:64(is_scalar)\n",
      "  4099057   13.195    0.000   39.352    0.000 tensor_shape.py:748(__init__)\n",
      "   699839   39.055    0.000   39.055    0.000 {built-in method tensorflow.python._pywrap_tfe.TFE_Py_Execute}\n",
      "   999724    4.478    0.000   38.568    0.000 text.py:93(standardize)\n",
      "  1199724    3.683    0.000   37.449    0.000 <__array_function__ internals>:2(all)\n",
      "   699839   15.459    0.000   36.339    0.000 special_math_ops.py:1226(_einsum_v2_parse_and_resolve_equation)\n",
      " 68441660   35.083    0.000   35.083    0.000 {built-in method builtins.issubclass}\n",
      " 14715076   34.469    0.000   34.469    0.000 {built-in method numpy.core._multiarray_umath.count_nonzero}\n",
      "   599862    3.918    0.000   33.557    0.000 constant_op.py:348(_tensor_shape_tensor_conversion_function)\n",
      " 10197654   33.354    0.000   33.354    0.000 context.py:815(executing_eagerly)\n",
      " 66692058   32.331    0.000   32.635    0.000 {built-in method builtins.getattr}\n",
      "   999724    1.357    0.000   32.517    0.000 re.py:203(sub)\n",
      "   399908   13.423    0.000   32.494    0.000 embedding_sgram.py:322(dWs)\n",
      " 17672919   32.457    0.000   32.457    0.000 {method 'flatten' of 'numpy.ndarray' objects}\n",
      "   699839    1.194    0.000   31.833    0.000 constant_op.py:336(_constant_tensor_conversion_function)\n",
      "   399908   12.224    0.000   30.750    0.000 embedding_sgram.py:283(dWc)\n",
      "  1199724    5.214    0.000   30.305    0.000 fromnumeric.py:2355(all)\n",
      " 17389483   19.268    0.000   29.642    0.000 base.py:84(is_tensor)\n",
      "   399908   11.058    0.000   28.096    0.000 embedding_sgram.py:261(dWe)\n",
      "  4398988    8.607    0.000   27.911    0.000 context.py:1874(executing_eagerly)\n",
      "    99977    3.251    0.000   27.877    0.000 embedding_sgram.py:1523(g)\n",
      "    99977    0.712    0.000   27.113    0.000 tf.py:47(full)\n",
      "   999724   27.008    0.000   27.008    0.000 {method 'sub' of 're.Pattern' objects}\n",
      "   999724   12.270    0.000   26.720    0.000 text.py:252(<listcomp>)\n",
      "  1999448    8.971    0.000   26.121    0.000 arraypad.py:454(_as_pairs)\n",
      "    99977   11.110    0.000   26.057    0.000 function.py:751(sigmoid_cross_entropy_log_loss)\n",
      "  1899563    8.107    0.000   25.924    0.000 ops.py:6402(name_scope)\n",
      "  2599402    5.381    0.000   25.388    0.000 ops.py:991(__bool__)\n",
      "  4099057   11.791    0.000   25.235    0.000 tensor_shape.py:758(<listcomp>)\n",
      "   299931    2.401    0.000   25.022    0.000 math_ops.py:1718(tensor_equals)\n",
      "   499885    2.979    0.000   24.915    0.000 embedding_sgram.py:255(Be)\n",
      "  1399678    3.660    0.000   24.845    0.000 <__array_function__ internals>:2(reshape)\n",
      "  2399448    8.381    0.000   24.426    0.000 base.py:93(is_float_tensor)\n",
      " 15814777   24.405    0.000   24.405    0.000 {method 'tolist' of 'numpy.ndarray' objects}\n",
      "     1000    0.006    0.000   22.482    0.022 <__array_function__ internals>:2(mean)\n",
      "     1000    0.017    0.000   22.470    0.022 fromnumeric.py:3301(mean)\n",
      "     1000    0.052    0.000   22.453    0.022 _methods.py:161(_mean)\n",
      "  2699379    3.454    0.000   21.222    0.000 ops.py:1035(_numpy)\n",
      " 16614639   20.831    0.000   20.831    0.000 fromnumeric.py:71(<dictcomp>)\n",
      "   999724    3.460    0.000   20.472    0.000 numerictypes.py:599(find_common_type)\n",
      "  2499425    3.923    0.000   20.278    0.000 ops.py:1181(get_shape)\n",
      "   299931    1.327    0.000   20.253    0.000 gen_math_ops.py:3130(equal)\n",
      "   699839    1.159    0.000   20.101    0.000 array_ops.py:1512(_autopacking_conversion_function)\n",
      "   299931    0.511    0.000   20.079    0.000 array_ops.py:804(rank)\n",
      "   299931    4.248    0.000   19.568    0.000 array_ops.py:840(rank_internal)\n",
      " 17672919   13.687    0.000   19.094    0.000 arraysetops.py:125(_unpack_tuple)\n",
      "   699839    3.919    0.000   18.942    0.000 array_ops.py:1502(_should_not_autopack)\n",
      "   899793    1.336    0.000   18.015    0.000 tensor_conversion_registry.py:50(_default_conversion_function)\n",
      "  1999540   17.956    0.000   17.956    0.000 {built-in method numpy.arange}\n",
      "  2699379   17.768    0.000   17.768    0.000 {method '_numpy_internal' of 'tensorflow.python.framework.ops.EagerTensor' objects}\n",
      "  1399678    4.097    0.000   17.262    0.000 fromnumeric.py:199(reshape)\n",
      "    99977    1.103    0.000   17.075    0.000 gen_array_ops.py:3372(fill_eager_fallback)\n",
      "   199954    0.628    0.000   16.794    0.000 <__array_function__ internals>:2(isin)\n",
      " 17114432   16.326    0.000   16.326    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
      "  3099195    2.701    0.000   15.825    0.000 _asarray.py:23(asarray)\n",
      "   199954    1.212    0.000   15.732    0.000 arraysetops.py:615(isin)\n",
      "  1999448   11.030    0.000   15.273    0.000 numerictypes.py:575(_can_coerce_all)\n",
      "   999724    9.273    0.000   14.410    0.000 arraypad.py:86(_pad_simple)\n",
      " 44145228   13.841    0.000   13.841    0.000 preprocess.py:109(vocabulary)\n",
      " 14715076   13.594    0.000   13.594    0.000 embedding_sgram.py:201(dictionary)\n",
      "   199954    0.515    0.000   13.502    0.000 <__array_function__ internals>:2(in1d)\n",
      "  9197884   11.640    0.000   13.443    0.000 tensor_shape.py:187(__init__)\n",
      "   699839    2.566    0.000   13.396    0.000 re.py:188(match)\n",
      "    99977    0.606    0.000   13.192    0.000 tf.py:169(add)\n",
      "   599862    0.860    0.000   12.722    0.000 base.py:76(is_float_scalar)\n",
      "    99977    1.619    0.000   12.586    0.000 gen_math_ops.py:299(add)\n",
      "   199954    8.093    0.000   12.295    0.000 arraysetops.py:498(in1d)\n",
      "    99977    1.122    0.000   12.272    0.000 function.py:733(check_binary_classification_X_T)\n",
      "   299931    1.312    0.000   12.175    0.000 gen_math_ops.py:7157(_range)\n",
      "   599862    2.014    0.000   11.862    0.000 base.py:33(is_np_float_scalar)\n",
      "   999770    4.108    0.000   11.662    0.000 base.py:259(T)\n",
      "    99977    1.156    0.000   11.638    0.000 base.py:99(T)\n",
      "  2199494    9.565    0.000   11.615    0.000 tensor_shape.py:858(__iter__)\n",
      "   699839    2.187    0.000   11.298    0.000 nest.py:274(flatten)\n",
      "   999724    1.916    0.000   11.067    0.000 <__array_function__ internals>:2(concatenate)\n",
      "   999724    1.420    0.000   11.040    0.000 <__array_function__ internals>:2(round_)\n",
      "  3599080   10.519    0.000   10.519    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "    99977    2.012    0.000   10.482    0.000 embedding_sgram.py:1499(f)\n",
      "  4398988   10.391    0.000   10.391    0.000 {method '_shape_tuple' of 'tensorflow.python.framework.ops.EagerTensor' objects}\n",
      "  1699609   10.292    0.000   10.292    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
      "   899793    3.233    0.000   10.106    0.000 math_ops.py:907(cast)\n",
      "  1699563    6.407    0.000   10.097    0.000 re.py:289(_compile)\n",
      "  1599632    3.810    0.000   10.067    0.000 {built-in method builtins.all}\n",
      "  5898643    9.982    0.000    9.982    0.000 context.py:837(device_name)\n",
      "  1299701    2.648    0.000    9.613    0.000 tensor_util.py:1042(maybe_set_static_shape)\n",
      "   999770    1.985    0.000    9.542    0.000 base.py:126(is_same_shape)\n",
      "  2099517    9.074    0.000    9.506    0.000 dtypes.py:606(as_dtype)\n",
      "   899793    3.109    0.000    9.345    0.000 base.py:191(X)\n",
      "  2399448    3.086    0.000    9.171    0.000 dtypes.py:84(base_dtype)\n",
      "   699839    9.110    0.000    9.111    0.000 {built-in method tensorflow.python._pywrap_utils.Flatten}\n",
      " 31929509    8.996    0.000    8.996    0.000 {method 'rstrip' of 'str' objects}\n",
      "   199954    0.807    0.000    8.895    0.000 composite.py:146(T)\n",
      "  2099471    3.535    0.000    8.815    0.000 base.py:130(tensor_rank)\n",
      "   199954    5.725    0.000    8.701    0.000 embedding_sgram.py:270(Wc)\n",
      "  5798666    8.673    0.000    8.673    0.000 _internal.py:826(npy_ctypes_check)\n",
      "  4998850    6.608    0.000    8.109    0.000 ops.py:1041(dtype)\n",
      "   999724    1.243    0.000    8.071    0.000 fromnumeric.py:3709(round_)\n",
      "    99977    0.449    0.000    8.034    0.000 tf.py:162(concat)\n",
      "  2999172    3.278    0.000    8.001    0.000 <__array_function__ internals>:2(ndim)\n",
      " 15015007    7.974    0.000    7.974    0.000 fromnumeric.py:2907(_prod_dispatcher)\n",
      "  8098137    5.491    0.000    7.691    0.000 tensor_shape.py:864(<genexpr>)\n",
      "   199954    5.314    0.000    7.609    0.000 embedding_sgram.py:315(Ws)\n",
      "   499885    3.469    0.000    7.582    0.000 arrayprint.py:60(_make_options_dict)\n",
      "    99977    1.853    0.000    7.570    0.000 embedding_sgram.py:1456(_adapt_function_handle_Y)\n",
      "    99977    1.575    0.000    7.340    0.000 embedding_sgram.py:159(X)\n",
      "    99977    0.665    0.000    7.180    0.000 array_ops.py:1585(concat)\n",
      "   999724    1.196    0.000    6.828    0.000 <__array_function__ internals>:2(around)\n",
      "  2099517    6.808    0.000    6.808    0.000 dtypes.py:192(__eq__)\n",
      "   199954    4.839    0.000    6.762    0.000 embedding_sgram.py:216(negative_sample_indices)\n",
      "   100010    0.146    0.000    6.730    0.000 {built-in method builtins.next}\n",
      " 16814593    6.588    0.000    6.588    0.000 embedding_sgram.py:211(SL)\n",
      "   100000    0.759    0.000    6.584    0.000 <ipython-input-33-0580c1f4e1b7>:1(sentences_generator)\n",
      "   999724    5.045    0.000    6.501    0.000 arraypad.py:129(_set_pad_area)\n",
      "    99977    0.789    0.000    6.419    0.000 gen_array_ops.py:1170(concat_v2)\n",
      " 17114525    6.352    0.000    6.352    0.000 {method 'items' of 'dict' objects}\n",
      " 14715076    6.308    0.000    6.308    0.000 numeric.py:420(_count_nonzero_dispatcher)\n",
      "  6398528    6.148    0.000    6.148    0.000 base.py:233(N)\n",
      "  2399448    6.086    0.000    6.086    0.000 dtypes.py:71(_is_ref_dtype)\n",
      " 17672919    6.074    0.000    6.074    0.000 arraysetops.py:133(_unique_dispatcher)\n",
      " 17672919    6.019    0.000    6.019    0.000 fromnumeric.py:2442(_cumsum_dispatcher)\n",
      "  1399678    2.609    0.000    5.931    0.000 tensor_shape.py:1180(as_list)\n",
      "   599862    5.921    0.000    5.921    0.000 {built-in method numpy.zeros}\n",
      "   199954    0.542    0.000    5.729    0.000 <__array_function__ internals>:2(sum)\n",
      "   199954    0.993    0.000    5.653    0.000 composite.py:199(_set_label)\n",
      "   199954    3.842    0.000    5.363    0.000 embedding_sgram.py:190(context_indices)\n",
      "  2299471    2.072    0.000    5.255    0.000 math_ops.py:1915(_may_reduce_to_scalar)\n",
      "   599862    1.592    0.000    5.129    0.000 tensor_shape.py:1166(is_fully_defined)\n",
      "  5898643    5.017    0.000    5.017    0.000 context.py:501(ensure_initialized)\n",
      "   699839    1.620    0.000    5.008    0.000 execute.py:284(<listcomp>)\n",
      " 14715076    4.983    0.000    4.983    0.000 preprocess.py:119(probabilities)\n",
      "   699839    4.883    0.000    4.883    0.000 {method 'match' of 're.Pattern' objects}\n",
      "   199954    0.921    0.000    4.676    0.000 fromnumeric.py:2111(sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    99977    0.606    0.000    4.357    0.000 numeric.py:148(ones)\n",
      " 27159920    4.257    0.000    4.257    0.000 {method 'lower' of 'str' objects}\n",
      "   999724    1.306    0.000    4.237    0.000 fromnumeric.py:3199(around)\n",
      "  1799586    3.879    0.000    4.230    0.000 base.py:357(logger)\n",
      "   199954    2.937    0.000    4.101    0.000 embedding_sgram.py:182(target_indices)\n",
      "  1599632    2.510    0.000    3.992    0.000 tensor_conversion_registry.py:114(get)\n",
      "   499885    3.976    0.000    3.976    0.000 arrayprint.py:358(_get_formatdict)\n",
      "  2899333    2.766    0.000    3.911    0.000 tensor_shape.py:821(rank)\n",
      "   299931    1.864    0.000    3.637    0.000 dtypes.py:172(is_compatible_with)\n",
      "  5898643    3.617    0.000    3.617    0.000 context.py:773(_handle)\n",
      " 11697309    3.559    0.000    3.559    0.000 tensor_shape.py:249(value)\n",
      "   499885    3.407    0.000    3.407    0.000 {built-in method builtins.locals}\n",
      "   699839    0.906    0.000    3.389    0.000 ops.py:6817(_is_keras_symbolic_tensor)\n",
      "   699839    3.385    0.000    3.385    0.000 {method 'format' of 'str' objects}\n",
      "  1899563    3.359    0.000    3.360    0.000 __init__.py:1677(isEnabledFor)\n",
      "   899793    1.573    0.000    3.327    0.000 __init__.py:1412(debug)\n",
      "  1399678    2.390    0.000    3.323    0.000 tensor_shape.py:1191(<listcomp>)\n",
      "  1000770    1.604    0.000    3.239    0.000 {built-in method builtins.hasattr}\n",
      "  4598966    3.207    0.000    3.207    0.000 base.py:185(D)\n",
      "  2299471    2.535    0.000    3.183    0.000 math_ops.py:1910(_has_fully_defined_shape)\n",
      "  5998344    3.149    0.000    3.149    0.000 numerictypes.py:584(<listcomp>)\n",
      "  2199494    1.961    0.000    3.078    0.000 tensor_shape.py:845(__len__)\n",
      "   299931    1.229    0.000    3.039    0.000 base.py:557(update)\n",
      "    99977    0.475    0.000    2.890    0.000 ops.py:1047(numpy)\n",
      "   100000    0.697    0.000    2.850    0.000 utility_file.py:221(take)\n",
      "   699839    1.374    0.000    2.803    0.000 __init__.py:1436(warning)\n",
      " 10197654    2.763    0.000    2.763    0.000 context.py:1849(context_safe)\n",
      "   599868    0.972    0.000    2.700    0.000 abc.py:96(__instancecheck__)\n",
      "   699839    1.329    0.000    2.521    0.000 backprop.py:170(_must_record_gradient)\n",
      "   199954    0.924    0.000    2.435    0.000 _ufunc_config.py:32(seterr)\n",
      "    99977    0.274    0.000    2.369    0.000 <__array_function__ internals>:2(amax)\n",
      "  5698689    2.219    0.000    2.219    0.000 {method '_datatype_enum' of 'tensorflow.python.framework.ops.EagerTensor' objects}\n",
      "  2499425    2.198    0.000    2.198    0.000 array_ops.py:1508(<genexpr>)\n",
      "  9097921    2.169    0.000    2.169    0.000 {method 'append' of 'list' objects}\n",
      "   999747    1.705    0.000    2.128    0.000 utility_file.py:280(file_line_stream)\n",
      "   499885    0.876    0.000    2.122    0.000 base.py:102(is_integer_tensor)\n",
      "  1199724    0.903    0.000    2.095    0.000 math_ops.py:1865(<genexpr>)\n",
      "    99977    0.315    0.000    2.058    0.000 _ufunc_config.py:433(__enter__)\n",
      "  2399448    1.538    0.000    1.964    0.000 tensor_shape.py:1169(<genexpr>)\n",
      "  1299701    1.940    0.000    1.940    0.000 ops.py:1225(graph)\n",
      "    99977    0.379    0.000    1.799    0.000 fromnumeric.py:2617(amax)\n",
      "   199954    1.770    0.000    1.770    0.000 base.py:122(L)\n",
      "   399908    1.124    0.000    1.746    0.000 objective.py:147(P)\n",
      "   599868    1.727    0.000    1.728    0.000 {built-in method _abc._abc_instancecheck}\n",
      "   299931    0.483    0.000    1.683    0.000 ops.py:5744(executing_eagerly_outside_functions)\n",
      "  2799356    1.667    0.000    1.667    0.000 {method 'group' of 're.Match' objects}\n",
      "   999724    1.056    0.000    1.664    0.000 types.py:171(__get__)\n",
      "    99977    0.878    0.000    1.655    0.000 function.py:411(transform_X_T)\n",
      "   999724    1.636    0.000    1.636    0.000 arraypad.py:58(_view_roi)\n",
      "  1999448    1.611    0.000    1.611    0.000 arraypad.py:109(<genexpr>)\n",
      "    99977    0.371    0.000    1.540    0.000 <__array_function__ internals>:2(copyto)\n",
      "   999724    1.517    0.000    1.517    0.000 numerictypes.py:651(<listcomp>)\n",
      "  1899563    1.476    0.000    1.476    0.000 ops.py:175(__exit__)\n",
      "  1999448    1.456    0.000    1.456    0.000 arraypad.py:33(_slice_at_axis)\n",
      "    99977    0.965    0.000    1.441    0.000 embedding_sgram.py:248(We)\n",
      "       24    0.000    0.000    1.409    0.059 base.py:590(save)\n",
      "       24    1.069    0.045    1.408    0.059 utility_file.py:198(serialize)\n",
      "  1999448    1.394    0.000    1.394    0.000 arraypad.py:120(<genexpr>)\n",
      "    99977    0.169    0.000    1.365    0.000 <__array_function__ internals>:2(amin)\n",
      "   999724    1.360    0.000    1.360    0.000 {method 'round' of 'numpy.ndarray' objects}\n",
      "  2599402    1.336    0.000    1.336    0.000 embedding_sgram.py:172(C)\n",
      "   299931    1.321    0.000    1.321    0.000 dtypes.py:103(as_numpy_dtype)\n",
      "   699839    0.750    0.000    1.246    0.000 base.py:180(M)\n",
      "  2199494    1.225    0.000    1.225    0.000 tensor_shape.py:792(_v2_behavior)\n",
      "   699839    1.192    0.000    1.192    0.000 {built-in method tensorflow.python._pywrap_tfe.TFE_Py_TapeSetIsEmpty}\n",
      "    99977    1.180    0.000    1.180    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
      "  1199724    1.172    0.000    1.172    0.000 base.py:253(T)\n",
      "   299931    0.566    0.000    1.139    0.000 math_ops.py:1866(<listcomp>)\n",
      "    99977    0.301    0.000    1.139    0.000 base.py:202(to_list)\n",
      "  2999172    1.127    0.000    1.127    0.000 fromnumeric.py:3106(ndim)\n",
      "  1599632    1.087    0.000    1.087    0.000 embedding_sgram.py:167(E)\n",
      "    99977    0.240    0.000    1.030    0.000 fromnumeric.py:2742(amin)\n",
      "    99977    0.299    0.000    0.977    0.000 <__array_function__ internals>:2(squeeze)\n",
      "   199954    0.848    0.000    0.895    0.000 _ufunc_config.py:132(geterr)\n",
      "    99977    0.201    0.000    0.894    0.000 _ufunc_config.py:438(__exit__)\n",
      "    99977    0.224    0.000    0.862    0.000 embedding_sgram.py:154(X)\n",
      "   199954    0.596    0.000    0.831    0.000 embedding_sgram.py:177(window_size)\n",
      "  2199494    0.826    0.000    0.826    0.000 {built-in method builtins.iter}\n",
      "  1399678    0.800    0.000    0.800    0.000 fromnumeric.py:194(_reshape_dispatcher)\n",
      "  2999172    0.797    0.000    0.797    0.000 fromnumeric.py:3102(_ndim_dispatcher)\n",
      "   599862    0.335    0.000    0.792    0.000 tensor_shape.py:840(ndims)\n",
      "   699839    0.668    0.000    0.779    0.000 base.py:325(dS)\n",
      "  1899563    0.746    0.000    0.746    0.000 ops.py:169(__init__)\n",
      "   799840    0.739    0.000    0.739    0.000 base.py:170(name)\n",
      "  1099701    0.691    0.000    0.691    0.000 preprocess.py:491(window_size)\n",
      "  1199724    0.650    0.000    0.650    0.000 fromnumeric.py:2350(_all_dispatcher)\n",
      "   699839    0.615    0.000    0.615    0.000 {method 'replace' of 'str' objects}\n",
      "   499885    0.613    0.000    0.613    0.000 {method 'update' of 'dict' objects}\n",
      "   999724    0.609    0.000    0.609    0.000 enum.py:664(value)\n",
      "   799816    0.608    0.000    0.608    0.000 tensor_shape.py:851(__bool__)\n",
      "  1999448    0.585    0.000    0.585    0.000 {method 'strip' of 'str' objects}\n",
      "  1999448    0.584    0.000    0.584    0.000 fromnumeric.py:3195(_around_dispatcher)\n",
      "   499885    0.577    0.000    0.577    0.000 arrayprint.py:65(<dictcomp>)\n",
      "   199954    0.360    0.000    0.576    0.000 composite.py:141(T)\n",
      "   499885    0.545    0.000    0.545    0.000 {method 'discard' of 'set' objects}\n",
      "   499885    0.514    0.000    0.514    0.000 {method 'copy' of 'dict' objects}\n",
      "   699839    0.496    0.000    0.496    0.000 base.py:175(num_nodes)\n",
      "  1899563    0.495    0.000    0.495    0.000 ops.py:172(__enter__)\n",
      "  1399678    0.492    0.000    0.492    0.000 {method 'pop' of 'dict' objects}\n",
      "  1099701    0.462    0.000    0.462    0.000 preprocess.py:503(event_size)\n",
      "   499885    0.458    0.000    0.458    0.000 {built-in method builtins.id}\n",
      "    99977    0.249    0.000    0.438    0.000 base.py:143(layer_inference)\n",
      "    99977    0.307    0.000    0.437    0.000 objective.py:156(J)\n",
      "   299931    0.259    0.000    0.435    0.000 __init__.py:1424(info)\n",
      "    99977    0.173    0.000    0.420    0.000 fromnumeric.py:1439(squeeze)\n",
      "   100025    0.405    0.000    0.405    0.000 {method 'join' of 'str' objects}\n",
      "   499885    0.401    0.000    0.401    0.000 {built-in method _thread.get_ident}\n",
      "   199954    0.397    0.000    0.397    0.000 {built-in method numpy.seterrobj}\n",
      "   499885    0.362    0.000    0.362    0.000 {method 'add' of 'set' objects}\n",
      "   499885    0.343    0.000    0.343    0.000 arrayprint.py:854(_none_or_positive_arg)\n",
      "   299931    0.343    0.000    0.343    0.000 base.py:63(lr)\n",
      "   999724    0.321    0.000    0.321    0.000 arraypad.py:521(_pad_dispatcher)\n",
      "   999724    0.312    0.000    0.312    0.000 multiarray.py:143(concatenate)\n",
      "   299931    0.302    0.000    0.302    0.000 embedding_sgram.py:355(optimizer)\n",
      "  1099701    0.297    0.000    0.297    0.000 {built-in method builtins.callable}\n",
      "       24    0.289    0.012    0.289    0.012 {built-in method _pickle.dump}\n",
      "   299931    0.288    0.000    0.288    0.000 composite.py:168(layers)\n",
      "    99977    0.259    0.000    0.282    0.000 base.py:157(layers_all)\n",
      "   399908    0.266    0.000    0.266    0.000 {built-in method numpy.geterrobj}\n",
      "    99977    0.247    0.000    0.247    0.000 {method 'squeeze' of 'numpy.ndarray' objects}\n",
      "   299931    0.237    0.000    0.237    0.000 base.py:75(l2)\n",
      "     1023    0.013    0.000    0.234    0.000 {built-in method builtins.print}\n",
      "   999724    0.222    0.000    0.222    0.000 numerictypes.py:652(<listcomp>)\n",
      "     2046    0.018    0.000    0.221    0.000 iostream.py:386(write)\n",
      "    99977    0.157    0.000    0.195    0.000 base.py:335(objective)\n",
      "     3069    0.024    0.000    0.185    0.000 iostream.py:197(schedule)\n",
      "    99977    0.133    0.000    0.175    0.000 function.py:619(transform_scalar_X_T)\n",
      "    99977    0.161    0.000    0.161    0.000 _ufunc_config.py:429(__init__)\n",
      "    99977    0.095    0.000    0.156    0.000 base.py:182(assure_tensor)\n",
      "   199978    0.150    0.000    0.150    0.000 embedding_sgram.py:234(WO)\n",
      "    14829    0.073    0.000    0.135    0.000 codecs.py:319(decode)\n",
      "   199978    0.132    0.000    0.132    0.000 embedding_sgram.py:229(W)\n",
      "     3069    0.132    0.000    0.132    0.000 socket.py:432(send)\n",
      "   199954    0.123    0.000    0.123    0.000 arraysetops.py:494(_in1d_dispatcher)\n",
      "   100977    0.111    0.000    0.111    0.000 {built-in method time.time}\n",
      "    99977    0.095    0.000    0.095    0.000 preprocess.py:104(min_sequence_length)\n",
      "   199954    0.090    0.000    0.090    0.000 arraysetops.py:611(_isin_dispatcher)\n",
      "   199954    0.089    0.000    0.089    0.000 fromnumeric.py:2106(_sum_dispatcher)\n",
      "        1    0.000    0.000    0.067    0.067 <ipython-input-33-0580c1f4e1b7>:59(<module>)\n",
      "    99977    0.065    0.000    0.065    0.000 multiarray.py:1054(copyto)\n",
      "    14829    0.062    0.000    0.062    0.000 {built-in method _codecs.utf_8_decode}\n",
      "    99977    0.062    0.000    0.062    0.000 fromnumeric.py:1435(_squeeze_dispatcher)\n",
      "       48    0.051    0.001    0.055    0.001 {built-in method io.open}\n",
      "    99977    0.051    0.000    0.051    0.000 fromnumeric.py:2612(_amax_dispatcher)\n",
      "    99977    0.038    0.000    0.038    0.000 fromnumeric.py:2737(_amin_dispatcher)\n",
      "     1000    0.033    0.000    0.035    0.000 _methods.py:65(_count_reduce_items)\n",
      "     2046    0.004    0.000    0.034    0.000 iostream.py:323(_schedule_flush)\n",
      "     3069    0.010    0.000    0.022    0.000 threading.py:1071(is_alive)\n",
      "     2046    0.006    0.000    0.013    0.000 iostream.py:310(_is_master_process)\n",
      "     3069    0.004    0.000    0.010    0.000 threading.py:1017(_wait_for_tstate_lock)\n",
      "     2046    0.007    0.000    0.007    0.000 {built-in method posix.getpid}\n",
      "     3069    0.006    0.000    0.006    0.000 iostream.py:93(_event_pipe)\n",
      "       24    0.000    0.000    0.006    0.000 pathlib.py:1210(open)\n",
      "     3069    0.006    0.000    0.006    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "       24    0.000    0.000    0.004    0.000 pathlib.py:1414(is_file)\n",
      "       24    0.000    0.000    0.004    0.000 pathlib.py:1189(stat)\n",
      "       24    0.004    0.000    0.004    0.000 {built-in method posix.stat}\n",
      "     1000    0.003    0.000    0.003    0.000 base.py:129(history)\n",
      "       48    0.000    0.000    0.003    0.000 pathlib.py:1035(__new__)\n",
      "       24    0.000    0.000    0.003    0.000 pathlib.py:1072(_opener)\n",
      "       24    0.003    0.000    0.003    0.000 {built-in method posix.open}\n",
      "       48    0.000    0.000    0.002    0.000 pathlib.py:674(_from_parts)\n",
      "       48    0.000    0.000    0.002    0.000 pathlib.py:654(_parse_args)\n",
      "     1000    0.002    0.000    0.002    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}\n",
      "       48    0.001    0.000    0.002    0.000 pathlib.py:63(parse_parts)\n",
      "       24    0.000    0.000    0.002    0.000 utility_file.py:108(is_path_creatable)\n",
      "     3069    0.001    0.000    0.001    0.000 {method 'append' of 'collections.deque' objects}\n",
      "     3069    0.001    0.000    0.001    0.000 threading.py:513(is_set)\n",
      "       24    0.001    0.000    0.001    0.000 {built-in method posix.access}\n",
      "   154/10    0.000    0.000    0.001    0.000 abc.py:100(__subclasscheck__)\n",
      "   154/10    0.001    0.000    0.001    0.000 {built-in method _abc._abc_subclasscheck}\n",
      "        5    0.000    0.000    0.001    0.000 codeop.py:140(__call__)\n",
      "       72    0.000    0.000    0.001    0.000 pathlib.py:724(__fspath__)\n",
      "        5    0.001    0.000    0.001    0.000 {built-in method builtins.compile}\n",
      "     1000    0.001    0.000    0.001    0.000 fromnumeric.py:3296(_mean_dispatcher)\n",
      "       72    0.000    0.000    0.001    0.000 pathlib.py:714(__str__)\n",
      "      240    0.001    0.000    0.001    0.000 {built-in method sys.intern}\n",
      "       24    0.000    0.000    0.000    0.000 embedding_sgram.py:393(S)\n",
      "       24    0.000    0.000    0.000    0.000 _bootlocale.py:33(getpreferredencoding)\n",
      "       24    0.000    0.000    0.000    0.000 posixpath.py:150(dirname)\n",
      "       48    0.000    0.000    0.000    0.000 pathlib.py:292(splitroot)\n",
      "       24    0.000    0.000    0.000    0.000 {built-in method _locale.nl_langinfo}\n",
      "       48    0.000    0.000    0.000    0.000 pathlib.py:697(_format_parsed_parts)\n",
      "       48    0.000    0.000    0.000    0.000 pathlib.py:1045(_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       24    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
      "       23    0.000    0.000    0.000    0.000 utility_file.py:35(__init__)\n",
      "       24    0.000    0.000    0.000    0.000 __init__.py:145(_DType_reduce)\n",
      "       24    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
      "        6    0.000    0.000    0.000    0.000 tensor_conversion_registry.py:135(<genexpr>)\n",
      "       24    0.000    0.000    0.000    0.000 posixpath.py:41(_get_sep)\n",
      "       48    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x55e106cb7ac0}\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:238(helper)\n",
      "       48    0.000    0.000    0.000    0.000 {method 'lstrip' of 'str' objects}\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:108(__enter__)\n",
      "       72    0.000    0.000    0.000    0.000 {built-in method posix.fspath}\n",
      "       46    0.000    0.000    0.000    0.000 {method 'close' of 'generator' objects}\n",
      "        9    0.000    0.000    0.000    0.000 __init__.py:214(_acquireLock)\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:82(__init__)\n",
      "       24    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
      "      116    0.000    0.000    0.000    0.000 _collections_abc.py:392(__subclasshook__)\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:117(__exit__)\n",
      "       10    0.000    0.000    0.000    0.000 compilerop.py:138(extra_flags)\n",
      "       24    0.000    0.000    0.000    0.000 {built-in method _stat.S_ISREG}\n",
      "       24    0.000    0.000    0.000    0.000 embedding_sgram.py:381(state_elements)\n",
      "        5    0.000    0.000    0.000    0.000 traitlets.py:564(__get__)\n",
      "       48    0.000    0.000    0.000    0.000 {method 'reverse' of 'list' objects}\n",
      "        5    0.000    0.000    0.000    0.000 hooks.py:103(__call__)\n",
      "        9    0.000    0.000    0.000    0.000 __init__.py:223(_releaseLock)\n",
      "       24    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
      "        5    0.000    0.000    0.000    0.000 interactiveshell.py:3315(compare)\n",
      "        9    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}\n",
      "        5    0.000    0.000    0.000    0.000 traitlets.py:533(get)\n",
      "        9    0.000    0.000    0.000    0.000 __init__.py:1663(getEffectiveLevel)\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-33-0580c1f4e1b7>:61(<module>)\n",
      "        5    0.000    0.000    0.000    0.000 ipstruct.py:125(__getattr__)\n",
      "       26    0.000    0.000    0.000    0.000 _collections_abc.py:302(__subclasshook__)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
      "        5    0.000    0.000    0.000    0.000 interactiveshell.py:1277(user_global_ns)\n",
      "        9    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}\n",
      "        5    0.000    0.000    0.000    0.000 hooks.py:168(pre_run_code_hook)\n",
      "        2    0.000    0.000    0.000    0.000 _collections_abc.py:367(__subclasshook__)\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-33-0580c1f4e1b7>:23(<module>)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-33-0580c1f4e1b7>:22(<module>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sentences_generator(path_to_file, num_sentences):\n",
    "    stream = fileio.Function.file_line_stream(path_to_file)\n",
    "    try:\n",
    "        while True:\n",
    "            _lines = fileio.Function.take(num_sentences, stream)\n",
    "            yield np.array(_lines)\n",
    "    finally:\n",
    "        stream.close()\n",
    "\n",
    "# Sentences for the trainig\n",
    "source = sentences_generator(\n",
    "    path_to_file=path_to_corpus, num_sentences=NUM_SENTENCES\n",
    ")\n",
    "\n",
    "# Restore the state if exists.\n",
    "state = embedding.load(STATE_FILE)\n",
    "\n",
    "# Continue training\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "\n",
    "total_sentences = 0\n",
    "epochs = 0\n",
    "\n",
    "for i in range(MAX_ITERATIONS):\n",
    "    try:\n",
    "        sentences = next(source)\n",
    "        total_sentences += len(sentences)\n",
    "\n",
    "        start = time.time()\n",
    "        network.train(X=sentences, T=np.array([0]))\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                f\"Batch {i:05d} of {NUM_SENTENCES} sentences: \"\n",
    "                f\"Average Loss: {np.mean(network.history):10f} \"\n",
    "                f\"Duration {time.time() - start:3f}\"\n",
    "            )\n",
    "        if i % 1000 == 0:\n",
    "            # embedding.save(STATE_FILE)\n",
    "            pass\n",
    "        \n",
    "    except fileio.Function.GenearatorHasNoMore as e:\n",
    "        source.close()\n",
    "        embedding.save(STATE_FILE)\n",
    "\n",
    "        # Next epoch\n",
    "        print(f\"epoch {epochs} batches {i:05d} done\")\n",
    "        epochs += 1\n",
    "        source = sentences_generator(\n",
    "            path_to_file=path_to_corpus, num_sentences=NUM_SENTENCES\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        source.close()\n",
    "        raise e\n",
    "\n",
    "embedding.save(STATE_FILE)\n",
    "\n",
    "profiler.disable()\n",
    "profiler.print_stats(sort=\"cumtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluate the vector space\n",
    "\n",
    "Verify if the trained model, or the vector space W, has encoded the words in a way that **similar** words are close in the vector space.\n",
    "\n",
    "* [How to measure the similarity among vectors](https://math.stackexchange.com/questions/4132458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words ['cash']\n",
      "Word indices [413]\n",
      "prediction for ['cash']:\n",
      "[['dividends' 'exceed' 'amount' 'borrowings' 'expense' 'exceeding' 'emhart' 'liabilities' 'riskier' 'flow']\n",
      " ['borrowings' 'emhart' 'unpaid' 'sums' 'exceeding' 'riskier' 'financings' 'exceed' 'expenditures' 'beneficiaries']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oonisim/home/repository/git/oonisim/python_programs/nlp/src/layer/preprocessing/preprocess.py:216: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return self._vocabulary[list(iter(indices))]\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "context = \"cash\".split()\n",
    "word_indices = np.array(word_indexing.list_indices(context), dtype=TYPE_INT)\n",
    "\n",
    "print(f\"Words {context}\")\n",
    "print(f\"Word indices {word_indices}\")\n",
    "print(f\"prediction for {context}:\\n{word_indexing.list_events([embedding.predict(word_indices, n)])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Compare with [gensim word2vec](https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import (\n",
    "    Word2Vec\n",
    ")\n",
    "from gensim.models.word2vec import (\n",
    "    LineSentence    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = LineSentence(source=path_to_corpus)\n",
    "w2v = Word2Vec(\n",
    "    sentences=sentences, \n",
    "    sg=0,\n",
    "    window=5, \n",
    "    negative=5,\n",
    "    vector_size=100, \n",
    "    min_count=1, \n",
    "    workers=4\n",
    ")\n",
    "del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amount', 0.8858454823493958),\n",
       " ('debt', 0.8851679563522339),\n",
       " ('assets', 0.8694609999656677),\n",
       " ('payments', 0.8522050380706787),\n",
       " ('value', 0.8282856941223145),\n",
       " ('proceeds', 0.8225603103637695),\n",
       " ('dividend', 0.8094569444656372),\n",
       " ('reserves', 0.808117687702179),\n",
       " ('face', 0.8079915046691895),\n",
       " ('total', 0.8020584583282471)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(context, topn=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
