{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec implementation\n",
    "\n",
    "## Original papers\n",
    "* [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "* [word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method](https://arxiv.org/abs/1402.3722)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The original paper proposed using:\n",
    "1. Matmul to extract word vectors from ```Win```.\n",
    "2. Softmax to calculate scores with all the word vectors from ```Wout```.\n",
    "\n",
    "<img src=\"image/word2vec_cbow_mechanism.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "### Motivation\n",
    "\n",
    "The original implementation was computationally expensive.\n",
    "\n",
    "1. Matmal to extract word vectors from ```Win```.\n",
    "2. Softmax can happen vocabulary size times with ```Wout```.\n",
    "\n",
    "<img src=\"image/word2vec_negative_sampling_motivation.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1. Use index to extract word vectors from Win.\n",
    "2. Instead of calculating softmax with all the word vectors from ```Wout```, sample small number (SL) of negative/false word vectors from ```Wout``` and calculate logistic log loss with each sample. \n",
    "\n",
    "<img src=\"image/wors2vec_neg_sample_backprop.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using only one Word vector space W\n",
    "\n",
    "There is no reasoning nor proof why two word vector space are required. In the end, we only use one word vector space, which appears to be ```Win```. \n",
    "\n",
    "However, if we use one vector space ```W``` for ```event```, ```context``` and ```negative samples```,then an event vector ```event=W[i]``` in a sentence can be used as a negative sample in another setence. Then the weight ```W[i]``` is updated for both positive and negative labels in the same gradient descent on ```W```. The actual [experiment of using only one vector space](./layer/embedding_single_vector_space.py) ```W``` did not work well.\n",
    "\n",
    "* [Why do we need 2 matrices for word2vec or GloVe](https://datascience.stackexchange.com/a/94422/68313)\n",
    "\n",
    "\n",
    "<img src=\"image/word2vec_why_not_one_W.png\" align=\"left\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from itertools import islice\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=400) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Google Colab environment\n",
    "\n",
    "Colab gets disconnected within approx 20 min. Hence not suitable for training (or need to upgrade to the pro version)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone github to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_GOOGLE_COLAB = True\n",
    "except:\n",
    "    IN_GOOGLE_COLAB = False\n",
    "    \n",
    "if IN_GOOGLE_COLAB:\n",
    "    !pip install line_profiler\n",
    "    !google.colab.drive.mount('/content/gdrive')\n",
    "    !rm -rf /content/drive/MyDrive/github\n",
    "    !mkdir -p /content/drive/MyDrive/github\n",
    "    !git clone https://github.com/oonisim/python-programs.git /content/drive/MyDrive/github\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone github to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_GOOGLE_COLAB = True\n",
    "except:\n",
    "    IN_GOOGLE_COLAB = False\n",
    "    \n",
    "if IN_GOOGLE_COLAB:\n",
    "    !pip install line_profiler\n",
    "    !google.colab.drive.mount('/content/gdrive')\n",
    "    !rm -rf /content/github\n",
    "    !mkdir -p /content/github\n",
    "    !git clone https://github.com/oonisim/python-programs.git /content/github\n",
    "        \n",
    "    import sys\n",
    "    sys.path.append('/content/github/nlp/src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook setups\n",
    "\n",
    "Auto reolaod causes an error in Jupyter notebooks. Restart the Jupyter kernel for the error:\n",
    "```TypeError: super(type, obj): obj must be an instance or subtype of type```\n",
    "See\n",
    "- https://stackoverflow.com/a/52927102/4281353\n",
    "- http://thomas-cokelaer.info/blog/2011/09/382/\n",
    "\n",
    "> The problem resides in the mechanism of reloading modules.\n",
    "> Reloading a module often changes the internal object in memory which\n",
    "> makes the isinstance test of super return False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "import function.fileio as fileio\n",
    "import function.text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.constant import (\n",
    "    TYPE_INT,\n",
    "    TYPE_FLOAT,\n",
    "    TYPE_LABEL,\n",
    "    TYPE_TENSOR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants\n",
    "\n",
    "Too large LR generates unusable event vector space.\n",
    "\n",
    "Uniform weight distribution does not work (Why?)\n",
    "Weights from the normal distribution sampling with small std (0.01) works (Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PTB = True\n",
    "USE_TEXT8 = False\n",
    "\n",
    "CORPUS_FILE = \"text8_512\" if USE_TEXT8 else \"ptb_train\"\n",
    "CORPUS_URL = \"https://data.deepai.org/text8.zip\" \\\n",
    "    if USE_TEXT8 else f'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt' \\\n",
    "\n",
    "TARGET_SIZE = TYPE_INT(1)       # Size of the target event (word)\n",
    "CONTEXT_SIZE = TYPE_INT(10)     # Size of the context in which the target event occurs.\n",
    "WINDOW_SIZE = TARGET_SIZE + CONTEXT_SIZE\n",
    "SAMPLE_SIZE = TYPE_INT(5)       # Size of the negative samples\n",
    "\n",
    "VECTOR_SIZE = TYPE_INT(100)     # Number of features in the event vector.\n",
    "WEIGHT_SCHEME = \"normal\"\n",
    "WEIGHT_PARAMS = {\n",
    "    \"std\": 0.01\n",
    "}\n",
    "\n",
    "LR = TYPE_FLOAT(5)\n",
    "NUM_SENTENCES = 10\n",
    "\n",
    "STATE_FILE = \"../models/word2vec_%s_E%s_C%s_S%s_W%s_%s_%s_V%s_LR%s_N%s.pkl\" % (\n",
    "    CORPUS_FILE,\n",
    "    TARGET_SIZE,\n",
    "    CONTEXT_SIZE,\n",
    "    SAMPLE_SIZE,\n",
    "    WEIGHT_SCHEME,\n",
    "    \"std\",\n",
    "    WEIGHT_PARAMS[\"std\"],\n",
    "    VECTOR_SIZE,\n",
    "    LR,\n",
    "    NUM_SENTENCES,\n",
    ")\n",
    "\n",
    "MAX_ITERATIONS = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oonisim/.keras/datasets/ptb_train\n"
     ]
    }
   ],
   "source": [
    "path_to_corpus = f\"~/.keras/datasets/{CORPUS_FILE}\"\n",
    "if fileio.Function.is_file(path_to_corpus):\n",
    "    pass\n",
    "else:\n",
    "    # text8, run \"cat text8 | xargs -n 512 > text8_512\" after download\n",
    "    path_to_corpus = tf.keras.utils.get_file(\n",
    "        fname=CORPUS_FILE,\n",
    "        origin=CORPUS_URL,\n",
    "        extract=True\n",
    "    )\n",
    "corpus = fileio.Function.read_file(path_to_corpus)\n",
    "print(path_to_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n"
     ]
    }
   ],
   "source": [
    "examples = corpus.split('\\n')[:1]\n",
    "for line in examples:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Event (word) indexing\n",
    "Index the events that have occurred in the event sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oonisim/conda/envs/python_programs/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "from layer.preprocessing import (\n",
    "    EventIndexing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexing = EventIndexing(\n",
    "    name=\"word_indexing\",\n",
    "    corpus=corpus,\n",
    "    min_sequence_length=WINDOW_SIZE\n",
    ")\n",
    "del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EventIndexing  for the corpus\n",
    "\n",
    "Adapt to the ```corpus``` and provides:\n",
    "* event_to_index dictionary\n",
    "* vocaburary of the corpus\n",
    "* word occurrence probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EventIndexing.vocabulary[10]:\n",
      "['<nil>' '<unk>' 'aer' 'banknote' 'berlitz' 'calloway' 'centrust' 'cluett' 'fromstein' 'gitano']\n",
      "\n",
      "EventIndexing.event_to_index[10]:\n",
      "('<nil>', 0)\n",
      "('<unk>', 1)\n",
      "('aer', 2)\n",
      "('banknote', 3)\n",
      "('berlitz', 4)\n",
      "('calloway', 5)\n",
      "('centrust', 6)\n",
      "('cluett', 7)\n",
      "('fromstein', 8)\n",
      "('gitano', 9)\n",
      "\n",
      "EventIndexing.probabilities[10]:\n",
      "<nil>                : 0.00000e+00\n",
      "<unk>                : 1.65308e-02\n",
      "aer                  : 5.34860e-06\n",
      "banknote             : 5.34860e-06\n",
      "berlitz              : 5.34860e-06\n",
      "calloway             : 5.34860e-06\n",
      "centrust             : 5.34860e-06\n",
      "cluett               : 5.34860e-06\n",
      "fromstein            : 5.34860e-06\n",
      "gitano               : 5.34860e-06\n"
     ]
    }
   ],
   "source": [
    "words = word_indexing.list_events(range(10))\n",
    "print(f\"EventIndexing.vocabulary[10]:\\n{words}\\n\")\n",
    "\n",
    "indices = word_indexing.list_indices(words)\n",
    "print(f\"EventIndexing.event_to_index[10]:\")\n",
    "for item in zip(words, indices):\n",
    "    print(item)\n",
    "\n",
    "probabilities = word_indexing.list_probabilities(words)\n",
    "print(f\"\\nEventIndexing.probabilities[10]:\")\n",
    "for word, p in zip(words, probabilities):\n",
    "    print(f\"{word:20s} : {p:.5e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling using the probability\n",
    "\n",
    "Sample events according to their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['expanding', 'ways', 'day', 'anything', 'of']\n"
     ]
    }
   ],
   "source": [
    "sample = word_indexing.sample(size=5)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "Sample events not including those events already sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_indices=[162, 484, 1670, 582, 3050] \n",
      "events=['are' 'interests' 'temporarily' 'makers' 'jury']\n"
     ]
    }
   ],
   "source": [
    "negative_indices = word_indexing.negative_sample_indices(\n",
    "    size=5, excludes=word_indexing.list_indices(sample)\n",
    ")\n",
    "print(f\"negative_indices={negative_indices} \\nevents={word_indexing.list_events(negative_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the             :    34\n",
      "asbestos        :    63\n",
      "fiber           :    86\n",
      "<unk>           :     1\n",
      "is              :    42\n",
      "unusually       :    87\n",
      "<unk>           :     1\n",
      "once            :    64\n",
      "it              :    80\n",
      "enters          :    88\n",
      "the             :    34\n",
      "<unk>           :     1\n"
     ]
    }
   ],
   "source": [
    "# sentences = \"\\n\".join(corpus.split('\\n')[5:6])\n",
    "sentences = \"\"\"\n",
    "the asbestos fiber <unk> is unusually <unk> once it enters the <unk> \n",
    "\"\"\"\n",
    "sequences = word_indexing.function(sentences)\n",
    "for pair in zip(sentences.strip().split(\" \"), sequences[0]):\n",
    "    print(f\"{pair[0]:15} : {pair[1]:5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EventContext\n",
    "\n",
    "EventContext layer generates ```(event, context)``` pairs from sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer.preprocessing import (\n",
    "    EventContext\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_context = EventContext(\n",
    "    name=\"ev\",\n",
    "    window_size=WINDOW_SIZE,\n",
    "    event_size=TARGET_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context of an event (word) in a sentence\n",
    "\n",
    "In the sentence ```\"a form of asbestos once used to make kent cigarette filters\"```, one of the context windows ```a form of asbestos once``` of size 5 and event size 1 has.\n",
    "* ```of``` as a target word.\n",
    "* ```(a, form) and (asbestos, once)``` as its context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence of the word indices for the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[37, 62, 44, 63, 64, 65, 66, 67, 68, 69, 70],\n",
       "       [29, 30, 31, 50, 51, 43, 44, 52, 53,  0,  0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \"\"\"\n",
    "a form of asbestos once used to make kent cigarette filters\n",
    "\n",
    "N years old and former chairman of consolidated gold \n",
    "\"\"\"\n",
    "\n",
    "sequence = word_indexing.function(sentences)\n",
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (event, context) pairs\n",
    "\n",
    "For each word (event) in the setence ```(of, asbestos, ... , kent)``` excludnig the ends of the sentence, create ```(target, context)``` as:\n",
    "\n",
    "```\n",
    "[\n",
    "  [of, a, form, asbestos, once],              # target is 'of', context is (a, form, asbestos, once)\n",
    "  ['asbestos', 'form', 'of', 'once', 'used'],\n",
    "  ['once', 'of', 'asbestos', 'used', 'to'],\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "### Format of the (event, context) pairs\n",
    "\n",
    "* **E** is the target event indices\n",
    "* **C** is the context indices\n",
    "\n",
    "<img src=\"image/event_context_format.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event context pairs. Shape (2, 11), Target event size 1, Window size 11.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[65, 37, 62, 44, 63, 64, 66, 67, 68, 69, 70],\n",
       "       [43, 29, 30, 31, 50, 51, 44, 52, 53,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_context_pairs = event_context.function(sequence)\n",
    "print(\n",
    "    f\"Event context pairs. Shape %s, Target event size %s, Window size %s.\" % \n",
    "    (event_context_pairs.shape, event_context.event_size, event_context.window_size)\n",
    ")\n",
    "event_context_pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (event, context) pairs in textual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['used',\n",
       "  'a',\n",
       "  'form',\n",
       "  'of',\n",
       "  'asbestos',\n",
       "  'once',\n",
       "  'to',\n",
       "  'make',\n",
       "  'kent',\n",
       "  'cigarette',\n",
       "  'filters'],\n",
       " ['chairman',\n",
       "  'n',\n",
       "  'years',\n",
       "  'old',\n",
       "  'and',\n",
       "  'former',\n",
       "  'of',\n",
       "  'consolidated',\n",
       "  'gold',\n",
       "  '<nil>',\n",
       "  '<nil>']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indexing.sequence_to_sentence(event_context_pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Word Embedding\n",
    "\n",
    "Embedding is to train the model to group similar events in a close proximity in the event vector space. If two events e.g. 'pencil' and 'pen' are similar concepts, then their event vectors resides in a close distance in the event space. \n",
    "\n",
    "* [Thought Vectors](https://wiki.pathmind.com/thought-vectors)\n",
    "\n",
    "## Training process\n",
    "\n",
    "1. Calculate ```Bc```, the BoW (Bag of Words) from context event vectors.\n",
    "2. Calculate ```Be```,  the BoW (Bag of Words) from target event vectors.\n",
    "3. The dot product ```Ye = dot(Bc, Be)``` is given the label 1 to get them closer.\n",
    "4. For each negative sample ```Ws(s)```, the dot product with ```Ys = dot(Be, Ws(s)``` is given the label 0 to get them apart. \n",
    "5. ```np.c_[Ye, Ys]``` is fowarded to the logistic log loss layer.\n",
    "\n",
    "<img src=\"image/word2vec_backprop_Be.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer import (\n",
    "    Embedding\n",
    ")\n",
    "from optimizer import (\n",
    "    SGD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding: Embedding = Embedding(\n",
    "    name=\"embedding\",\n",
    "    num_nodes=WINDOW_SIZE,\n",
    "    target_size=TARGET_SIZE,\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    negative_sample_size=SAMPLE_SIZE,\n",
    "    event_vector_size=VECTOR_SIZE,\n",
    "    optimizer=SGD(lr=LR),\n",
    "    dictionary=word_indexing,\n",
    "    weight_initialization_scheme=WEIGHT_SCHEME,\n",
    "    weight_initialization_parameters=WEIGHT_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores ```np.c_[Ye, Ys]``` from the Embedding\n",
    "\n",
    "The 0th column is the scores for ```dot(Bc, Be``` for positive labels. The rest are the scores for ```dot(Bc, Ws)``` for negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "[[ 4.69841558e-04  1.67092177e-04 -2.82329624e-04  1.15030198e-04 -2.25383643e-04 -4.19924036e-04]\n",
      " [-1.51731292e-04  5.58194588e-04 -1.18213458e-04  1.13727045e-04 -1.78170449e-05  1.23736536e-04]]\n",
      "\n",
      "Scores for dot(Bc, Be): \n",
      "[[ 0.00046984]\n",
      " [-0.00015173]]\n"
     ]
    }
   ],
   "source": [
    "scores = embedding.function(event_context_pairs)\n",
    "print(f\"Scores:\\n{scores[:5]}\\n\")\n",
    "print(f\"Scores for dot(Bc, Be): \\n{scores[:5, :1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer.normalization import (\n",
    "    BatchNormalization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = BatchNormalization(\n",
    "    name=\"bn\",\n",
    "    num_nodes=1+SAMPLE_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Logistic Log Loss\n",
    "\n",
    "Train the model to get:\n",
    "1. BoW of the context event vectors close to the target event vector. Label 1\n",
    "2. BoW of the context event vectors away from each of the negative sample event vectors Label 0.\n",
    "\n",
    "This is a binary logistic classification, hence use Logistic Log Loss as the network objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.function import (\n",
    "    sigmoid_cross_entropy_log_loss,\n",
    "    sigmoid\n",
    ")\n",
    "from layer.objective import (\n",
    "    CrossEntropyLogLoss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CrossEntropyLogLoss(\n",
    "    name=\"loss\",\n",
    "    num_nodes=1,  # Logistic log loss\n",
    "    log_loss_function=sigmoid_cross_entropy_log_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Adapter\n",
    "\n",
    "The logistic log loss layer expects the input of shape ```(N,M=1)```, however Embedding outputs ```(N,(1+SL)``` where ```SL``` is SAMPLE_SIZE. The ```Adapter``` layer bridges between Embedding and the Loss layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer.adapter import (\n",
    "    Adapter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_function = embedding.adapt_function_to_logistic_log_loss(loss=loss)\n",
    "adapter_gradient = embedding.adapt_gradient_to_logistic_log_loss()\n",
    "\n",
    "adapter: Adapter = Adapter(\n",
    "    name=\"adapter\",\n",
    "    num_nodes=1,    # Number of output M=1 \n",
    "    function=adapter_function,\n",
    "    gradient=adapter_gradient\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Word2vec Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a sequential network\n",
    "\n",
    "$ \\text {Sentences} \\rightarrow EventIndexing \\rightarrow EventContext \\rightarrow  Embedding \\rightarrow Adapter \\rightarrow LogisticLogLoss$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.sequential import (\n",
    "    SequentialNetwork\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = SequentialNetwork(\n",
    "    name=\"word2vec\",\n",
    "    num_nodes=1,\n",
    "    inference_layers=[\n",
    "        word_indexing,\n",
    "        event_context,\n",
    "        embedding,\n",
    "        adapter\n",
    "    ],\n",
    "    objective_layers=[\n",
    "        loss\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved model file if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "STATE_FILE: ../models/word2vec_ptb_train_E1_C10_Wnormal_std_0.01_V100_LR5.0_S5_N10.pkl\n",
      "Model loaded.\n",
      "    event_size 1\n",
      "    context_size: 10\n",
      "    event_vector_size: 100\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "if fileio.Function.is_file(STATE_FILE):\n",
    "    print(\"Loading model...\\nSTATE_FILE: %s\" % STATE_FILE)\n",
    "    state = embedding.load(STATE_FILE)\n",
    "\n",
    "    fmt=\"\"\"Model loaded.\n",
    "    event_size %s\n",
    "    context_size: %s\n",
    "    event_vector_size: %s\n",
    "    \"\"\"\n",
    "    print(fmt % (\n",
    "        state[\"target_size\"], \n",
    "        state[\"context_size\"], \n",
    "        state[\"event_vector_size\"]\n",
    "    ))\n",
    "else:\n",
    "    print(\"State file does not exist. Saving the initial model to %s.\" % STATE_FILE)\n",
    "    embedding.save(STATE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 00000 of 10 sentences: Average Loss:   0.377837 Duration 0.253498\n",
      "Batch 00100 of 10 sentences: Average Loss:   0.300687 Duration 0.143290\n",
      "Batch 00200 of 10 sentences: Average Loss:   0.293107 Duration 0.067000\n",
      "Batch 00300 of 10 sentences: Average Loss:   0.287961 Duration 0.170852\n",
      "Batch 00400 of 10 sentences: Average Loss:   0.283184 Duration 0.160169\n",
      "Batch 00500 of 10 sentences: Average Loss:   0.281906 Duration 0.162519\n",
      "Batch 00600 of 10 sentences: Average Loss:   0.282994 Duration 0.127383\n",
      "Batch 00700 of 10 sentences: Average Loss:   0.281182 Duration 0.176040\n",
      "Batch 00800 of 10 sentences: Average Loss:   0.280507 Duration 0.165288\n",
      "Batch 00900 of 10 sentences: Average Loss:   0.280256 Duration 0.140948\n",
      "Batch 01000 of 10 sentences: Average Loss:   0.279790 Duration 0.166355\n",
      "Batch 01100 of 10 sentences: Average Loss:   0.279876 Duration 0.146388\n",
      "Batch 01200 of 10 sentences: Average Loss:   0.279638 Duration 0.137938\n",
      "Batch 01300 of 10 sentences: Average Loss:   0.278991 Duration 0.181569\n",
      "Batch 01400 of 10 sentences: Average Loss:   0.277850 Duration 0.164105\n",
      "Batch 01500 of 10 sentences: Average Loss:   0.278057 Duration 0.170189\n",
      "Batch 01600 of 10 sentences: Average Loss:   0.277776 Duration 0.125877\n",
      "Batch 01700 of 10 sentences: Average Loss:   0.277427 Duration 0.119789\n",
      "Batch 01800 of 10 sentences: Average Loss:   0.277427 Duration 0.183687\n",
      "Batch 01900 of 10 sentences: Average Loss:   0.276936 Duration 0.153116\n",
      "Batch 02000 of 10 sentences: Average Loss:   0.276343 Duration 0.137354\n",
      "Batch 02100 of 10 sentences: Average Loss:   0.276119 Duration 0.136997\n",
      "Batch 02200 of 10 sentences: Average Loss:   0.275582 Duration 0.161773\n",
      "Batch 02300 of 10 sentences: Average Loss:   0.274738 Duration 0.105567\n",
      "Batch 02400 of 10 sentences: Average Loss:   0.274815 Duration 0.140662\n",
      "Batch 02500 of 10 sentences: Average Loss:   0.274118 Duration 0.182591\n",
      "Batch 02600 of 10 sentences: Average Loss:   0.273682 Duration 0.147052\n",
      "Batch 02700 of 10 sentences: Average Loss:   0.273956 Duration 0.139359\n",
      "Batch 02800 of 10 sentences: Average Loss:   0.273856 Duration 0.159112\n",
      "Batch 02900 of 10 sentences: Average Loss:   0.273753 Duration 0.056455\n",
      "Batch 03000 of 10 sentences: Average Loss:   0.273440 Duration 0.131843\n",
      "Batch 03100 of 10 sentences: Average Loss:   0.273547 Duration 0.178867\n",
      "Batch 03200 of 10 sentences: Average Loss:   0.273284 Duration 0.193338\n",
      "Batch 03300 of 10 sentences: Average Loss:   0.273139 Duration 0.101927\n",
      "Batch 03400 of 10 sentences: Average Loss:   0.272827 Duration 0.136708\n",
      "Batch 03500 of 10 sentences: Average Loss:   0.272565 Duration 0.110763\n",
      "Batch 03600 of 10 sentences: Average Loss:   0.272361 Duration 0.192427\n",
      "Batch 03700 of 10 sentences: Average Loss:   0.272438 Duration 0.169122\n",
      "Batch 03800 of 10 sentences: Average Loss:   0.272296 Duration 0.145765\n",
      "Batch 03900 of 10 sentences: Average Loss:   0.272057 Duration 0.144139\n",
      "Batch 04000 of 10 sentences: Average Loss:   0.271730 Duration 0.122567\n",
      "Batch 04100 of 10 sentences: Average Loss:   0.271563 Duration 0.152551\n",
      "Batch 04200 of 10 sentences: Average Loss:   0.271383 Duration 0.152621\n",
      "epoch 0 batches 04207 done\n",
      "Batch 04300 of 10 sentences: Average Loss:   0.271459 Duration 0.209335\n",
      "Batch 04400 of 10 sentences: Average Loss:   0.271493 Duration 0.157186\n",
      "Batch 04500 of 10 sentences: Average Loss:   0.271417 Duration 0.326414\n",
      "Batch 04600 of 10 sentences: Average Loss:   0.271091 Duration 0.200399\n",
      "Batch 04700 of 10 sentences: Average Loss:   0.271061 Duration 0.213619\n",
      "Batch 04800 of 10 sentences: Average Loss:   0.271181 Duration 0.120387\n",
      "Batch 04900 of 10 sentences: Average Loss:   0.271022 Duration 0.147453\n",
      "Batch 05000 of 10 sentences: Average Loss:   0.270914 Duration 0.177957\n",
      "Batch 05100 of 10 sentences: Average Loss:   0.270938 Duration 0.270280\n",
      "Batch 05200 of 10 sentences: Average Loss:   0.270847 Duration 0.106804\n",
      "Batch 05300 of 10 sentences: Average Loss:   0.270930 Duration 0.095705\n",
      "Batch 05400 of 10 sentences: Average Loss:   0.270952 Duration 0.177558\n",
      "Batch 05500 of 10 sentences: Average Loss:   0.270887 Duration 0.102531\n",
      "Batch 05600 of 10 sentences: Average Loss:   0.270631 Duration 0.153026\n",
      "Batch 05700 of 10 sentences: Average Loss:   0.270715 Duration 0.146709\n",
      "Batch 05800 of 10 sentences: Average Loss:   0.270682 Duration 0.156187\n",
      "Batch 05900 of 10 sentences: Average Loss:   0.270605 Duration 0.106295\n",
      "Batch 06000 of 10 sentences: Average Loss:   0.270623 Duration 0.147206\n",
      "Batch 06100 of 10 sentences: Average Loss:   0.270501 Duration 0.194231\n",
      "Batch 06200 of 10 sentences: Average Loss:   0.270410 Duration 0.149337\n",
      "Batch 06300 of 10 sentences: Average Loss:   0.270332 Duration 0.170178\n",
      "Batch 06400 of 10 sentences: Average Loss:   0.270168 Duration 0.159103\n",
      "Batch 06500 of 10 sentences: Average Loss:   0.269882 Duration 0.092045\n",
      "Batch 06600 of 10 sentences: Average Loss:   0.269881 Duration 0.086924\n",
      "Batch 06700 of 10 sentences: Average Loss:   0.269744 Duration 0.127417\n",
      "Batch 06800 of 10 sentences: Average Loss:   0.269484 Duration 0.224966\n",
      "Batch 06900 of 10 sentences: Average Loss:   0.269554 Duration 0.139096\n",
      "Batch 07000 of 10 sentences: Average Loss:   0.269523 Duration 0.099745\n",
      "Batch 07100 of 10 sentences: Average Loss:   0.269491 Duration 0.174809\n",
      "Batch 07200 of 10 sentences: Average Loss:   0.269339 Duration 0.126693\n",
      "Batch 07300 of 10 sentences: Average Loss:   0.269391 Duration 0.231983\n",
      "Batch 07400 of 10 sentences: Average Loss:   0.269271 Duration 0.077577\n",
      "Batch 07500 of 10 sentences: Average Loss:   0.269214 Duration 0.122562\n",
      "Batch 07600 of 10 sentences: Average Loss:   0.269093 Duration 0.122439\n",
      "Batch 07700 of 10 sentences: Average Loss:   0.268985 Duration 0.158569\n",
      "Batch 07800 of 10 sentences: Average Loss:   0.268875 Duration 0.248313\n",
      "Batch 07900 of 10 sentences: Average Loss:   0.268879 Duration 0.117201\n",
      "Batch 08000 of 10 sentences: Average Loss:   0.268815 Duration 0.082326\n",
      "Batch 08100 of 10 sentences: Average Loss:   0.268693 Duration 0.089378\n",
      "Batch 08200 of 10 sentences: Average Loss:   0.268509 Duration 0.171379\n",
      "Batch 08300 of 10 sentences: Average Loss:   0.268425 Duration 0.138952\n",
      "Batch 08400 of 10 sentences: Average Loss:   0.268348 Duration 0.120713\n",
      "epoch 1 batches 08415 done\n",
      "Batch 08500 of 10 sentences: Average Loss:   0.268391 Duration 0.122211\n",
      "Batch 08600 of 10 sentences: Average Loss:   0.268377 Duration 0.149619\n",
      "Batch 08700 of 10 sentences: Average Loss:   0.268357 Duration 0.109917\n",
      "Batch 08800 of 10 sentences: Average Loss:   0.268191 Duration 0.106251\n",
      "Batch 08900 of 10 sentences: Average Loss:   0.268161 Duration 0.145382\n",
      "Batch 09000 of 10 sentences: Average Loss:   0.268217 Duration 0.116600\n",
      "Batch 09100 of 10 sentences: Average Loss:   0.268156 Duration 0.149468\n",
      "Batch 09200 of 10 sentences: Average Loss:   0.268080 Duration 0.167264\n",
      "Batch 09300 of 10 sentences: Average Loss:   0.268105 Duration 0.131238\n",
      "Batch 09400 of 10 sentences: Average Loss:   0.268069 Duration 0.078846\n",
      "Batch 09500 of 10 sentences: Average Loss:   0.268112 Duration 0.213382\n",
      "Batch 09600 of 10 sentences: Average Loss:   0.268078 Duration 0.131073\n",
      "Batch 09700 of 10 sentences: Average Loss:   0.268065 Duration 0.099635\n",
      "Batch 09800 of 10 sentences: Average Loss:   0.267887 Duration 0.141061\n",
      "Batch 09900 of 10 sentences: Average Loss:   0.267916 Duration 0.172245\n",
      "Batch 10000 of 10 sentences: Average Loss:   0.267914 Duration 0.110472\n",
      "Batch 10100 of 10 sentences: Average Loss:   0.267859 Duration 0.110921\n",
      "Batch 10200 of 10 sentences: Average Loss:   0.267858 Duration 0.122406\n",
      "Batch 10300 of 10 sentences: Average Loss:   0.267783 Duration 0.170204\n",
      "Batch 10400 of 10 sentences: Average Loss:   0.267730 Duration 0.167379\n",
      "Batch 10500 of 10 sentences: Average Loss:   0.267645 Duration 0.103167\n",
      "Batch 10600 of 10 sentences: Average Loss:   0.267539 Duration 0.146025\n",
      "Batch 10700 of 10 sentences: Average Loss:   0.267408 Duration 0.115204\n",
      "Batch 10800 of 10 sentences: Average Loss:   0.267368 Duration 0.123035\n",
      "Batch 10900 of 10 sentences: Average Loss:   0.267318 Duration 0.142413\n",
      "Batch 11000 of 10 sentences: Average Loss:   0.267167 Duration 0.112075\n",
      "Batch 11100 of 10 sentences: Average Loss:   0.267182 Duration 0.152375\n",
      "Batch 11200 of 10 sentences: Average Loss:   0.267196 Duration 0.165537\n",
      "Batch 11300 of 10 sentences: Average Loss:   0.267140 Duration 0.149283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11400 of 10 sentences: Average Loss:   0.267056 Duration 0.247263\n",
      "Batch 11500 of 10 sentences: Average Loss:   0.267063 Duration 0.123812\n",
      "Batch 11600 of 10 sentences: Average Loss:   0.266976 Duration 0.128988\n",
      "Batch 11700 of 10 sentences: Average Loss:   0.266924 Duration 0.100958\n",
      "Batch 11800 of 10 sentences: Average Loss:   0.266836 Duration 0.123029\n",
      "Batch 11900 of 10 sentences: Average Loss:   0.266797 Duration 0.141814\n",
      "Batch 12000 of 10 sentences: Average Loss:   0.266719 Duration 0.183207\n",
      "Batch 12100 of 10 sentences: Average Loss:   0.266704 Duration 0.151992\n",
      "Batch 12200 of 10 sentences: Average Loss:   0.266672 Duration 0.063920\n",
      "Batch 12300 of 10 sentences: Average Loss:   0.266562 Duration 0.089968\n",
      "Batch 12400 of 10 sentences: Average Loss:   0.266467 Duration 0.113766\n",
      "Batch 12500 of 10 sentences: Average Loss:   0.266428 Duration 0.120758\n",
      "Batch 12600 of 10 sentences: Average Loss:   0.266367 Duration 0.233368\n",
      "epoch 2 batches 12623 done\n",
      "Batch 12700 of 10 sentences: Average Loss:   0.266341 Duration 0.104983\n",
      "Batch 12800 of 10 sentences: Average Loss:   0.266308 Duration 0.173264\n",
      "Batch 12900 of 10 sentences: Average Loss:   0.266320 Duration 0.145050\n",
      "Batch 13000 of 10 sentences: Average Loss:   0.266187 Duration 0.150375\n",
      "Batch 13100 of 10 sentences: Average Loss:   0.266144 Duration 0.140624\n",
      "Batch 13200 of 10 sentences: Average Loss:   0.266163 Duration 0.145415\n",
      "Batch 13300 of 10 sentences: Average Loss:   0.266132 Duration 0.130075\n",
      "Batch 13400 of 10 sentences: Average Loss:   0.266059 Duration 0.141663\n",
      "Batch 13500 of 10 sentences: Average Loss:   0.266080 Duration 0.155059\n",
      "Batch 13600 of 10 sentences: Average Loss:   0.266048 Duration 0.179919\n",
      "Batch 13700 of 10 sentences: Average Loss:   0.266057 Duration 0.157826\n",
      "Batch 13800 of 10 sentences: Average Loss:   0.266063 Duration 0.150283\n",
      "Batch 13900 of 10 sentences: Average Loss:   0.266031 Duration 0.111388\n",
      "Batch 14000 of 10 sentences: Average Loss:   0.265913 Duration 0.234703\n",
      "Batch 14100 of 10 sentences: Average Loss:   0.265911 Duration 0.096971\n",
      "Batch 14200 of 10 sentences: Average Loss:   0.265906 Duration 0.173926\n",
      "Batch 14300 of 10 sentences: Average Loss:   0.265857 Duration 0.168584\n",
      "Batch 14400 of 10 sentences: Average Loss:   0.265868 Duration 0.128594\n",
      "Batch 14500 of 10 sentences: Average Loss:   0.265808 Duration 0.104891\n",
      "Batch 14600 of 10 sentences: Average Loss:   0.265747 Duration 0.126593\n",
      "Batch 14700 of 10 sentences: Average Loss:   0.265683 Duration 0.124867\n",
      "Batch 14800 of 10 sentences: Average Loss:   0.265645 Duration 0.199767\n",
      "Batch 14900 of 10 sentences: Average Loss:   0.265529 Duration 0.154771\n",
      "Batch 15000 of 10 sentences: Average Loss:   0.265447 Duration 0.133371\n",
      "Batch 15100 of 10 sentences: Average Loss:   0.265406 Duration 0.139950\n",
      "Batch 15200 of 10 sentences: Average Loss:   0.265303 Duration 0.124221\n",
      "Batch 15300 of 10 sentences: Average Loss:   0.265288 Duration 0.136009\n",
      "Batch 15400 of 10 sentences: Average Loss:   0.265283 Duration 0.120401\n",
      "Batch 15500 of 10 sentences: Average Loss:   0.265232 Duration 0.154900\n",
      "Batch 15600 of 10 sentences: Average Loss:   0.265176 Duration 0.135004\n",
      "Batch 15700 of 10 sentences: Average Loss:   0.265173 Duration 0.123934\n",
      "Batch 15800 of 10 sentences: Average Loss:   0.265096 Duration 0.184170\n",
      "Batch 15900 of 10 sentences: Average Loss:   0.265068 Duration 0.171581\n",
      "Batch 16000 of 10 sentences: Average Loss:   0.264984 Duration 0.153408\n",
      "Batch 16100 of 10 sentences: Average Loss:   0.264963 Duration 0.077943\n",
      "Batch 16200 of 10 sentences: Average Loss:   0.264905 Duration 0.242289\n",
      "Batch 16300 of 10 sentences: Average Loss:   0.264885 Duration 0.156030\n",
      "Batch 16400 of 10 sentences: Average Loss:   0.264840 Duration 0.233665\n",
      "Batch 16500 of 10 sentences: Average Loss:   0.264767 Duration 0.106469\n",
      "Batch 16600 of 10 sentences: Average Loss:   0.264672 Duration 0.153056\n",
      "Batch 16700 of 10 sentences: Average Loss:   0.264638 Duration 0.238268\n",
      "Batch 16800 of 10 sentences: Average Loss:   0.264578 Duration 0.138353\n",
      "epoch 3 batches 16831 done\n",
      "Batch 16900 of 10 sentences: Average Loss:   0.264545 Duration 0.104157\n",
      "Batch 17000 of 10 sentences: Average Loss:   0.264543 Duration 0.113828\n",
      "Batch 17100 of 10 sentences: Average Loss:   0.264544 Duration 0.207734\n",
      "Batch 17200 of 10 sentences: Average Loss:   0.264428 Duration 0.112624\n",
      "Batch 17300 of 10 sentences: Average Loss:   0.264390 Duration 0.142387\n",
      "Batch 17400 of 10 sentences: Average Loss:   0.264407 Duration 0.093902\n",
      "Batch 17500 of 10 sentences: Average Loss:   0.264387 Duration 0.102135\n",
      "Batch 17600 of 10 sentences: Average Loss:   0.264305 Duration 0.089717\n",
      "Batch 17700 of 10 sentences: Average Loss:   0.264317 Duration 0.152210\n",
      "Batch 17800 of 10 sentences: Average Loss:   0.264296 Duration 0.159377\n",
      "Batch 17900 of 10 sentences: Average Loss:   0.264286 Duration 0.197645\n",
      "Batch 18000 of 10 sentences: Average Loss:   0.264279 Duration 0.150429\n",
      "Batch 18100 of 10 sentences: Average Loss:   0.264279 Duration 0.239457\n",
      "Batch 18200 of 10 sentences: Average Loss:   0.264209 Duration 0.114757\n",
      "Batch 18300 of 10 sentences: Average Loss:   0.264185 Duration 0.135283\n",
      "Batch 18400 of 10 sentences: Average Loss:   0.264191 Duration 0.164260\n",
      "Batch 18500 of 10 sentences: Average Loss:   0.264136 Duration 0.136486\n",
      "Batch 18600 of 10 sentences: Average Loss:   0.264146 Duration 0.195352\n",
      "Batch 18700 of 10 sentences: Average Loss:   0.264088 Duration 0.181181\n",
      "Batch 18800 of 10 sentences: Average Loss:   0.264034 Duration 0.080439\n",
      "Batch 18900 of 10 sentences: Average Loss:   0.263985 Duration 0.099884\n",
      "Batch 19000 of 10 sentences: Average Loss:   0.263940 Duration 0.123916\n",
      "Batch 19100 of 10 sentences: Average Loss:   0.263854 Duration 0.190122\n",
      "Batch 19200 of 10 sentences: Average Loss:   0.263794 Duration 0.111363\n",
      "Batch 19300 of 10 sentences: Average Loss:   0.263763 Duration 0.085247\n",
      "Batch 19400 of 10 sentences: Average Loss:   0.263691 Duration 0.203174\n",
      "Batch 19500 of 10 sentences: Average Loss:   0.263642 Duration 0.169548\n",
      "Batch 19600 of 10 sentences: Average Loss:   0.263648 Duration 0.174765\n",
      "Batch 19700 of 10 sentences: Average Loss:   0.263601 Duration 0.290227\n",
      "Batch 19800 of 10 sentences: Average Loss:   0.263546 Duration 0.114004\n",
      "Batch 19900 of 10 sentences: Average Loss:   0.263522 Duration 0.106791\n",
      "Batch 20000 of 10 sentences: Average Loss:   0.263456 Duration 0.232801\n",
      "Batch 20100 of 10 sentences: Average Loss:   0.263441 Duration 0.125130\n",
      "Batch 20200 of 10 sentences: Average Loss:   0.263362 Duration 0.133641\n",
      "Batch 20300 of 10 sentences: Average Loss:   0.263352 Duration 0.145681\n",
      "Batch 20400 of 10 sentences: Average Loss:   0.263269 Duration 0.125457\n",
      "Batch 20500 of 10 sentences: Average Loss:   0.263251 Duration 0.142054\n",
      "Batch 20600 of 10 sentences: Average Loss:   0.263226 Duration 0.155422\n",
      "Batch 20700 of 10 sentences: Average Loss:   0.263154 Duration 0.136200\n",
      "Batch 20800 of 10 sentences: Average Loss:   0.263087 Duration 0.119891\n",
      "Batch 20900 of 10 sentences: Average Loss:   0.263043 Duration 0.144221\n",
      "Batch 21000 of 10 sentences: Average Loss:   0.262988 Duration 0.268640\n",
      "epoch 4 batches 21039 done\n",
      "Batch 21100 of 10 sentences: Average Loss:   0.262967 Duration 0.193717\n",
      "Batch 21200 of 10 sentences: Average Loss:   0.262951 Duration 0.167099\n",
      "Batch 21300 of 10 sentences: Average Loss:   0.262952 Duration 0.129399\n",
      "Batch 21400 of 10 sentences: Average Loss:   0.262869 Duration 0.157507\n",
      "Batch 21500 of 10 sentences: Average Loss:   0.262831 Duration 0.402707\n",
      "Batch 21600 of 10 sentences: Average Loss:   0.262845 Duration 0.159734\n",
      "Batch 21700 of 10 sentences: Average Loss:   0.262815 Duration 0.194513\n",
      "Batch 21800 of 10 sentences: Average Loss:   0.262744 Duration 0.122080\n",
      "Batch 21900 of 10 sentences: Average Loss:   0.262733 Duration 0.219673\n",
      "Batch 22000 of 10 sentences: Average Loss:   0.262725 Duration 0.120339\n",
      "Batch 22100 of 10 sentences: Average Loss:   0.262702 Duration 0.146120\n",
      "Batch 22200 of 10 sentences: Average Loss:   0.262696 Duration 0.134315\n",
      "Batch 22300 of 10 sentences: Average Loss:   0.262699 Duration 0.163749\n",
      "Batch 22400 of 10 sentences: Average Loss:   0.262639 Duration 0.165246\n",
      "Batch 22500 of 10 sentences: Average Loss:   0.262601 Duration 0.086045\n",
      "Batch 22600 of 10 sentences: Average Loss:   0.262600 Duration 0.105802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 22700 of 10 sentences: Average Loss:   0.262544 Duration 0.134503\n",
      "Batch 22800 of 10 sentences: Average Loss:   0.262552 Duration 0.212068\n",
      "Batch 22900 of 10 sentences: Average Loss:   0.262518 Duration 0.155276\n",
      "Batch 23000 of 10 sentences: Average Loss:   0.262459 Duration 0.159432\n",
      "Batch 23100 of 10 sentences: Average Loss:   0.262419 Duration 0.219003\n",
      "Batch 23200 of 10 sentences: Average Loss:   0.262392 Duration 0.156004\n",
      "Batch 23300 of 10 sentences: Average Loss:   0.262318 Duration 0.106737\n",
      "Batch 23400 of 10 sentences: Average Loss:   0.262236 Duration 0.096243\n",
      "Batch 23500 of 10 sentences: Average Loss:   0.262212 Duration 0.141431\n",
      "Batch 23600 of 10 sentences: Average Loss:   0.262146 Duration 0.211659\n",
      "Batch 23700 of 10 sentences: Average Loss:   0.262106 Duration 0.142459\n",
      "Batch 23800 of 10 sentences: Average Loss:   0.262116 Duration 0.197581\n",
      "Batch 23900 of 10 sentences: Average Loss:   0.262076 Duration 0.154832\n",
      "Batch 24000 of 10 sentences: Average Loss:   0.262027 Duration 0.137698\n",
      "Batch 24100 of 10 sentences: Average Loss:   0.262004 Duration 0.123760\n",
      "Batch 24200 of 10 sentences: Average Loss:   0.261956 Duration 0.102907\n",
      "Batch 24300 of 10 sentences: Average Loss:   0.261932 Duration 0.195123\n",
      "Batch 24400 of 10 sentences: Average Loss:   0.261865 Duration 0.124218\n",
      "Batch 24500 of 10 sentences: Average Loss:   0.261859 Duration 0.214787\n",
      "Batch 24600 of 10 sentences: Average Loss:   0.261781 Duration 0.141855\n",
      "Batch 24700 of 10 sentences: Average Loss:   0.261753 Duration 0.304676\n",
      "Batch 24800 of 10 sentences: Average Loss:   0.261733 Duration 0.121342\n",
      "Batch 24900 of 10 sentences: Average Loss:   0.261675 Duration 0.151455\n",
      "Batch 25000 of 10 sentences: Average Loss:   0.261631 Duration 0.171981\n",
      "Batch 25100 of 10 sentences: Average Loss:   0.261561 Duration 0.150858\n",
      "Batch 25200 of 10 sentences: Average Loss:   0.261530 Duration 0.118803\n",
      "epoch 5 batches 25247 done\n",
      "Batch 25300 of 10 sentences: Average Loss:   0.261498 Duration 0.117188\n",
      "Batch 25400 of 10 sentences: Average Loss:   0.261488 Duration 0.108408\n",
      "Batch 25500 of 10 sentences: Average Loss:   0.261488 Duration 0.127535\n",
      "Batch 25600 of 10 sentences: Average Loss:   0.261408 Duration 0.267461\n",
      "Batch 25700 of 10 sentences: Average Loss:   0.261377 Duration 0.142958\n",
      "Batch 25800 of 10 sentences: Average Loss:   0.261381 Duration 0.137271\n",
      "Batch 25900 of 10 sentences: Average Loss:   0.261357 Duration 0.201118\n",
      "Batch 26000 of 10 sentences: Average Loss:   0.261293 Duration 0.116468\n",
      "Batch 26100 of 10 sentences: Average Loss:   0.261290 Duration 0.134144\n",
      "Batch 26200 of 10 sentences: Average Loss:   0.261273 Duration 0.203655\n",
      "Batch 26300 of 10 sentences: Average Loss:   0.261256 Duration 0.135816\n",
      "Batch 26400 of 10 sentences: Average Loss:   0.261245 Duration 0.171502\n",
      "Batch 26500 of 10 sentences: Average Loss:   0.261247 Duration 0.155646\n",
      "Batch 26600 of 10 sentences: Average Loss:   0.261201 Duration 0.151791\n",
      "Batch 26700 of 10 sentences: Average Loss:   0.261161 Duration 0.139806\n",
      "Batch 26800 of 10 sentences: Average Loss:   0.261161 Duration 0.135395\n",
      "Batch 26900 of 10 sentences: Average Loss:   0.261113 Duration 0.120288\n",
      "Batch 27000 of 10 sentences: Average Loss:   0.261106 Duration 0.156996\n",
      "Batch 27100 of 10 sentences: Average Loss:   0.261079 Duration 0.177567\n",
      "Batch 27200 of 10 sentences: Average Loss:   0.261018 Duration 0.106436\n",
      "Batch 27300 of 10 sentences: Average Loss:   0.260973 Duration 0.110501\n",
      "Batch 27400 of 10 sentences: Average Loss:   0.260950 Duration 0.112421\n",
      "Batch 27500 of 10 sentences: Average Loss:   0.260884 Duration 0.117420\n",
      "Batch 27600 of 10 sentences: Average Loss:   0.260821 Duration 0.176780\n",
      "Batch 27700 of 10 sentences: Average Loss:   0.260797 Duration 0.185979\n",
      "Batch 27800 of 10 sentences: Average Loss:   0.260739 Duration 0.124666\n",
      "Batch 27900 of 10 sentences: Average Loss:   0.260710 Duration 0.133035\n",
      "Batch 28000 of 10 sentences: Average Loss:   0.260710 Duration 0.241050\n",
      "Batch 28100 of 10 sentences: Average Loss:   0.260684 Duration 0.141428\n",
      "Batch 28200 of 10 sentences: Average Loss:   0.260636 Duration 0.115699\n",
      "Batch 28300 of 10 sentences: Average Loss:   0.260612 Duration 0.101055\n",
      "Batch 28400 of 10 sentences: Average Loss:   0.260568 Duration 0.106359\n",
      "Batch 28500 of 10 sentences: Average Loss:   0.260543 Duration 0.075604\n",
      "Batch 28600 of 10 sentences: Average Loss:   0.260492 Duration 0.124284\n",
      "Batch 28700 of 10 sentences: Average Loss:   0.260474 Duration 0.097408\n",
      "Batch 28800 of 10 sentences: Average Loss:   0.260407 Duration 0.142679\n",
      "Batch 28900 of 10 sentences: Average Loss:   0.260380 Duration 0.085396\n",
      "Batch 29000 of 10 sentences: Average Loss:   0.260353 Duration 0.155627\n",
      "Batch 29100 of 10 sentences: Average Loss:   0.260294 Duration 0.168973\n",
      "Batch 29200 of 10 sentences: Average Loss:   0.260257 Duration 0.192918\n",
      "Batch 29300 of 10 sentences: Average Loss:   0.260194 Duration 0.137642\n",
      "Batch 29400 of 10 sentences: Average Loss:   0.260163 Duration 0.155180\n",
      "epoch 6 batches 29455 done\n",
      "Batch 29500 of 10 sentences: Average Loss:   0.260131 Duration 0.133647\n",
      "Batch 29600 of 10 sentences: Average Loss:   0.260121 Duration 0.087046\n",
      "Batch 29700 of 10 sentences: Average Loss:   0.260114 Duration 0.157468\n",
      "Batch 29800 of 10 sentences: Average Loss:   0.260056 Duration 0.111505\n",
      "Batch 29900 of 10 sentences: Average Loss:   0.260010 Duration 0.123636\n",
      "Batch 30000 of 10 sentences: Average Loss:   0.260004 Duration 0.229400\n",
      "Batch 30100 of 10 sentences: Average Loss:   0.259991 Duration 0.147416\n",
      "Batch 30200 of 10 sentences: Average Loss:   0.259937 Duration 0.140959\n",
      "Batch 30300 of 10 sentences: Average Loss:   0.259915 Duration 0.113765\n",
      "Batch 30400 of 10 sentences: Average Loss:   0.259899 Duration 0.178808\n",
      "Batch 30500 of 10 sentences: Average Loss:   0.259875 Duration 0.090678\n",
      "Batch 30600 of 10 sentences: Average Loss:   0.259872 Duration 0.129026\n",
      "Batch 30700 of 10 sentences: Average Loss:   0.259867 Duration 0.139732\n",
      "Batch 30800 of 10 sentences: Average Loss:   0.259822 Duration 0.143821\n",
      "Batch 30900 of 10 sentences: Average Loss:   0.259785 Duration 0.135104\n",
      "Batch 31000 of 10 sentences: Average Loss:   0.259779 Duration 0.159711\n",
      "Batch 31100 of 10 sentences: Average Loss:   0.259747 Duration 0.160954\n",
      "Batch 31200 of 10 sentences: Average Loss:   0.259736 Duration 0.110491\n",
      "Batch 31300 of 10 sentences: Average Loss:   0.259709 Duration 0.116093\n",
      "Batch 31400 of 10 sentences: Average Loss:   0.259656 Duration 0.133296\n",
      "Batch 31500 of 10 sentences: Average Loss:   0.259619 Duration 0.143898\n",
      "Batch 31600 of 10 sentences: Average Loss:   0.259586 Duration 0.122443\n",
      "Batch 31700 of 10 sentences: Average Loss:   0.259536 Duration 0.131686\n",
      "Batch 31800 of 10 sentences: Average Loss:   0.259467 Duration 0.080824\n",
      "Batch 31900 of 10 sentences: Average Loss:   0.259447 Duration 0.100998\n",
      "Batch 32000 of 10 sentences: Average Loss:   0.259394 Duration 0.100973\n",
      "Batch 32100 of 10 sentences: Average Loss:   0.259355 Duration 0.129397\n",
      "Batch 32200 of 10 sentences: Average Loss:   0.259354 Duration 0.187888\n",
      "Batch 32300 of 10 sentences: Average Loss:   0.259327 Duration 0.145683\n",
      "Batch 32400 of 10 sentences: Average Loss:   0.259285 Duration 0.105796\n",
      "Batch 32500 of 10 sentences: Average Loss:   0.259259 Duration 0.156551\n",
      "Batch 32600 of 10 sentences: Average Loss:   0.259223 Duration 0.112306\n",
      "Batch 32700 of 10 sentences: Average Loss:   0.259198 Duration 0.145369\n",
      "Batch 32800 of 10 sentences: Average Loss:   0.259158 Duration 0.098570\n",
      "Batch 32900 of 10 sentences: Average Loss:   0.259137 Duration 0.176818\n",
      "Batch 33000 of 10 sentences: Average Loss:   0.259081 Duration 0.070289\n",
      "Batch 33100 of 10 sentences: Average Loss:   0.259052 Duration 0.180516\n",
      "Batch 33200 of 10 sentences: Average Loss:   0.259034 Duration 0.132926\n",
      "Batch 33300 of 10 sentences: Average Loss:   0.258968 Duration 0.238794\n",
      "Batch 33400 of 10 sentences: Average Loss:   0.258928 Duration 0.190557\n",
      "Batch 33500 of 10 sentences: Average Loss:   0.258885 Duration 0.110148\n",
      "Batch 33600 of 10 sentences: Average Loss:   0.258843 Duration 0.155249\n",
      "epoch 7 batches 33663 done\n",
      "Batch 33700 of 10 sentences: Average Loss:   0.258814 Duration 0.201835\n",
      "Batch 33800 of 10 sentences: Average Loss:   0.258801 Duration 0.104074\n",
      "Batch 33900 of 10 sentences: Average Loss:   0.258785 Duration 0.111332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 34000 of 10 sentences: Average Loss:   0.258742 Duration 0.260818\n",
      "Batch 34100 of 10 sentences: Average Loss:   0.258691 Duration 0.110821\n",
      "Batch 34200 of 10 sentences: Average Loss:   0.258680 Duration 0.185052\n",
      "Batch 34300 of 10 sentences: Average Loss:   0.258675 Duration 0.139431\n",
      "Batch 34400 of 10 sentences: Average Loss:   0.258632 Duration 0.114731\n",
      "Batch 34500 of 10 sentences: Average Loss:   0.258607 Duration 0.142235\n",
      "Batch 34600 of 10 sentences: Average Loss:   0.258591 Duration 0.164371\n",
      "Batch 34700 of 10 sentences: Average Loss:   0.258568 Duration 0.133128\n",
      "Batch 34800 of 10 sentences: Average Loss:   0.258563 Duration 0.117076\n",
      "Batch 34900 of 10 sentences: Average Loss:   0.258553 Duration 0.125717\n",
      "Batch 35000 of 10 sentences: Average Loss:   0.258513 Duration 0.141819\n",
      "Batch 35100 of 10 sentences: Average Loss:   0.258481 Duration 0.236598\n",
      "Batch 35200 of 10 sentences: Average Loss:   0.258475 Duration 0.116731\n",
      "Batch 35300 of 10 sentences: Average Loss:   0.258442 Duration 0.123366\n",
      "Batch 35400 of 10 sentences: Average Loss:   0.258433 Duration 0.188784\n",
      "Batch 35500 of 10 sentences: Average Loss:   0.258421 Duration 0.162670\n",
      "Batch 35600 of 10 sentences: Average Loss:   0.258363 Duration 0.124459\n",
      "Batch 35700 of 10 sentences: Average Loss:   0.258335 Duration 0.235197\n",
      "Batch 35800 of 10 sentences: Average Loss:   0.258295 Duration 0.068944\n",
      "Batch 35900 of 10 sentences: Average Loss:   0.258249 Duration 0.168572\n",
      "Batch 36000 of 10 sentences: Average Loss:   0.258176 Duration 0.109331\n",
      "Batch 36100 of 10 sentences: Average Loss:   0.258162 Duration 0.149184\n",
      "Batch 36200 of 10 sentences: Average Loss:   0.258115 Duration 0.196126\n",
      "Batch 36300 of 10 sentences: Average Loss:   0.258068 Duration 0.239881\n",
      "Batch 36400 of 10 sentences: Average Loss:   0.258064 Duration 0.150443\n",
      "Batch 36500 of 10 sentences: Average Loss:   0.258036 Duration 0.137845\n",
      "Batch 36600 of 10 sentences: Average Loss:   0.258000 Duration 0.168671\n",
      "Batch 36700 of 10 sentences: Average Loss:   0.257975 Duration 0.250701\n",
      "Batch 36800 of 10 sentences: Average Loss:   0.257950 Duration 0.142812\n",
      "Batch 36900 of 10 sentences: Average Loss:   0.257917 Duration 0.141892\n",
      "Batch 37000 of 10 sentences: Average Loss:   0.257887 Duration 0.193969\n",
      "Batch 37100 of 10 sentences: Average Loss:   0.257866 Duration 0.180252\n",
      "Batch 37200 of 10 sentences: Average Loss:   0.257817 Duration 0.044468\n",
      "Batch 37300 of 10 sentences: Average Loss:   0.257787 Duration 0.139817\n",
      "Batch 37400 of 10 sentences: Average Loss:   0.257778 Duration 0.129202\n",
      "Batch 37500 of 10 sentences: Average Loss:   0.257721 Duration 0.169695\n",
      "Batch 37600 of 10 sentences: Average Loss:   0.257679 Duration 0.154225\n",
      "Batch 37700 of 10 sentences: Average Loss:   0.257631 Duration 0.101603\n",
      "Batch 37800 of 10 sentences: Average Loss:   0.257595 Duration 0.117706\n",
      "epoch 8 batches 37871 done\n",
      "Batch 37900 of 10 sentences: Average Loss:   0.257563 Duration 0.159522\n",
      "Batch 38000 of 10 sentences: Average Loss:   0.257556 Duration 0.160426\n",
      "Batch 38100 of 10 sentences: Average Loss:   0.257533 Duration 0.252346\n",
      "Batch 38200 of 10 sentences: Average Loss:   0.257494 Duration 0.155802\n",
      "Batch 38300 of 10 sentences: Average Loss:   0.257455 Duration 0.183810\n",
      "Batch 38400 of 10 sentences: Average Loss:   0.257437 Duration 0.169523\n",
      "Batch 38500 of 10 sentences: Average Loss:   0.257436 Duration 0.122701\n",
      "Batch 38600 of 10 sentences: Average Loss:   0.257395 Duration 0.152112\n",
      "Batch 38700 of 10 sentences: Average Loss:   0.257363 Duration 0.154456\n",
      "Batch 38800 of 10 sentences: Average Loss:   0.257351 Duration 0.179434\n",
      "Batch 38900 of 10 sentences: Average Loss:   0.257328 Duration 0.185861\n",
      "Batch 39000 of 10 sentences: Average Loss:   0.257325 Duration 0.168132\n",
      "Batch 39100 of 10 sentences: Average Loss:   0.257311 Duration 0.117433\n",
      "Batch 39200 of 10 sentences: Average Loss:   0.257273 Duration 0.168210\n",
      "Batch 39300 of 10 sentences: Average Loss:   0.257237 Duration 0.154316\n",
      "Batch 39400 of 10 sentences: Average Loss:   0.257236 Duration 0.139533\n",
      "Batch 39500 of 10 sentences: Average Loss:   0.257201 Duration 0.238441\n",
      "Batch 39600 of 10 sentences: Average Loss:   0.257187 Duration 0.164464\n",
      "Batch 39700 of 10 sentences: Average Loss:   0.257180 Duration 0.110182\n",
      "Batch 39800 of 10 sentences: Average Loss:   0.257127 Duration 0.066181\n",
      "Batch 39900 of 10 sentences: Average Loss:   0.257095 Duration 0.158226\n",
      "Batch 40000 of 10 sentences: Average Loss:   0.257059 Duration 0.044132\n",
      "Batch 40100 of 10 sentences: Average Loss:   0.257014 Duration 0.136425\n",
      "Batch 40200 of 10 sentences: Average Loss:   0.256949 Duration 0.137078\n",
      "Batch 40300 of 10 sentences: Average Loss:   0.256941 Duration 0.139030\n",
      "Batch 40400 of 10 sentences: Average Loss:   0.256900 Duration 0.131900\n",
      "Batch 40500 of 10 sentences: Average Loss:   0.256860 Duration 0.110244\n",
      "Batch 40600 of 10 sentences: Average Loss:   0.256850 Duration 0.168566\n",
      "Batch 40700 of 10 sentences: Average Loss:   0.256829 Duration 0.160558\n",
      "Batch 40800 of 10 sentences: Average Loss:   0.256800 Duration 0.203031\n",
      "Batch 40900 of 10 sentences: Average Loss:   0.256768 Duration 0.143571\n",
      "Batch 41000 of 10 sentences: Average Loss:   0.256753 Duration 0.152098\n",
      "Batch 41100 of 10 sentences: Average Loss:   0.256721 Duration 0.149340\n",
      "Batch 41200 of 10 sentences: Average Loss:   0.256693 Duration 0.116150\n",
      "Batch 41300 of 10 sentences: Average Loss:   0.256671 Duration 0.134088\n",
      "Batch 41400 of 10 sentences: Average Loss:   0.256625 Duration 0.134401\n",
      "Batch 41500 of 10 sentences: Average Loss:   0.256595 Duration 0.205164\n",
      "Batch 41600 of 10 sentences: Average Loss:   0.256588 Duration 0.175702\n",
      "Batch 41700 of 10 sentences: Average Loss:   0.256544 Duration 0.172075\n",
      "Batch 41800 of 10 sentences: Average Loss:   0.256512 Duration 0.192530\n",
      "Batch 41900 of 10 sentences: Average Loss:   0.256461 Duration 0.118071\n",
      "Batch 42000 of 10 sentences: Average Loss:   0.256423 Duration 0.136605\n",
      "epoch 9 batches 42079 done\n",
      "Batch 42100 of 10 sentences: Average Loss:   0.256395 Duration 0.101107\n",
      "Batch 42200 of 10 sentences: Average Loss:   0.256387 Duration 0.151295\n",
      "Batch 42300 of 10 sentences: Average Loss:   0.256370 Duration 0.144130\n",
      "Batch 42400 of 10 sentences: Average Loss:   0.256335 Duration 0.171559\n",
      "Batch 42500 of 10 sentences: Average Loss:   0.256300 Duration 0.119554\n",
      "Batch 42600 of 10 sentences: Average Loss:   0.256275 Duration 0.138000\n",
      "Batch 42700 of 10 sentences: Average Loss:   0.256274 Duration 0.153988\n",
      "Batch 42800 of 10 sentences: Average Loss:   0.256235 Duration 0.127875\n",
      "Batch 42900 of 10 sentences: Average Loss:   0.256206 Duration 0.133661\n",
      "Batch 43000 of 10 sentences: Average Loss:   0.256190 Duration 0.092015\n",
      "Batch 43100 of 10 sentences: Average Loss:   0.256174 Duration 0.138771\n",
      "Batch 43200 of 10 sentences: Average Loss:   0.256170 Duration 0.181141\n",
      "Batch 43300 of 10 sentences: Average Loss:   0.256154 Duration 0.177043\n",
      "Batch 43400 of 10 sentences: Average Loss:   0.256119 Duration 0.170171\n",
      "Batch 43500 of 10 sentences: Average Loss:   0.256081 Duration 0.108989\n",
      "Batch 43600 of 10 sentences: Average Loss:   0.256078 Duration 0.139415\n",
      "Batch 43700 of 10 sentences: Average Loss:   0.256050 Duration 0.218920\n",
      "Batch 43800 of 10 sentences: Average Loss:   0.256030 Duration 0.189201\n",
      "Batch 43900 of 10 sentences: Average Loss:   0.256025 Duration 0.212882\n",
      "Batch 44000 of 10 sentences: Average Loss:   0.255979 Duration 0.137383\n",
      "Batch 44100 of 10 sentences: Average Loss:   0.255946 Duration 0.163735\n",
      "Batch 44200 of 10 sentences: Average Loss:   0.255918 Duration 0.150744\n",
      "Batch 44300 of 10 sentences: Average Loss:   0.255872 Duration 0.165693\n",
      "Batch 44400 of 10 sentences: Average Loss:   0.255818 Duration 0.139529\n",
      "Batch 44500 of 10 sentences: Average Loss:   0.255812 Duration 0.132283\n",
      "Batch 44600 of 10 sentences: Average Loss:   0.255768 Duration 0.217790\n",
      "Batch 44700 of 10 sentences: Average Loss:   0.255730 Duration 0.076973\n",
      "Batch 44800 of 10 sentences: Average Loss:   0.255723 Duration 0.178549\n",
      "Batch 44900 of 10 sentences: Average Loss:   0.255699 Duration 0.119473\n",
      "Batch 45000 of 10 sentences: Average Loss:   0.255673 Duration 0.209365\n",
      "Batch 45100 of 10 sentences: Average Loss:   0.255649 Duration 0.161695\n",
      "Batch 45200 of 10 sentences: Average Loss:   0.255635 Duration 0.150823\n",
      "Batch 45300 of 10 sentences: Average Loss:   0.255605 Duration 0.158121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 45400 of 10 sentences: Average Loss:   0.255576 Duration 0.163643\n",
      "Batch 45500 of 10 sentences: Average Loss:   0.255553 Duration 0.152171\n",
      "Batch 45600 of 10 sentences: Average Loss:   0.255507 Duration 0.165099\n",
      "Batch 45700 of 10 sentences: Average Loss:   0.255480 Duration 0.169909\n",
      "Batch 45800 of 10 sentences: Average Loss:   0.255471 Duration 0.133723\n",
      "Batch 45900 of 10 sentences: Average Loss:   0.255427 Duration 0.176315\n",
      "Batch 46000 of 10 sentences: Average Loss:   0.255397 Duration 0.136685\n",
      "Batch 46100 of 10 sentences: Average Loss:   0.255349 Duration 0.135832\n",
      "Batch 46200 of 10 sentences: Average Loss:   0.255314 Duration 0.114719\n",
      "epoch 10 batches 46287 done\n",
      "Batch 46300 of 10 sentences: Average Loss:   0.255285 Duration 0.153073\n",
      "Batch 46400 of 10 sentences: Average Loss:   0.255280 Duration 0.146082\n",
      "Batch 46500 of 10 sentences: Average Loss:   0.255257 Duration 0.087204\n",
      "Batch 46600 of 10 sentences: Average Loss:   0.255227 Duration 0.163894\n",
      "Batch 46700 of 10 sentences: Average Loss:   0.255190 Duration 0.154778\n",
      "Batch 46800 of 10 sentences: Average Loss:   0.255159 Duration 0.155629\n",
      "Batch 46900 of 10 sentences: Average Loss:   0.255158 Duration 0.125665\n",
      "Batch 47000 of 10 sentences: Average Loss:   0.255122 Duration 0.151388\n",
      "Batch 47100 of 10 sentences: Average Loss:   0.255095 Duration 0.146763\n",
      "Batch 47200 of 10 sentences: Average Loss:   0.255080 Duration 0.096933\n",
      "Batch 47300 of 10 sentences: Average Loss:   0.255063 Duration 0.155775\n",
      "Batch 47400 of 10 sentences: Average Loss:   0.255058 Duration 0.164461\n",
      "Batch 47500 of 10 sentences: Average Loss:   0.255042 Duration 0.187318\n",
      "Batch 47600 of 10 sentences: Average Loss:   0.255014 Duration 0.086528\n",
      "Batch 47700 of 10 sentences: Average Loss:   0.254978 Duration 0.168471\n",
      "Batch 47800 of 10 sentences: Average Loss:   0.254968 Duration 0.145328\n",
      "Batch 47900 of 10 sentences: Average Loss:   0.254943 Duration 0.249418\n",
      "Batch 48000 of 10 sentences: Average Loss:   0.254919 Duration 0.112021\n",
      "Batch 48100 of 10 sentences: Average Loss:   0.254913 Duration 0.124927\n",
      "Batch 48200 of 10 sentences: Average Loss:   0.254863 Duration 0.163910\n",
      "Batch 48300 of 10 sentences: Average Loss:   0.254833 Duration 0.167603\n",
      "Batch 48400 of 10 sentences: Average Loss:   0.254802 Duration 0.198928\n",
      "Batch 48500 of 10 sentences: Average Loss:   0.254766 Duration 0.171115\n",
      "Batch 48600 of 10 sentences: Average Loss:   0.254720 Duration 0.137363\n",
      "Batch 48700 of 10 sentences: Average Loss:   0.254709 Duration 0.125667\n",
      "Batch 48800 of 10 sentences: Average Loss:   0.254659 Duration 0.249022\n",
      "Batch 48900 of 10 sentences: Average Loss:   0.254621 Duration 0.260667\n",
      "Batch 49000 of 10 sentences: Average Loss:   0.254618 Duration 0.177968\n",
      "Batch 49100 of 10 sentences: Average Loss:   0.254596 Duration 0.147007\n",
      "Batch 49200 of 10 sentences: Average Loss:   0.254566 Duration 0.127582\n",
      "Batch 49300 of 10 sentences: Average Loss:   0.254540 Duration 0.190579\n",
      "Batch 49400 of 10 sentences: Average Loss:   0.254530 Duration 0.152138\n",
      "Batch 49500 of 10 sentences: Average Loss:   0.254500 Duration 0.094273\n",
      "Batch 49600 of 10 sentences: Average Loss:   0.254476 Duration 0.188835\n",
      "Batch 49700 of 10 sentences: Average Loss:   0.254450 Duration 0.173309\n",
      "Batch 49800 of 10 sentences: Average Loss:   0.254413 Duration 0.138062\n",
      "Batch 49900 of 10 sentences: Average Loss:   0.254385 Duration 0.152966\n",
      "Batch 50000 of 10 sentences: Average Loss:   0.254374 Duration 0.178508\n",
      "Batch 50100 of 10 sentences: Average Loss:   0.254335 Duration 0.124009\n",
      "Batch 50200 of 10 sentences: Average Loss:   0.254305 Duration 0.146490\n",
      "Batch 50300 of 10 sentences: Average Loss:   0.254256 Duration 0.132704\n",
      "Batch 50400 of 10 sentences: Average Loss:   0.254228 Duration 0.242002\n",
      "epoch 11 batches 50495 done\n",
      "Batch 50500 of 10 sentences: Average Loss:   0.254203 Duration 0.140091\n",
      "Batch 50600 of 10 sentences: Average Loss:   0.254194 Duration 0.140971\n",
      "Batch 50700 of 10 sentences: Average Loss:   0.254175 Duration 0.241900\n",
      "Batch 50800 of 10 sentences: Average Loss:   0.254147 Duration 0.087400\n",
      "Batch 50900 of 10 sentences: Average Loss:   0.254104 Duration 0.140158\n",
      "Batch 51000 of 10 sentences: Average Loss:   0.254074 Duration 0.211807\n",
      "Batch 51100 of 10 sentences: Average Loss:   0.254074 Duration 0.167876\n",
      "Batch 51200 of 10 sentences: Average Loss:   0.254040 Duration 0.167386\n",
      "Batch 51300 of 10 sentences: Average Loss:   0.254012 Duration 0.135204\n",
      "Batch 51400 of 10 sentences: Average Loss:   0.253997 Duration 0.100578\n",
      "Batch 51500 of 10 sentences: Average Loss:   0.253979 Duration 0.155364\n",
      "Batch 51600 of 10 sentences: Average Loss:   0.253974 Duration 0.151324\n",
      "Batch 51700 of 10 sentences: Average Loss:   0.253960 Duration 0.146178\n",
      "Batch 51800 of 10 sentences: Average Loss:   0.253935 Duration 0.143607\n",
      "Batch 51900 of 10 sentences: Average Loss:   0.253900 Duration 0.135894\n",
      "Batch 52000 of 10 sentences: Average Loss:   0.253889 Duration 0.141846\n",
      "Batch 52100 of 10 sentences: Average Loss:   0.253866 Duration 0.154125\n",
      "Batch 52200 of 10 sentences: Average Loss:   0.253844 Duration 0.153975\n",
      "Batch 52300 of 10 sentences: Average Loss:   0.253835 Duration 0.088659\n",
      "Batch 52400 of 10 sentences: Average Loss:   0.253801 Duration 0.187916\n",
      "Batch 52500 of 10 sentences: Average Loss:   0.253766 Duration 0.078870\n",
      "Batch 52600 of 10 sentences: Average Loss:   0.253745 Duration 0.119139\n",
      "Batch 52700 of 10 sentences: Average Loss:   0.253707 Duration 0.128135\n",
      "Batch 52800 of 10 sentences: Average Loss:   0.253663 Duration 0.147346\n",
      "Batch 52900 of 10 sentences: Average Loss:   0.253646 Duration 0.192166\n",
      "Batch 53000 of 10 sentences: Average Loss:   0.253597 Duration 0.162308\n",
      "Batch 53100 of 10 sentences: Average Loss:   0.253556 Duration 0.078339\n",
      "Batch 53200 of 10 sentences: Average Loss:   0.253555 Duration 0.129433\n",
      "Batch 53300 of 10 sentences: Average Loss:   0.253530 Duration 0.106148\n",
      "Batch 53400 of 10 sentences: Average Loss:   0.253505 Duration 0.059461\n",
      "Batch 53500 of 10 sentences: Average Loss:   0.253480 Duration 0.122758\n",
      "Batch 53600 of 10 sentences: Average Loss:   0.253469 Duration 0.158167\n",
      "Batch 53700 of 10 sentences: Average Loss:   0.253444 Duration 0.094435\n",
      "Batch 53800 of 10 sentences: Average Loss:   0.253423 Duration 0.164039\n",
      "Batch 53900 of 10 sentences: Average Loss:   0.253389 Duration 0.226801\n",
      "Batch 54000 of 10 sentences: Average Loss:   0.253358 Duration 0.101115\n",
      "Batch 54100 of 10 sentences: Average Loss:   0.253333 Duration 0.179443\n",
      "Batch 54200 of 10 sentences: Average Loss:   0.253318 Duration 0.137503\n",
      "Batch 54300 of 10 sentences: Average Loss:   0.253286 Duration 0.198688\n",
      "Batch 54400 of 10 sentences: Average Loss:   0.253252 Duration 0.086227\n",
      "Batch 54500 of 10 sentences: Average Loss:   0.253209 Duration 0.125324\n",
      "Batch 54600 of 10 sentences: Average Loss:   0.253177 Duration 0.148608\n",
      "Batch 54700 of 10 sentences: Average Loss:   0.253148 Duration 0.179774\n",
      "epoch 12 batches 54703 done\n",
      "Batch 54800 of 10 sentences: Average Loss:   0.253141 Duration 0.148476\n",
      "Batch 54900 of 10 sentences: Average Loss:   0.253123 Duration 0.260472\n",
      "Batch 55000 of 10 sentences: Average Loss:   0.253103 Duration 0.150457\n",
      "Batch 55100 of 10 sentences: Average Loss:   0.253053 Duration 0.153867\n",
      "Batch 55200 of 10 sentences: Average Loss:   0.253031 Duration 0.252538\n",
      "Batch 55300 of 10 sentences: Average Loss:   0.253029 Duration 0.171169\n",
      "Batch 55400 of 10 sentences: Average Loss:   0.253001 Duration 0.203708\n",
      "Batch 55500 of 10 sentences: Average Loss:   0.252973 Duration 0.095021\n",
      "Batch 55600 of 10 sentences: Average Loss:   0.252956 Duration 0.117106\n",
      "Batch 55700 of 10 sentences: Average Loss:   0.252940 Duration 0.129001\n",
      "Batch 55800 of 10 sentences: Average Loss:   0.252934 Duration 0.139966\n",
      "Batch 55900 of 10 sentences: Average Loss:   0.252921 Duration 0.254481\n",
      "Batch 56000 of 10 sentences: Average Loss:   0.252898 Duration 0.157413\n",
      "Batch 56100 of 10 sentences: Average Loss:   0.252857 Duration 0.179021\n",
      "Batch 56200 of 10 sentences: Average Loss:   0.252852 Duration 0.097598\n",
      "Batch 56300 of 10 sentences: Average Loss:   0.252832 Duration 0.164181\n",
      "Batch 56400 of 10 sentences: Average Loss:   0.252810 Duration 0.137799\n",
      "Batch 56500 of 10 sentences: Average Loss:   0.252797 Duration 0.247536\n",
      "Batch 56600 of 10 sentences: Average Loss:   0.252769 Duration 0.161002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 56700 of 10 sentences: Average Loss:   0.252735 Duration 0.118174\n",
      "Batch 56800 of 10 sentences: Average Loss:   0.252711 Duration 0.197571\n",
      "Batch 56900 of 10 sentences: Average Loss:   0.252676 Duration 0.119242\n",
      "Batch 57000 of 10 sentences: Average Loss:   0.252629 Duration 0.184046\n",
      "Batch 57100 of 10 sentences: Average Loss:   0.252616 Duration 0.130990\n",
      "Batch 57200 of 10 sentences: Average Loss:   0.252583 Duration 0.139043\n",
      "Batch 57300 of 10 sentences: Average Loss:   0.252543 Duration 0.142762\n",
      "Batch 57400 of 10 sentences: Average Loss:   0.252531 Duration 0.177178\n",
      "Batch 57500 of 10 sentences: Average Loss:   0.252511 Duration 0.181979\n",
      "Batch 57600 of 10 sentences: Average Loss:   0.252490 Duration 0.138298\n",
      "Batch 57700 of 10 sentences: Average Loss:   0.252457 Duration 0.138046\n",
      "Batch 57800 of 10 sentences: Average Loss:   0.252452 Duration 0.150717\n",
      "Batch 57900 of 10 sentences: Average Loss:   0.252423 Duration 0.207544\n",
      "Batch 58000 of 10 sentences: Average Loss:   0.252399 Duration 0.174485\n",
      "Batch 58100 of 10 sentences: Average Loss:   0.252369 Duration 0.179424\n",
      "Batch 58200 of 10 sentences: Average Loss:   0.252335 Duration 0.286651\n",
      "Batch 58300 of 10 sentences: Average Loss:   0.252308 Duration 0.238710\n",
      "Batch 58400 of 10 sentences: Average Loss:   0.252297 Duration 0.118861\n",
      "Batch 58500 of 10 sentences: Average Loss:   0.252266 Duration 0.199997\n",
      "Batch 58600 of 10 sentences: Average Loss:   0.252234 Duration 0.104338\n",
      "Batch 58700 of 10 sentences: Average Loss:   0.252191 Duration 0.185049\n",
      "Batch 58800 of 10 sentences: Average Loss:   0.252161 Duration 0.158981\n",
      "Batch 58900 of 10 sentences: Average Loss:   0.252135 Duration 0.138923\n",
      "epoch 13 batches 58911 done\n",
      "Batch 59000 of 10 sentences: Average Loss:   0.252124 Duration 0.124009\n",
      "Batch 59100 of 10 sentences: Average Loss:   0.252103 Duration 0.147306\n",
      "Batch 59200 of 10 sentences: Average Loss:   0.252081 Duration 0.149970\n",
      "Batch 59300 of 10 sentences: Average Loss:   0.252038 Duration 0.162024\n",
      "Batch 59400 of 10 sentences: Average Loss:   0.252017 Duration 0.159482\n",
      "Batch 59500 of 10 sentences: Average Loss:   0.252011 Duration 0.162587\n",
      "Batch 59600 of 10 sentences: Average Loss:   0.251986 Duration 0.106189\n",
      "Batch 59700 of 10 sentences: Average Loss:   0.251956 Duration 0.168671\n",
      "Batch 59800 of 10 sentences: Average Loss:   0.251943 Duration 0.114996\n",
      "Batch 59900 of 10 sentences: Average Loss:   0.251923 Duration 0.198016\n",
      "Batch 60000 of 10 sentences: Average Loss:   0.251914 Duration 0.175379\n",
      "Batch 60100 of 10 sentences: Average Loss:   0.251900 Duration 0.226549\n",
      "Batch 60200 of 10 sentences: Average Loss:   0.251881 Duration 0.120374\n",
      "Batch 60300 of 10 sentences: Average Loss:   0.251840 Duration 0.102621\n",
      "Batch 60400 of 10 sentences: Average Loss:   0.251832 Duration 0.145019\n",
      "Batch 60500 of 10 sentences: Average Loss:   0.251817 Duration 0.085291\n",
      "Batch 60600 of 10 sentences: Average Loss:   0.251795 Duration 0.141745\n",
      "Batch 60700 of 10 sentences: Average Loss:   0.251779 Duration 0.155488\n",
      "Batch 60800 of 10 sentences: Average Loss:   0.251754 Duration 0.165619\n",
      "Batch 60900 of 10 sentences: Average Loss:   0.251731 Duration 0.117495\n",
      "Batch 61000 of 10 sentences: Average Loss:   0.251705 Duration 0.152710\n",
      "Batch 61100 of 10 sentences: Average Loss:   0.251667 Duration 0.158276\n",
      "Batch 61200 of 10 sentences: Average Loss:   0.251628 Duration 0.105141\n",
      "Batch 61300 of 10 sentences: Average Loss:   0.251608 Duration 0.176495\n",
      "Batch 61400 of 10 sentences: Average Loss:   0.251584 Duration 0.132848\n",
      "Batch 61500 of 10 sentences: Average Loss:   0.251536 Duration 0.126148\n",
      "Batch 61600 of 10 sentences: Average Loss:   0.251524 Duration 0.107644\n",
      "Batch 61700 of 10 sentences: Average Loss:   0.251513 Duration 0.160232\n",
      "Batch 61800 of 10 sentences: Average Loss:   0.251492 Duration 0.156041\n",
      "Batch 61900 of 10 sentences: Average Loss:   0.251462 Duration 0.134150\n",
      "Batch 62000 of 10 sentences: Average Loss:   0.251455 Duration 0.203373\n",
      "Batch 62100 of 10 sentences: Average Loss:   0.251426 Duration 0.167198\n",
      "Batch 62200 of 10 sentences: Average Loss:   0.251406 Duration 0.134029\n",
      "Batch 62300 of 10 sentences: Average Loss:   0.251378 Duration 0.158189\n",
      "Batch 62400 of 10 sentences: Average Loss:   0.251349 Duration 0.157725\n",
      "Batch 62500 of 10 sentences: Average Loss:   0.251324 Duration 0.175895\n",
      "Batch 62600 of 10 sentences: Average Loss:   0.251311 Duration 0.077434\n",
      "Batch 62700 of 10 sentences: Average Loss:   0.251291 Duration 0.197023\n",
      "Batch 62800 of 10 sentences: Average Loss:   0.251254 Duration 0.133281\n",
      "Batch 62900 of 10 sentences: Average Loss:   0.251219 Duration 0.105002\n",
      "Batch 63000 of 10 sentences: Average Loss:   0.251190 Duration 0.202501\n",
      "Batch 63100 of 10 sentences: Average Loss:   0.251166 Duration 0.216959\n",
      "epoch 14 batches 63119 done\n",
      "Batch 63200 of 10 sentences: Average Loss:   0.251151 Duration 0.121233\n",
      "Batch 63300 of 10 sentences: Average Loss:   0.251132 Duration 0.160445\n",
      "Batch 63400 of 10 sentences: Average Loss:   0.251116 Duration 0.152745\n",
      "Batch 63500 of 10 sentences: Average Loss:   0.251073 Duration 0.157192\n",
      "Batch 63600 of 10 sentences: Average Loss:   0.251051 Duration 0.201523\n",
      "Batch 63700 of 10 sentences: Average Loss:   0.251046 Duration 0.093002\n",
      "Batch 63800 of 10 sentences: Average Loss:   0.251023 Duration 0.177167\n",
      "Batch 63900 of 10 sentences: Average Loss:   0.250989 Duration 0.157185\n",
      "Batch 64000 of 10 sentences: Average Loss:   0.250981 Duration 0.178338\n",
      "Batch 64100 of 10 sentences: Average Loss:   0.250960 Duration 0.165603\n",
      "Batch 64200 of 10 sentences: Average Loss:   0.250949 Duration 0.168423\n",
      "Batch 64300 of 10 sentences: Average Loss:   0.250933 Duration 0.203292\n",
      "Batch 64400 of 10 sentences: Average Loss:   0.250914 Duration 0.155223\n",
      "Batch 64500 of 10 sentences: Average Loss:   0.250875 Duration 0.177133\n",
      "Batch 64600 of 10 sentences: Average Loss:   0.250863 Duration 0.179789\n",
      "Batch 64700 of 10 sentences: Average Loss:   0.250851 Duration 0.131188\n",
      "Batch 64800 of 10 sentences: Average Loss:   0.250827 Duration 0.166398\n",
      "Batch 64900 of 10 sentences: Average Loss:   0.250820 Duration 0.196299\n",
      "Batch 65000 of 10 sentences: Average Loss:   0.250791 Duration 0.190106\n",
      "Batch 65100 of 10 sentences: Average Loss:   0.250766 Duration 0.159867\n",
      "Batch 65200 of 10 sentences: Average Loss:   0.250737 Duration 0.103379\n",
      "Batch 65300 of 10 sentences: Average Loss:   0.250707 Duration 0.144758\n",
      "Batch 65400 of 10 sentences: Average Loss:   0.250669 Duration 0.142380\n",
      "Batch 65500 of 10 sentences: Average Loss:   0.250644 Duration 0.130586\n",
      "Batch 65600 of 10 sentences: Average Loss:   0.250622 Duration 0.216711\n",
      "Batch 65700 of 10 sentences: Average Loss:   0.250585 Duration 0.148160\n",
      "Batch 65800 of 10 sentences: Average Loss:   0.250567 Duration 0.125029\n",
      "Batch 65900 of 10 sentences: Average Loss:   0.250557 Duration 0.162452\n",
      "Batch 66000 of 10 sentences: Average Loss:   0.250535 Duration 0.173056\n",
      "Batch 66100 of 10 sentences: Average Loss:   0.250507 Duration 0.192849\n",
      "Batch 66200 of 10 sentences: Average Loss:   0.250494 Duration 0.202838\n",
      "Batch 66300 of 10 sentences: Average Loss:   0.250468 Duration 0.227340\n",
      "Batch 66400 of 10 sentences: Average Loss:   0.250448 Duration 0.125434\n",
      "Batch 66500 of 10 sentences: Average Loss:   0.250418 Duration 0.167463\n",
      "Batch 66600 of 10 sentences: Average Loss:   0.250393 Duration 0.142623\n",
      "Batch 66700 of 10 sentences: Average Loss:   0.250366 Duration 0.292045\n",
      "Batch 66800 of 10 sentences: Average Loss:   0.250351 Duration 0.173049\n",
      "Batch 66900 of 10 sentences: Average Loss:   0.250329 Duration 0.129510\n",
      "Batch 67000 of 10 sentences: Average Loss:   0.250297 Duration 0.218208\n",
      "Batch 67100 of 10 sentences: Average Loss:   0.250262 Duration 0.138431\n",
      "Batch 67200 of 10 sentences: Average Loss:   0.250238 Duration 0.145365\n",
      "Batch 67300 of 10 sentences: Average Loss:   0.250208 Duration 0.183695\n",
      "epoch 15 batches 67327 done\n",
      "Batch 67400 of 10 sentences: Average Loss:   0.250191 Duration 0.149092\n",
      "Batch 67500 of 10 sentences: Average Loss:   0.250172 Duration 0.224339\n",
      "Batch 67600 of 10 sentences: Average Loss:   0.250162 Duration 0.166528\n",
      "Batch 67700 of 10 sentences: Average Loss:   0.250118 Duration 0.175753\n",
      "Batch 67800 of 10 sentences: Average Loss:   0.250096 Duration 0.173531\n",
      "Batch 67900 of 10 sentences: Average Loss:   0.250091 Duration 0.107827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 68000 of 10 sentences: Average Loss:   0.250072 Duration 0.201376\n",
      "Batch 68100 of 10 sentences: Average Loss:   0.250035 Duration 0.113745\n",
      "Batch 68200 of 10 sentences: Average Loss:   0.250026 Duration 0.182832\n",
      "Batch 68300 of 10 sentences: Average Loss:   0.250007 Duration 0.152914\n",
      "Batch 68400 of 10 sentences: Average Loss:   0.249995 Duration 0.141424\n",
      "Batch 68500 of 10 sentences: Average Loss:   0.249984 Duration 0.173476\n",
      "Batch 68600 of 10 sentences: Average Loss:   0.249969 Duration 0.175088\n",
      "Batch 68700 of 10 sentences: Average Loss:   0.249931 Duration 0.119848\n",
      "Batch 68800 of 10 sentences: Average Loss:   0.249916 Duration 0.170043\n",
      "Batch 68900 of 10 sentences: Average Loss:   0.249906 Duration 0.148525\n",
      "Batch 69000 of 10 sentences: Average Loss:   0.249881 Duration 0.153543\n",
      "Batch 69100 of 10 sentences: Average Loss:   0.249871 Duration 0.156125\n",
      "Batch 69200 of 10 sentences: Average Loss:   0.249844 Duration 0.103979\n",
      "Batch 69300 of 10 sentences: Average Loss:   0.249818 Duration 0.138359\n",
      "Batch 69400 of 10 sentences: Average Loss:   0.249791 Duration 0.212449\n",
      "Batch 69500 of 10 sentences: Average Loss:   0.249766 Duration 0.137576\n",
      "Batch 69600 of 10 sentences: Average Loss:   0.249731 Duration 0.120594\n",
      "Batch 69700 of 10 sentences: Average Loss:   0.249702 Duration 0.118738\n",
      "Batch 69800 of 10 sentences: Average Loss:   0.249682 Duration 0.106735\n",
      "Batch 69900 of 10 sentences: Average Loss:   0.249648 Duration 0.195529\n",
      "Batch 70000 of 10 sentences: Average Loss:   0.249624 Duration 0.195773\n",
      "Batch 70100 of 10 sentences: Average Loss:   0.249614 Duration 0.166949\n",
      "Batch 70200 of 10 sentences: Average Loss:   0.249591 Duration 0.146405\n",
      "Batch 70300 of 10 sentences: Average Loss:   0.249566 Duration 0.174980\n",
      "Batch 70400 of 10 sentences: Average Loss:   0.249555 Duration 0.240712\n",
      "Batch 70500 of 10 sentences: Average Loss:   0.249525 Duration 0.186645\n",
      "Batch 70600 of 10 sentences: Average Loss:   0.249510 Duration 0.095283\n",
      "Batch 70700 of 10 sentences: Average Loss:   0.249479 Duration 0.209280\n",
      "Batch 70800 of 10 sentences: Average Loss:   0.249460 Duration 0.157407\n",
      "Batch 70900 of 10 sentences: Average Loss:   0.249428 Duration 0.144003\n",
      "Batch 71000 of 10 sentences: Average Loss:   0.249414 Duration 0.161580\n",
      "Batch 71100 of 10 sentences: Average Loss:   0.249394 Duration 0.163792\n",
      "Batch 71200 of 10 sentences: Average Loss:   0.249364 Duration 0.184919\n",
      "Batch 71300 of 10 sentences: Average Loss:   0.249330 Duration 0.180836\n",
      "Batch 71400 of 10 sentences: Average Loss:   0.249308 Duration 0.199102\n",
      "Batch 71500 of 10 sentences: Average Loss:   0.249280 Duration 0.131244\n",
      "epoch 16 batches 71535 done\n",
      "Batch 71600 of 10 sentences: Average Loss:   0.249262 Duration 0.140452\n",
      "Batch 71700 of 10 sentences: Average Loss:   0.249246 Duration 0.139033\n",
      "Batch 71800 of 10 sentences: Average Loss:   0.249237 Duration 0.188629\n",
      "Batch 71900 of 10 sentences: Average Loss:   0.249195 Duration 0.182734\n",
      "Batch 72000 of 10 sentences: Average Loss:   0.249172 Duration 0.222212\n",
      "Batch 72100 of 10 sentences: Average Loss:   0.249163 Duration 0.143179\n",
      "Batch 72200 of 10 sentences: Average Loss:   0.249146 Duration 0.263871\n",
      "Batch 72300 of 10 sentences: Average Loss:   0.249113 Duration 0.173677\n",
      "Batch 72400 of 10 sentences: Average Loss:   0.249102 Duration 0.181001\n",
      "Batch 72500 of 10 sentences: Average Loss:   0.249086 Duration 0.177636\n",
      "Batch 72600 of 10 sentences: Average Loss:   0.249069 Duration 0.160605\n",
      "Batch 72700 of 10 sentences: Average Loss:   0.249057 Duration 0.079566\n",
      "Batch 72800 of 10 sentences: Average Loss:   0.249045 Duration 0.210744\n",
      "Batch 72900 of 10 sentences: Average Loss:   0.249015 Duration 0.188852\n",
      "Batch 73000 of 10 sentences: Average Loss:   0.248994 Duration 0.211291\n",
      "Batch 73100 of 10 sentences: Average Loss:   0.248983 Duration 0.173909\n",
      "Batch 73200 of 10 sentences: Average Loss:   0.248955 Duration 0.154683\n",
      "Batch 73300 of 10 sentences: Average Loss:   0.248948 Duration 0.135696\n",
      "Batch 73400 of 10 sentences: Average Loss:   0.248925 Duration 0.133512\n",
      "Batch 73500 of 10 sentences: Average Loss:   0.248897 Duration 0.186660\n",
      "Batch 73600 of 10 sentences: Average Loss:   0.248873 Duration 0.240021\n",
      "Batch 73700 of 10 sentences: Average Loss:   0.248852 Duration 0.148972\n",
      "Batch 73800 of 10 sentences: Average Loss:   0.248818 Duration 0.185332\n",
      "Batch 73900 of 10 sentences: Average Loss:   0.248786 Duration 0.140573\n",
      "Batch 74000 of 10 sentences: Average Loss:   0.248767 Duration 0.160730\n",
      "Batch 74100 of 10 sentences: Average Loss:   0.248738 Duration 0.145293\n",
      "Batch 74200 of 10 sentences: Average Loss:   0.248713 Duration 0.173570\n",
      "Batch 74300 of 10 sentences: Average Loss:   0.248707 Duration 0.162618\n",
      "Batch 74400 of 10 sentences: Average Loss:   0.248684 Duration 0.207166\n",
      "Batch 74500 of 10 sentences: Average Loss:   0.248658 Duration 0.097512\n",
      "Batch 74600 of 10 sentences: Average Loss:   0.248644 Duration 0.150043\n",
      "Batch 74700 of 10 sentences: Average Loss:   0.248619 Duration 0.202552\n",
      "Batch 74800 of 10 sentences: Average Loss:   0.248604 Duration 0.162316\n",
      "Batch 74900 of 10 sentences: Average Loss:   0.248571 Duration 0.174282\n",
      "Batch 75000 of 10 sentences: Average Loss:   0.248555 Duration 0.136920\n",
      "Batch 75100 of 10 sentences: Average Loss:   0.248518 Duration 0.125659\n",
      "Batch 75200 of 10 sentences: Average Loss:   0.248507 Duration 0.160395\n",
      "Batch 75300 of 10 sentences: Average Loss:   0.248492 Duration 0.240389\n",
      "Batch 75400 of 10 sentences: Average Loss:   0.248460 Duration 0.265728\n",
      "Batch 75500 of 10 sentences: Average Loss:   0.248434 Duration 0.121329\n",
      "Batch 75600 of 10 sentences: Average Loss:   0.248402 Duration 0.218779\n",
      "Batch 75700 of 10 sentences: Average Loss:   0.248378 Duration 0.114851\n",
      "epoch 17 batches 75743 done\n",
      "Batch 75800 of 10 sentences: Average Loss:   0.248359 Duration 0.197722\n",
      "Batch 75900 of 10 sentences: Average Loss:   0.248342 Duration 0.091408\n",
      "Batch 76000 of 10 sentences: Average Loss:   0.248332 Duration 0.184890\n",
      "Batch 76100 of 10 sentences: Average Loss:   0.248295 Duration 0.151361\n",
      "Batch 76200 of 10 sentences: Average Loss:   0.248270 Duration 0.230363\n",
      "Batch 76300 of 10 sentences: Average Loss:   0.248264 Duration 0.120505\n",
      "Batch 76400 of 10 sentences: Average Loss:   0.248248 Duration 0.146494\n",
      "Batch 76500 of 10 sentences: Average Loss:   0.248216 Duration 0.142619\n",
      "Batch 76600 of 10 sentences: Average Loss:   0.248205 Duration 0.233927\n",
      "Batch 76700 of 10 sentences: Average Loss:   0.248191 Duration 0.143502\n",
      "Batch 76800 of 10 sentences: Average Loss:   0.248173 Duration 0.145756\n",
      "Batch 76900 of 10 sentences: Average Loss:   0.248162 Duration 0.191042\n",
      "Batch 77000 of 10 sentences: Average Loss:   0.248152 Duration 0.141173\n",
      "Batch 77100 of 10 sentences: Average Loss:   0.248127 Duration 0.143006\n",
      "Batch 77200 of 10 sentences: Average Loss:   0.248103 Duration 0.211565\n",
      "Batch 77300 of 10 sentences: Average Loss:   0.248093 Duration 0.118303\n",
      "Batch 77400 of 10 sentences: Average Loss:   0.248068 Duration 0.143708\n",
      "Batch 77500 of 10 sentences: Average Loss:   0.248060 Duration 0.138857\n",
      "Batch 77600 of 10 sentences: Average Loss:   0.248038 Duration 0.190229\n",
      "Batch 77700 of 10 sentences: Average Loss:   0.248010 Duration 0.188641\n",
      "Batch 77800 of 10 sentences: Average Loss:   0.247985 Duration 0.127381\n",
      "Batch 77900 of 10 sentences: Average Loss:   0.247966 Duration 0.175134\n",
      "Batch 78000 of 10 sentences: Average Loss:   0.247932 Duration 0.206597\n",
      "Batch 78100 of 10 sentences: Average Loss:   0.247900 Duration 0.189123\n",
      "Batch 78200 of 10 sentences: Average Loss:   0.247885 Duration 0.262566\n",
      "Batch 78300 of 10 sentences: Average Loss:   0.247857 Duration 0.184966\n",
      "Batch 78400 of 10 sentences: Average Loss:   0.247836 Duration 0.154153\n",
      "Batch 78500 of 10 sentences: Average Loss:   0.247829 Duration 0.206424\n",
      "Batch 78600 of 10 sentences: Average Loss:   0.247806 Duration 0.180374\n",
      "Batch 78700 of 10 sentences: Average Loss:   0.247782 Duration 0.187085\n",
      "Batch 78800 of 10 sentences: Average Loss:   0.247768 Duration 0.098570\n",
      "Batch 78900 of 10 sentences: Average Loss:   0.247747 Duration 0.104045\n",
      "Batch 79000 of 10 sentences: Average Loss:   0.247732 Duration 0.169621\n",
      "Batch 79100 of 10 sentences: Average Loss:   0.247699 Duration 0.167412\n",
      "Batch 79200 of 10 sentences: Average Loss:   0.247688 Duration 0.148311\n",
      "Batch 79300 of 10 sentences: Average Loss:   0.247654 Duration 0.126195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 79400 of 10 sentences: Average Loss:   0.247639 Duration 0.130252\n",
      "Batch 79500 of 10 sentences: Average Loss:   0.247623 Duration 0.187007\n",
      "Batch 79600 of 10 sentences: Average Loss:   0.247594 Duration 0.107959\n",
      "Batch 79700 of 10 sentences: Average Loss:   0.247568 Duration 0.182772\n",
      "Batch 79800 of 10 sentences: Average Loss:   0.247536 Duration 0.150264\n",
      "Batch 79900 of 10 sentences: Average Loss:   0.247519 Duration 0.174709\n",
      "epoch 18 batches 79951 done\n",
      "Batch 80000 of 10 sentences: Average Loss:   0.247495 Duration 0.219630\n",
      "Batch 80100 of 10 sentences: Average Loss:   0.247481 Duration 0.147173\n",
      "Batch 80200 of 10 sentences: Average Loss:   0.247470 Duration 0.196369\n",
      "Batch 80300 of 10 sentences: Average Loss:   0.247435 Duration 0.248974\n",
      "Batch 80400 of 10 sentences: Average Loss:   0.247412 Duration 0.256680\n",
      "Batch 80500 of 10 sentences: Average Loss:   0.247403 Duration 0.146693\n",
      "Batch 80600 of 10 sentences: Average Loss:   0.247386 Duration 0.152255\n",
      "Batch 80700 of 10 sentences: Average Loss:   0.247356 Duration 0.085011\n",
      "Batch 80800 of 10 sentences: Average Loss:   0.247343 Duration 0.164715\n",
      "Batch 80900 of 10 sentences: Average Loss:   0.247328 Duration 0.152713\n",
      "Batch 81000 of 10 sentences: Average Loss:   0.247311 Duration 0.155930\n",
      "Batch 81100 of 10 sentences: Average Loss:   0.247299 Duration 0.127493\n",
      "Batch 81200 of 10 sentences: Average Loss:   0.247291 Duration 0.170392\n",
      "Batch 81300 of 10 sentences: Average Loss:   0.247264 Duration 0.159044\n",
      "Batch 81400 of 10 sentences: Average Loss:   0.247239 Duration 0.161275\n",
      "Batch 81500 of 10 sentences: Average Loss:   0.247230 Duration 0.157474\n",
      "Batch 81600 of 10 sentences: Average Loss:   0.247207 Duration 0.161621\n",
      "Batch 81700 of 10 sentences: Average Loss:   0.247196 Duration 0.188935\n",
      "Batch 81800 of 10 sentences: Average Loss:   0.247177 Duration 0.197327\n",
      "Batch 81900 of 10 sentences: Average Loss:   0.247150 Duration 0.120795\n",
      "Batch 82000 of 10 sentences: Average Loss:   0.247126 Duration 0.092277\n",
      "Batch 82100 of 10 sentences: Average Loss:   0.247105 Duration 0.190006\n",
      "Batch 82200 of 10 sentences: Average Loss:   0.247075 Duration 0.215250\n",
      "Batch 82300 of 10 sentences: Average Loss:   0.247043 Duration 0.169484\n",
      "Batch 82400 of 10 sentences: Average Loss:   0.247029 Duration 0.169111\n",
      "Batch 82500 of 10 sentences: Average Loss:   0.247001 Duration 0.149386\n",
      "Batch 82600 of 10 sentences: Average Loss:   0.246978 Duration 0.187582\n",
      "Batch 82700 of 10 sentences: Average Loss:   0.246968 Duration 0.112541\n",
      "Batch 82800 of 10 sentences: Average Loss:   0.246948 Duration 0.133862\n",
      "Batch 82900 of 10 sentences: Average Loss:   0.246926 Duration 0.212273\n",
      "Batch 83000 of 10 sentences: Average Loss:   0.246910 Duration 0.206743\n",
      "Batch 83100 of 10 sentences: Average Loss:   0.246889 Duration 0.158139\n",
      "Batch 83200 of 10 sentences: Average Loss:   0.246873 Duration 0.213644\n",
      "Batch 83300 of 10 sentences: Average Loss:   0.246852 Duration 0.073507\n",
      "Batch 83400 of 10 sentences: Average Loss:   0.246835 Duration 0.200344\n",
      "Batch 83500 of 10 sentences: Average Loss:   0.246805 Duration 0.157167\n",
      "Batch 83600 of 10 sentences: Average Loss:   0.246788 Duration 0.170663\n",
      "Batch 83700 of 10 sentences: Average Loss:   0.246773 Duration 0.154163\n",
      "Batch 83800 of 10 sentences: Average Loss:   0.246742 Duration 0.136140\n",
      "Batch 83900 of 10 sentences: Average Loss:   0.246717 Duration 0.162455\n",
      "Batch 84000 of 10 sentences: Average Loss:   0.246687 Duration 0.122409\n",
      "Batch 84100 of 10 sentences: Average Loss:   0.246663 Duration 0.177439\n",
      "epoch 19 batches 84159 done\n",
      "Batch 84200 of 10 sentences: Average Loss:   0.246644 Duration 0.148454\n",
      "Batch 84300 of 10 sentences: Average Loss:   0.246628 Duration 0.277539\n",
      "Batch 84400 of 10 sentences: Average Loss:   0.246616 Duration 0.258892\n",
      "Batch 84500 of 10 sentences: Average Loss:   0.246589 Duration 0.150388\n",
      "Batch 84600 of 10 sentences: Average Loss:   0.246562 Duration 0.147473\n",
      "Batch 84700 of 10 sentences: Average Loss:   0.246551 Duration 0.167349\n",
      "Batch 84800 of 10 sentences: Average Loss:   0.246540 Duration 0.169627\n",
      "Batch 84900 of 10 sentences: Average Loss:   0.246511 Duration 0.160623\n",
      "Batch 85000 of 10 sentences: Average Loss:   0.246493 Duration 0.187154\n",
      "Batch 85100 of 10 sentences: Average Loss:   0.246480 Duration 0.161816\n",
      "Batch 85200 of 10 sentences: Average Loss:   0.246463 Duration 0.186144\n",
      "Batch 85300 of 10 sentences: Average Loss:   0.246453 Duration 0.163610\n",
      "Batch 85400 of 10 sentences: Average Loss:   0.246440 Duration 0.179007\n",
      "Batch 85500 of 10 sentences: Average Loss:   0.246415 Duration 0.222410\n",
      "Batch 85600 of 10 sentences: Average Loss:   0.246394 Duration 0.118828\n",
      "Batch 85700 of 10 sentences: Average Loss:   0.246383 Duration 0.192275\n",
      "Batch 85800 of 10 sentences: Average Loss:   0.246362 Duration 0.190692\n",
      "Batch 85900 of 10 sentences: Average Loss:   0.246350 Duration 0.193034\n",
      "Batch 86000 of 10 sentences: Average Loss:   0.246333 Duration 0.176521\n",
      "Batch 86100 of 10 sentences: Average Loss:   0.246307 Duration 0.149846\n",
      "Batch 86200 of 10 sentences: Average Loss:   0.246287 Duration 0.146603\n",
      "Batch 86300 of 10 sentences: Average Loss:   0.246262 Duration 0.207012\n",
      "Batch 86400 of 10 sentences: Average Loss:   0.246235 Duration 0.168355\n",
      "Batch 86500 of 10 sentences: Average Loss:   0.246202 Duration 0.170705\n",
      "Batch 86600 of 10 sentences: Average Loss:   0.246189 Duration 0.095222\n",
      "Batch 86700 of 10 sentences: Average Loss:   0.246163 Duration 0.112696\n",
      "Batch 86800 of 10 sentences: Average Loss:   0.246139 Duration 0.187309\n",
      "Batch 86900 of 10 sentences: Average Loss:   0.246132 Duration 0.172082\n",
      "Batch 87000 of 10 sentences: Average Loss:   0.246114 Duration 0.151713\n",
      "Batch 87100 of 10 sentences: Average Loss:   0.246093 Duration 0.227166\n",
      "Batch 87200 of 10 sentences: Average Loss:   0.246073 Duration 0.110725\n",
      "Batch 87300 of 10 sentences: Average Loss:   0.246056 Duration 0.189682\n",
      "Batch 87400 of 10 sentences: Average Loss:   0.246038 Duration 0.205225\n",
      "Batch 87500 of 10 sentences: Average Loss:   0.246016 Duration 0.198909\n",
      "Batch 87600 of 10 sentences: Average Loss:   0.245999 Duration 0.207503\n",
      "Batch 87700 of 10 sentences: Average Loss:   0.245968 Duration 0.058499\n",
      "Batch 87800 of 10 sentences: Average Loss:   0.245950 Duration 0.187359\n",
      "Batch 87900 of 10 sentences: Average Loss:   0.245940 Duration 0.158801\n",
      "Batch 88000 of 10 sentences: Average Loss:   0.245907 Duration 0.188208\n",
      "Batch 88100 of 10 sentences: Average Loss:   0.245882 Duration 0.176299\n",
      "Batch 88200 of 10 sentences: Average Loss:   0.245853 Duration 0.180598\n",
      "Batch 88300 of 10 sentences: Average Loss:   0.245832 Duration 0.144451\n",
      "epoch 20 batches 88367 done\n",
      "Batch 88400 of 10 sentences: Average Loss:   0.245811 Duration 0.139494\n",
      "Batch 88500 of 10 sentences: Average Loss:   0.245797 Duration 0.152743\n",
      "Batch 88600 of 10 sentences: Average Loss:   0.245782 Duration 0.143159\n",
      "Batch 88700 of 10 sentences: Average Loss:   0.245756 Duration 0.146024\n",
      "Batch 88800 of 10 sentences: Average Loss:   0.245729 Duration 0.132024\n",
      "Batch 88900 of 10 sentences: Average Loss:   0.245717 Duration 0.190353\n",
      "Batch 89000 of 10 sentences: Average Loss:   0.245710 Duration 0.179863\n",
      "Batch 89100 of 10 sentences: Average Loss:   0.245682 Duration 0.116409\n",
      "Batch 89200 of 10 sentences: Average Loss:   0.245662 Duration 0.233359\n",
      "Batch 89300 of 10 sentences: Average Loss:   0.245651 Duration 0.237816\n",
      "Batch 89400 of 10 sentences: Average Loss:   0.245630 Duration 0.149446\n",
      "Batch 89500 of 10 sentences: Average Loss:   0.245623 Duration 0.160884\n",
      "Batch 89600 of 10 sentences: Average Loss:   0.245608 Duration 0.166065\n",
      "Batch 89700 of 10 sentences: Average Loss:   0.245583 Duration 0.106629\n",
      "Batch 89800 of 10 sentences: Average Loss:   0.245560 Duration 0.156798\n",
      "Batch 89900 of 10 sentences: Average Loss:   0.245551 Duration 0.125516\n",
      "Batch 90000 of 10 sentences: Average Loss:   0.245530 Duration 0.230433\n",
      "Batch 90100 of 10 sentences: Average Loss:   0.245516 Duration 0.165007\n",
      "Batch 90200 of 10 sentences: Average Loss:   0.245508 Duration 0.245063\n",
      "Batch 90300 of 10 sentences: Average Loss:   0.245477 Duration 0.169244\n",
      "Batch 90400 of 10 sentences: Average Loss:   0.245454 Duration 0.211895\n",
      "Batch 90500 of 10 sentences: Average Loss:   0.245433 Duration 0.132174\n",
      "Batch 90600 of 10 sentences: Average Loss:   0.245406 Duration 0.176785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 90700 of 10 sentences: Average Loss:   0.245370 Duration 0.256352\n",
      "Batch 90800 of 10 sentences: Average Loss:   0.245360 Duration 0.170192\n",
      "Batch 90900 of 10 sentences: Average Loss:   0.245334 Duration 0.141894\n",
      "Batch 91000 of 10 sentences: Average Loss:   0.245308 Duration 0.165189\n",
      "Batch 91100 of 10 sentences: Average Loss:   0.245300 Duration 0.176229\n",
      "Batch 91200 of 10 sentences: Average Loss:   0.245281 Duration 0.135576\n",
      "Batch 91300 of 10 sentences: Average Loss:   0.245261 Duration 0.138963\n",
      "Batch 91400 of 10 sentences: Average Loss:   0.245240 Duration 0.160622\n",
      "Batch 91500 of 10 sentences: Average Loss:   0.245226 Duration 0.165834\n",
      "Batch 91600 of 10 sentences: Average Loss:   0.245207 Duration 0.175700\n",
      "Batch 91700 of 10 sentences: Average Loss:   0.245189 Duration 0.196054\n",
      "Batch 91800 of 10 sentences: Average Loss:   0.245171 Duration 0.145123\n",
      "Batch 91900 of 10 sentences: Average Loss:   0.245141 Duration 0.153350\n",
      "Batch 92000 of 10 sentences: Average Loss:   0.245124 Duration 0.313132\n",
      "Batch 92100 of 10 sentences: Average Loss:   0.245116 Duration 0.173989\n",
      "Batch 92200 of 10 sentences: Average Loss:   0.245085 Duration 0.307242\n",
      "Batch 92300 of 10 sentences: Average Loss:   0.245061 Duration 0.272289\n",
      "Batch 92400 of 10 sentences: Average Loss:   0.245036 Duration 0.143027\n",
      "Batch 92500 of 10 sentences: Average Loss:   0.245013 Duration 0.213856\n",
      "epoch 21 batches 92575 done\n",
      "Batch 92600 of 10 sentences: Average Loss:   0.244993 Duration 0.140296\n",
      "Batch 92700 of 10 sentences: Average Loss:   0.244982 Duration 0.224091\n",
      "Batch 92800 of 10 sentences: Average Loss:   0.244964 Duration 0.192749\n",
      "Batch 92900 of 10 sentences: Average Loss:   0.244942 Duration 0.144243\n",
      "Batch 93000 of 10 sentences: Average Loss:   0.244918 Duration 0.167122\n",
      "Batch 93100 of 10 sentences: Average Loss:   0.244901 Duration 0.146372\n",
      "Batch 93200 of 10 sentences: Average Loss:   0.244894 Duration 0.197480\n",
      "Batch 93300 of 10 sentences: Average Loss:   0.244868 Duration 0.170655\n",
      "Batch 93400 of 10 sentences: Average Loss:   0.244847 Duration 0.118162\n",
      "Batch 93500 of 10 sentences: Average Loss:   0.244836 Duration 0.172903\n",
      "Batch 93600 of 10 sentences: Average Loss:   0.244820 Duration 0.174178\n",
      "Batch 93700 of 10 sentences: Average Loss:   0.244812 Duration 0.188767\n",
      "Batch 93800 of 10 sentences: Average Loss:   0.244797 Duration 0.179603\n",
      "Batch 93900 of 10 sentences: Average Loss:   0.244774 Duration 0.097119\n",
      "Batch 94000 of 10 sentences: Average Loss:   0.244752 Duration 0.121792\n",
      "Batch 94100 of 10 sentences: Average Loss:   0.244743 Duration 0.098604\n",
      "Batch 94200 of 10 sentences: Average Loss:   0.244725 Duration 0.204455\n",
      "Batch 94300 of 10 sentences: Average Loss:   0.244710 Duration 0.213866\n",
      "Batch 94400 of 10 sentences: Average Loss:   0.244700 Duration 0.147885\n",
      "Batch 94500 of 10 sentences: Average Loss:   0.244671 Duration 0.125501\n",
      "Batch 94600 of 10 sentences: Average Loss:   0.244649 Duration 0.194426\n",
      "Batch 94700 of 10 sentences: Average Loss:   0.244628 Duration 0.065986\n",
      "Batch 94800 of 10 sentences: Average Loss:   0.244599 Duration 0.191004\n",
      "Batch 94900 of 10 sentences: Average Loss:   0.244566 Duration 0.166385\n",
      "Batch 95000 of 10 sentences: Average Loss:   0.244557 Duration 0.132871\n",
      "Batch 95100 of 10 sentences: Average Loss:   0.244529 Duration 0.170991\n",
      "Batch 95200 of 10 sentences: Average Loss:   0.244502 Duration 0.211857\n",
      "Batch 95300 of 10 sentences: Average Loss:   0.244491 Duration 0.141662\n",
      "Batch 95400 of 10 sentences: Average Loss:   0.244475 Duration 0.147019\n",
      "Batch 95500 of 10 sentences: Average Loss:   0.244455 Duration 0.119927\n",
      "Batch 95600 of 10 sentences: Average Loss:   0.244434 Duration 0.149773\n",
      "Batch 95700 of 10 sentences: Average Loss:   0.244422 Duration 0.193578\n",
      "Batch 95800 of 10 sentences: Average Loss:   0.244400 Duration 0.165798\n",
      "Batch 95900 of 10 sentences: Average Loss:   0.244383 Duration 0.200017\n",
      "Batch 96000 of 10 sentences: Average Loss:   0.244367 Duration 0.178844\n",
      "Batch 96100 of 10 sentences: Average Loss:   0.244340 Duration 0.158262\n",
      "Batch 96200 of 10 sentences: Average Loss:   0.244319 Duration 0.200643\n",
      "Batch 96300 of 10 sentences: Average Loss:   0.244311 Duration 0.165945\n",
      "Batch 96400 of 10 sentences: Average Loss:   0.244282 Duration 0.160010\n",
      "Batch 96500 of 10 sentences: Average Loss:   0.244261 Duration 0.108653\n",
      "Batch 96600 of 10 sentences: Average Loss:   0.244231 Duration 0.299660\n",
      "Batch 96700 of 10 sentences: Average Loss:   0.244209 Duration 0.161934\n",
      "epoch 22 batches 96783 done\n",
      "Batch 96800 of 10 sentences: Average Loss:   0.244189 Duration 0.181999\n",
      "Batch 96900 of 10 sentences: Average Loss:   0.244178 Duration 0.116254\n",
      "Batch 97000 of 10 sentences: Average Loss:   0.244163 Duration 0.136782\n",
      "Batch 97100 of 10 sentences: Average Loss:   0.244139 Duration 0.155272\n",
      "Batch 97200 of 10 sentences: Average Loss:   0.244118 Duration 0.111301\n",
      "Batch 97300 of 10 sentences: Average Loss:   0.244097 Duration 0.176307\n",
      "Batch 97400 of 10 sentences: Average Loss:   0.244092 Duration 0.133274\n",
      "Batch 97500 of 10 sentences: Average Loss:   0.244067 Duration 0.190568\n",
      "Batch 97600 of 10 sentences: Average Loss:   0.244048 Duration 0.173850\n",
      "Batch 97700 of 10 sentences: Average Loss:   0.244036 Duration 0.191480\n",
      "Batch 97800 of 10 sentences: Average Loss:   0.244021 Duration 0.147486\n",
      "Batch 97900 of 10 sentences: Average Loss:   0.244015 Duration 0.137882\n",
      "Batch 98000 of 10 sentences: Average Loss:   0.244000 Duration 0.184788\n",
      "Batch 98100 of 10 sentences: Average Loss:   0.243980 Duration 0.064217\n",
      "Batch 98200 of 10 sentences: Average Loss:   0.243957 Duration 0.242910\n",
      "Batch 98300 of 10 sentences: Average Loss:   0.243949 Duration 0.227195\n",
      "Batch 98400 of 10 sentences: Average Loss:   0.243927 Duration 0.182532\n",
      "Batch 98500 of 10 sentences: Average Loss:   0.243913 Duration 0.131271\n",
      "Batch 98600 of 10 sentences: Average Loss:   0.243903 Duration 0.195998\n",
      "Batch 98700 of 10 sentences: Average Loss:   0.243874 Duration 0.182437\n",
      "Batch 98800 of 10 sentences: Average Loss:   0.243852 Duration 0.165372\n",
      "Batch 98900 of 10 sentences: Average Loss:   0.243834 Duration 0.172096\n",
      "Batch 99000 of 10 sentences: Average Loss:   0.243807 Duration 0.212821\n",
      "Batch 99100 of 10 sentences: Average Loss:   0.243776 Duration 0.161408\n",
      "Batch 99200 of 10 sentences: Average Loss:   0.243765 Duration 0.150665\n",
      "Batch 99300 of 10 sentences: Average Loss:   0.243737 Duration 0.179657\n",
      "Batch 99400 of 10 sentences: Average Loss:   0.243711 Duration 0.156318\n",
      "Batch 99500 of 10 sentences: Average Loss:   0.243700 Duration 0.204019\n",
      "Batch 99600 of 10 sentences: Average Loss:   0.243684 Duration 0.140712\n",
      "Batch 99700 of 10 sentences: Average Loss:   0.243663 Duration 0.174914\n",
      "Batch 99800 of 10 sentences: Average Loss:   0.243643 Duration 0.171163\n",
      "Batch 99900 of 10 sentences: Average Loss:   0.243633 Duration 0.133696\n",
      "         9143442729 function calls (8819225871 primitive calls) in 13934.218 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        5    0.000    0.000 13934.218 2786.844 interactiveshell.py:3377(run_code)\n",
      "        5    0.000    0.000 13934.218 2786.844 {built-in method builtins.exec}\n",
      "        1    1.382    1.382 13934.186 13934.186 <ipython-input-33-0580c1f4e1b7>:25(<module>)\n",
      "    99977    4.115    0.000 13908.333    0.139 base.py:258(train)\n",
      "299931/199954    3.214    0.000 11575.733    0.058 function.py:1111(_)\n",
      "    99977    0.610    0.000 6537.607    0.065 base.py:211(function)\n",
      "    99977   16.890    0.000 6077.892    0.061 embedding.py:636(function)\n",
      "    99977   88.758    0.001 5616.799    0.056 embedding.py:758(<listcomp>)\n",
      " 14715076  461.885    0.000 5416.351    0.000 preprocess.py:283(negative_sample_indices)\n",
      "    99977    0.696    0.000 5057.249    0.051 base.py:230(gradient)\n",
      " 14715076  381.077    0.000 4447.438    0.000 preprocess.py:254(sample)\n",
      "   499885    2.328    0.000 4362.992    0.009 arrayprint.py:1500(_array_str_implementation)\n",
      "   499885    4.135    0.000 4360.664    0.009 arrayprint.py:516(array2string)\n",
      "   499885    4.037    0.000 4349.693    0.009 arrayprint.py:461(wrapper)\n",
      "   499885    6.843    0.000 4344.087    0.009 arrayprint.py:478(_array2string)\n",
      "    99977   27.992    0.000 4316.935    0.043 embedding.py:805(gradient)\n",
      " 14715076 2173.759    0.000 4029.400    0.000 {method 'choice' of 'numpy.random.mtrand.RandomState' objects}\n",
      "   499885   18.236    0.000 3157.650    0.006 base.py:239(dX)\n",
      "   499885    1.577    0.000 2967.556    0.006 arrayprint.py:736(_formatArray)\n",
      "320517657/499885  781.407    0.000 2965.979    0.006 arrayprint.py:745(recurser)\n",
      "    99977    1.528    0.000 2108.930    0.021 base.py:239(update)\n",
      "    99977    0.485    0.000 2106.904    0.021 composite.py:207(update)\n",
      "    99977    0.827    0.000 2106.309    0.021 composite.py:214(<listcomp>)\n",
      "    99977    8.777    0.000 2103.100    0.021 embedding.py:1248(update)\n",
      "   299931    7.327    0.000 2055.682    0.007 embedding.py:1234(_gradient_descent)\n",
      "   299931 1851.415    0.006 1851.415    0.006 {method 'at' of 'numpy.ufunc' objects}\n",
      "75470043/73270641  129.518    0.000 1799.719    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "289587896  937.818    0.000 1447.987    0.000 arrayprint.py:974(__call__)\n",
      "   499885    5.526    0.000 1368.311    0.003 arrayprint.py:409(_get_format_function)\n",
      "   499885    1.889    0.000 1359.159    0.003 arrayprint.py:366(<lambda>)\n",
      "   499885    8.089    0.000 1357.270    0.003 arrayprint.py:863(__init__)\n",
      "   499885  249.454    0.000 1348.658    0.003 arrayprint.py:890(fillFormat)\n",
      "7398298/5498735   12.533    0.000  989.349    0.000 dispatch.py:198(wrapper)\n",
      " 17670837   19.134    0.000  850.806    0.000 <__array_function__ internals>:2(cumsum)\n",
      " 17670837   22.798    0.000  810.565    0.000 fromnumeric.py:2446(cumsum)\n",
      " 19770308   18.711    0.000  797.524    0.000 fromnumeric.py:52(_wrapfunc)\n",
      " 17670837  765.492    0.000  765.492    0.000 {method 'cumsum' of 'numpy.ndarray' objects}\n",
      "289587896  231.084    0.000  666.738    0.000 arrayprint.py:709(_extendLine_pretty)\n",
      "  7698229  642.938    0.000  642.938    0.000 {built-in method tensorflow.python._pywrap_tfe.TFE_Py_FastPathExecute}\n",
      "402595960  627.324    0.000  627.324    0.000 {built-in method numpy.core._multiarray_umath.dragon4_positional}\n",
      "    99977    0.797    0.000  585.725    0.006 preprocess.py:655(gradient)\n",
      "201697903  101.189    0.000  570.751    0.000 arrayprint.py:945(<genexpr>)\n",
      " 17670837   22.321    0.000  497.432    0.000 <__array_function__ internals>:2(unique)\n",
      " 29430152   25.085    0.000  458.821    0.000 preprocess.py:233(list_indices)\n",
      " 17670837   44.732    0.000  443.780    0.000 arraysetops.py:138(unique)\n",
      " 29430152  183.175    0.000  433.735    0.000 preprocess.py:238(<listcomp>)\n",
      "201697903  113.043    0.000  386.577    0.000 arrayprint.py:940(<genexpr>)\n",
      " 17670837  238.140    0.000  361.762    0.000 arraysetops.py:310(_unique1d)\n",
      "  2099517   11.963    0.000  352.976    0.000 tf.py:48(is_finite)\n",
      "289587896  255.642    0.000  350.930    0.000 arrayprint.py:695(_extendLine)\n",
      " 15015007   26.962    0.000  331.671    0.000 <__array_function__ internals>:2(prod)\n",
      "176579832  278.184    0.000  278.184    0.000 {built-in method numpy.core._multiarray_umath.dragon4_scientific}\n",
      " 33129194   99.356    0.000  271.109    0.000 {built-in method builtins.max}\n",
      "   499885    2.866    0.000  256.507    0.001 tf.py:200(einsum)\n",
      " 15015007   32.239    0.000  256.272    0.000 fromnumeric.py:2912(prod)\n",
      "   499885    1.908    0.000  252.011    0.001 special_math_ops.py:606(einsum)\n",
      "   499885   12.127    0.000  250.103    0.001 special_math_ops.py:1152(_einsum_v2)\n",
      "2660867848  249.434    0.000  249.434    0.000 {built-in method builtins.len}\n",
      " 16614639   70.126    0.000  247.580    0.000 fromnumeric.py:70(_wrapreduction)\n",
      "    99977    4.382    0.000  247.173    0.002 preprocess.py:550(function)\n",
      "  1399678    8.856    0.000  237.332    0.000 base.py:271(Y)\n",
      " 88389878   36.641    0.000  223.563    0.000 arrayprint.py:918(<genexpr>)\n",
      "    99977    4.360    0.000  220.038    0.002 preprocess.py:601(<listcomp>)\n",
      "  2099517   10.344    0.000  218.044    0.000 math_ops.py:2815(reduce_all)\n",
      "   999724   42.333    0.000  214.982    0.000 utility.py:43(event_context_pairs)\n",
      "490943895  212.521    0.000  212.521    0.000 {method 'get' of 'dict' objects}\n",
      "   499885    3.581    0.000  204.803    0.000 gen_linalg_ops.py:979(einsum)\n",
      "  1899563    7.169    0.000  199.721    0.000 base.py:232(reshape)\n",
      "   999770    4.764    0.000  176.311    0.000 tf.py:174(multiply)\n",
      "   799816    3.539    0.000  173.587    0.000 array_ops.py:59(reshape)\n",
      "   199954    0.541    0.000  170.936    0.001 embedding.py:621(_bagging)\n",
      "   999770    2.596    0.000  169.616    0.000 math_ops.py:472(multiply)\n",
      "   999770    6.863    0.000  167.020    0.000 gen_math_ops.py:6046(mul)\n",
      "   799816   10.403    0.000  166.016    0.000 gen_array_ops.py:8288(reshape)\n",
      " 88389878   41.601    0.000  163.406    0.000 arrayprint.py:915(<genexpr>)\n",
      "    99977    2.099    0.000  150.102    0.002 preprocess.py:341(function)\n",
      "    99977    1.106    0.000  133.162    0.001 preprocess.py:304(sentence_to_sequence)\n",
      "    99977    5.655    0.000  131.470    0.001 text.py:218(sentence_to_sequence)\n",
      " 14715076   35.401    0.000  129.192    0.000 {method 'prod' of 'numpy.generic' objects}\n",
      "  2099517   11.365    0.000  128.875    0.000 math_ops.py:1891(_ReductionDims)\n",
      "  2099517   15.551    0.000  119.790    0.000 gen_math_ops.py:4546(is_finite)\n",
      "   799816    9.074    0.000  119.412    0.000 gen_array_ops.py:8392(reshape_eager_fallback)\n",
      " 21613489   50.841    0.000  113.556    0.000 numerictypes.py:359(issubdtype)\n",
      "291187530  112.102    0.000  112.102    0.000 {method 'split' of 'str' objects}\n",
      " 16615639  108.247    0.000  108.247    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "    99977    3.531    0.000  106.051    0.001 objective.py:175(function)\n",
      "  2799356    4.333    0.000  105.589    0.000 trace.py:158(wrapped)\n",
      "   999724  105.485    0.000  105.485    0.000 utility.py:84(<listcomp>)\n",
      " 14715076   40.452    0.000  101.805    0.000 base.py:217(to_flat_list)\n",
      "  2799356   16.603    0.000  101.256    0.000 ops.py:1481(convert_to_tensor)\n",
      " 45240029   96.330    0.000   96.330    0.000 {built-in method numpy.array}\n",
      " 14715076    8.448    0.000   93.791    0.000 _methods.py:49(_prod)\n",
      "  2199494   16.792    0.000   93.180    0.000 execute.py:236(args_to_matching_eager)\n",
      "  4498965    4.420    0.000   92.046    0.000 constant_op.py:166(constant)\n",
      "  4498965   11.204    0.000   87.626    0.000 constant_op.py:268(_constant_impl)\n",
      "   299931    1.431    0.000   80.602    0.000 tf.py:41(full)\n",
      "   299931    1.149    0.000   78.552    0.000 base.py:308(dY)\n",
      "  7591829   24.657    0.000   76.931    0.000 base.py:119(tensor_shape)\n",
      "  2099517    5.776    0.000   75.192    0.000 gen_math_ops.py:509(_all)\n",
      "   299931    1.036    0.000   73.793    0.000 array_ops.py:200(fill)\n",
      " 29430152   60.445    0.000   73.412    0.000 preprocess.py:114(vocabulary_size)\n",
      "  1099747    3.803    0.000   73.046    0.000 execute.py:33(quick_execute)\n",
      " 14715076   17.279    0.000   71.618    0.000 <__array_function__ internals>:2(count_nonzero)\n",
      "   299931    2.457    0.000   70.954    0.000 gen_array_ops.py:3304(fill)\n",
      "    99977    3.445    0.000   68.399    0.001 text.py:266(<listcomp>)\n",
      "  1099747   66.973    0.000   66.973    0.000 {built-in method tensorflow.python._pywrap_tfe.TFE_Py_Execute}\n",
      "  4498965    3.891    0.000   66.345    0.000 constant_op.py:299(_constant_eager_impl)\n",
      "   699839   10.909    0.000   62.599    0.000 base.py:205(X)\n",
      " 35303869   38.021    0.000   62.581    0.000 tensor_util.py:992(is_tensor)\n",
      "  4498965   50.537    0.000   62.454    0.000 constant_op.py:70(convert_to_eager_tensor)\n",
      "   999724    1.546    0.000   61.924    0.000 <__array_function__ internals>:2(pad)\n",
      "   299931    2.325    0.000   61.072    0.000 gen_array_ops.py:3372(fill_eager_fallback)\n",
      "    99977    0.978    0.000   60.595    0.001 preprocess.py:380(gradient)\n",
      "   299931    5.711    0.000   60.580    0.000 sgd.py:78(differential)\n",
      " 17670837   57.695    0.000   57.695    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
      "   999724   11.285    0.000   57.542    0.000 arraypad.py:529(pad)\n",
      "438352673   56.544    0.000   56.544    0.000 preprocess.py:129(event_to_index)\n",
      "201697903   38.880    0.000   55.199    0.000 arrayprint.py:949(<genexpr>)\n",
      "201697903   38.366    0.000   54.345    0.000 arrayprint.py:950(<genexpr>)\n",
      " 43226978   35.043    0.000   54.200    0.000 numerictypes.py:285(issubclass_)\n",
      "  2499425    5.805    0.000   54.023    0.000 base.py:142(tensor_size)\n",
      " 35342674   16.223    0.000   53.411    0.000 _asarray.py:110(asanyarray)\n",
      "289587896   50.316    0.000   50.316    0.000 {method 'splitlines' of 'str' objects}\n",
      "    99977    0.572    0.000   48.314    0.000 adapter.py:69(gradient)\n",
      "   999724   14.279    0.000   47.629    0.000 index_tricks.py:317(__getitem__)\n",
      " 29430152   39.223    0.000   46.817    0.000 getlimits.py:382(__new__)\n",
      " 88389878   17.977    0.000   45.013    0.000 arrayprint.py:919(<genexpr>)\n",
      "    99977    0.703    0.000   44.822    0.000 adapter.py:64(function)\n",
      "   299931    0.478    0.000   41.185    0.000 array_ops.py:700(size_v2)\n",
      "155603698   39.088    0.000   41.078    0.000 {built-in method builtins.isinstance}\n",
      "    99977    6.048    0.000   40.963    0.000 objective.py:251(gradient)\n",
      "   299931    0.479    0.000   40.437    0.000 array_ops.py:734(size)\n",
      "   299931    3.298    0.000   39.957    0.000 array_ops.py:766(size_internal)\n",
      " 19264000   37.273    0.000   37.273    0.000 {built-in method numpy.empty}\n",
      "    99977    0.596    0.000   35.739    0.000 tf.py:163(add)\n",
      " 14715076   11.534    0.000   35.677    0.000 numeric.py:424(count_nonzero)\n",
      "    99977    1.198    0.000   35.142    0.000 gen_math_ops.py:299(add)\n",
      "  5398758   11.966    0.000   35.135    0.000 ops.py:1168(shape)\n",
      "  2299471    3.234    0.000   33.206    0.000 base.py:149(tensor_dtype)\n",
      "   299931    4.006    0.000   32.554    0.000 math_ops.py:1796(range)\n",
      "   299931   30.424    0.000   32.158    0.000 embedding.py:597(_extract_event_vectors)\n",
      "   999724    3.506    0.000   31.283    0.000 text.py:93(standardize)\n",
      "  1099747    1.112    0.000   30.522    0.000 constant_op.py:336(_constant_tensor_conversion_function)\n",
      "    99977    0.641    0.000   30.365    0.000 tf.py:188(divide)\n",
      "    99977    0.911    0.000   29.462    0.000 math_ops.py:438(divide)\n",
      "  2299471    4.102    0.000   28.825    0.000 base.py:64(is_scalar)\n",
      "    99977    0.928    0.000   28.160    0.000 math_ops.py:1161(binary_op_wrapper)\n",
      "  1199724    2.687    0.000   28.055    0.000 <__array_function__ internals>:2(all)\n",
      "   299931    0.404    0.000   27.994    0.000 base.py:165(to_tensor)\n",
      "   999724    0.982    0.000   26.508    0.000 re.py:203(sub)\n",
      "    99977    0.484    0.000   25.229    0.000 math_ops.py:1306(truediv)\n",
      "    99977    2.311    0.000   24.745    0.000 math_ops.py:1259(_truediv_python3)\n",
      " 14715076   24.143    0.000   24.143    0.000 {built-in method numpy.core._multiarray_umath.count_nonzero}\n",
      " 66689976   23.601    0.000   23.845    0.000 {built-in method builtins.getattr}\n",
      " 88289916   23.516    0.000   23.516    0.000 {method 'partition' of 'str' objects}\n",
      " 68941524   23.390    0.000   23.390    0.000 {built-in method builtins.issubclass}\n",
      "  1199724    3.629    0.000   22.647    0.000 fromnumeric.py:2355(all)\n",
      "   999724    9.759    0.000   22.432    0.000 text.py:252(<listcomp>)\n",
      "   999724   22.068    0.000   22.068    0.000 {method 'sub' of 're.Pattern' objects}\n",
      "   499885    8.642    0.000   21.347    0.000 special_math_ops.py:1226(_einsum_v2_parse_and_resolve_equation)\n",
      " 88389878   14.930    0.000   21.084    0.000 arrayprint.py:923(<genexpr>)\n",
      " 17670837   20.932    0.000   20.932    0.000 {method 'flatten' of 'numpy.ndarray' objects}\n",
      " 88389878   14.621    0.000   20.675    0.000 arrayprint.py:920(<genexpr>)\n",
      " 88389878   14.370    0.000   20.450    0.000 arrayprint.py:930(<genexpr>)\n",
      "   399908    8.589    0.000   19.881    0.000 embedding.py:322(dWs)\n",
      "  8198114   19.846    0.000   19.846    0.000 context.py:815(executing_eagerly)\n",
      "  1999448    6.755    0.000   19.729    0.000 arraypad.py:454(_as_pairs)\n",
      "  1099747    1.177    0.000   19.725    0.000 array_ops.py:1512(_autopacking_conversion_function)\n",
      "   999770    1.207    0.000   18.961    0.000 tensor_conversion_registry.py:50(_default_conversion_function)\n",
      " 17389483   11.572    0.000   18.712    0.000 base.py:84(is_tensor)\n",
      "  1099747    3.619    0.000   18.548    0.000 array_ops.py:1502(_should_not_autopack)\n",
      "  2899333    6.901    0.000   18.489    0.000 tensor_shape.py:748(__init__)\n",
      "    99977    2.682    0.000   18.344    0.000 embedding.py:1658(g)\n",
      "     1000    0.004    0.000   18.033    0.018 <__array_function__ internals>:2(mean)\n",
      "     1000    0.013    0.000   18.025    0.018 fromnumeric.py:3301(mean)\n",
      "     1000    0.045    0.000   18.012    0.018 _methods.py:161(_mean)\n",
      "   499885    2.442    0.000   17.609    0.000 embedding.py:277(Bc)\n",
      "    99977    6.759    0.000   17.587    0.000 function.py:751(sigmoid_cross_entropy_log_loss)\n",
      "   299931    1.704    0.000   16.878    0.000 math_ops.py:1718(tensor_equals)\n",
      "  2399448    2.420    0.000   16.449    0.000 ops.py:1181(get_shape)\n",
      "  2399448    4.972    0.000   16.084    0.000 base.py:93(is_float_tensor)\n",
      " 15814777   15.990    0.000   15.990    0.000 {method 'tolist' of 'numpy.ndarray' objects}\n",
      "  2399448    3.328    0.000   15.913    0.000 ops.py:991(__bool__)\n",
      "   999724    2.484    0.000   15.328    0.000 numerictypes.py:599(find_common_type)\n",
      "  2999310    4.551    0.000   15.321    0.000 context.py:1874(executing_eagerly)\n",
      " 16614639   14.794    0.000   14.794    0.000 fromnumeric.py:71(<dictcomp>)\n",
      " 17670837    9.824    0.000   14.702    0.000 arraysetops.py:125(_unpack_tuple)\n",
      "   199954    5.643    0.000   14.255    0.000 embedding.py:261(dWe)\n",
      "  1299701    4.090    0.000   13.945    0.000 ops.py:6402(name_scope)\n",
      "  1099747    1.816    0.000   13.558    0.000 <__array_function__ internals>:2(reshape)\n",
      "  2499425    2.140    0.000   13.453    0.000 ops.py:1035(_numpy)\n",
      "   299931    0.968    0.000   13.332    0.000 gen_math_ops.py:3130(equal)\n",
      "   299931    0.361    0.000   13.267    0.000 array_ops.py:804(rank)\n",
      "   299931    2.839    0.000   12.906    0.000 array_ops.py:840(rank_internal)\n",
      "   199954    1.071    0.000   12.554    0.000 numeric.py:148(ones)\n",
      "   199954    0.459    0.000   12.247    0.000 <__array_function__ internals>:2(isin)\n",
      " 17114432   11.717    0.000   11.717    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
      "  3099195    1.957    0.000   11.575    0.000 _asarray.py:23(asarray)\n",
      "  1999448    8.187    0.000   11.554    0.000 numerictypes.py:575(_can_coerce_all)\n",
      "   199954    0.914    0.000   11.452    0.000 arraysetops.py:615(isin)\n",
      "  1799586   11.321    0.000   11.321    0.000 {built-in method numpy.arange}\n",
      "  2499425   11.313    0.000   11.313    0.000 {method '_numpy_internal' of 'tensorflow.python.framework.ops.EagerTensor' objects}\n",
      "   199954    4.052    0.000   11.243    0.000 embedding.py:283(dWc)\n",
      "  2899333    5.804    0.000   11.095    0.000 tensor_shape.py:758(<listcomp>)\n",
      "   999724    6.939    0.000   11.043    0.000 arraypad.py:86(_pad_simple)\n",
      "  1099747    2.299    0.000   10.724    0.000 nest.py:274(flatten)\n",
      "   299931    1.345    0.000   10.091    0.000 embedding.py:255(Be)\n",
      "   199954    0.410    0.000    9.882    0.000 <__array_function__ internals>:2(in1d)\n",
      "  1099747    2.051    0.000    9.613    0.000 fromnumeric.py:199(reshape)\n",
      "   199954    0.604    0.000    9.449    0.000 <__array_function__ internals>:2(copyto)\n",
      " 44145228    9.437    0.000    9.437    0.000 preprocess.py:109(vocabulary)\n",
      "   199954    5.883    0.000    8.937    0.000 arraysetops.py:498(in1d)\n",
      "    99977    0.816    0.000    8.790    0.000 function.py:733(check_binary_classification_X_T)\n",
      "    99977    0.609    0.000    8.739    0.000 gen_math_ops.py:7303(real_div)\n",
      "    99977    0.945    0.000    8.658    0.000 base.py:99(T)\n",
      "   299931    5.441    0.000    8.512    0.000 embedding.py:315(Ws)\n",
      "  1099747    8.424    0.000    8.425    0.000 {built-in method tensorflow.python._pywrap_utils.Flatten}\n",
      "   599862    0.563    0.000    8.361    0.000 base.py:76(is_float_scalar)\n",
      "   999724    1.053    0.000    8.288    0.000 <__array_function__ internals>:2(round_)\n",
      "   999724    1.434    0.000    8.140    0.000 <__array_function__ internals>:2(concatenate)\n",
      "   999770    2.751    0.000    8.140    0.000 base.py:259(T)\n",
      "   499885    1.480    0.000    8.126    0.000 re.py:188(match)\n",
      "   599862    1.262    0.000    7.799    0.000 base.py:33(is_np_float_scalar)\n",
      "    99977    1.617    0.000    7.719    0.000 embedding.py:1634(f)\n",
      "  2099517    6.241    0.000    7.712    0.000 tensor_shape.py:858(__iter__)\n",
      "   899793    2.447    0.000    7.414    0.000 base.py:191(X)\n",
      "  5598712    7.360    0.000    7.360    0.000 context.py:837(device_name)\n",
      " 31929509    7.332    0.000    7.332    0.000 {method 'rstrip' of 'str' objects}\n",
      "  3599080    7.215    0.000    7.215    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "   899793    2.257    0.000    7.154    0.000 math_ops.py:907(cast)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   299931    0.795    0.000    7.043    0.000 gen_math_ops.py:7157(_range)\n",
      "  1499609    4.455    0.000    6.777    0.000 re.py:289(_compile)\n",
      "   999770    1.351    0.000    6.679    0.000 base.py:126(is_same_shape)\n",
      "   199954    0.654    0.000    6.531    0.000 composite.py:146(T)\n",
      "  1399678    6.293    0.000    6.293    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
      "  2099471    2.467    0.000    6.266    0.000 base.py:130(tensor_rank)\n",
      " 14715076    6.184    0.000    6.184    0.000 embedding.py:201(dictionary)\n",
      "   999724    0.931    0.000    6.077    0.000 fromnumeric.py:3709(round_)\n",
      "  1399678    2.139    0.000    6.040    0.000 {built-in method builtins.all}\n",
      "  1099747    1.598    0.000    5.835    0.000 tensor_util.py:1042(maybe_set_static_shape)\n",
      "  2999172    2.415    0.000    5.782    0.000 <__array_function__ internals>:2(ndim)\n",
      "   499885    2.661    0.000    5.698    0.000 arrayprint.py:60(_make_options_dict)\n",
      "  5798666    5.573    0.000    5.573    0.000 _internal.py:826(npy_ctypes_check)\n",
      "  5598712    4.337    0.000    5.544    0.000 ops.py:1041(dtype)\n",
      "    99977    1.422    0.000    5.530    0.000 embedding.py:1591(_adapt_function_handle_Y)\n",
      "  5398758    4.693    0.000    5.291    0.000 tensor_shape.py:187(__init__)\n",
      "    99977    1.109    0.000    5.279    0.000 embedding.py:159(X)\n",
      "  3199264    5.216    0.000    5.216    0.000 {method '_shape_tuple' of 'tensorflow.python.framework.ops.EagerTensor' objects}\n",
      "   999724    4.027    0.000    5.189    0.000 arraypad.py:129(_set_pad_area)\n",
      "  7198344    3.832    0.000    5.174    0.000 tensor_shape.py:864(<genexpr>)\n",
      "   999724    0.940    0.000    5.146    0.000 <__array_function__ internals>:2(around)\n",
      "  1099747    1.556    0.000    5.024    0.000 execute.py:284(<listcomp>)\n",
      "   100010    0.111    0.000    5.004    0.000 {built-in method builtins.next}\n",
      "   100000    0.610    0.000    4.893    0.000 <ipython-input-33-0580c1f4e1b7>:1(sentences_generator)\n",
      " 16614639    4.770    0.000    4.770    0.000 embedding.py:211(SL)\n",
      "    99977    0.226    0.000    4.769    0.000 tf.py:156(concat)\n",
      " 15015007    4.682    0.000    4.682    0.000 fromnumeric.py:2907(_prod_dispatcher)\n",
      "   199954    0.424    0.000    4.502    0.000 <__array_function__ internals>:2(sum)\n",
      "   199954    3.075    0.000    4.483    0.000 embedding.py:216(negative_sample_indices)\n",
      "   199954    3.214    0.000    4.434    0.000 embedding.py:190(context_indices)\n",
      "    99977    0.411    0.000    4.372    0.000 array_ops.py:1585(concat)\n",
      " 17114524    4.294    0.000    4.294    0.000 {method 'items' of 'dict' objects}\n",
      "   599862    4.280    0.000    4.280    0.000 {built-in method numpy.zeros}\n",
      " 17670837    4.193    0.000    4.193    0.000 fromnumeric.py:2442(_cumsum_dispatcher)\n",
      " 17670837    4.188    0.000    4.188    0.000 arraysetops.py:133(_unique_dispatcher)\n",
      " 14715076    4.152    0.000    4.152    0.000 numeric.py:420(_count_nonzero_dispatcher)\n",
      "  1699609    1.349    0.000    4.147    0.000 dtypes.py:84(base_dtype)\n",
      "   199954    0.668    0.000    4.097    0.000 composite.py:199(_set_label)\n",
      "    99977    0.315    0.000    3.907    0.000 gen_array_ops.py:1170(concat_v2)\n",
      "  2099517    2.412    0.000    3.743    0.000 tensor_conversion_registry.py:114(get)\n",
      "  5798666    3.683    0.000    3.683    0.000 base.py:233(N)\n",
      "   199954    0.701    0.000    3.662    0.000 fromnumeric.py:2111(sum)\n",
      "  2099517    1.376    0.000    3.632    0.000 math_ops.py:1915(_may_reduce_to_scalar)\n",
      " 27159920    3.505    0.000    3.505    0.000 {method 'lower' of 'str' objects}\n",
      "  1099747    0.818    0.000    3.468    0.000 ops.py:6817(_is_keras_symbolic_tensor)\n",
      "  1799586    3.109    0.000    3.382    0.000 base.py:357(logger)\n",
      " 14715076    3.377    0.000    3.377    0.000 preprocess.py:119(probabilities)\n",
      "   499885    3.327    0.000    3.327    0.000 {method 'match' of 're.Pattern' objects}\n",
      "  1400678    1.672    0.000    3.238    0.000 {built-in method builtins.hasattr}\n",
      "   199954    2.345    0.000    3.221    0.000 embedding.py:182(target_indices)\n",
      "   499885    3.191    0.000    3.191    0.000 arrayprint.py:358(_get_formatdict)\n",
      "   999724    0.963    0.000    3.157    0.000 fromnumeric.py:3199(around)\n",
      "  1599632    2.881    0.000    3.097    0.000 dtypes.py:606(as_dtype)\n",
      "  1099747    1.462    0.000    2.860    0.000 backprop.py:170(_must_record_gradient)\n",
      "   299931    1.423    0.000    2.856    0.000 dtypes.py:172(is_compatible_with)\n",
      "  1699609    2.798    0.000    2.798    0.000 dtypes.py:71(_is_ref_dtype)\n",
      "  2699379    1.908    0.000    2.647    0.000 tensor_shape.py:821(rank)\n",
      "  5598712    2.585    0.000    2.585    0.000 context.py:501(ensure_initialized)\n",
      "  5598712    2.544    0.000    2.544    0.000 context.py:773(_handle)\n",
      "  3999080    2.536    0.000    2.536    0.000 array_ops.py:1508(<genexpr>)\n",
      "  4299035    2.521    0.000    2.521    0.000 base.py:185(D)\n",
      "  5998344    2.478    0.000    2.478    0.000 numerictypes.py:584(<listcomp>)\n",
      "   499885    2.449    0.000    2.449    0.000 {built-in method builtins.locals}\n",
      "   299931    1.005    0.000    2.383    0.000 base.py:557(update)\n",
      "   899793    1.192    0.000    2.271    0.000 __init__.py:1412(debug)\n",
      "   100000    0.561    0.000    2.264    0.000 utility_file.py:221(take)\n",
      "  2099517    1.754    0.000    2.257    0.000 math_ops.py:1910(_has_fully_defined_shape)\n",
      "  1899563    2.185    0.000    2.185    0.000 __init__.py:1677(isEnabledFor)\n",
      "   499885    2.176    0.000    2.176    0.000 {method 'format' of 'str' objects}\n",
      "  2099517    1.372    0.000    2.118    0.000 tensor_shape.py:845(__len__)\n",
      "   699839    1.076    0.000    2.049    0.000 __init__.py:1436(warning)\n",
      "  1099747    2.037    0.000    2.037    0.000 dtypes.py:192(__eq__)\n",
      "   599865    0.574    0.000    1.990    0.000 abc.py:96(__instancecheck__)\n",
      "    99977    0.331    0.000    1.986    0.000 ops.py:1047(numpy)\n",
      "  8198114    1.945    0.000    1.945    0.000 context.py:1849(context_safe)\n",
      "   199954    0.751    0.000    1.938    0.000 _ufunc_config.py:32(seterr)\n",
      "  6698459    1.916    0.000    1.916    0.000 {method '_datatype_enum' of 'tensorflow.python.framework.ops.EagerTensor' objects}\n",
      "  1699609    1.809    0.000    1.809    0.000 ops.py:1225(graph)\n",
      "    99977    0.198    0.000    1.698    0.000 <__array_function__ internals>:2(amax)\n",
      "   999747    1.333    0.000    1.681    0.000 utility_file.py:280(file_line_stream)\n",
      "    99977    0.259    0.000    1.666    0.000 _ufunc_config.py:433(__enter__)\n",
      "    99977    1.145    0.000    1.644    0.000 embedding.py:270(Wc)\n",
      "  8997944    1.640    0.000    1.640    0.000 {method 'append' of 'list' objects}\n",
      "   499885    0.564    0.000    1.492    0.000 base.py:102(is_integer_tensor)\n",
      "  5698689    1.456    0.000    1.456    0.000 tensor_shape.py:249(value)\n",
      "   599865    1.416    0.000    1.416    0.000 {built-in method _abc._abc_instancecheck}\n",
      "  1099747    1.398    0.000    1.398    0.000 {built-in method tensorflow.python._pywrap_tfe.TFE_Py_TapeSetIsEmpty}\n",
      "  1199724    0.642    0.000    1.365    0.000 math_ops.py:1865(<genexpr>)\n",
      "  1999448    1.327    0.000    1.327    0.000 arraypad.py:109(<genexpr>)\n",
      "   299931    0.389    0.000    1.317    0.000 ops.py:5744(executing_eagerly_outside_functions)\n",
      "   199954    1.303    0.000    1.303    0.000 base.py:122(L)\n",
      "    99977    0.237    0.000    1.285    0.000 fromnumeric.py:2617(amax)\n",
      "   399908    0.839    0.000    1.273    0.000 objective.py:147(P)\n",
      "   999724    1.250    0.000    1.250    0.000 arraypad.py:58(_view_roi)\n",
      "  1999448    1.163    0.000    1.163    0.000 arraypad.py:33(_slice_at_axis)\n",
      "    99977    0.619    0.000    1.155    0.000 function.py:411(transform_X_T)\n",
      "   999724    0.820    0.000    1.115    0.000 types.py:171(__get__)\n",
      "   999724    1.113    0.000    1.113    0.000 numerictypes.py:651(<listcomp>)\n",
      "    99977    0.170    0.000    1.112    0.000 <__array_function__ internals>:2(amin)\n",
      "    99977    0.740    0.000    1.111    0.000 embedding.py:248(We)\n",
      "  1999448    1.096    0.000    1.096    0.000 arraypad.py:120(<genexpr>)\n",
      "  1999540    1.045    0.000    1.045    0.000 {method 'group' of 're.Match' objects}\n",
      "   999724    0.968    0.000    0.968    0.000 {method 'round' of 'numpy.ndarray' objects}\n",
      "       24    0.001    0.000    0.965    0.040 base.py:590(save)\n",
      "       24    0.729    0.030    0.964    0.040 utility_file.py:198(serialize)\n",
      "   299931    0.908    0.000    0.908    0.000 dtypes.py:103(as_numpy_dtype)\n",
      "    99977    0.254    0.000    0.903    0.000 base.py:202(to_list)\n",
      "   299931    0.416    0.000    0.896    0.000 tensor_shape.py:1180(as_list)\n",
      "  2099517    0.864    0.000    0.864    0.000 tensor_shape.py:792(_v2_behavior)\n",
      "   299931    0.408    0.000    0.840    0.000 math_ops.py:1866(<listcomp>)\n",
      "   699839    0.522    0.000    0.808    0.000 base.py:180(M)\n",
      "    99977    0.235    0.000    0.807    0.000 <__array_function__ internals>:2(squeeze)\n",
      "  1199724    0.797    0.000    0.797    0.000 base.py:253(T)\n",
      "    99977    0.200    0.000    0.795    0.000 fromnumeric.py:2742(amin)\n",
      "    99977    0.774    0.000    0.774    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
      "  2999172    0.749    0.000    0.749    0.000 fromnumeric.py:3106(ndim)\n",
      "   199954    0.206    0.000    0.735    0.000 dtypes.py:205(__ne__)\n",
      "    99977    0.201    0.000    0.705    0.000 embedding.py:154(X)\n",
      "  1299701    0.705    0.000    0.705    0.000 embedding.py:172(C)\n",
      "   199954    0.656    0.000    0.696    0.000 _ufunc_config.py:132(geterr)\n",
      "  1299701    0.693    0.000    0.693    0.000 embedding.py:167(E)\n",
      "    99977    0.158    0.000    0.688    0.000 _ufunc_config.py:438(__exit__)\n",
      "   199954    0.440    0.000    0.617    0.000 embedding.py:177(window_size)\n",
      "  1299701    0.608    0.000    0.608    0.000 ops.py:175(__exit__)\n",
      "  2099517    0.608    0.000    0.608    0.000 {built-in method builtins.iter}\n",
      "   599862    0.253    0.000    0.594    0.000 tensor_shape.py:840(ndims)\n",
      "  2999172    0.580    0.000    0.580    0.000 fromnumeric.py:3102(_ndim_dispatcher)\n",
      "   699839    0.481    0.000    0.570    0.000 base.py:325(dS)\n",
      "   499885    0.503    0.000    0.503    0.000 {method 'update' of 'dict' objects}\n",
      "   499885    0.488    0.000    0.488    0.000 arrayprint.py:65(<dictcomp>)\n",
      "   299931    0.366    0.000    0.480    0.000 tensor_shape.py:1191(<listcomp>)\n",
      "   499885    0.478    0.000    0.478    0.000 {method 'discard' of 'set' objects}\n",
      "   499885    0.471    0.000    0.471    0.000 {method 'copy' of 'dict' objects}\n",
      "  1999448    0.464    0.000    0.464    0.000 {method 'strip' of 'str' objects}\n",
      "   499885    0.460    0.000    0.460    0.000 {built-in method builtins.id}\n",
      "   199954    0.297    0.000    0.459    0.000 composite.py:141(T)\n",
      "  1999448    0.442    0.000    0.442    0.000 fromnumeric.py:3195(_around_dispatcher)\n",
      "    99977    0.167    0.000    0.441    0.000 base.py:143(layer_inference)\n",
      "   799840    0.436    0.000    0.436    0.000 base.py:170(name)\n",
      "  1099701    0.419    0.000    0.419    0.000 preprocess.py:491(window_size)\n",
      "  1199724    0.407    0.000    0.407    0.000 fromnumeric.py:2350(_all_dispatcher)\n",
      "  1299701    0.384    0.000    0.384    0.000 ops.py:169(__init__)\n",
      "  1099747    0.371    0.000    0.371    0.000 fromnumeric.py:194(_reshape_dispatcher)\n",
      "   499885    0.370    0.000    0.370    0.000 {method 'replace' of 'str' objects}\n",
      "   100025    0.369    0.000    0.369    0.000 {method 'join' of 'str' objects}\n",
      "    99977    0.141    0.000    0.342    0.000 fromnumeric.py:1439(squeeze)\n",
      "   299931    0.204    0.000    0.338    0.000 __init__.py:1424(info)\n",
      "  1099701    0.336    0.000    0.336    0.000 preprocess.py:503(event_size)\n",
      "   499885    0.325    0.000    0.325    0.000 {method 'add' of 'set' objects}\n",
      "   199954    0.316    0.000    0.316    0.000 {built-in method numpy.seterrobj}\n",
      "   499885    0.306    0.000    0.306    0.000 {built-in method _thread.get_ident}\n",
      "   499885    0.300    0.000    0.300    0.000 arrayprint.py:854(_none_or_positive_arg)\n",
      "   999724    0.296    0.000    0.296    0.000 enum.py:664(value)\n",
      "    99977    0.212    0.000    0.292    0.000 objective.py:156(J)\n",
      "   699839    0.286    0.000    0.286    0.000 base.py:175(num_nodes)\n",
      "   299931    0.275    0.000    0.275    0.000 base.py:63(lr)\n",
      "   999770    0.257    0.000    0.257    0.000 {method 'pop' of 'dict' objects}\n",
      "  1299701    0.253    0.000    0.253    0.000 ops.py:172(__enter__)\n",
      "   999724    0.248    0.000    0.248    0.000 arraypad.py:521(_pad_dispatcher)\n",
      "   299931    0.248    0.000    0.248    0.000 embedding.py:355(optimizer)\n",
      "   299931    0.245    0.000    0.245    0.000 composite.py:168(layers)\n",
      "  1099701    0.229    0.000    0.229    0.000 {built-in method builtins.callable}\n",
      "   999724    0.227    0.000    0.227    0.000 multiarray.py:143(concatenate)\n",
      "   399908    0.215    0.000    0.215    0.000 {built-in method numpy.geterrobj}\n",
      "       24    0.204    0.008    0.204    0.008 {built-in method _pickle.dump}\n",
      "    99977    0.200    0.000    0.200    0.000 {method 'squeeze' of 'numpy.ndarray' objects}\n",
      "   199978    0.190    0.000    0.190    0.000 embedding.py:229(W)\n",
      "   299931    0.186    0.000    0.186    0.000 base.py:75(l2)\n",
      "    99977    0.162    0.000    0.181    0.000 base.py:157(layers_all)\n",
      "    99977    0.148    0.000    0.178    0.000 base.py:335(objective)\n",
      "   999724    0.176    0.000    0.176    0.000 numerictypes.py:652(<listcomp>)\n",
      "     1023    0.010    0.000    0.163    0.000 {built-in method builtins.print}\n",
      "     2046    0.013    0.000    0.153    0.000 iostream.py:386(write)\n",
      "   199978    0.145    0.000    0.145    0.000 embedding.py:234(WO)\n",
      "    99977    0.139    0.000    0.139    0.000 _ufunc_config.py:429(__init__)\n",
      "    99977    0.092    0.000    0.138    0.000 base.py:182(assure_tensor)\n",
      "    99977    0.107    0.000    0.137    0.000 function.py:619(transform_scalar_X_T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   299931    0.135    0.000    0.135    0.000 tensor_shape.py:851(__bool__)\n",
      "     3069    0.017    0.000    0.127    0.000 iostream.py:197(schedule)\n",
      "    99977    0.124    0.000    0.124    0.000 preprocess.py:104(min_sequence_length)\n",
      "    14829    0.061    0.000    0.112    0.000 codecs.py:319(decode)\n",
      "   199954    0.104    0.000    0.104    0.000 multiarray.py:1054(copyto)\n",
      "   199954    0.094    0.000    0.094    0.000 arraysetops.py:494(_in1d_dispatcher)\n",
      "   100977    0.093    0.000    0.093    0.000 {built-in method time.time}\n",
      "     3069    0.086    0.000    0.086    0.000 socket.py:432(send)\n",
      "   199954    0.077    0.000    0.077    0.000 arraysetops.py:611(_isin_dispatcher)\n",
      "   199954    0.073    0.000    0.073    0.000 fromnumeric.py:2106(_sum_dispatcher)\n",
      "    14829    0.051    0.000    0.051    0.000 {built-in method _codecs.utf_8_decode}\n",
      "    99977    0.045    0.000    0.045    0.000 fromnumeric.py:2612(_amax_dispatcher)\n",
      "    99977    0.043    0.000    0.043    0.000 fromnumeric.py:1435(_squeeze_dispatcher)\n",
      "       48    0.033    0.001    0.037    0.001 {built-in method io.open}\n",
      "     1000    0.034    0.000    0.036    0.000 _methods.py:65(_count_reduce_items)\n",
      "    99977    0.033    0.000    0.033    0.000 fromnumeric.py:2737(_amin_dispatcher)\n",
      "        1    0.000    0.000    0.032    0.032 <ipython-input-33-0580c1f4e1b7>:59(<module>)\n",
      "     2046    0.003    0.000    0.019    0.000 iostream.py:323(_schedule_flush)\n",
      "     3069    0.008    0.000    0.017    0.000 threading.py:1071(is_alive)\n",
      "     2046    0.004    0.000    0.009    0.000 iostream.py:310(_is_master_process)\n",
      "     3069    0.003    0.000    0.008    0.000 threading.py:1017(_wait_for_tstate_lock)\n",
      "       24    0.000    0.000    0.007    0.000 pathlib.py:1210(open)\n",
      "     3069    0.005    0.000    0.005    0.000 iostream.py:93(_event_pipe)\n",
      "     3069    0.005    0.000    0.005    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "     2046    0.005    0.000    0.005    0.000 {built-in method posix.getpid}\n",
      "       24    0.000    0.000    0.003    0.000 pathlib.py:1072(_opener)\n",
      "       24    0.003    0.000    0.003    0.000 {built-in method posix.open}\n",
      "     1000    0.002    0.000    0.003    0.000 base.py:129(history)\n",
      "       24    0.000    0.000    0.003    0.000 pathlib.py:1414(is_file)\n",
      "       24    0.000    0.000    0.003    0.000 pathlib.py:1189(stat)\n",
      "       24    0.002    0.000    0.002    0.000 {built-in method posix.stat}\n",
      "       48    0.000    0.000    0.002    0.000 pathlib.py:1035(__new__)\n",
      "       48    0.000    0.000    0.002    0.000 pathlib.py:674(_from_parts)\n",
      "       48    0.000    0.000    0.002    0.000 pathlib.py:654(_parse_args)\n",
      "     1000    0.001    0.000    0.001    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}\n",
      "     3069    0.001    0.000    0.001    0.000 {method 'append' of 'collections.deque' objects}\n",
      "       48    0.001    0.000    0.001    0.000 pathlib.py:63(parse_parts)\n",
      "       24    0.000    0.000    0.001    0.000 utility_file.py:108(is_path_creatable)\n",
      "     3069    0.001    0.000    0.001    0.000 threading.py:513(is_set)\n",
      "       24    0.001    0.000    0.001    0.000 {built-in method posix.access}\n",
      "        5    0.000    0.000    0.001    0.000 codeop.py:140(__call__)\n",
      "        5    0.001    0.000    0.001    0.000 {built-in method builtins.compile}\n",
      "     76/4    0.000    0.000    0.001    0.000 abc.py:100(__subclasscheck__)\n",
      "     76/4    0.000    0.000    0.001    0.000 {built-in method _abc._abc_subclasscheck}\n",
      "       72    0.000    0.000    0.000    0.000 pathlib.py:724(__fspath__)\n",
      "     1000    0.000    0.000    0.000    0.000 fromnumeric.py:3296(_mean_dispatcher)\n",
      "       24    0.000    0.000    0.000    0.000 embedding.py:393(S)\n",
      "       24    0.000    0.000    0.000    0.000 posixpath.py:150(dirname)\n",
      "       72    0.000    0.000    0.000    0.000 pathlib.py:714(__str__)\n",
      "      240    0.000    0.000    0.000    0.000 {built-in method sys.intern}\n",
      "       24    0.000    0.000    0.000    0.000 _bootlocale.py:33(getpreferredencoding)\n",
      "       48    0.000    0.000    0.000    0.000 pathlib.py:292(splitroot)\n",
      "       48    0.000    0.000    0.000    0.000 pathlib.py:697(_format_parsed_parts)\n",
      "       24    0.000    0.000    0.000    0.000 {built-in method _locale.nl_langinfo}\n",
      "       24    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
      "       23    0.000    0.000    0.000    0.000 utility_file.py:35(__init__)\n",
      "       24    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}\n",
      "       48    0.000    0.000    0.000    0.000 pathlib.py:1045(_init)\n",
      "       24    0.000    0.000    0.000    0.000 posixpath.py:41(_get_sep)\n",
      "       48    0.000    0.000    0.000    0.000 {method 'lstrip' of 'str' objects}\n",
      "        9    0.000    0.000    0.000    0.000 __init__.py:214(_acquireLock)\n",
      "       48    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x556e7a05bac0}\n",
      "       72    0.000    0.000    0.000    0.000 {built-in method posix.fspath}\n",
      "       24    0.000    0.000    0.000    0.000 __init__.py:145(_DType_reduce)\n",
      "       46    0.000    0.000    0.000    0.000 {method 'close' of 'generator' objects}\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:238(helper)\n",
      "       24    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:117(__exit__)\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:82(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:108(__enter__)\n",
      "       58    0.000    0.000    0.000    0.000 _collections_abc.py:392(__subclasshook__)\n",
      "       24    0.000    0.000    0.000    0.000 {built-in method _stat.S_ISREG}\n",
      "       48    0.000    0.000    0.000    0.000 {method 'reverse' of 'list' objects}\n",
      "        5    0.000    0.000    0.000    0.000 hooks.py:103(__call__)\n",
      "        9    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}\n",
      "       10    0.000    0.000    0.000    0.000 compilerop.py:138(extra_flags)\n",
      "        9    0.000    0.000    0.000    0.000 __init__.py:223(_releaseLock)\n",
      "        5    0.000    0.000    0.000    0.000 traitlets.py:564(__get__)\n",
      "       24    0.000    0.000    0.000    0.000 embedding.py:381(state_elements)\n",
      "       24    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
      "        9    0.000    0.000    0.000    0.000 __init__.py:1663(getEffectiveLevel)\n",
      "        5    0.000    0.000    0.000    0.000 interactiveshell.py:3315(compare)\n",
      "        5    0.000    0.000    0.000    0.000 traitlets.py:533(get)\n",
      "        5    0.000    0.000    0.000    0.000 ipstruct.py:125(__getattr__)\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-33-0580c1f4e1b7>:61(<module>)\n",
      "       13    0.000    0.000    0.000    0.000 _collections_abc.py:302(__subclasshook__)\n",
      "        9    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}\n",
      "        5    0.000    0.000    0.000    0.000 interactiveshell.py:1277(user_global_ns)\n",
      "        5    0.000    0.000    0.000    0.000 hooks.py:168(pre_run_code_hook)\n",
      "        1    0.000    0.000    0.000    0.000 _collections_abc.py:367(__subclasshook__)\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-33-0580c1f4e1b7>:23(<module>)\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-33-0580c1f4e1b7>:22(<module>)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sentences_generator(path_to_file, num_sentences):\n",
    "    stream = fileio.Function.file_line_stream(path_to_file)\n",
    "    try:\n",
    "        while True:\n",
    "            _lines = fileio.Function.take(num_sentences, stream)\n",
    "            yield np.array(_lines)\n",
    "    finally:\n",
    "        stream.close()\n",
    "\n",
    "# Sentences for the trainig\n",
    "source = sentences_generator(\n",
    "    path_to_file=path_to_corpus, num_sentences=NUM_SENTENCES\n",
    ")\n",
    "\n",
    "# Restore the state if exists.\n",
    "state = embedding.load(STATE_FILE)\n",
    "\n",
    "# Continue training\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "\n",
    "total_sentences = 0\n",
    "epochs = 0\n",
    "\n",
    "for i in range(MAX_ITERATIONS):\n",
    "    try:\n",
    "        sentences = next(source)\n",
    "        total_sentences += len(sentences)\n",
    "\n",
    "        start = time.time()\n",
    "        network.train(X=sentences, T=np.array([0]))\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                f\"Batch {i:05d} of {NUM_SENTENCES} sentences: \"\n",
    "                f\"Average Loss: {np.mean(network.history):10f} \"\n",
    "                f\"Duration {time.time() - start:3f}\"\n",
    "            )\n",
    "        if i % 1000 == 0:\n",
    "            # embedding.save(STATE_FILE)\n",
    "            pass\n",
    "        \n",
    "    except fileio.Function.GenearatorHasNoMore as e:\n",
    "        source.close()\n",
    "        embedding.save(STATE_FILE)\n",
    "\n",
    "        # Next epoch\n",
    "        print(f\"epoch {epochs} batches {i:05d} done\")\n",
    "        epochs += 1\n",
    "        source = sentences_generator(\n",
    "            path_to_file=path_to_corpus, num_sentences=NUM_SENTENCES\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        source.close()\n",
    "        raise e\n",
    "\n",
    "embedding.save(STATE_FILE)\n",
    "\n",
    "profiler.disable()\n",
    "profiler.print_stats(sort=\"cumtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluate the vector space\n",
    "\n",
    "Verify if the trained model, or the vector space W, has encoded the words in a way that **similar** words are close in the vector space.\n",
    "\n",
    "* [How to measure the similarity among vectors](https://math.stackexchange.com/questions/4132458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words ['cash']\n",
      "Word indices [413]\n",
      "prediction for ['cash']:\n",
      "[['interest' 'demand' 'fourth' 'debt' 'higher' 'lower' 'orders' 'tax' 'transactions' 'costs']\n",
      " ['demand' 'debt' 'fourth' 'orders' 'costs' 'assets' 'strong' 'drop' 'spending' 'fall']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oonisim/home/repository/git/oonisim/python_programs/nlp/src/layer/preprocessing/preprocess.py:216: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return self._vocabulary[list(iter(indices))]\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "context = \"cash\".split()\n",
    "word_indices = np.array(word_indexing.list_indices(context), dtype=TYPE_INT)\n",
    "\n",
    "print(f\"Words {context}\")\n",
    "print(f\"Word indices {word_indices}\")\n",
    "print(f\"prediction for {context}:\\n{word_indexing.list_events([embedding.predict(word_indices, n)])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Compare with [gensim word2vec](https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import (\n",
    "    Word2Vec\n",
    ")\n",
    "from gensim.models.word2vec import (\n",
    "    LineSentence    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = LineSentence(source=path_to_corpus)\n",
    "w2v = Word2Vec(\n",
    "    sentences=sentences, \n",
    "    sg=0,\n",
    "    window=5, \n",
    "    negative=5,\n",
    "    vector_size=100, \n",
    "    min_count=1, \n",
    "    workers=4\n",
    ")\n",
    "del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amount', 0.8982115983963013),\n",
       " ('debt', 0.8907380104064941),\n",
       " ('assets', 0.8691324591636658),\n",
       " ('value', 0.8579981923103333),\n",
       " ('flow', 0.8463666439056396),\n",
       " ('face', 0.8402033448219299),\n",
       " ('payment', 0.8359115719795227),\n",
       " ('dividends', 0.8225480318069458),\n",
       " ('holders', 0.8186897039413452),\n",
       " ('payments', 0.8037282824516296)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(context, topn=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
