{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec implementation\n",
    "\n",
    "## Original papers\n",
    "* [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "* [word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method](https://arxiv.org/abs/1402.3722)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The original paper proposed using:\n",
    "1. Matmul to extract word vectors from ```Win```.\n",
    "2. Softmax to calculate scores with all the word vectors from ```Wout```.\n",
    "\n",
    "<img src=\"image/word2vec_cbow_mechanism.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "### Motivation\n",
    "\n",
    "The original implementation was computationally expensive.\n",
    "\n",
    "1. Matmal to extract word vectors from ```Win```.\n",
    "2. Softmax can happen vocabulary size times with ```Wout```.\n",
    "\n",
    "<img src=\"image/word2vec_negative_sampling_motivation.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1. Use index to extract word vectors from Win.\n",
    "2. Instead of calculating softmax with all the word vectors from ```Wout```, sample small number (SL) of negative/false word vectors from ```Wout``` and calculate logistic log loss with each sample. \n",
    "\n",
    "<img src=\"image/wors2vec_neg_sample_backprop.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using only one Word vector space W\n",
    "\n",
    "There is no reasoning nor proof why two word vector space are required. In the end, we only use one word vector space, which appears to be ```Win```. \n",
    "\n",
    "However, if we use one vector space ```W``` for ```event```, ```context``` and ```negative samples```,then an event vector ```event=W[i]``` in a sentence can be used as a negative sample in another setence. Then the weight ```W[i]``` is updated for both positive and negative labels in the same gradient descent on ```W```. The actual [experiment of using only one vector space](./layer/embedding_single_vector_space.py) ```W``` did not work well.\n",
    "\n",
    "* [Why do we need 2 matrices for word2vec or GloVe](https://datascience.stackexchange.com/a/94422/68313)\n",
    "\n",
    "\n",
    "<img src=\"image/word2vec_why_not_one_W.png\" align=\"left\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from itertools import islice\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=400) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Google Colab environment\n",
    "\n",
    "Colab gets disconnected within approx 20 min. Hence not suitable for training (or need to upgrade to the pro version)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone github to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_GOOGLE_COLAB = True\n",
    "except:\n",
    "    IN_GOOGLE_COLAB = False\n",
    "    \n",
    "if IN_GOOGLE_COLAB:\n",
    "    !pip install line_profiler\n",
    "    !google.colab.drive.mount('/content/gdrive')\n",
    "    !rm -rf /content/drive/MyDrive/github\n",
    "    !mkdir -p /content/drive/MyDrive/github\n",
    "    !git clone https://github.com/oonisim/python-programs.git /content/drive/MyDrive/github\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone github to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_GOOGLE_COLAB = True\n",
    "except:\n",
    "    IN_GOOGLE_COLAB = False\n",
    "    \n",
    "if IN_GOOGLE_COLAB:\n",
    "    !pip install line_profiler\n",
    "    !google.colab.drive.mount('/content/gdrive')\n",
    "    !rm -rf /content/github\n",
    "    !mkdir -p /content/github\n",
    "    !git clone https://github.com/oonisim/python-programs.git /content/github\n",
    "        \n",
    "    import sys\n",
    "    sys.path.append('/content/github/nlp/src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook setups\n",
    "\n",
    "Auto reolaod causes an error in Jupyter notebooks. Restart the Jupyter kernel for the error:\n",
    "```TypeError: super(type, obj): obj must be an instance or subtype of type```\n",
    "See\n",
    "- https://stackoverflow.com/a/52927102/4281353\n",
    "- http://thomas-cokelaer.info/blog/2011/09/382/\n",
    "\n",
    "> The problem resides in the mechanism of reloading modules.\n",
    "> Reloading a module often changes the internal object in memory which\n",
    "> makes the isinstance test of super return False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "import function.fileio as fileio\n",
    "import function.text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.constant import (\n",
    "    TYPE_INT,\n",
    "    TYPE_FLOAT,\n",
    "    TYPE_LABEL,\n",
    "    TYPE_TENSOR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants\n",
    "\n",
    "Too large LR generates unusable event vector space.\n",
    "\n",
    "Uniform weight distribution does not work (Why?)\n",
    "Weights from the normal distribution sampling with small std (0.01) works (Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TEXT8 = False\n",
    "USE_PTB = not USE_TEXT8\n",
    "USE_CBOW = False\n",
    "USE_SGRAM = not USE_CBOW\n",
    "\n",
    "CORPUS_FILE = \"text8_256\" if USE_TEXT8 else \"ptb_train\"\n",
    "CORPUS_URL = \"https://data.deepai.org/text8.zip\" \\\n",
    "    if USE_TEXT8 else f'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt' \\\n",
    "\n",
    "TARGET_SIZE = TYPE_INT(1)       # Size of the target event (word)\n",
    "CONTEXT_SIZE = TYPE_INT(10)     # Size of the context in which the target event occurs.\n",
    "WINDOW_SIZE = TARGET_SIZE + CONTEXT_SIZE\n",
    "SAMPLE_SIZE = TYPE_INT(5)       # Size of the negative samples\n",
    "\n",
    "VECTOR_SIZE = TYPE_INT(100)     # Number of features in the event vector.\n",
    "WEIGHT_SCHEME = \"normal\"\n",
    "WEIGHT_PARAMS = {\n",
    "    \"std\": 0.01\n",
    "}\n",
    "\n",
    "LR = TYPE_FLOAT(20)\n",
    "NUM_SENTENCES = 10\n",
    "\n",
    "STATE_FILE = \"../models/word2vec_sgram_%s_E%s_C%s_S%s_W%s_%s_%s_V%s_LR%s_N%s.pkl\" % (\n",
    "    CORPUS_FILE,\n",
    "    TARGET_SIZE,\n",
    "    CONTEXT_SIZE,\n",
    "    SAMPLE_SIZE,\n",
    "    WEIGHT_SCHEME,\n",
    "    \"std\",\n",
    "    WEIGHT_PARAMS[\"std\"],\n",
    "    VECTOR_SIZE,\n",
    "    LR,\n",
    "    NUM_SENTENCES,\n",
    ")\n",
    "\n",
    "MAX_ITERATIONS = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oonisim/.keras/datasets/ptb_train\n"
     ]
    }
   ],
   "source": [
    "path_to_corpus = f\"~/.keras/datasets/{CORPUS_FILE}\"\n",
    "if fileio.Function.is_file(path_to_corpus):\n",
    "    pass\n",
    "else:\n",
    "    # text8, run \"cat text8 | xargs -n 512 > text8_512\" after download\n",
    "    path_to_corpus = tf.keras.utils.get_file(\n",
    "        fname=CORPUS_FILE,\n",
    "        origin=CORPUS_URL,\n",
    "        extract=True\n",
    "    )\n",
    "corpus = fileio.Function.read_file(path_to_corpus)\n",
    "print(path_to_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n"
     ]
    }
   ],
   "source": [
    "examples = corpus.split('\\n')[:1]\n",
    "for line in examples:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Event (word) indexing\n",
    "Index the events that have occurred in the event sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oonisim/conda/envs/python_programs/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "from layer.preprocessing import (\n",
    "    EventIndexing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexing = EventIndexing(\n",
    "    name=\"word_indexing\",\n",
    "    corpus=corpus,\n",
    "    min_sequence_length=WINDOW_SIZE\n",
    ")\n",
    "del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EventIndexing  for the corpus\n",
    "\n",
    "Adapt to the ```corpus``` and provides:\n",
    "* event_to_index dictionary\n",
    "* vocaburary of the corpus\n",
    "* word occurrence probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EventIndexing.vocabulary[10]:\n",
      "['<nil>' '<unk>' 'aer' 'banknote' 'berlitz' 'calloway' 'centrust' 'cluett' 'fromstein' 'gitano']\n",
      "\n",
      "EventIndexing.event_to_index[10]:\n",
      "('<nil>', 0)\n",
      "('<unk>', 1)\n",
      "('aer', 2)\n",
      "('banknote', 3)\n",
      "('berlitz', 4)\n",
      "('calloway', 5)\n",
      "('centrust', 6)\n",
      "('cluett', 7)\n",
      "('fromstein', 8)\n",
      "('gitano', 9)\n",
      "\n",
      "EventIndexing.probabilities[10]:\n",
      "<nil>                : 0.00000e+00\n",
      "<unk>                : 1.65308e-02\n",
      "aer                  : 5.34860e-06\n",
      "banknote             : 5.34860e-06\n",
      "berlitz              : 5.34860e-06\n",
      "calloway             : 5.34860e-06\n",
      "centrust             : 5.34860e-06\n",
      "cluett               : 5.34860e-06\n",
      "fromstein            : 5.34860e-06\n",
      "gitano               : 5.34860e-06\n"
     ]
    }
   ],
   "source": [
    "words = word_indexing.list_events(range(10))\n",
    "print(f\"EventIndexing.vocabulary[10]:\\n{words}\\n\")\n",
    "\n",
    "indices = word_indexing.list_indices(words)\n",
    "print(f\"EventIndexing.event_to_index[10]:\")\n",
    "for item in zip(words, indices):\n",
    "    print(item)\n",
    "\n",
    "probabilities = word_indexing.list_probabilities(words)\n",
    "print(f\"\\nEventIndexing.probabilities[10]:\")\n",
    "for word, p in zip(words, probabilities):\n",
    "    print(f\"{word:20s} : {p:.5e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling using the probability\n",
    "\n",
    "Sample events according to their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shapiro', 'personal', 'estimated', 'cutbacks', 'growth']\n"
     ]
    }
   ],
   "source": [
    "sample = word_indexing.sample(size=5)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "Sample events not including those events already sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_indices=[9254, 2566, 6284, 2188, 8245] \n",
      "events=['azt' 'series' '80s' 'third' 'ralph']\n"
     ]
    }
   ],
   "source": [
    "negative_indices = word_indexing.negative_sample_indices(\n",
    "    size=5, excludes=word_indexing.list_indices(sample)\n",
    ")\n",
    "print(f\"negative_indices={negative_indices} \\nevents={word_indexing.list_events(negative_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the             :    34\n",
      "asbestos        :    63\n",
      "fiber           :    86\n",
      "<unk>           :     1\n",
      "is              :    42\n",
      "unusually       :    87\n",
      "<unk>           :     1\n",
      "once            :    64\n",
      "it              :    80\n",
      "enters          :    88\n",
      "the             :    34\n",
      "<unk>           :     1\n"
     ]
    }
   ],
   "source": [
    "# sentences = \"\\n\".join(corpus.split('\\n')[5:6])\n",
    "sentences = \"\"\"\n",
    "the asbestos fiber <unk> is unusually <unk> once it enters the <unk> \n",
    "\"\"\"\n",
    "sequences = word_indexing.function(sentences)\n",
    "for pair in zip(sentences.strip().split(\" \"), sequences[0]):\n",
    "    print(f\"{pair[0]:15} : {pair[1]:5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EventContext\n",
    "\n",
    "EventContext layer generates ```(event, context)``` pairs from sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer.preprocessing import (\n",
    "    EventContext\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_context = EventContext(\n",
    "    name=\"ev\",\n",
    "    window_size=WINDOW_SIZE,\n",
    "    event_size=TARGET_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context of an event (word) in a sentence\n",
    "\n",
    "In the sentence ```\"a form of asbestos once used to make kent cigarette filters\"```, one of the context windows ```a form of asbestos once``` of size 5 and event size 1 has.\n",
    "* ```of``` as a target word.\n",
    "* ```(a, form) and (asbestos, once)``` as its context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence of the word indices for the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[37, 62, 44, 63, 64, 65, 66, 67, 68, 69, 70],\n",
       "       [29, 30, 31, 50, 51, 43, 44, 52, 53,  0,  0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \"\"\"\n",
    "a form of asbestos once used to make kent cigarette filters\n",
    "\n",
    "N years old and former chairman of consolidated gold \n",
    "\"\"\"\n",
    "\n",
    "sequence = word_indexing.function(sentences)\n",
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (event, context) pairs\n",
    "\n",
    "For each word (event) in the setence ```(of, asbestos, ... , kent)``` excludnig the ends of the sentence, create ```(target, context)``` as:\n",
    "\n",
    "```\n",
    "[\n",
    "  [of, a, form, asbestos, once],              # target is 'of', context is (a, form, asbestos, once)\n",
    "  ['asbestos', 'form', 'of', 'once', 'used'],\n",
    "  ['once', 'of', 'asbestos', 'used', 'to'],\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "### Format of the (event, context) pairs\n",
    "\n",
    "* **E** is the target event indices\n",
    "* **C** is the context indices\n",
    "\n",
    "<img src=\"image/event_context_format.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event context pairs. Shape (2, 11), Target event size 1, Window size 11.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[65, 37, 62, 44, 63, 64, 66, 67, 68, 69, 70],\n",
       "       [43, 29, 30, 31, 50, 51, 44, 52, 53,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_context_pairs = event_context.function(sequence)\n",
    "print(\n",
    "    f\"Event context pairs. Shape %s, Target event size %s, Window size %s.\" % \n",
    "    (event_context_pairs.shape, event_context.event_size, event_context.window_size)\n",
    ")\n",
    "event_context_pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (event, context) pairs in textual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['used',\n",
       "  'a',\n",
       "  'form',\n",
       "  'of',\n",
       "  'asbestos',\n",
       "  'once',\n",
       "  'to',\n",
       "  'make',\n",
       "  'kent',\n",
       "  'cigarette',\n",
       "  'filters'],\n",
       " ['chairman',\n",
       "  'n',\n",
       "  'years',\n",
       "  'old',\n",
       "  'and',\n",
       "  'former',\n",
       "  'of',\n",
       "  'consolidated',\n",
       "  'gold',\n",
       "  '<nil>',\n",
       "  '<nil>']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indexing.sequence_to_sentence(event_context_pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Word Embedding\n",
    "\n",
    "Embedding is to train the model to group similar events in a close proximity in the event vector space. If two events e.g. 'pencil' and 'pen' are similar concepts, then their event vectors resides in a close distance in the event space. \n",
    "\n",
    "* [Thought Vectors](https://wiki.pathmind.com/thought-vectors)\n",
    "\n",
    "## Training process\n",
    "\n",
    "```EL``` is the number or length of the event words as the targets. The number of the event words can be more than 1. ```CL``` is the number of the context words sorrounding the event words. ```SL``` is the number of words to use for **negative sampling**.\n",
    "\n",
    "For instance, **know why you say good bye and I say hello**, if ```(EL=2,CL=8)```, then event words are ```(good, bye)``` and context words are ```(know, why, you, say, and, I, say, hello)```. ```Be``` is generated as the mean of the event word embedeing vectors.\n",
    "\n",
    "1. Calculate ```Bc```, the BoW (Bag of Words) from context event vectors.\n",
    "2. Calculate ```Be```,  the BoW (Bag of Words) from target event vectors.\n",
    "3. The dot product ```Ye = dot(Bc, Be)``` is given the label 1 to get them closer.\n",
    "4. For each negative sample ```Ws(s)```, the dot product with ```Ys = dot(Be, Ws(s)``` is given the label 0 to get them apart. \n",
    "5. ```np.c_[Ye, Ys]``` is fowarded to the logistic log loss layer.\n",
    "\n",
    "<img src=\"image/word2vec_backprop_Be.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec has two approaches, 1. CBOW and 2. Skipgram.\n",
    "%autoreload 2\n",
    "if USE_CBOW:\n",
    "    # CBOW\n",
    "    from layer.embedding_cbow_dual_vector_spaces.py import (\n",
    "        Embedding\n",
    "    )\n",
    "else:\n",
    "    # Skipgram\n",
    "    from layer.embedding_sgram import (\n",
    "        Embedding\n",
    "    )\n",
    "\n",
    "from optimizer import (\n",
    "    SGD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding: Embedding = Embedding(\n",
    "    name=\"embedding\",\n",
    "    num_nodes=WINDOW_SIZE,\n",
    "    target_size=TARGET_SIZE,\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    negative_sample_size=SAMPLE_SIZE,\n",
    "    event_vector_size=VECTOR_SIZE,\n",
    "    optimizer=SGD(lr=LR),\n",
    "    dictionary=word_indexing,\n",
    "    weight_initialization_scheme=WEIGHT_SCHEME,\n",
    "    weight_initialization_parameters=WEIGHT_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores ```np.c_[Ye, Ys]``` from the Embedding\n",
    "\n",
    "The 0th column is the scores for ```dot(Bc, Be)``` for positive labels. The rest are the scores for ```dot(Bc, Ws)``` for negative labels.\n",
    "\n",
    "The idea is the vectors ```(Bc, Be)``` should be closer (cosine similarity -> 1 or dot product is large positive), and the vectors ```(Be, Bs)``` should be apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "[[-2.4349254e-04 -2.1856527e-03 -8.7675056e-04 -9.7383186e-04  2.3010063e-03  1.2452344e-03  1.2126924e-03 -2.4789597e-03  7.9648133e-04 -2.6054615e-03  4.3943583e-04  4.5977812e-04  1.3538190e-03 -1.0263928e-03  1.1331388e-03]\n",
      " [ 1.0284438e-03  1.5731137e-03  2.7410622e-04  8.6363230e-04  2.6896014e-04 -1.6666343e-04 -5.3010433e-04 -1.1244978e-04  0.0000000e+00  0.0000000e+00 -6.9077860e-04 -1.6753969e-05  5.4561766e-04  9.2070817e-04 -8.8071707e-04]]\n",
      "\n",
      "Scores for dot(Bc, Be): \n",
      "[[-0.00024349]\n",
      " [ 0.00102844]]\n"
     ]
    }
   ],
   "source": [
    "scores = embedding.function(event_context_pairs)\n",
    "print(f\"Scores:\\n{scores[:5]}\\n\")\n",
    "print(f\"Scores for dot(Bc, Be): \\n{scores[:5, :1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer.normalization import (\n",
    "    BatchNormalization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = BatchNormalization(\n",
    "    name=\"bn\",\n",
    "    num_nodes=1+SAMPLE_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Logistic Log Loss\n",
    "\n",
    "Train the model to get:\n",
    "1. BoW of the context event vectors close to the target event vector. Label 1\n",
    "2. BoW of the context event vectors away from each of the negative sample event vectors Label 0.\n",
    "\n",
    "This is a binary logistic classification, hence use Logistic Log Loss as the network objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.function import (\n",
    "    sigmoid_cross_entropy_log_loss,\n",
    "    sigmoid\n",
    ")\n",
    "from layer.objective import (\n",
    "    CrossEntropyLogLoss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CrossEntropyLogLoss(\n",
    "    name=\"loss\",\n",
    "    num_nodes=1,  # Logistic log loss\n",
    "    log_loss_function=sigmoid_cross_entropy_log_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Adapter\n",
    "\n",
    "The logistic log loss layer expects the input of shape ```(N,M=1)```, however Embedding outputs ```(N,(1+SL)``` where ```SL``` is SAMPLE_SIZE. The ```Adapter``` layer bridges between Embedding and the Loss layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer.adapter import (\n",
    "    Adapter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_function = embedding.adapt_function_to_logistic_log_loss(loss=loss)\n",
    "adapter_gradient = embedding.adapt_gradient_to_logistic_log_loss()\n",
    "\n",
    "adapter: Adapter = Adapter(\n",
    "    name=\"adapter\",\n",
    "    num_nodes=1,    # Number of output M=1 \n",
    "    function=adapter_function,\n",
    "    gradient=adapter_gradient\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Word2vec Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a sequential network\n",
    "\n",
    "$ \\text {Sentences} \\rightarrow EventIndexing \\rightarrow EventContext \\rightarrow  Embedding \\rightarrow Adapter \\rightarrow LogisticLogLoss$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.sequential import (\n",
    "    SequentialNetwork\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = SequentialNetwork(\n",
    "    name=\"word2vec\",\n",
    "    num_nodes=1,\n",
    "    inference_layers=[\n",
    "        word_indexing,\n",
    "        event_context,\n",
    "        embedding,\n",
    "        adapter\n",
    "    ],\n",
    "    objective_layers=[\n",
    "        loss\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved model file if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State file does not exist. Saving the initial model to ../models/word2vec_sgram_ptb_train_E1_C10_S5_Wnormal_std_0.01_V100_LR20.0_N10.pkl.\n"
     ]
    }
   ],
   "source": [
    "if fileio.Function.is_file(STATE_FILE):\n",
    "    print(\"Loading model...\\nSTATE_FILE: %s\" % STATE_FILE)\n",
    "    state = embedding.load(STATE_FILE)\n",
    "\n",
    "    fmt=\"\"\"Model loaded.\n",
    "    event_size %s\n",
    "    context_size: %s\n",
    "    event_vector_size: %s\n",
    "    \"\"\"\n",
    "    print(fmt % (\n",
    "        state[\"target_size\"], \n",
    "        state[\"context_size\"], \n",
    "        state[\"event_vector_size\"]\n",
    "    ))\n",
    "else:\n",
    "    print(\"State file does not exist. Saving the initial model to %s.\" % STATE_FILE)\n",
    "    embedding.save(STATE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 00000 of 10 sentences: Average Loss:   0.693131 Duration 0.285150\n",
      "Batch 00100 of 10 sentences: Average Loss:   0.667208 Duration 0.245759\n",
      "Batch 00200 of 10 sentences: Average Loss:   0.634716 Duration 0.089798\n",
      "Batch 00300 of 10 sentences: Average Loss:   0.611182 Duration 0.200067\n",
      "Batch 00400 of 10 sentences: Average Loss:   0.593024 Duration 0.223902\n",
      "Batch 00500 of 10 sentences: Average Loss:   0.579920 Duration 0.279604\n",
      "Batch 00600 of 10 sentences: Average Loss:   0.571002 Duration 0.216216\n",
      "Batch 00700 of 10 sentences: Average Loss:   0.561259 Duration 0.336020\n",
      "Batch 00800 of 10 sentences: Average Loss:   0.553482 Duration 0.352396\n",
      "Batch 00900 of 10 sentences: Average Loss:   0.547468 Duration 0.237554\n",
      "Batch 01000 of 10 sentences: Average Loss:   0.541967 Duration 0.271538\n",
      "Batch 01100 of 10 sentences: Average Loss:   0.537164 Duration 0.265858\n",
      "Batch 01200 of 10 sentences: Average Loss:   0.532819 Duration 0.206694\n",
      "Batch 01300 of 10 sentences: Average Loss:   0.528261 Duration 0.281933\n",
      "Batch 01400 of 10 sentences: Average Loss:   0.523725 Duration 0.292323\n",
      "Batch 01500 of 10 sentences: Average Loss:   0.520667 Duration 0.242438\n",
      "Batch 01600 of 10 sentences: Average Loss:   0.517284 Duration 0.230874\n",
      "Batch 01700 of 10 sentences: Average Loss:   0.514046 Duration 0.145148\n",
      "Batch 01800 of 10 sentences: Average Loss:   0.511316 Duration 0.267757\n",
      "Batch 01900 of 10 sentences: Average Loss:   0.508766 Duration 0.205471\n",
      "Batch 02000 of 10 sentences: Average Loss:   0.505911 Duration 0.168908\n",
      "Batch 02100 of 10 sentences: Average Loss:   0.503642 Duration 0.196697\n",
      "Batch 02200 of 10 sentences: Average Loss:   0.501030 Duration 0.230423\n",
      "Batch 02300 of 10 sentences: Average Loss:   0.498293 Duration 0.145831\n",
      "Batch 02400 of 10 sentences: Average Loss:   0.496583 Duration 0.235054\n",
      "Batch 02500 of 10 sentences: Average Loss:   0.494252 Duration 0.212774\n",
      "Batch 02600 of 10 sentences: Average Loss:   0.492290 Duration 0.200628\n",
      "Batch 02700 of 10 sentences: Average Loss:   0.490865 Duration 0.242051\n",
      "Batch 02800 of 10 sentences: Average Loss:   0.489184 Duration 0.239694\n",
      "Batch 02900 of 10 sentences: Average Loss:   0.487704 Duration 0.065882\n",
      "Batch 03000 of 10 sentences: Average Loss:   0.486091 Duration 0.186269\n",
      "Batch 03100 of 10 sentences: Average Loss:   0.484784 Duration 0.249379\n",
      "Batch 03200 of 10 sentences: Average Loss:   0.483313 Duration 0.259567\n",
      "Batch 03300 of 10 sentences: Average Loss:   0.481872 Duration 0.157992\n",
      "Batch 03400 of 10 sentences: Average Loss:   0.480614 Duration 0.216387\n",
      "Batch 03500 of 10 sentences: Average Loss:   0.479326 Duration 0.191730\n",
      "Batch 03600 of 10 sentences: Average Loss:   0.478106 Duration 0.296875\n",
      "Batch 03700 of 10 sentences: Average Loss:   0.477131 Duration 0.236106\n",
      "Batch 03800 of 10 sentences: Average Loss:   0.475998 Duration 0.235581\n",
      "Batch 03900 of 10 sentences: Average Loss:   0.474800 Duration 0.279645\n",
      "Batch 04000 of 10 sentences: Average Loss:   0.473574 Duration 0.189859\n",
      "Batch 04100 of 10 sentences: Average Loss:   0.472480 Duration 0.183734\n",
      "Batch 04200 of 10 sentences: Average Loss:   0.471589 Duration 0.292931\n",
      "epoch 0 batches 04207 done\n",
      "Batch 04300 of 10 sentences: Average Loss:   0.470796 Duration 0.317887\n",
      "Batch 04400 of 10 sentences: Average Loss:   0.469957 Duration 0.230951\n",
      "Batch 04500 of 10 sentences: Average Loss:   0.469076 Duration 0.264803\n",
      "Batch 04600 of 10 sentences: Average Loss:   0.468042 Duration 0.268645\n",
      "Batch 04700 of 10 sentences: Average Loss:   0.467233 Duration 0.303776\n",
      "Batch 04800 of 10 sentences: Average Loss:   0.466608 Duration 0.144166\n",
      "Batch 04900 of 10 sentences: Average Loss:   0.465759 Duration 0.228220\n",
      "Batch 05000 of 10 sentences: Average Loss:   0.464938 Duration 0.278847\n",
      "Batch 05100 of 10 sentences: Average Loss:   0.464297 Duration 0.460923\n",
      "Batch 05200 of 10 sentences: Average Loss:   0.463638 Duration 0.121965\n",
      "Batch 05300 of 10 sentences: Average Loss:   0.463078 Duration 0.140540\n",
      "Batch 05400 of 10 sentences: Average Loss:   0.462471 Duration 0.275158\n",
      "Batch 05500 of 10 sentences: Average Loss:   0.461849 Duration 0.155462\n",
      "Batch 05600 of 10 sentences: Average Loss:   0.461039 Duration 0.194761\n",
      "Batch 05700 of 10 sentences: Average Loss:   0.460513 Duration 0.175020\n",
      "Batch 05800 of 10 sentences: Average Loss:   0.459894 Duration 0.185516\n",
      "Batch 05900 of 10 sentences: Average Loss:   0.459298 Duration 0.148863\n",
      "Batch 06000 of 10 sentences: Average Loss:   0.458743 Duration 0.162844\n",
      "Batch 06100 of 10 sentences: Average Loss:   0.458207 Duration 0.208590\n",
      "Batch 06200 of 10 sentences: Average Loss:   0.457625 Duration 0.182673\n",
      "Batch 06300 of 10 sentences: Average Loss:   0.457055 Duration 0.219178\n",
      "Batch 06400 of 10 sentences: Average Loss:   0.456419 Duration 0.205318\n",
      "Batch 06500 of 10 sentences: Average Loss:   0.455664 Duration 0.112310\n",
      "Batch 06600 of 10 sentences: Average Loss:   0.455196 Duration 0.123086\n",
      "Batch 06700 of 10 sentences: Average Loss:   0.454595 Duration 0.164613\n",
      "Batch 06800 of 10 sentences: Average Loss:   0.453974 Duration 0.211483\n",
      "Batch 06900 of 10 sentences: Average Loss:   0.453583 Duration 0.167450\n",
      "Batch 07000 of 10 sentences: Average Loss:   0.453097 Duration 0.127413\n",
      "Batch 07100 of 10 sentences: Average Loss:   0.452670 Duration 0.206163\n",
      "Batch 07200 of 10 sentences: Average Loss:   0.452112 Duration 0.177057\n",
      "Batch 07300 of 10 sentences: Average Loss:   0.451740 Duration 0.163110\n",
      "Batch 07400 of 10 sentences: Average Loss:   0.451239 Duration 0.194264\n",
      "Batch 07500 of 10 sentences: Average Loss:   0.450743 Duration 0.153218\n",
      "Batch 07600 of 10 sentences: Average Loss:   0.450320 Duration 0.321175\n",
      "Batch 07700 of 10 sentences: Average Loss:   0.449887 Duration 0.330439\n",
      "Batch 07800 of 10 sentences: Average Loss:   0.449440 Duration 0.274222\n",
      "Batch 07900 of 10 sentences: Average Loss:   0.449123 Duration 0.123968\n",
      "Batch 08000 of 10 sentences: Average Loss:   0.448695 Duration 0.115869\n",
      "Batch 08100 of 10 sentences: Average Loss:   0.448224 Duration 0.123975\n",
      "Batch 08200 of 10 sentences: Average Loss:   0.447734 Duration 0.290536\n",
      "Batch 08300 of 10 sentences: Average Loss:   0.447297 Duration 0.169406\n",
      "Batch 08400 of 10 sentences: Average Loss:   0.446970 Duration 0.154596\n",
      "epoch 1 batches 08415 done\n",
      "Batch 08500 of 10 sentences: Average Loss:   0.446634 Duration 0.168085\n",
      "Batch 08600 of 10 sentences: Average Loss:   0.446314 Duration 0.167397\n",
      "Batch 08700 of 10 sentences: Average Loss:   0.445977 Duration 0.145801\n",
      "Batch 08800 of 10 sentences: Average Loss:   0.445520 Duration 0.119269\n",
      "Batch 08900 of 10 sentences: Average Loss:   0.445165 Duration 0.174398\n",
      "Batch 09000 of 10 sentences: Average Loss:   0.444919 Duration 0.172770\n",
      "Batch 09100 of 10 sentences: Average Loss:   0.444565 Duration 0.172224\n",
      "Batch 09200 of 10 sentences: Average Loss:   0.444166 Duration 0.185950\n",
      "Batch 09300 of 10 sentences: Average Loss:   0.443917 Duration 0.156561\n",
      "Batch 09400 of 10 sentences: Average Loss:   0.443607 Duration 0.105481\n",
      "Batch 09500 of 10 sentences: Average Loss:   0.443369 Duration 0.267426\n",
      "Batch 09600 of 10 sentences: Average Loss:   0.443078 Duration 0.148210\n",
      "Batch 09700 of 10 sentences: Average Loss:   0.442823 Duration 0.151481\n",
      "Batch 09800 of 10 sentences: Average Loss:   0.442404 Duration 0.287854\n",
      "Batch 09900 of 10 sentences: Average Loss:   0.442152 Duration 0.210876\n",
      "Batch 10000 of 10 sentences: Average Loss:   0.441877 Duration 0.130185\n",
      "Batch 10100 of 10 sentences: Average Loss:   0.441568 Duration 0.190568\n",
      "Batch 10200 of 10 sentences: Average Loss:   0.441323 Duration 0.156363\n",
      "Batch 10300 of 10 sentences: Average Loss:   0.441042 Duration 0.223258\n",
      "Batch 10400 of 10 sentences: Average Loss:   0.440762 Duration 0.230505\n",
      "Batch 10500 of 10 sentences: Average Loss:   0.440437 Duration 0.152577\n",
      "Batch 10600 of 10 sentences: Average Loss:   0.440091 Duration 0.177487\n",
      "Batch 10700 of 10 sentences: Average Loss:   0.439732 Duration 0.184030\n",
      "Batch 10800 of 10 sentences: Average Loss:   0.439441 Duration 0.163314\n",
      "Batch 10900 of 10 sentences: Average Loss:   0.439152 Duration 0.163286\n",
      "Batch 11000 of 10 sentences: Average Loss:   0.438804 Duration 0.140091\n",
      "Batch 11100 of 10 sentences: Average Loss:   0.438601 Duration 0.177845\n",
      "Batch 11200 of 10 sentences: Average Loss:   0.438353 Duration 0.191812\n",
      "Batch 11300 of 10 sentences: Average Loss:   0.438123 Duration 0.195580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11400 of 10 sentences: Average Loss:   0.437829 Duration 0.175656\n",
      "Batch 11500 of 10 sentences: Average Loss:   0.437598 Duration 0.131591\n",
      "Batch 11600 of 10 sentences: Average Loss:   0.437322 Duration 0.162459\n",
      "Batch 11700 of 10 sentences: Average Loss:   0.437063 Duration 0.133105\n",
      "Batch 11800 of 10 sentences: Average Loss:   0.436793 Duration 0.139187\n",
      "Batch 11900 of 10 sentences: Average Loss:   0.436575 Duration 0.187415\n",
      "Batch 12000 of 10 sentences: Average Loss:   0.436337 Duration 0.224208\n",
      "Batch 12100 of 10 sentences: Average Loss:   0.436142 Duration 0.176519\n",
      "Batch 12200 of 10 sentences: Average Loss:   0.435892 Duration 0.107115\n",
      "Batch 12300 of 10 sentences: Average Loss:   0.435579 Duration 0.125227\n",
      "Batch 12400 of 10 sentences: Average Loss:   0.435288 Duration 0.170385\n",
      "Batch 12500 of 10 sentences: Average Loss:   0.435058 Duration 0.171922\n",
      "Batch 12600 of 10 sentences: Average Loss:   0.434845 Duration 0.199426\n",
      "epoch 2 batches 12623 done\n",
      "Batch 12700 of 10 sentences: Average Loss:   0.434650 Duration 0.126377\n",
      "Batch 12800 of 10 sentences: Average Loss:   0.434461 Duration 0.204320\n",
      "Batch 12900 of 10 sentences: Average Loss:   0.434273 Duration 0.186021\n",
      "Batch 13000 of 10 sentences: Average Loss:   0.433990 Duration 0.166213\n",
      "Batch 13100 of 10 sentences: Average Loss:   0.433770 Duration 0.166783\n",
      "Batch 13200 of 10 sentences: Average Loss:   0.433639 Duration 0.183565\n",
      "Batch 13300 of 10 sentences: Average Loss:   0.433436 Duration 0.209935\n",
      "Batch 13400 of 10 sentences: Average Loss:   0.433181 Duration 0.118459\n",
      "Batch 13500 of 10 sentences: Average Loss:   0.433036 Duration 0.157055\n",
      "Batch 13600 of 10 sentences: Average Loss:   0.432864 Duration 0.180095\n",
      "Batch 13700 of 10 sentences: Average Loss:   0.432712 Duration 0.184921\n",
      "Batch 13800 of 10 sentences: Average Loss:   0.432563 Duration 0.139930\n",
      "Batch 13900 of 10 sentences: Average Loss:   0.432399 Duration 0.153521\n",
      "Batch 14000 of 10 sentences: Average Loss:   0.432125 Duration 0.210002\n",
      "Batch 14100 of 10 sentences: Average Loss:   0.431949 Duration 0.140727\n",
      "Batch 14200 of 10 sentences: Average Loss:   0.431803 Duration 0.157755\n",
      "Batch 14300 of 10 sentences: Average Loss:   0.431605 Duration 0.199728\n",
      "Batch 14400 of 10 sentences: Average Loss:   0.431450 Duration 0.144048\n",
      "Batch 14500 of 10 sentences: Average Loss:   0.431259 Duration 0.132344\n",
      "Batch 14600 of 10 sentences: Average Loss:   0.431074 Duration 0.154201\n",
      "Batch 14700 of 10 sentences: Average Loss:   0.430893 Duration 0.155962\n",
      "Batch 14800 of 10 sentences: Average Loss:   0.430694 Duration 0.164776\n",
      "Batch 14900 of 10 sentences: Average Loss:   0.430457 Duration 0.201709\n",
      "Batch 15000 of 10 sentences: Average Loss:   0.430229 Duration 0.185526\n",
      "Batch 15100 of 10 sentences: Average Loss:   0.430062 Duration 0.153481\n",
      "Batch 15200 of 10 sentences: Average Loss:   0.429850 Duration 0.140387\n",
      "Batch 15300 of 10 sentences: Average Loss:   0.429699 Duration 0.121098\n",
      "Batch 15400 of 10 sentences: Average Loss:   0.429539 Duration 0.145967\n",
      "Batch 15500 of 10 sentences: Average Loss:   0.429378 Duration 0.197759\n",
      "Batch 15600 of 10 sentences: Average Loss:   0.429213 Duration 0.152242\n",
      "Batch 15700 of 10 sentences: Average Loss:   0.429062 Duration 0.165424\n",
      "Batch 15800 of 10 sentences: Average Loss:   0.428871 Duration 0.216518\n",
      "Batch 15900 of 10 sentences: Average Loss:   0.428694 Duration 0.198931\n",
      "Batch 16000 of 10 sentences: Average Loss:   0.428493 Duration 0.188268\n",
      "Batch 16100 of 10 sentences: Average Loss:   0.428350 Duration 0.082156\n",
      "Batch 16200 of 10 sentences: Average Loss:   0.428160 Duration 0.219813\n",
      "Batch 16300 of 10 sentences: Average Loss:   0.428041 Duration 0.182939\n",
      "Batch 16400 of 10 sentences: Average Loss:   0.427871 Duration 0.221551\n",
      "Batch 16500 of 10 sentences: Average Loss:   0.427664 Duration 0.146811\n",
      "Batch 16600 of 10 sentences: Average Loss:   0.427456 Duration 0.152774\n",
      "Batch 16700 of 10 sentences: Average Loss:   0.427291 Duration 0.186873\n",
      "Batch 16800 of 10 sentences: Average Loss:   0.427145 Duration 0.178676\n",
      "epoch 3 batches 16831 done\n",
      "Batch 16900 of 10 sentences: Average Loss:   0.427002 Duration 0.129621\n",
      "Batch 17000 of 10 sentences: Average Loss:   0.426880 Duration 0.161483\n",
      "Batch 17100 of 10 sentences: Average Loss:   0.426766 Duration 0.126894\n",
      "Batch 17200 of 10 sentences: Average Loss:   0.426549 Duration 0.143481\n",
      "Batch 17300 of 10 sentences: Average Loss:   0.426389 Duration 0.160375\n",
      "Batch 17400 of 10 sentences: Average Loss:   0.426305 Duration 0.137468\n",
      "Batch 17500 of 10 sentences: Average Loss:   0.426179 Duration 0.133188\n",
      "Batch 17600 of 10 sentences: Average Loss:   0.425989 Duration 0.222700\n",
      "Batch 17700 of 10 sentences: Average Loss:   0.425888 Duration 0.185156\n",
      "Batch 17800 of 10 sentences: Average Loss:   0.425761 Duration 0.176603\n",
      "Batch 17900 of 10 sentences: Average Loss:   0.425653 Duration 0.231004\n",
      "Batch 18000 of 10 sentences: Average Loss:   0.425559 Duration 0.177650\n",
      "Batch 18100 of 10 sentences: Average Loss:   0.425452 Duration 0.222236\n",
      "Batch 18200 of 10 sentences: Average Loss:   0.425283 Duration 0.181859\n",
      "Batch 18300 of 10 sentences: Average Loss:   0.425133 Duration 0.184256\n",
      "Batch 18400 of 10 sentences: Average Loss:   0.425051 Duration 0.214049\n",
      "Batch 18500 of 10 sentences: Average Loss:   0.424905 Duration 0.148105\n",
      "Batch 18600 of 10 sentences: Average Loss:   0.424810 Duration 0.231443\n",
      "Batch 18700 of 10 sentences: Average Loss:   0.424671 Duration 0.220968\n",
      "Batch 18800 of 10 sentences: Average Loss:   0.424540 Duration 0.090776\n",
      "Batch 18900 of 10 sentences: Average Loss:   0.424408 Duration 0.124428\n",
      "Batch 19000 of 10 sentences: Average Loss:   0.424260 Duration 0.238801\n",
      "Batch 19100 of 10 sentences: Average Loss:   0.424098 Duration 0.189528\n",
      "Batch 19200 of 10 sentences: Average Loss:   0.423929 Duration 0.137471\n",
      "Batch 19300 of 10 sentences: Average Loss:   0.423810 Duration 0.129272\n",
      "Batch 19400 of 10 sentences: Average Loss:   0.423665 Duration 0.300513\n",
      "Batch 19500 of 10 sentences: Average Loss:   0.423530 Duration 0.186425\n",
      "Batch 19600 of 10 sentences: Average Loss:   0.423427 Duration 0.174985\n",
      "Batch 19700 of 10 sentences: Average Loss:   0.423307 Duration 0.292514\n",
      "Batch 19800 of 10 sentences: Average Loss:   0.423178 Duration 0.142750\n",
      "Batch 19900 of 10 sentences: Average Loss:   0.423066 Duration 0.151116\n",
      "Batch 20000 of 10 sentences: Average Loss:   0.422923 Duration 0.242373\n",
      "Batch 20100 of 10 sentences: Average Loss:   0.422813 Duration 0.158015\n",
      "Batch 20200 of 10 sentences: Average Loss:   0.422647 Duration 0.179314\n",
      "Batch 20300 of 10 sentences: Average Loss:   0.422559 Duration 0.164155\n",
      "Batch 20400 of 10 sentences: Average Loss:   0.422400 Duration 0.156468\n",
      "Batch 20500 of 10 sentences: Average Loss:   0.422331 Duration 0.184461\n",
      "Batch 20600 of 10 sentences: Average Loss:   0.422216 Duration 0.168077\n",
      "Batch 20700 of 10 sentences: Average Loss:   0.422043 Duration 0.174736\n",
      "Batch 20800 of 10 sentences: Average Loss:   0.421890 Duration 0.139493\n",
      "Batch 20900 of 10 sentences: Average Loss:   0.421751 Duration 0.241247\n",
      "Batch 21000 of 10 sentences: Average Loss:   0.421630 Duration 0.186299\n",
      "epoch 4 batches 21039 done\n",
      "Batch 21100 of 10 sentences: Average Loss:   0.421534 Duration 0.216040\n",
      "Batch 21200 of 10 sentences: Average Loss:   0.421438 Duration 0.230485\n",
      "Batch 21300 of 10 sentences: Average Loss:   0.421353 Duration 0.158425\n",
      "Batch 21400 of 10 sentences: Average Loss:   0.421188 Duration 0.150666\n",
      "Batch 21500 of 10 sentences: Average Loss:   0.421066 Duration 0.283541\n",
      "Batch 21600 of 10 sentences: Average Loss:   0.420989 Duration 0.212040\n",
      "Batch 21700 of 10 sentences: Average Loss:   0.420878 Duration 0.249252\n",
      "Batch 21800 of 10 sentences: Average Loss:   0.420734 Duration 0.231236\n",
      "Batch 21900 of 10 sentences: Average Loss:   0.420644 Duration 0.262915\n",
      "Batch 22000 of 10 sentences: Average Loss:   0.420560 Duration 0.143933\n",
      "Batch 22100 of 10 sentences: Average Loss:   0.420457 Duration 0.153748\n",
      "Batch 22200 of 10 sentences: Average Loss:   0.420386 Duration 0.157479\n",
      "Batch 22300 of 10 sentences: Average Loss:   0.420319 Duration 0.200838\n",
      "Batch 22400 of 10 sentences: Average Loss:   0.420188 Duration 0.232983\n",
      "Batch 22500 of 10 sentences: Average Loss:   0.420063 Duration 0.076049\n",
      "Batch 22600 of 10 sentences: Average Loss:   0.419999 Duration 0.145920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 22700 of 10 sentences: Average Loss:   0.419879 Duration 0.116651\n",
      "Batch 22800 of 10 sentences: Average Loss:   0.419806 Duration 0.187972\n",
      "Batch 22900 of 10 sentences: Average Loss:   0.419701 Duration 0.201726\n",
      "Batch 23000 of 10 sentences: Average Loss:   0.419586 Duration 0.218563\n",
      "Batch 23100 of 10 sentences: Average Loss:   0.419477 Duration 0.170224\n",
      "Batch 23200 of 10 sentences: Average Loss:   0.419366 Duration 0.218204\n",
      "Batch 23300 of 10 sentences: Average Loss:   0.419227 Duration 0.131817\n",
      "Batch 23400 of 10 sentences: Average Loss:   0.419072 Duration 0.113211\n",
      "Batch 23500 of 10 sentences: Average Loss:   0.418984 Duration 0.149859\n",
      "Batch 23600 of 10 sentences: Average Loss:   0.418870 Duration 0.250694\n",
      "Batch 23700 of 10 sentences: Average Loss:   0.418761 Duration 0.159725\n",
      "Batch 23800 of 10 sentences: Average Loss:   0.418676 Duration 0.163515\n",
      "Batch 23900 of 10 sentences: Average Loss:   0.418580 Duration 0.191605\n",
      "Batch 24000 of 10 sentences: Average Loss:   0.418475 Duration 0.203308\n",
      "Batch 24100 of 10 sentences: Average Loss:   0.418378 Duration 0.174923\n",
      "Batch 24200 of 10 sentences: Average Loss:   0.418271 Duration 0.147212\n",
      "Batch 24300 of 10 sentences: Average Loss:   0.418182 Duration 0.317185\n",
      "Batch 24400 of 10 sentences: Average Loss:   0.418052 Duration 0.178882\n",
      "Batch 24500 of 10 sentences: Average Loss:   0.417993 Duration 0.195565\n",
      "Batch 24600 of 10 sentences: Average Loss:   0.417845 Duration 0.132342\n",
      "Batch 24700 of 10 sentences: Average Loss:   0.417778 Duration 0.323526\n",
      "Batch 24800 of 10 sentences: Average Loss:   0.417689 Duration 0.163746\n",
      "Batch 24900 of 10 sentences: Average Loss:   0.417566 Duration 0.171832\n",
      "Batch 25000 of 10 sentences: Average Loss:   0.417454 Duration 0.235348\n",
      "Batch 25100 of 10 sentences: Average Loss:   0.417311 Duration 0.286334\n",
      "Batch 25200 of 10 sentences: Average Loss:   0.417227 Duration 0.164417\n",
      "epoch 5 batches 25247 done\n",
      "Batch 25300 of 10 sentences: Average Loss:   0.417147 Duration 0.156911\n",
      "Batch 25400 of 10 sentences: Average Loss:   0.417066 Duration 0.122997\n",
      "Batch 25500 of 10 sentences: Average Loss:   0.417007 Duration 0.184508\n",
      "Batch 25600 of 10 sentences: Average Loss:   0.416868 Duration 0.259397\n",
      "Batch 25700 of 10 sentences: Average Loss:   0.416769 Duration 0.169569\n",
      "Batch 25800 of 10 sentences: Average Loss:   0.416710 Duration 0.175002\n",
      "Batch 25900 of 10 sentences: Average Loss:   0.416627 Duration 0.275852\n",
      "Batch 26000 of 10 sentences: Average Loss:   0.416507 Duration 0.149998\n",
      "Batch 26100 of 10 sentences: Average Loss:   0.416437 Duration 0.162511\n",
      "Batch 26200 of 10 sentences: Average Loss:   0.416367 Duration 0.162088\n",
      "Batch 26300 of 10 sentences: Average Loss:   0.416291 Duration 0.178880\n",
      "Batch 26400 of 10 sentences: Average Loss:   0.416232 Duration 0.216072\n",
      "Batch 26500 of 10 sentences: Average Loss:   0.416173 Duration 0.197506\n",
      "Batch 26600 of 10 sentences: Average Loss:   0.416086 Duration 0.211480\n",
      "Batch 26700 of 10 sentences: Average Loss:   0.415977 Duration 0.145023\n",
      "Batch 26800 of 10 sentences: Average Loss:   0.415927 Duration 0.167133\n",
      "Batch 26900 of 10 sentences: Average Loss:   0.415822 Duration 0.142074\n",
      "Batch 27000 of 10 sentences: Average Loss:   0.415756 Duration 0.217898\n",
      "Batch 27100 of 10 sentences: Average Loss:   0.415672 Duration 0.157542\n",
      "Batch 27200 of 10 sentences: Average Loss:   0.415580 Duration 0.137097\n",
      "Batch 27300 of 10 sentences: Average Loss:   0.415486 Duration 0.145855\n",
      "Batch 27400 of 10 sentences: Average Loss:   0.415394 Duration 0.150221\n",
      "Batch 27500 of 10 sentences: Average Loss:   0.415283 Duration 0.151191\n",
      "Batch 27600 of 10 sentences: Average Loss:   0.415151 Duration 0.185564\n",
      "Batch 27700 of 10 sentences: Average Loss:   0.415079 Duration 0.220221\n",
      "Batch 27800 of 10 sentences: Average Loss:   0.414975 Duration 0.150354\n",
      "Batch 27900 of 10 sentences: Average Loss:   0.414882 Duration 0.158499\n",
      "Batch 28000 of 10 sentences: Average Loss:   0.414813 Duration 0.206675\n",
      "Batch 28100 of 10 sentences: Average Loss:   0.414740 Duration 0.194139\n",
      "Batch 28200 of 10 sentences: Average Loss:   0.414660 Duration 0.173077\n",
      "Batch 28300 of 10 sentences: Average Loss:   0.414575 Duration 0.125129\n",
      "Batch 28400 of 10 sentences: Average Loss:   0.414483 Duration 0.119244\n",
      "Batch 28500 of 10 sentences: Average Loss:   0.414410 Duration 0.106324\n",
      "Batch 28600 of 10 sentences: Average Loss:   0.414309 Duration 0.191005\n",
      "Batch 28700 of 10 sentences: Average Loss:   0.414253 Duration 0.123462\n",
      "Batch 28800 of 10 sentences: Average Loss:   0.414138 Duration 0.171816\n",
      "Batch 28900 of 10 sentences: Average Loss:   0.414079 Duration 0.134074\n",
      "Batch 29000 of 10 sentences: Average Loss:   0.413995 Duration 0.222131\n",
      "Batch 29100 of 10 sentences: Average Loss:   0.413884 Duration 0.231081\n",
      "Batch 29200 of 10 sentences: Average Loss:   0.413789 Duration 0.203360\n",
      "Batch 29300 of 10 sentences: Average Loss:   0.413671 Duration 0.192700\n",
      "Batch 29400 of 10 sentences: Average Loss:   0.413595 Duration 0.187408\n",
      "epoch 6 batches 29455 done\n",
      "Batch 29500 of 10 sentences: Average Loss:   0.413523 Duration 0.169580\n",
      "Batch 29600 of 10 sentences: Average Loss:   0.413460 Duration 0.127693\n",
      "Batch 29700 of 10 sentences: Average Loss:   0.413411 Duration 0.190667\n",
      "Batch 29800 of 10 sentences: Average Loss:   0.413304 Duration 0.123810\n",
      "Batch 29900 of 10 sentences: Average Loss:   0.413206 Duration 0.167064\n",
      "Batch 30000 of 10 sentences: Average Loss:   0.413156 Duration 0.169331\n",
      "Batch 30100 of 10 sentences: Average Loss:   0.413099 Duration 0.192066\n",
      "Batch 30200 of 10 sentences: Average Loss:   0.412999 Duration 0.162354\n",
      "Batch 30300 of 10 sentences: Average Loss:   0.412929 Duration 0.172184\n",
      "Batch 30400 of 10 sentences: Average Loss:   0.412874 Duration 0.206912\n",
      "Batch 30500 of 10 sentences: Average Loss:   0.412804 Duration 0.131701\n",
      "Batch 30600 of 10 sentences: Average Loss:   0.412764 Duration 0.159430\n",
      "Batch 30700 of 10 sentences: Average Loss:   0.412715 Duration 0.150518\n",
      "Batch 30800 of 10 sentences: Average Loss:   0.412637 Duration 0.202206\n",
      "Batch 30900 of 10 sentences: Average Loss:   0.412550 Duration 0.179447\n",
      "Batch 31000 of 10 sentences: Average Loss:   0.412503 Duration 0.191258\n",
      "Batch 31100 of 10 sentences: Average Loss:   0.412426 Duration 0.133173\n",
      "Batch 31200 of 10 sentences: Average Loss:   0.412372 Duration 0.161187\n",
      "Batch 31300 of 10 sentences: Average Loss:   0.412303 Duration 0.146972\n",
      "Batch 31400 of 10 sentences: Average Loss:   0.412228 Duration 0.173624\n",
      "Batch 31500 of 10 sentences: Average Loss:   0.412147 Duration 0.194075\n",
      "Batch 31600 of 10 sentences: Average Loss:   0.412057 Duration 0.165623\n",
      "Batch 31700 of 10 sentences: Average Loss:   0.411974 Duration 0.163192\n",
      "Batch 31800 of 10 sentences: Average Loss:   0.411855 Duration 0.096099\n",
      "Batch 31900 of 10 sentences: Average Loss:   0.411799 Duration 0.158288\n",
      "Batch 32000 of 10 sentences: Average Loss:   0.411712 Duration 0.141214\n",
      "Batch 32100 of 10 sentences: Average Loss:   0.411630 Duration 0.185296\n",
      "Batch 32200 of 10 sentences: Average Loss:   0.411573 Duration 0.218665\n",
      "Batch 32300 of 10 sentences: Average Loss:   0.411511 Duration 0.145099\n",
      "Batch 32400 of 10 sentences: Average Loss:   0.411433 Duration 0.146321\n",
      "Batch 32500 of 10 sentences: Average Loss:   0.411360 Duration 0.165546\n",
      "Batch 32600 of 10 sentences: Average Loss:   0.411291 Duration 0.156986\n",
      "Batch 32700 of 10 sentences: Average Loss:   0.411223 Duration 0.185180\n",
      "Batch 32800 of 10 sentences: Average Loss:   0.411146 Duration 0.134370\n",
      "Batch 32900 of 10 sentences: Average Loss:   0.411094 Duration 0.266947\n",
      "Batch 33000 of 10 sentences: Average Loss:   0.411001 Duration 0.082437\n",
      "Batch 33100 of 10 sentences: Average Loss:   0.410949 Duration 0.226691\n",
      "Batch 33200 of 10 sentences: Average Loss:   0.410895 Duration 0.171795\n",
      "Batch 33300 of 10 sentences: Average Loss:   0.410797 Duration 0.215889\n",
      "Batch 33400 of 10 sentences: Average Loss:   0.410717 Duration 0.151085\n",
      "Batch 33500 of 10 sentences: Average Loss:   0.410622 Duration 0.148967\n",
      "Batch 33600 of 10 sentences: Average Loss:   0.410548 Duration 0.202011\n",
      "epoch 7 batches 33663 done\n",
      "Batch 33700 of 10 sentences: Average Loss:   0.410493 Duration 0.208019\n",
      "Batch 33800 of 10 sentences: Average Loss:   0.410432 Duration 0.132164\n",
      "Batch 33900 of 10 sentences: Average Loss:   0.410384 Duration 0.154648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 34000 of 10 sentences: Average Loss:   0.410302 Duration 0.176282\n",
      "Batch 34100 of 10 sentences: Average Loss:   0.410217 Duration 0.144981\n",
      "Batch 34200 of 10 sentences: Average Loss:   0.410171 Duration 0.198678\n",
      "Batch 34300 of 10 sentences: Average Loss:   0.410124 Duration 0.168010\n",
      "Batch 34400 of 10 sentences: Average Loss:   0.410039 Duration 0.149536\n",
      "Batch 34500 of 10 sentences: Average Loss:   0.409972 Duration 0.221091\n",
      "Batch 34600 of 10 sentences: Average Loss:   0.409921 Duration 0.282004\n",
      "Batch 34700 of 10 sentences: Average Loss:   0.409864 Duration 0.197598\n",
      "Batch 34800 of 10 sentences: Average Loss:   0.409832 Duration 0.149493\n",
      "Batch 34900 of 10 sentences: Average Loss:   0.409786 Duration 0.169722\n",
      "Batch 35000 of 10 sentences: Average Loss:   0.409716 Duration 0.180265\n",
      "Batch 35100 of 10 sentences: Average Loss:   0.409637 Duration 0.252486\n",
      "Batch 35200 of 10 sentences: Average Loss:   0.409599 Duration 0.147384\n",
      "Batch 35300 of 10 sentences: Average Loss:   0.409530 Duration 0.147488\n",
      "Batch 35400 of 10 sentences: Average Loss:   0.409486 Duration 0.244457\n",
      "Batch 35500 of 10 sentences: Average Loss:   0.409428 Duration 0.169334\n",
      "Batch 35600 of 10 sentences: Average Loss:   0.409359 Duration 0.182276\n",
      "Batch 35700 of 10 sentences: Average Loss:   0.409286 Duration 0.209478\n",
      "Batch 35800 of 10 sentences: Average Loss:   0.409206 Duration 0.083520\n",
      "Batch 35900 of 10 sentences: Average Loss:   0.409135 Duration 0.243738\n",
      "Batch 36000 of 10 sentences: Average Loss:   0.409029 Duration 0.134172\n",
      "Batch 36100 of 10 sentences: Average Loss:   0.408984 Duration 0.197776\n",
      "Batch 36200 of 10 sentences: Average Loss:   0.408908 Duration 0.160734\n",
      "Batch 36300 of 10 sentences: Average Loss:   0.408834 Duration 0.153517\n",
      "Batch 36400 of 10 sentences: Average Loss:   0.408782 Duration 0.205913\n",
      "Batch 36500 of 10 sentences: Average Loss:   0.408725 Duration 0.176633\n",
      "Batch 36600 of 10 sentences: Average Loss:   0.408660 Duration 0.156403\n",
      "Batch 36700 of 10 sentences: Average Loss:   0.408600 Duration 0.151684\n",
      "Batch 36800 of 10 sentences: Average Loss:   0.408540 Duration 0.179731\n",
      "Batch 36900 of 10 sentences: Average Loss:   0.408474 Duration 0.217429\n",
      "Batch 37000 of 10 sentences: Average Loss:   0.408403 Duration 0.213288\n",
      "Batch 37100 of 10 sentences: Average Loss:   0.408355 Duration 0.261520\n",
      "Batch 37200 of 10 sentences: Average Loss:   0.408271 Duration 0.065062\n",
      "Batch 37300 of 10 sentences: Average Loss:   0.408215 Duration 0.149936\n",
      "Batch 37400 of 10 sentences: Average Loss:   0.408166 Duration 0.149029\n",
      "Batch 37500 of 10 sentences: Average Loss:   0.408078 Duration 0.202911\n",
      "Batch 37600 of 10 sentences: Average Loss:   0.407999 Duration 0.195442\n",
      "Batch 37700 of 10 sentences: Average Loss:   0.407909 Duration 0.115862\n",
      "Batch 37800 of 10 sentences: Average Loss:   0.407840 Duration 0.160117\n",
      "epoch 8 batches 37871 done\n",
      "Batch 37900 of 10 sentences: Average Loss:   0.407790 Duration 0.222206\n",
      "Batch 38000 of 10 sentences: Average Loss:   0.407743 Duration 0.172514\n",
      "Batch 38100 of 10 sentences: Average Loss:   0.407692 Duration 0.304330\n",
      "Batch 38200 of 10 sentences: Average Loss:   0.407622 Duration 0.172596\n",
      "Batch 38300 of 10 sentences: Average Loss:   0.407550 Duration 0.340217\n",
      "Batch 38400 of 10 sentences: Average Loss:   0.407501 Duration 0.148583\n",
      "Batch 38500 of 10 sentences: Average Loss:   0.407470 Duration 0.155224\n",
      "Batch 38600 of 10 sentences: Average Loss:   0.407396 Duration 0.194632\n",
      "Batch 38700 of 10 sentences: Average Loss:   0.407329 Duration 0.190614\n",
      "Batch 38800 of 10 sentences: Average Loss:   0.407292 Duration 0.226415\n",
      "Batch 38900 of 10 sentences: Average Loss:   0.407241 Duration 0.208966\n",
      "Batch 39000 of 10 sentences: Average Loss:   0.407214 Duration 0.196877\n",
      "Batch 39100 of 10 sentences: Average Loss:   0.407170 Duration 0.151159\n",
      "Batch 39200 of 10 sentences: Average Loss:   0.407103 Duration 0.216358\n",
      "Batch 39300 of 10 sentences: Average Loss:   0.407032 Duration 0.168564\n",
      "Batch 39400 of 10 sentences: Average Loss:   0.406995 Duration 0.207780\n",
      "Batch 39500 of 10 sentences: Average Loss:   0.406939 Duration 0.182987\n",
      "Batch 39600 of 10 sentences: Average Loss:   0.406892 Duration 0.175575\n",
      "Batch 39700 of 10 sentences: Average Loss:   0.406851 Duration 0.159467\n",
      "Batch 39800 of 10 sentences: Average Loss:   0.406787 Duration 0.108662\n",
      "Batch 39900 of 10 sentences: Average Loss:   0.406726 Duration 0.211836\n",
      "Batch 40000 of 10 sentences: Average Loss:   0.406657 Duration 0.057967\n",
      "Batch 40100 of 10 sentences: Average Loss:   0.406585 Duration 0.185458\n",
      "Batch 40200 of 10 sentences: Average Loss:   0.406491 Duration 0.176091\n",
      "Batch 40300 of 10 sentences: Average Loss:   0.406452 Duration 0.170501\n",
      "Batch 40400 of 10 sentences: Average Loss:   0.406381 Duration 0.152181\n",
      "Batch 40500 of 10 sentences: Average Loss:   0.406315 Duration 0.152410\n",
      "Batch 40600 of 10 sentences: Average Loss:   0.406269 Duration 0.191849\n",
      "Batch 40700 of 10 sentences: Average Loss:   0.406218 Duration 0.220072\n",
      "Batch 40800 of 10 sentences: Average Loss:   0.406160 Duration 0.218080\n",
      "Batch 40900 of 10 sentences: Average Loss:   0.406105 Duration 0.168306\n",
      "Batch 41000 of 10 sentences: Average Loss:   0.406056 Duration 0.201142\n",
      "Batch 41100 of 10 sentences: Average Loss:   0.405993 Duration 0.184468\n",
      "Batch 41200 of 10 sentences: Average Loss:   0.405936 Duration 0.135622\n",
      "Batch 41300 of 10 sentences: Average Loss:   0.405888 Duration 0.152152\n",
      "Batch 41400 of 10 sentences: Average Loss:   0.405817 Duration 0.153715\n",
      "Batch 41500 of 10 sentences: Average Loss:   0.405753 Duration 0.224233\n",
      "Batch 41600 of 10 sentences: Average Loss:   0.405716 Duration 0.198451\n",
      "Batch 41700 of 10 sentences: Average Loss:   0.405639 Duration 0.208614\n",
      "Batch 41800 of 10 sentences: Average Loss:   0.405569 Duration 0.225737\n",
      "Batch 41900 of 10 sentences: Average Loss:   0.405483 Duration 0.160028\n",
      "Batch 42000 of 10 sentences: Average Loss:   0.405423 Duration 0.183154\n",
      "epoch 9 batches 42079 done\n",
      "Batch 42100 of 10 sentences: Average Loss:   0.405373 Duration 0.147392\n",
      "Batch 42200 of 10 sentences: Average Loss:   0.405333 Duration 0.160022\n",
      "Batch 42300 of 10 sentences: Average Loss:   0.405291 Duration 0.164212\n",
      "Batch 42400 of 10 sentences: Average Loss:   0.405221 Duration 0.192718\n",
      "Batch 42500 of 10 sentences: Average Loss:   0.405158 Duration 0.174018\n",
      "Batch 42600 of 10 sentences: Average Loss:   0.405106 Duration 0.133676\n",
      "Batch 42700 of 10 sentences: Average Loss:   0.405073 Duration 0.199820\n",
      "Batch 42800 of 10 sentences: Average Loss:   0.405010 Duration 0.215889\n",
      "Batch 42900 of 10 sentences: Average Loss:   0.404943 Duration 0.269455\n",
      "Batch 43000 of 10 sentences: Average Loss:   0.404907 Duration 0.108169\n",
      "Batch 43100 of 10 sentences: Average Loss:   0.404865 Duration 0.195130\n",
      "Batch 43200 of 10 sentences: Average Loss:   0.404841 Duration 0.218887\n",
      "Batch 43300 of 10 sentences: Average Loss:   0.404800 Duration 0.272135\n",
      "Batch 43400 of 10 sentences: Average Loss:   0.404747 Duration 0.202281\n",
      "Batch 43500 of 10 sentences: Average Loss:   0.404684 Duration 0.140938\n",
      "Batch 43600 of 10 sentences: Average Loss:   0.404650 Duration 0.183607\n",
      "Batch 43700 of 10 sentences: Average Loss:   0.404601 Duration 0.210308\n",
      "Batch 43800 of 10 sentences: Average Loss:   0.404558 Duration 0.291265\n",
      "Batch 43900 of 10 sentences: Average Loss:   0.404515 Duration 0.242939\n",
      "Batch 44000 of 10 sentences: Average Loss:   0.404461 Duration 0.154557\n",
      "Batch 44100 of 10 sentences: Average Loss:   0.404402 Duration 0.195667\n",
      "Batch 44200 of 10 sentences: Average Loss:   0.404353 Duration 0.199628\n",
      "Batch 44300 of 10 sentences: Average Loss:   0.404274 Duration 0.216676\n",
      "Batch 44400 of 10 sentences: Average Loss:   0.404194 Duration 0.152450\n",
      "Batch 44500 of 10 sentences: Average Loss:   0.404161 Duration 0.175528\n",
      "Batch 44600 of 10 sentences: Average Loss:   0.404089 Duration 0.253336\n",
      "Batch 44700 of 10 sentences: Average Loss:   0.404021 Duration 0.092170\n",
      "Batch 44800 of 10 sentences: Average Loss:   0.403979 Duration 0.221063\n",
      "Batch 44900 of 10 sentences: Average Loss:   0.403935 Duration 0.154231\n",
      "Batch 45000 of 10 sentences: Average Loss:   0.403884 Duration 0.241710\n",
      "Batch 45100 of 10 sentences: Average Loss:   0.403837 Duration 0.175257\n",
      "Batch 45200 of 10 sentences: Average Loss:   0.403795 Duration 0.178489\n",
      "Batch 45300 of 10 sentences: Average Loss:   0.403739 Duration 0.310074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 45400 of 10 sentences: Average Loss:   0.403682 Duration 0.210803\n",
      "Batch 45500 of 10 sentences: Average Loss:   0.403645 Duration 0.189276\n",
      "Batch 45600 of 10 sentences: Average Loss:   0.403578 Duration 0.232171\n",
      "Batch 45700 of 10 sentences: Average Loss:   0.403527 Duration 0.224334\n",
      "Batch 45800 of 10 sentences: Average Loss:   0.403489 Duration 0.210731\n",
      "Batch 45900 of 10 sentences: Average Loss:   0.403418 Duration 0.215115\n",
      "Batch 46000 of 10 sentences: Average Loss:   0.403353 Duration 0.154949\n",
      "Batch 46100 of 10 sentences: Average Loss:   0.403277 Duration 0.187279\n",
      "Batch 46200 of 10 sentences: Average Loss:   0.403223 Duration 0.149619\n",
      "epoch 10 batches 46287 done\n",
      "Batch 46300 of 10 sentences: Average Loss:   0.403178 Duration 0.190695\n",
      "Batch 46400 of 10 sentences: Average Loss:   0.403141 Duration 0.174913\n",
      "Batch 46500 of 10 sentences: Average Loss:   0.403101 Duration 0.116006\n",
      "Batch 46600 of 10 sentences: Average Loss:   0.403044 Duration 0.218864\n",
      "Batch 46700 of 10 sentences: Average Loss:   0.402985 Duration 0.153436\n",
      "Batch 46800 of 10 sentences: Average Loss:   0.402931 Duration 0.201479\n",
      "Batch 46900 of 10 sentences: Average Loss:   0.402904 Duration 0.160470\n",
      "Batch 47000 of 10 sentences: Average Loss:   0.402842 Duration 0.212355\n",
      "Batch 47100 of 10 sentences: Average Loss:   0.402788 Duration 0.131717\n",
      "Batch 47200 of 10 sentences: Average Loss:   0.402751 Duration 0.130428\n",
      "Batch 47300 of 10 sentences: Average Loss:   0.402712 Duration 0.206727\n",
      "Batch 47400 of 10 sentences: Average Loss:   0.402689 Duration 0.209967\n",
      "Batch 47500 of 10 sentences: Average Loss:   0.402648 Duration 0.253915\n",
      "Batch 47600 of 10 sentences: Average Loss:   0.402607 Duration 0.121911\n",
      "Batch 47700 of 10 sentences: Average Loss:   0.402543 Duration 0.205620\n",
      "Batch 47800 of 10 sentences: Average Loss:   0.402510 Duration 0.165290\n",
      "Batch 47900 of 10 sentences: Average Loss:   0.402464 Duration 0.377118\n",
      "Batch 48000 of 10 sentences: Average Loss:   0.402419 Duration 0.151457\n",
      "Batch 48100 of 10 sentences: Average Loss:   0.402386 Duration 0.142372\n",
      "Batch 48200 of 10 sentences: Average Loss:   0.402331 Duration 0.200236\n",
      "Batch 48300 of 10 sentences: Average Loss:   0.402276 Duration 0.208465\n",
      "Batch 48400 of 10 sentences: Average Loss:   0.402227 Duration 0.172847\n",
      "Batch 48500 of 10 sentences: Average Loss:   0.402159 Duration 0.135471\n",
      "Batch 48600 of 10 sentences: Average Loss:   0.402089 Duration 0.156792\n",
      "Batch 48700 of 10 sentences: Average Loss:   0.402050 Duration 0.180367\n",
      "Batch 48800 of 10 sentences: Average Loss:   0.401982 Duration 0.237982\n",
      "Batch 48900 of 10 sentences: Average Loss:   0.401919 Duration 0.199523\n",
      "Batch 49000 of 10 sentences: Average Loss:   0.401878 Duration 0.214512\n",
      "Batch 49100 of 10 sentences: Average Loss:   0.401840 Duration 0.186446\n",
      "Batch 49200 of 10 sentences: Average Loss:   0.401793 Duration 0.173315\n",
      "Batch 49300 of 10 sentences: Average Loss:   0.401750 Duration 0.199680\n",
      "Batch 49400 of 10 sentences: Average Loss:   0.401710 Duration 0.206760\n",
      "Batch 49500 of 10 sentences: Average Loss:   0.401659 Duration 0.113132\n",
      "Batch 49600 of 10 sentences: Average Loss:   0.401604 Duration 0.190236\n",
      "Batch 49700 of 10 sentences: Average Loss:   0.401562 Duration 0.146118\n",
      "Batch 49800 of 10 sentences: Average Loss:   0.401507 Duration 0.156886\n",
      "Batch 49900 of 10 sentences: Average Loss:   0.401460 Duration 0.183180\n",
      "Batch 50000 of 10 sentences: Average Loss:   0.401423 Duration 0.221849\n",
      "Batch 50100 of 10 sentences: Average Loss:   0.401361 Duration 0.152336\n",
      "Batch 50200 of 10 sentences: Average Loss:   0.401301 Duration 0.159569\n",
      "Batch 50300 of 10 sentences: Average Loss:   0.401228 Duration 0.178134\n",
      "Batch 50400 of 10 sentences: Average Loss:   0.401177 Duration 0.240944\n",
      "epoch 11 batches 50495 done\n",
      "Batch 50500 of 10 sentences: Average Loss:   0.401140 Duration 0.186177\n",
      "Batch 50600 of 10 sentences: Average Loss:   0.401105 Duration 0.119146\n",
      "Batch 50700 of 10 sentences: Average Loss:   0.401072 Duration 0.169212\n",
      "Batch 50800 of 10 sentences: Average Loss:   0.401016 Duration 0.121305\n",
      "Batch 50900 of 10 sentences: Average Loss:   0.400958 Duration 0.197922\n",
      "Batch 51000 of 10 sentences: Average Loss:   0.400912 Duration 0.245060\n",
      "Batch 51100 of 10 sentences: Average Loss:   0.400889 Duration 0.189086\n",
      "Batch 51200 of 10 sentences: Average Loss:   0.400838 Duration 0.212612\n",
      "Batch 51300 of 10 sentences: Average Loss:   0.400785 Duration 0.176840\n",
      "Batch 51400 of 10 sentences: Average Loss:   0.400749 Duration 0.136723\n",
      "Batch 51500 of 10 sentences: Average Loss:   0.400715 Duration 0.191063\n",
      "Batch 51600 of 10 sentences: Average Loss:   0.400691 Duration 0.193137\n",
      "Batch 51700 of 10 sentences: Average Loss:   0.400656 Duration 0.200389\n",
      "Batch 51800 of 10 sentences: Average Loss:   0.400621 Duration 0.190131\n",
      "Batch 51900 of 10 sentences: Average Loss:   0.400564 Duration 0.143698\n",
      "Batch 52000 of 10 sentences: Average Loss:   0.400534 Duration 0.157549\n",
      "Batch 52100 of 10 sentences: Average Loss:   0.400492 Duration 0.200159\n",
      "Batch 52200 of 10 sentences: Average Loss:   0.400451 Duration 0.255731\n",
      "Batch 52300 of 10 sentences: Average Loss:   0.400414 Duration 0.120192\n",
      "Batch 52400 of 10 sentences: Average Loss:   0.400371 Duration 0.277783\n",
      "Batch 52500 of 10 sentences: Average Loss:   0.400322 Duration 0.128856\n",
      "Batch 52600 of 10 sentences: Average Loss:   0.400278 Duration 0.138155\n",
      "Batch 52700 of 10 sentences: Average Loss:   0.400212 Duration 0.195692\n",
      "Batch 52800 of 10 sentences: Average Loss:   0.400149 Duration 0.199096\n",
      "Batch 52900 of 10 sentences: Average Loss:   0.400116 Duration 0.244702\n",
      "Batch 53000 of 10 sentences: Average Loss:   0.400057 Duration 0.213451\n",
      "Batch 53100 of 10 sentences: Average Loss:   0.399999 Duration 0.120458\n",
      "Batch 53200 of 10 sentences: Average Loss:   0.399967 Duration 0.155276\n",
      "Batch 53300 of 10 sentences: Average Loss:   0.399924 Duration 0.152641\n",
      "Batch 53400 of 10 sentences: Average Loss:   0.399885 Duration 0.084077\n",
      "Batch 53500 of 10 sentences: Average Loss:   0.399842 Duration 0.147226\n",
      "Batch 53600 of 10 sentences: Average Loss:   0.399808 Duration 0.250892\n",
      "Batch 53700 of 10 sentences: Average Loss:   0.399766 Duration 0.119421\n",
      "Batch 53800 of 10 sentences: Average Loss:   0.399723 Duration 0.232971\n",
      "Batch 53900 of 10 sentences: Average Loss:   0.399677 Duration 0.317060\n",
      "Batch 54000 of 10 sentences: Average Loss:   0.399626 Duration 0.155171\n",
      "Batch 54100 of 10 sentences: Average Loss:   0.399579 Duration 0.253708\n",
      "Batch 54200 of 10 sentences: Average Loss:   0.399550 Duration 0.167604\n",
      "Batch 54300 of 10 sentences: Average Loss:   0.399495 Duration 0.280660\n",
      "Batch 54400 of 10 sentences: Average Loss:   0.399437 Duration 0.127608\n",
      "Batch 54500 of 10 sentences: Average Loss:   0.399372 Duration 0.167058\n",
      "Batch 54600 of 10 sentences: Average Loss:   0.399318 Duration 0.197347\n",
      "Batch 54700 of 10 sentences: Average Loss:   0.399281 Duration 0.319590\n",
      "epoch 12 batches 54703 done\n",
      "Batch 54800 of 10 sentences: Average Loss:   0.399245 Duration 0.181736\n",
      "Batch 54900 of 10 sentences: Average Loss:   0.399212 Duration 0.376817\n",
      "Batch 55000 of 10 sentences: Average Loss:   0.399166 Duration 0.235036\n",
      "Batch 55100 of 10 sentences: Average Loss:   0.399104 Duration 0.225386\n",
      "Batch 55200 of 10 sentences: Average Loss:   0.399062 Duration 0.230071\n",
      "Batch 55300 of 10 sentences: Average Loss:   0.399037 Duration 0.270577\n",
      "Batch 55400 of 10 sentences: Average Loss:   0.398991 Duration 0.232283\n",
      "Batch 55500 of 10 sentences: Average Loss:   0.398944 Duration 0.117200\n",
      "Batch 55600 of 10 sentences: Average Loss:   0.398912 Duration 0.123043\n",
      "Batch 55700 of 10 sentences: Average Loss:   0.398874 Duration 0.181499\n",
      "Batch 55800 of 10 sentences: Average Loss:   0.398852 Duration 0.178398\n",
      "Batch 55900 of 10 sentences: Average Loss:   0.398821 Duration 0.296771\n",
      "Batch 56000 of 10 sentences: Average Loss:   0.398788 Duration 0.169092\n",
      "Batch 56100 of 10 sentences: Average Loss:   0.398727 Duration 0.198708\n",
      "Batch 56200 of 10 sentences: Average Loss:   0.398702 Duration 0.138728\n",
      "Batch 56300 of 10 sentences: Average Loss:   0.398666 Duration 0.221325\n",
      "Batch 56400 of 10 sentences: Average Loss:   0.398630 Duration 0.180797\n",
      "Batch 56500 of 10 sentences: Average Loss:   0.398596 Duration 0.243540\n",
      "Batch 56600 of 10 sentences: Average Loss:   0.398557 Duration 0.220696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 56700 of 10 sentences: Average Loss:   0.398509 Duration 0.148686\n",
      "Batch 56800 of 10 sentences: Average Loss:   0.398470 Duration 0.295860\n",
      "Batch 56900 of 10 sentences: Average Loss:   0.398411 Duration 0.207232\n",
      "Batch 57000 of 10 sentences: Average Loss:   0.398350 Duration 0.145264\n",
      "Batch 57100 of 10 sentences: Average Loss:   0.398317 Duration 0.221088\n",
      "Batch 57200 of 10 sentences: Average Loss:   0.398267 Duration 0.168620\n",
      "Batch 57300 of 10 sentences: Average Loss:   0.398213 Duration 0.202826\n",
      "Batch 57400 of 10 sentences: Average Loss:   0.398179 Duration 0.150028\n",
      "Batch 57500 of 10 sentences: Average Loss:   0.398140 Duration 0.281765\n",
      "Batch 57600 of 10 sentences: Average Loss:   0.398109 Duration 0.180932\n",
      "Batch 57700 of 10 sentences: Average Loss:   0.398059 Duration 0.197020\n",
      "Batch 57800 of 10 sentences: Average Loss:   0.398033 Duration 0.254252\n",
      "Batch 57900 of 10 sentences: Average Loss:   0.397993 Duration 0.260480\n",
      "Batch 58000 of 10 sentences: Average Loss:   0.397949 Duration 0.232803\n",
      "Batch 58100 of 10 sentences: Average Loss:   0.397910 Duration 0.196405\n",
      "Batch 58200 of 10 sentences: Average Loss:   0.397863 Duration 0.267571\n",
      "Batch 58300 of 10 sentences: Average Loss:   0.397821 Duration 0.325445\n",
      "Batch 58400 of 10 sentences: Average Loss:   0.397792 Duration 0.160857\n",
      "Batch 58500 of 10 sentences: Average Loss:   0.397742 Duration 0.260469\n",
      "Batch 58600 of 10 sentences: Average Loss:   0.397690 Duration 0.146790\n",
      "Batch 58700 of 10 sentences: Average Loss:   0.397632 Duration 0.246432\n",
      "Batch 58800 of 10 sentences: Average Loss:   0.397580 Duration 0.176814\n",
      "Batch 58900 of 10 sentences: Average Loss:   0.397543 Duration 0.187008\n",
      "epoch 13 batches 58911 done\n",
      "Batch 59000 of 10 sentences: Average Loss:   0.397510 Duration 0.156338\n",
      "Batch 59100 of 10 sentences: Average Loss:   0.397478 Duration 0.206413\n",
      "Batch 59200 of 10 sentences: Average Loss:   0.397436 Duration 0.244662\n",
      "Batch 59300 of 10 sentences: Average Loss:   0.397381 Duration 0.224805\n",
      "Batch 59400 of 10 sentences: Average Loss:   0.397341 Duration 0.215959\n",
      "Batch 59500 of 10 sentences: Average Loss:   0.397313 Duration 0.230706\n",
      "Batch 59600 of 10 sentences: Average Loss:   0.397275 Duration 0.179902\n",
      "Batch 59700 of 10 sentences: Average Loss:   0.397224 Duration 0.197267\n",
      "Batch 59800 of 10 sentences: Average Loss:   0.397195 Duration 0.158899\n",
      "Batch 59900 of 10 sentences: Average Loss:   0.397160 Duration 0.291072\n",
      "Batch 60000 of 10 sentences: Average Loss:   0.397139 Duration 0.238086\n",
      "Batch 60100 of 10 sentences: Average Loss:   0.397108 Duration 0.227398\n",
      "Batch 60200 of 10 sentences: Average Loss:   0.397080 Duration 0.162490\n",
      "Batch 60300 of 10 sentences: Average Loss:   0.397025 Duration 0.133957\n",
      "Batch 60400 of 10 sentences: Average Loss:   0.396999 Duration 0.207123\n",
      "Batch 60500 of 10 sentences: Average Loss:   0.396965 Duration 0.102534\n",
      "Batch 60600 of 10 sentences: Average Loss:   0.396928 Duration 0.179496\n",
      "Batch 60700 of 10 sentences: Average Loss:   0.396894 Duration 0.221334\n",
      "Batch 60800 of 10 sentences: Average Loss:   0.396861 Duration 0.227496\n",
      "Batch 60900 of 10 sentences: Average Loss:   0.396820 Duration 0.170862\n",
      "Batch 61000 of 10 sentences: Average Loss:   0.396777 Duration 0.133850\n",
      "Batch 61100 of 10 sentences: Average Loss:   0.396717 Duration 0.307494\n",
      "Batch 61200 of 10 sentences: Average Loss:   0.396667 Duration 0.163116\n",
      "Batch 61300 of 10 sentences: Average Loss:   0.396629 Duration 0.381085\n",
      "Batch 61400 of 10 sentences: Average Loss:   0.396594 Duration 0.216951\n",
      "Batch 61500 of 10 sentences: Average Loss:   0.396530 Duration 0.158641\n",
      "Batch 61600 of 10 sentences: Average Loss:   0.396503 Duration 0.166049\n",
      "Batch 61700 of 10 sentences: Average Loss:   0.396470 Duration 0.220634\n",
      "Batch 61800 of 10 sentences: Average Loss:   0.396441 Duration 0.200297\n",
      "Batch 61900 of 10 sentences: Average Loss:   0.396391 Duration 0.180307\n",
      "Batch 62000 of 10 sentences: Average Loss:   0.396361 Duration 0.288844\n",
      "Batch 62100 of 10 sentences: Average Loss:   0.396319 Duration 0.229673\n",
      "Batch 62200 of 10 sentences: Average Loss:   0.396280 Duration 0.182209\n",
      "Batch 62300 of 10 sentences: Average Loss:   0.396238 Duration 0.272041\n",
      "Batch 62400 of 10 sentences: Average Loss:   0.396194 Duration 0.216653\n",
      "Batch 62500 of 10 sentences: Average Loss:   0.396151 Duration 0.238765\n",
      "Batch 62600 of 10 sentences: Average Loss:   0.396124 Duration 0.114017\n",
      "Batch 62700 of 10 sentences: Average Loss:   0.396084 Duration 0.256894\n",
      "Batch 62800 of 10 sentences: Average Loss:   0.396027 Duration 0.215383\n",
      "Batch 62900 of 10 sentences: Average Loss:   0.395973 Duration 0.153419\n",
      "Batch 63000 of 10 sentences: Average Loss:   0.395928 Duration 0.275740\n",
      "Batch 63100 of 10 sentences: Average Loss:   0.395893 Duration 0.468631\n",
      "epoch 14 batches 63119 done\n",
      "Batch 63200 of 10 sentences: Average Loss:   0.395857 Duration 0.185931\n",
      "Batch 63300 of 10 sentences: Average Loss:   0.395823 Duration 0.212602\n",
      "Batch 63400 of 10 sentences: Average Loss:   0.395790 Duration 0.180462\n",
      "Batch 63500 of 10 sentences: Average Loss:   0.395735 Duration 0.214747\n",
      "Batch 63600 of 10 sentences: Average Loss:   0.395695 Duration 0.279850\n",
      "Batch 63700 of 10 sentences: Average Loss:   0.395672 Duration 0.115567\n",
      "Batch 63800 of 10 sentences: Average Loss:   0.395637 Duration 0.240855\n",
      "Batch 63900 of 10 sentences: Average Loss:   0.395589 Duration 0.243880\n",
      "Batch 64000 of 10 sentences: Average Loss:   0.395566 Duration 0.255512\n",
      "Batch 64100 of 10 sentences: Average Loss:   0.395531 Duration 0.154038\n",
      "Batch 64200 of 10 sentences: Average Loss:   0.395510 Duration 0.220331\n",
      "Batch 64300 of 10 sentences: Average Loss:   0.395483 Duration 0.257470\n",
      "Batch 64400 of 10 sentences: Average Loss:   0.395452 Duration 0.296430\n",
      "Batch 64500 of 10 sentences: Average Loss:   0.395402 Duration 0.198067\n",
      "Batch 64600 of 10 sentences: Average Loss:   0.395375 Duration 0.243554\n",
      "Batch 64700 of 10 sentences: Average Loss:   0.395347 Duration 0.138387\n",
      "Batch 64800 of 10 sentences: Average Loss:   0.395309 Duration 0.234296\n",
      "Batch 64900 of 10 sentences: Average Loss:   0.395284 Duration 0.226210\n",
      "Batch 65000 of 10 sentences: Average Loss:   0.395245 Duration 0.232242\n",
      "Batch 65100 of 10 sentences: Average Loss:   0.395208 Duration 0.243945\n",
      "Batch 65200 of 10 sentences: Average Loss:   0.395168 Duration 0.140960\n",
      "Batch 65300 of 10 sentences: Average Loss:   0.395123 Duration 0.188595\n",
      "Batch 65400 of 10 sentences: Average Loss:   0.395076 Duration 0.245593\n",
      "Batch 65500 of 10 sentences: Average Loss:   0.395034 Duration 0.182394\n",
      "Batch 65600 of 10 sentences: Average Loss:   0.395000 Duration 0.223977\n",
      "Batch 65700 of 10 sentences: Average Loss:   0.394951 Duration 0.215147\n",
      "Batch 65800 of 10 sentences: Average Loss:   0.394924 Duration 0.291825\n",
      "Batch 65900 of 10 sentences: Average Loss:   0.394892 Duration 0.234766\n",
      "Batch 66000 of 10 sentences: Average Loss:   0.394862 Duration 0.223707\n",
      "Batch 66100 of 10 sentences: Average Loss:   0.394824 Duration 0.257070\n",
      "Batch 66200 of 10 sentences: Average Loss:   0.394795 Duration 0.249416\n",
      "Batch 66300 of 10 sentences: Average Loss:   0.394758 Duration 0.219703\n",
      "Batch 66400 of 10 sentences: Average Loss:   0.394723 Duration 0.179035\n",
      "Batch 66500 of 10 sentences: Average Loss:   0.394676 Duration 0.233759\n",
      "Batch 66600 of 10 sentences: Average Loss:   0.394642 Duration 0.194865\n",
      "Batch 66700 of 10 sentences: Average Loss:   0.394597 Duration 0.240628\n",
      "Batch 66800 of 10 sentences: Average Loss:   0.394571 Duration 0.231619\n",
      "Batch 66900 of 10 sentences: Average Loss:   0.394531 Duration 0.201289\n",
      "Batch 67000 of 10 sentences: Average Loss:   0.394483 Duration 0.283810\n",
      "Batch 67100 of 10 sentences: Average Loss:   0.394431 Duration 0.170749\n",
      "Batch 67200 of 10 sentences: Average Loss:   0.394393 Duration 0.170282\n",
      "Batch 67300 of 10 sentences: Average Loss:   0.394355 Duration 0.211685\n",
      "epoch 15 batches 67327 done\n",
      "Batch 67400 of 10 sentences: Average Loss:   0.394321 Duration 0.207591\n",
      "Batch 67500 of 10 sentences: Average Loss:   0.394288 Duration 0.301119\n",
      "Batch 67600 of 10 sentences: Average Loss:   0.394264 Duration 0.202381\n",
      "Batch 67700 of 10 sentences: Average Loss:   0.394209 Duration 0.261979\n",
      "Batch 67800 of 10 sentences: Average Loss:   0.394170 Duration 0.252712\n",
      "Batch 67900 of 10 sentences: Average Loss:   0.394148 Duration 0.161373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 68000 of 10 sentences: Average Loss:   0.394116 Duration 0.262549\n",
      "Batch 68100 of 10 sentences: Average Loss:   0.394069 Duration 0.129192\n",
      "Batch 68200 of 10 sentences: Average Loss:   0.394047 Duration 0.245481\n",
      "Batch 68300 of 10 sentences: Average Loss:   0.394012 Duration 0.215628\n",
      "Batch 68400 of 10 sentences: Average Loss:   0.393990 Duration 0.202138\n",
      "Batch 68500 of 10 sentences: Average Loss:   0.393966 Duration 0.255367\n",
      "Batch 68600 of 10 sentences: Average Loss:   0.393942 Duration 0.225496\n",
      "Batch 68700 of 10 sentences: Average Loss:   0.393895 Duration 0.205198\n",
      "Batch 68800 of 10 sentences: Average Loss:   0.393864 Duration 0.233027\n",
      "Batch 68900 of 10 sentences: Average Loss:   0.393839 Duration 0.186669\n",
      "Batch 69000 of 10 sentences: Average Loss:   0.393804 Duration 0.200268\n",
      "Batch 69100 of 10 sentences: Average Loss:   0.393776 Duration 0.190370\n",
      "Batch 69200 of 10 sentences: Average Loss:   0.393740 Duration 0.159379\n",
      "Batch 69300 of 10 sentences: Average Loss:   0.393704 Duration 0.173204\n",
      "Batch 69400 of 10 sentences: Average Loss:   0.393667 Duration 0.231700\n",
      "Batch 69500 of 10 sentences: Average Loss:   0.393623 Duration 0.159950\n",
      "Batch 69600 of 10 sentences: Average Loss:   0.393578 Duration 0.191646\n",
      "Batch 69700 of 10 sentences: Average Loss:   0.393531 Duration 0.153764\n",
      "Batch 69800 of 10 sentences: Average Loss:   0.393500 Duration 0.146829\n",
      "Batch 69900 of 10 sentences: Average Loss:   0.393457 Duration 0.223496\n",
      "Batch 70000 of 10 sentences: Average Loss:   0.393420 Duration 0.215185\n",
      "Batch 70100 of 10 sentences: Average Loss:   0.393391 Duration 0.228807\n",
      "Batch 70200 of 10 sentences: Average Loss:   0.393360 Duration 0.178859\n",
      "Batch 70300 of 10 sentences: Average Loss:   0.393322 Duration 0.217169\n",
      "Batch 70400 of 10 sentences: Average Loss:   0.393295 Duration 0.334789\n",
      "Batch 70500 of 10 sentences: Average Loss:   0.393258 Duration 0.241798\n",
      "Batch 70600 of 10 sentences: Average Loss:   0.393226 Duration 0.141588\n",
      "Batch 70700 of 10 sentences: Average Loss:   0.393182 Duration 0.220852\n",
      "Batch 70800 of 10 sentences: Average Loss:   0.393156 Duration 0.205302\n",
      "Batch 70900 of 10 sentences: Average Loss:   0.393108 Duration 0.195752\n",
      "Batch 71000 of 10 sentences: Average Loss:   0.393085 Duration 0.226808\n",
      "Batch 71100 of 10 sentences: Average Loss:   0.393050 Duration 0.147415\n",
      "Batch 71200 of 10 sentences: Average Loss:   0.393003 Duration 0.236240\n",
      "Batch 71300 of 10 sentences: Average Loss:   0.392953 Duration 0.243937\n",
      "Batch 71400 of 10 sentences: Average Loss:   0.392915 Duration 0.238612\n",
      "Batch 71500 of 10 sentences: Average Loss:   0.392877 Duration 0.205565\n",
      "epoch 16 batches 71535 done\n",
      "Batch 71600 of 10 sentences: Average Loss:   0.392845 Duration 0.184937\n",
      "Batch 71700 of 10 sentences: Average Loss:   0.392816 Duration 0.290415\n",
      "Batch 71800 of 10 sentences: Average Loss:   0.392791 Duration 0.259566\n",
      "Batch 71900 of 10 sentences: Average Loss:   0.392740 Duration 0.249778\n",
      "Batch 72000 of 10 sentences: Average Loss:   0.392704 Duration 0.223638\n",
      "Batch 72100 of 10 sentences: Average Loss:   0.392681 Duration 0.174477\n",
      "Batch 72200 of 10 sentences: Average Loss:   0.392652 Duration 0.242923\n",
      "Batch 72300 of 10 sentences: Average Loss:   0.392607 Duration 0.295025\n",
      "Batch 72400 of 10 sentences: Average Loss:   0.392582 Duration 0.246569\n",
      "Batch 72500 of 10 sentences: Average Loss:   0.392555 Duration 0.269678\n",
      "Batch 72600 of 10 sentences: Average Loss:   0.392528 Duration 0.379212\n",
      "Batch 72700 of 10 sentences: Average Loss:   0.392504 Duration 0.116445\n",
      "Batch 72800 of 10 sentences: Average Loss:   0.392482 Duration 0.398526\n",
      "Batch 72900 of 10 sentences: Average Loss:   0.392444 Duration 0.290888\n",
      "Batch 73000 of 10 sentences: Average Loss:   0.392405 Duration 0.260116\n",
      "Batch 73100 of 10 sentences: Average Loss:   0.392384 Duration 0.338467\n",
      "Batch 73200 of 10 sentences: Average Loss:   0.392347 Duration 0.214828\n",
      "Batch 73300 of 10 sentences: Average Loss:   0.392325 Duration 0.156677\n",
      "Batch 73400 of 10 sentences: Average Loss:   0.392290 Duration 0.184696\n",
      "Batch 73500 of 10 sentences: Average Loss:   0.392256 Duration 0.170738\n",
      "Batch 73600 of 10 sentences: Average Loss:   0.392221 Duration 0.220351\n",
      "Batch 73700 of 10 sentences: Average Loss:   0.392184 Duration 0.207114\n",
      "Batch 73800 of 10 sentences: Average Loss:   0.392143 Duration 0.276255\n",
      "Batch 73900 of 10 sentences: Average Loss:   0.392093 Duration 0.224399\n",
      "Batch 74000 of 10 sentences: Average Loss:   0.392063 Duration 0.215358\n",
      "Batch 74100 of 10 sentences: Average Loss:   0.392023 Duration 0.198653\n",
      "Batch 74200 of 10 sentences: Average Loss:   0.391986 Duration 0.266428\n",
      "Batch 74300 of 10 sentences: Average Loss:   0.391960 Duration 0.232044\n",
      "Batch 74400 of 10 sentences: Average Loss:   0.391927 Duration 0.315609\n",
      "Batch 74500 of 10 sentences: Average Loss:   0.391892 Duration 0.150115\n",
      "Batch 74600 of 10 sentences: Average Loss:   0.391864 Duration 0.188033\n",
      "Batch 74700 of 10 sentences: Average Loss:   0.391829 Duration 0.263114\n",
      "Batch 74800 of 10 sentences: Average Loss:   0.391801 Duration 0.234721\n",
      "Batch 74900 of 10 sentences: Average Loss:   0.391756 Duration 0.214571\n",
      "Batch 75000 of 10 sentences: Average Loss:   0.391735 Duration 0.181750\n",
      "Batch 75100 of 10 sentences: Average Loss:   0.391682 Duration 0.164863\n",
      "Batch 75200 of 10 sentences: Average Loss:   0.391660 Duration 0.237522\n",
      "Batch 75300 of 10 sentences: Average Loss:   0.391628 Duration 0.268080\n",
      "Batch 75400 of 10 sentences: Average Loss:   0.391580 Duration 0.231957\n",
      "Batch 75500 of 10 sentences: Average Loss:   0.391542 Duration 0.180380\n",
      "Batch 75600 of 10 sentences: Average Loss:   0.391496 Duration 0.295672\n",
      "Batch 75700 of 10 sentences: Average Loss:   0.391463 Duration 0.199629\n",
      "epoch 17 batches 75743 done\n",
      "Batch 75800 of 10 sentences: Average Loss:   0.391435 Duration 0.261792\n",
      "Batch 75900 of 10 sentences: Average Loss:   0.391407 Duration 0.131094\n",
      "Batch 76000 of 10 sentences: Average Loss:   0.391385 Duration 0.277165\n",
      "Batch 76100 of 10 sentences: Average Loss:   0.391339 Duration 0.249400\n",
      "Batch 76200 of 10 sentences: Average Loss:   0.391301 Duration 0.276595\n",
      "Batch 76300 of 10 sentences: Average Loss:   0.391280 Duration 0.191415\n",
      "Batch 76400 of 10 sentences: Average Loss:   0.391252 Duration 0.240427\n",
      "Batch 76500 of 10 sentences: Average Loss:   0.391209 Duration 0.181545\n",
      "Batch 76600 of 10 sentences: Average Loss:   0.391187 Duration 0.335025\n",
      "Batch 76700 of 10 sentences: Average Loss:   0.391163 Duration 0.205748\n",
      "Batch 76800 of 10 sentences: Average Loss:   0.391135 Duration 0.231246\n",
      "Batch 76900 of 10 sentences: Average Loss:   0.391117 Duration 0.228921\n",
      "Batch 77000 of 10 sentences: Average Loss:   0.391096 Duration 0.212608\n",
      "Batch 77100 of 10 sentences: Average Loss:   0.391068 Duration 0.211211\n",
      "Batch 77200 of 10 sentences: Average Loss:   0.391031 Duration 0.296276\n",
      "Batch 77300 of 10 sentences: Average Loss:   0.391010 Duration 0.171354\n",
      "Batch 77400 of 10 sentences: Average Loss:   0.390976 Duration 0.378229\n",
      "Batch 77500 of 10 sentences: Average Loss:   0.390952 Duration 0.201586\n",
      "Batch 77600 of 10 sentences: Average Loss:   0.390919 Duration 0.252488\n",
      "Batch 77700 of 10 sentences: Average Loss:   0.390885 Duration 0.267100\n",
      "Batch 77800 of 10 sentences: Average Loss:   0.390849 Duration 0.182064\n",
      "Batch 77900 of 10 sentences: Average Loss:   0.390813 Duration 0.224469\n",
      "Batch 78000 of 10 sentences: Average Loss:   0.390778 Duration 0.270428\n",
      "Batch 78100 of 10 sentences: Average Loss:   0.390731 Duration 0.276768\n",
      "Batch 78200 of 10 sentences: Average Loss:   0.390705 Duration 0.259562\n",
      "Batch 78300 of 10 sentences: Average Loss:   0.390669 Duration 0.239963\n",
      "Batch 78400 of 10 sentences: Average Loss:   0.390635 Duration 0.215822\n",
      "Batch 78500 of 10 sentences: Average Loss:   0.390610 Duration 0.295121\n",
      "Batch 78600 of 10 sentences: Average Loss:   0.390580 Duration 0.256169\n",
      "Batch 78700 of 10 sentences: Average Loss:   0.390548 Duration 0.238478\n",
      "Batch 78800 of 10 sentences: Average Loss:   0.390518 Duration 0.131094\n",
      "Batch 78900 of 10 sentences: Average Loss:   0.390487 Duration 0.129371\n",
      "Batch 79000 of 10 sentences: Average Loss:   0.390462 Duration 0.255053\n",
      "Batch 79100 of 10 sentences: Average Loss:   0.390419 Duration 0.221904\n",
      "Batch 79200 of 10 sentences: Average Loss:   0.390398 Duration 0.210335\n",
      "Batch 79300 of 10 sentences: Average Loss:   0.390351 Duration 0.175005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 79400 of 10 sentences: Average Loss:   0.390331 Duration 0.228678\n",
      "Batch 79500 of 10 sentences: Average Loss:   0.390301 Duration 0.248209\n",
      "Batch 79600 of 10 sentences: Average Loss:   0.390259 Duration 0.160702\n",
      "Batch 79700 of 10 sentences: Average Loss:   0.390220 Duration 0.233731\n",
      "Batch 79800 of 10 sentences: Average Loss:   0.390174 Duration 0.223936\n",
      "Batch 79900 of 10 sentences: Average Loss:   0.390145 Duration 0.236949\n",
      "epoch 18 batches 79951 done\n",
      "Batch 80000 of 10 sentences: Average Loss:   0.390116 Duration 0.267089\n",
      "Batch 80100 of 10 sentences: Average Loss:   0.390089 Duration 0.236761\n",
      "Batch 80200 of 10 sentences: Average Loss:   0.390070 Duration 0.223507\n",
      "Batch 80300 of 10 sentences: Average Loss:   0.390027 Duration 0.189871\n",
      "Batch 80400 of 10 sentences: Average Loss:   0.389990 Duration 0.207526\n",
      "Batch 80500 of 10 sentences: Average Loss:   0.389970 Duration 0.199607\n",
      "Batch 80600 of 10 sentences: Average Loss:   0.389946 Duration 0.207347\n",
      "Batch 80700 of 10 sentences: Average Loss:   0.389906 Duration 0.096062\n",
      "Batch 80800 of 10 sentences: Average Loss:   0.389883 Duration 0.241543\n",
      "Batch 80900 of 10 sentences: Average Loss:   0.389860 Duration 0.168200\n",
      "Batch 81000 of 10 sentences: Average Loss:   0.389833 Duration 0.243871\n",
      "Batch 81100 of 10 sentences: Average Loss:   0.389817 Duration 0.162840\n",
      "Batch 81200 of 10 sentences: Average Loss:   0.389796 Duration 0.164750\n",
      "Batch 81300 of 10 sentences: Average Loss:   0.389767 Duration 0.186784\n",
      "Batch 81400 of 10 sentences: Average Loss:   0.389732 Duration 0.206195\n",
      "Batch 81500 of 10 sentences: Average Loss:   0.389713 Duration 0.218114\n",
      "Batch 81600 of 10 sentences: Average Loss:   0.389678 Duration 0.155686\n",
      "Batch 81700 of 10 sentences: Average Loss:   0.389657 Duration 0.219510\n",
      "Batch 81800 of 10 sentences: Average Loss:   0.389625 Duration 0.250659\n",
      "Batch 81900 of 10 sentences: Average Loss:   0.389597 Duration 0.171746\n",
      "Batch 82000 of 10 sentences: Average Loss:   0.389564 Duration 0.112241\n",
      "Batch 82100 of 10 sentences: Average Loss:   0.389529 Duration 0.152920\n",
      "Batch 82200 of 10 sentences: Average Loss:   0.389496 Duration 0.237331\n",
      "Batch 82300 of 10 sentences: Average Loss:   0.389450 Duration 0.195315\n",
      "Batch 82400 of 10 sentences: Average Loss:   0.389424 Duration 0.234811\n",
      "Batch 82500 of 10 sentences: Average Loss:   0.389388 Duration 0.179744\n",
      "Batch 82600 of 10 sentences: Average Loss:   0.389353 Duration 0.249580\n",
      "Batch 82700 of 10 sentences: Average Loss:   0.389329 Duration 0.173692\n",
      "Batch 82800 of 10 sentences: Average Loss:   0.389300 Duration 0.180224\n",
      "Batch 82900 of 10 sentences: Average Loss:   0.389273 Duration 0.251547\n",
      "Batch 83000 of 10 sentences: Average Loss:   0.389246 Duration 0.265320\n",
      "Batch 83100 of 10 sentences: Average Loss:   0.389216 Duration 0.225362\n",
      "Batch 83200 of 10 sentences: Average Loss:   0.389188 Duration 0.230271\n",
      "Batch 83300 of 10 sentences: Average Loss:   0.389154 Duration 0.097608\n",
      "Batch 83400 of 10 sentences: Average Loss:   0.389131 Duration 0.166180\n",
      "Batch 83500 of 10 sentences: Average Loss:   0.389083 Duration 0.173795\n",
      "Batch 83600 of 10 sentences: Average Loss:   0.389063 Duration 0.217579\n",
      "Batch 83700 of 10 sentences: Average Loss:   0.389032 Duration 0.200432\n",
      "Batch 83800 of 10 sentences: Average Loss:   0.388990 Duration 0.201718\n",
      "Batch 83900 of 10 sentences: Average Loss:   0.388954 Duration 0.195455\n",
      "Batch 84000 of 10 sentences: Average Loss:   0.388912 Duration 0.229829\n",
      "Batch 84100 of 10 sentences: Average Loss:   0.388881 Duration 0.213331\n",
      "epoch 19 batches 84159 done\n",
      "Batch 84200 of 10 sentences: Average Loss:   0.388855 Duration 0.178569\n",
      "Batch 84300 of 10 sentences: Average Loss:   0.388827 Duration 0.230778\n",
      "Batch 84400 of 10 sentences: Average Loss:   0.388808 Duration 0.198675\n",
      "Batch 84500 of 10 sentences: Average Loss:   0.388772 Duration 0.228614\n",
      "Batch 84600 of 10 sentences: Average Loss:   0.388735 Duration 0.165631\n",
      "Batch 84700 of 10 sentences: Average Loss:   0.388713 Duration 0.213931\n",
      "Batch 84800 of 10 sentences: Average Loss:   0.388692 Duration 0.251362\n",
      "Batch 84900 of 10 sentences: Average Loss:   0.388656 Duration 0.187835\n",
      "Batch 85000 of 10 sentences: Average Loss:   0.388626 Duration 0.196232\n",
      "Batch 85100 of 10 sentences: Average Loss:   0.388604 Duration 0.260204\n",
      "Batch 85200 of 10 sentences: Average Loss:   0.388578 Duration 0.227032\n",
      "Batch 85300 of 10 sentences: Average Loss:   0.388563 Duration 0.194968\n",
      "Batch 85400 of 10 sentences: Average Loss:   0.388544 Duration 0.224976\n",
      "Batch 85500 of 10 sentences: Average Loss:   0.388513 Duration 0.226216\n",
      "Batch 85600 of 10 sentences: Average Loss:   0.388479 Duration 0.113439\n",
      "Batch 85700 of 10 sentences: Average Loss:   0.388458 Duration 0.225901\n",
      "Batch 85800 of 10 sentences: Average Loss:   0.388428 Duration 0.196720\n",
      "Batch 85900 of 10 sentences: Average Loss:   0.388406 Duration 0.151366\n",
      "Batch 86000 of 10 sentences: Average Loss:   0.388376 Duration 0.200677\n",
      "Batch 86100 of 10 sentences: Average Loss:   0.388350 Duration 0.189455\n",
      "Batch 86200 of 10 sentences: Average Loss:   0.388320 Duration 0.167824\n",
      "Batch 86300 of 10 sentences: Average Loss:   0.388284 Duration 0.255759\n",
      "Batch 86400 of 10 sentences: Average Loss:   0.388254 Duration 0.146681\n",
      "Batch 86500 of 10 sentences: Average Loss:   0.388209 Duration 0.215051\n",
      "Batch 86600 of 10 sentences: Average Loss:   0.388184 Duration 0.115298\n",
      "Batch 86700 of 10 sentences: Average Loss:   0.388152 Duration 0.153668\n",
      "Batch 86800 of 10 sentences: Average Loss:   0.388120 Duration 0.259908\n",
      "Batch 86900 of 10 sentences: Average Loss:   0.388096 Duration 0.210982\n",
      "Batch 87000 of 10 sentences: Average Loss:   0.388070 Duration 0.202579\n",
      "Batch 87100 of 10 sentences: Average Loss:   0.388043 Duration 0.171832\n",
      "Batch 87200 of 10 sentences: Average Loss:   0.388014 Duration 0.156754\n",
      "Batch 87300 of 10 sentences: Average Loss:   0.387988 Duration 0.275692\n",
      "Batch 87400 of 10 sentences: Average Loss:   0.387960 Duration 0.224955\n",
      "Batch 87500 of 10 sentences: Average Loss:   0.387928 Duration 0.242096\n",
      "Batch 87600 of 10 sentences: Average Loss:   0.387905 Duration 0.221705\n",
      "Batch 87700 of 10 sentences: Average Loss:   0.387863 Duration 0.073178\n",
      "Batch 87800 of 10 sentences: Average Loss:   0.387841 Duration 0.209020\n",
      "Batch 87900 of 10 sentences: Average Loss:   0.387816 Duration 0.243561\n",
      "Batch 88000 of 10 sentences: Average Loss:   0.387774 Duration 0.256245\n",
      "Batch 88100 of 10 sentences: Average Loss:   0.387737 Duration 0.191231\n",
      "Batch 88200 of 10 sentences: Average Loss:   0.387698 Duration 0.229424\n",
      "Batch 88300 of 10 sentences: Average Loss:   0.387668 Duration 0.186786\n",
      "epoch 20 batches 88367 done\n",
      "Batch 88400 of 10 sentences: Average Loss:   0.387646 Duration 0.300695\n",
      "Batch 88500 of 10 sentences: Average Loss:   0.387618 Duration 0.165974\n",
      "Batch 88600 of 10 sentences: Average Loss:   0.387594 Duration 0.214025\n",
      "Batch 88700 of 10 sentences: Average Loss:   0.387562 Duration 0.214747\n",
      "Batch 88800 of 10 sentences: Average Loss:   0.387530 Duration 0.167981\n",
      "Batch 88900 of 10 sentences: Average Loss:   0.387508 Duration 0.179652\n",
      "Batch 89000 of 10 sentences: Average Loss:   0.387491 Duration 0.186804\n",
      "Batch 89100 of 10 sentences: Average Loss:   0.387457 Duration 0.122843\n",
      "Batch 89200 of 10 sentences: Average Loss:   0.387424 Duration 0.285965\n",
      "Batch 89300 of 10 sentences: Average Loss:   0.387407 Duration 0.220186\n",
      "Batch 89400 of 10 sentences: Average Loss:   0.387381 Duration 0.169585\n",
      "Batch 89500 of 10 sentences: Average Loss:   0.387370 Duration 0.191422\n",
      "Batch 89600 of 10 sentences: Average Loss:   0.387349 Duration 0.177057\n",
      "Batch 89700 of 10 sentences: Average Loss:   0.387322 Duration 0.135892\n",
      "Batch 89800 of 10 sentences: Average Loss:   0.387290 Duration 0.208562\n",
      "Batch 89900 of 10 sentences: Average Loss:   0.387270 Duration 0.171776\n",
      "Batch 90000 of 10 sentences: Average Loss:   0.387245 Duration 0.182015\n",
      "Batch 90100 of 10 sentences: Average Loss:   0.387224 Duration 0.178989\n",
      "Batch 90200 of 10 sentences: Average Loss:   0.387200 Duration 0.247666\n",
      "Batch 90300 of 10 sentences: Average Loss:   0.387170 Duration 0.216212\n",
      "Batch 90400 of 10 sentences: Average Loss:   0.387140 Duration 0.256188\n",
      "Batch 90500 of 10 sentences: Average Loss:   0.387105 Duration 0.141274\n",
      "Batch 90600 of 10 sentences: Average Loss:   0.387071 Duration 0.223659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 90700 of 10 sentences: Average Loss:   0.387026 Duration 0.239290\n",
      "Batch 90800 of 10 sentences: Average Loss:   0.387004 Duration 0.130527\n",
      "Batch 90900 of 10 sentences: Average Loss:   0.386973 Duration 0.200252\n",
      "Batch 91000 of 10 sentences: Average Loss:   0.386939 Duration 0.204631\n",
      "Batch 91100 of 10 sentences: Average Loss:   0.386917 Duration 0.152196\n",
      "Batch 91200 of 10 sentences: Average Loss:   0.386894 Duration 0.153647\n",
      "Batch 91300 of 10 sentences: Average Loss:   0.386866 Duration 0.167943\n",
      "Batch 91400 of 10 sentences: Average Loss:   0.386839 Duration 0.222885\n",
      "Batch 91500 of 10 sentences: Average Loss:   0.386815 Duration 0.185552\n",
      "Batch 91600 of 10 sentences: Average Loss:   0.386788 Duration 0.210121\n",
      "Batch 91700 of 10 sentences: Average Loss:   0.386758 Duration 0.230412\n",
      "Batch 91800 of 10 sentences: Average Loss:   0.386736 Duration 0.184718\n",
      "Batch 91900 of 10 sentences: Average Loss:   0.386701 Duration 0.175697\n",
      "Batch 92000 of 10 sentences: Average Loss:   0.386673 Duration 0.271745\n",
      "Batch 92100 of 10 sentences: Average Loss:   0.386654 Duration 0.196613\n",
      "Batch 92200 of 10 sentences: Average Loss:   0.386615 Duration 0.216370\n",
      "Batch 92300 of 10 sentences: Average Loss:   0.386579 Duration 0.217048\n",
      "Batch 92400 of 10 sentences: Average Loss:   0.386544 Duration 0.211084\n",
      "Batch 92500 of 10 sentences: Average Loss:   0.386513 Duration 0.273960\n",
      "epoch 21 batches 92575 done\n",
      "Batch 92600 of 10 sentences: Average Loss:   0.386489 Duration 0.164652\n",
      "Batch 92700 of 10 sentences: Average Loss:   0.386466 Duration 0.242447\n",
      "Batch 92800 of 10 sentences: Average Loss:   0.386442 Duration 0.185777\n",
      "Batch 92900 of 10 sentences: Average Loss:   0.386411 Duration 0.168260\n",
      "Batch 93000 of 10 sentences: Average Loss:   0.386380 Duration 0.205309\n",
      "Batch 93100 of 10 sentences: Average Loss:   0.386356 Duration 0.176106\n",
      "Batch 93200 of 10 sentences: Average Loss:   0.386339 Duration 0.254261\n",
      "Batch 93300 of 10 sentences: Average Loss:   0.386309 Duration 0.197161\n",
      "Batch 93400 of 10 sentences: Average Loss:   0.386278 Duration 0.129583\n",
      "Batch 93500 of 10 sentences: Average Loss:   0.386260 Duration 0.203668\n",
      "Batch 93600 of 10 sentences: Average Loss:   0.386236 Duration 0.208200\n",
      "Batch 93700 of 10 sentences: Average Loss:   0.386222 Duration 0.247430\n",
      "Batch 93800 of 10 sentences: Average Loss:   0.386203 Duration 0.224040\n",
      "Batch 93900 of 10 sentences: Average Loss:   0.386174 Duration 0.130779\n",
      "Batch 94000 of 10 sentences: Average Loss:   0.386144 Duration 0.144633\n",
      "Batch 94100 of 10 sentences: Average Loss:   0.386124 Duration 0.120430\n",
      "Batch 94200 of 10 sentences: Average Loss:   0.386100 Duration 0.234017\n",
      "Batch 94300 of 10 sentences: Average Loss:   0.386077 Duration 0.263193\n",
      "Batch 94400 of 10 sentences: Average Loss:   0.386054 Duration 0.177464\n",
      "Batch 94500 of 10 sentences: Average Loss:   0.386030 Duration 0.177042\n",
      "Batch 94600 of 10 sentences: Average Loss:   0.385999 Duration 0.210305\n",
      "Batch 94700 of 10 sentences: Average Loss:   0.385970 Duration 0.086382\n",
      "Batch 94800 of 10 sentences: Average Loss:   0.385933 Duration 0.238536\n",
      "Batch 94900 of 10 sentences: Average Loss:   0.385892 Duration 0.198975\n",
      "Batch 95000 of 10 sentences: Average Loss:   0.385874 Duration 0.194000\n",
      "Batch 95100 of 10 sentences: Average Loss:   0.385840 Duration 0.199158\n",
      "Batch 95200 of 10 sentences: Average Loss:   0.385808 Duration 0.239056\n",
      "Batch 95300 of 10 sentences: Average Loss:   0.385786 Duration 0.313771\n",
      "Batch 95400 of 10 sentences: Average Loss:   0.385764 Duration 0.190425\n",
      "Batch 95500 of 10 sentences: Average Loss:   0.385738 Duration 0.170658\n",
      "Batch 95600 of 10 sentences: Average Loss:   0.385712 Duration 0.162406\n",
      "Batch 95700 of 10 sentences: Average Loss:   0.385691 Duration 0.353731\n",
      "Batch 95800 of 10 sentences: Average Loss:   0.385661 Duration 0.201372\n",
      "Batch 95900 of 10 sentences: Average Loss:   0.385632 Duration 0.247401\n",
      "Batch 96000 of 10 sentences: Average Loss:   0.385611 Duration 0.203556\n",
      "Batch 96100 of 10 sentences: Average Loss:   0.385578 Duration 0.172160\n",
      "Batch 96200 of 10 sentences: Average Loss:   0.385549 Duration 0.234968\n",
      "Batch 96300 of 10 sentences: Average Loss:   0.385531 Duration 0.207598\n",
      "Batch 96400 of 10 sentences: Average Loss:   0.385495 Duration 0.165481\n",
      "Batch 96500 of 10 sentences: Average Loss:   0.385462 Duration 0.133382\n",
      "Batch 96600 of 10 sentences: Average Loss:   0.385424 Duration 0.237137\n",
      "Batch 96700 of 10 sentences: Average Loss:   0.385396 Duration 0.203097\n",
      "epoch 22 batches 96783 done\n",
      "Batch 96800 of 10 sentences: Average Loss:   0.385370 Duration 0.214212\n",
      "Batch 96900 of 10 sentences: Average Loss:   0.385350 Duration 0.132664\n",
      "Batch 97000 of 10 sentences: Average Loss:   0.385329 Duration 0.147969\n",
      "Batch 97100 of 10 sentences: Average Loss:   0.385298 Duration 0.185246\n",
      "Batch 97200 of 10 sentences: Average Loss:   0.385270 Duration 0.129386\n",
      "Batch 97300 of 10 sentences: Average Loss:   0.385244 Duration 0.222609\n",
      "Batch 97400 of 10 sentences: Average Loss:   0.385225 Duration 0.134871\n",
      "Batch 97500 of 10 sentences: Average Loss:   0.385198 Duration 0.235090\n",
      "Batch 97600 of 10 sentences: Average Loss:   0.385168 Duration 0.222880\n",
      "Batch 97700 of 10 sentences: Average Loss:   0.385148 Duration 0.220660\n",
      "Batch 97800 of 10 sentences: Average Loss:   0.385128 Duration 0.188970\n",
      "Batch 97900 of 10 sentences: Average Loss:   0.385116 Duration 0.167803\n",
      "Batch 98000 of 10 sentences: Average Loss:   0.385095 Duration 0.242773\n",
      "Batch 98100 of 10 sentences: Average Loss:   0.385070 Duration 0.075688\n",
      "Batch 98200 of 10 sentences: Average Loss:   0.385043 Duration 0.215417\n",
      "Batch 98300 of 10 sentences: Average Loss:   0.385024 Duration 0.270417\n",
      "Batch 98400 of 10 sentences: Average Loss:   0.384998 Duration 0.218638\n",
      "Batch 98500 of 10 sentences: Average Loss:   0.384977 Duration 0.199327\n",
      "Batch 98600 of 10 sentences: Average Loss:   0.384956 Duration 0.274704\n",
      "Batch 98700 of 10 sentences: Average Loss:   0.384929 Duration 0.206388\n",
      "Batch 98800 of 10 sentences: Average Loss:   0.384897 Duration 0.186982\n",
      "Batch 98900 of 10 sentences: Average Loss:   0.384874 Duration 0.223594\n",
      "Batch 99000 of 10 sentences: Average Loss:   0.384836 Duration 0.278007\n",
      "Batch 99100 of 10 sentences: Average Loss:   0.384800 Duration 0.200428\n",
      "Batch 99200 of 10 sentences: Average Loss:   0.384779 Duration 0.192201\n",
      "Batch 99300 of 10 sentences: Average Loss:   0.384748 Duration 0.206973\n",
      "Batch 99400 of 10 sentences: Average Loss:   0.384718 Duration 0.180832\n",
      "Batch 99500 of 10 sentences: Average Loss:   0.384694 Duration 0.216173\n",
      "Batch 99600 of 10 sentences: Average Loss:   0.384672 Duration 0.172513\n",
      "Batch 99700 of 10 sentences: Average Loss:   0.384648 Duration 0.205153\n",
      "Batch 99800 of 10 sentences: Average Loss:   0.384625 Duration 0.225299\n",
      "Batch 99900 of 10 sentences: Average Loss:   0.384606 Duration 0.180933\n",
      "         12400641529 function calls (11943488958 primitive calls) in 18004.422 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        5    0.000    0.000 18004.418 3600.884 interactiveshell.py:3400(run_code)\n",
      "        5    0.000    0.000 18004.418 3600.884 {built-in method builtins.exec}\n",
      "        1    1.311    1.311 18004.395 18004.395 <ipython-input-33-0580c1f4e1b7>:25(<module>)\n",
      "    99977    4.313    0.000 17977.634    0.180 base.py:258(train)\n",
      "299931/199954    3.329    0.000 15385.145    0.077 function.py:1112(_)\n",
      "    99977    0.617    0.000 9093.398    0.091 base.py:230(gradient)\n",
      "    99977   41.067    0.000 8421.393    0.084 embedding_sgram.py:811(gradient)\n",
      "    99977    0.609    0.000 6310.781    0.063 base.py:211(function)\n",
      "    99977   16.602    0.000 5880.191    0.059 embedding_sgram.py:636(function)\n",
      "   499885    2.714    0.000 5609.759    0.011 arrayprint.py:1500(_array_str_implementation)\n",
      "   499885    4.333    0.000 5607.045    0.011 arrayprint.py:516(array2string)\n",
      "   499885    4.291    0.000 5595.687    0.011 arrayprint.py:461(wrapper)\n",
      "   499885    7.585    0.000 5589.692    0.011 arrayprint.py:478(_array2string)\n",
      "    99977   93.713    0.001 5557.408    0.056 embedding_sgram.py:760(<listcomp>)\n",
      " 14715076  450.012    0.000 5355.482    0.000 preprocess.py:283(negative_sample_indices)\n",
      " 14715076  357.882    0.000 4409.902    0.000 preprocess.py:254(sample)\n",
      " 14715076 2202.204    0.000 4020.349    0.000 {method 'choice' of 'numpy.random.mtrand.RandomState' objects}\n",
      "   499885    1.730    0.000 3637.602    0.007 arrayprint.py:736(_formatArray)\n",
      "452953341/499885  961.714    0.000 3635.872    0.007 arrayprint.py:745(recurser)\n",
      "8398068/5998620   20.827    0.000 3611.244    0.001 dispatch.py:203(wrapper)\n",
      "  8398068 3014.490    0.000 3014.490    0.000 {built-in method tensorflow.python._pywrap_tfe.TFE_Py_FastPathExecute}\n",
      "   499885   19.832    0.000 2836.442    0.006 base.py:239(dX)\n",
      "   699839    4.440    0.000 2488.294    0.004 tf.py:206(einsum)\n",
      "   699839    3.920    0.000 2481.497    0.004 special_math_ops.py:606(einsum)\n",
      "   699839   22.454    0.000 2477.577    0.004 special_math_ops.py:1152(_einsum_v2)\n",
      "   699839    7.030    0.000 2393.171    0.003 gen_linalg_ops.py:979(einsum)\n",
      "    99977    1.555    0.000 2357.577    0.024 base.py:239(update)\n",
      "    99977    0.461    0.000 2355.649    0.024 composite.py:207(update)\n",
      "    99977    0.776    0.000 2355.052    0.024 composite.py:214(<listcomp>)\n",
      "    99977    8.691    0.000 2351.918    0.024 embedding_sgram.py:1115(update)\n",
      "   299931    8.202    0.000 2301.284    0.008 embedding_sgram.py:1101(_gradient_descent)\n",
      "   299931 2012.744    0.007 2012.744    0.007 {method 'at' of 'numpy.ufunc' objects}\n",
      "   499885    5.363    0.000 1943.181    0.004 arrayprint.py:409(_get_format_function)\n",
      "   499885    1.940    0.000 1934.008    0.004 arrayprint.py:366(<lambda>)\n",
      "   499885   17.331    0.000 1932.069    0.004 arrayprint.py:863(__init__)\n",
      "   499885  445.608    0.001 1914.238    0.004 arrayprint.py:890(fillFormat)\n",
      "422023580 1164.864    0.000 1790.315    0.000 arrayprint.py:974(__call__)\n",
      "75673499/73474097  112.298    0.000 1766.549    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      " 17672588   17.755    0.000  836.170    0.000 <__array_function__ internals>:2(cumsum)\n",
      "422023580  299.311    0.000  820.014    0.000 arrayprint.py:709(_extendLine_pretty)\n",
      " 17672588   21.332    0.000  799.556    0.000 fromnumeric.py:2446(cumsum)\n",
      " 20071990   19.244    0.000  790.441    0.000 fromnumeric.py:52(_wrapfunc)\n",
      " 17672588  756.948    0.000  756.948    0.000 {method 'cumsum' of 'numpy.ndarray' objects}\n",
      "441263010  570.935    0.000  570.935    0.000 {built-in method numpy.core._multiarray_umath.dragon4_scientific}\n",
      "201792027   81.562    0.000  511.652    0.000 arrayprint.py:945(<genexpr>)\n",
      "402784150  511.328    0.000  511.328    0.000 {built-in method numpy.core._multiarray_umath.dragon4_positional}\n",
      "    99977    0.870    0.000  507.141    0.005 preprocess.py:655(gradient)\n",
      "  2299471   14.191    0.000  486.666    0.000 tf.py:54(is_finite)\n",
      " 17672588   22.359    0.000  484.482    0.000 <__array_function__ internals>:2(unique)\n",
      "220731438   76.693    0.000  461.115    0.000 arrayprint.py:918(<genexpr>)\n",
      " 29430152   22.918    0.000  450.403    0.000 preprocess.py:233(list_indices)\n",
      " 17672588   42.963    0.000  433.373    0.000 arraysetops.py:138(unique)\n",
      " 29430152  166.830    0.000  427.485    0.000 preprocess.py:238(<listcomp>)\n",
      "422023580  313.447    0.000  421.183    0.000 arrayprint.py:695(_extendLine)\n",
      " 17672588  237.566    0.000  356.367    0.000 arraysetops.py:310(_unique1d)\n",
      " 33129165  118.960    0.000  355.146    0.000 {built-in method builtins.max}\n",
      " 15015007   27.465    0.000  332.743    0.000 <__array_function__ internals>:2(prod)\n",
      "220731438   81.252    0.000  325.309    0.000 arrayprint.py:915(<genexpr>)\n",
      "   999770    5.582    0.000  305.620    0.000 tf.py:180(multiply)\n",
      "201792027   90.433    0.000  303.187    0.000 arrayprint.py:940(<genexpr>)\n",
      "   999770    3.597    0.000  296.660    0.000 math_ops.py:482(multiply)\n",
      "   999770   11.173    0.000  293.063    0.000 gen_math_ops.py:6218(mul)\n",
      "  2299471   14.546    0.000  289.622    0.000 math_ops.py:2981(reduce_all)\n",
      "3854196315  276.528    0.000  276.528    0.000 {built-in method builtins.len}\n",
      " 15015007   33.898    0.000  260.261    0.000 fromnumeric.py:2912(prod)\n",
      "  1399678    9.011    0.000  255.308    0.000 base.py:271(Y)\n",
      " 16614639   72.180    0.000  252.841    0.000 fromnumeric.py:70(_wrapreduction)\n",
      "423823168  236.805    0.000  236.805    0.000 {method 'split' of 'str' objects}\n",
      "490543988  230.576    0.000  230.576    0.000 {method 'get' of 'dict' objects}\n",
      "    99977    4.729    0.000  226.853    0.002 preprocess.py:550(function)\n",
      "   599862    2.639    0.000  215.471    0.000 tf.py:35(ones)\n",
      "   599862   21.338    0.000  208.193    0.000 array_ops.py:3173(ones)\n",
      "  1999540    7.807    0.000  202.647    0.000 base.py:232(reshape)\n",
      "    99977    4.392    0.000  198.574    0.002 preprocess.py:601(<listcomp>)\n",
      "   999724   38.949    0.000  193.510    0.000 utility.py:43(event_context_pairs)\n",
      "  2299471   21.664    0.000  178.785    0.000 gen_math_ops.py:4597(is_finite)\n",
      "  5798666    5.684    0.000  175.443    0.000 constant_op.py:166(constant)\n",
      "   699839    5.183    0.000  171.196    0.000 array_ops.py:200(fill)\n",
      "   599862    3.358    0.000  170.790    0.000 array_ops.py:59(reshape)\n",
      "  5798666   19.795    0.000  169.759    0.000 constant_op.py:268(_constant_impl)\n",
      "   599862   10.175    0.000  163.287    0.000 gen_array_ops.py:8308(reshape)\n",
      "   699839    6.276    0.000  159.952    0.000 gen_array_ops.py:3324(fill)\n",
      "  2299471   15.152    0.000  156.672    0.000 math_ops.py:1943(_ReductionDims)\n",
      "  3099287    4.558    0.000  154.018    0.000 trace.py:158(wrapped)\n",
      "  1699609   16.118    0.000  153.000    0.000 execute.py:236(args_to_matching_eager)\n",
      "  3099287   18.451    0.000  149.459    0.000 ops.py:1507(convert_to_tensor)\n",
      "220731438   39.807    0.000  147.410    0.000 arrayprint.py:919(<genexpr>)\n",
      "    99977    2.020    0.000  138.400    0.001 preprocess.py:341(function)\n",
      "  5798666    8.790    0.000  134.861    0.000 constant_op.py:299(_constant_eager_impl)\n",
      "   299931    3.483    0.000  126.435    0.000 sgd.py:78(differential)\n",
      "  5798666  106.942    0.000  126.071    0.000 constant_op.py:70(convert_to_eager_tensor)\n",
      " 14715076   35.550    0.000  125.446    0.000 {method 'prod' of 'numpy.generic' objects}\n",
      "   599862    6.381    0.000  122.582    0.000 gen_array_ops.py:8412(reshape_eager_fallback)\n",
      "    99977    1.151    0.000  122.135    0.001 preprocess.py:304(sentence_to_sequence)\n",
      "    99977    5.782    0.000  120.385    0.001 text.py:218(sentence_to_sequence)\n",
      "    99977    3.626    0.000  115.663    0.001 objective.py:175(function)\n",
      "  2299471    6.947    0.000  114.300    0.000 gen_math_ops.py:560(_all)\n",
      " 21613489   49.570    0.000  109.264    0.000 numerictypes.py:359(issubdtype)\n",
      " 16615639  108.247    0.000  108.247    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "   999770    3.965    0.000   99.571    0.000 execute.py:33(quick_execute)\n",
      " 14715076   40.534    0.000   99.370    0.000 base.py:217(to_flat_list)\n",
      "  7591829   29.292    0.000   96.538    0.000 base.py:119(tensor_shape)\n",
      "   999724   94.438    0.000   94.438    0.000 utility.py:84(<listcomp>)\n",
      " 45243531   91.996    0.000   91.996    0.000 {built-in method numpy.array}\n",
      "   999770   91.855    0.000   91.855    0.000 {built-in method tensorflow.python._pywrap_tfe.TFE_Py_Execute}\n",
      "   299931    4.554    0.000   91.321    0.000 gen_math_ops.py:6263(mul_eager_fallback)\n",
      " 14715076    8.918    0.000   89.895    0.000 _methods.py:49(_prod)\n",
      "   299931    1.420    0.000   86.393    0.000 base.py:308(dY)\n",
      "    99977    0.715    0.000   69.063    0.001 tf.py:47(full)\n",
      " 29430152   55.458    0.000   67.523    0.000 preprocess.py:114(vocabulary_size)\n",
      " 14715076   16.558    0.000   66.981    0.000 <__array_function__ internals>:2(count_nonzero)\n",
      "   699839   11.759    0.000   64.387    0.000 base.py:205(X)\n",
      "    99977    0.313    0.000   64.108    0.001 embedding_sgram.py:621(_bagging)\n",
      "    99977    0.918    0.000   63.590    0.001 preprocess.py:380(gradient)\n",
      "    99977    3.233    0.000   63.458    0.001 text.py:266(<listcomp>)\n",
      " 35403846   43.019    0.000   62.804    0.000 tensor_util.py:1031(is_tf_type)\n",
      "422023580   62.797    0.000   62.797    0.000 {method 'splitlines' of 'str' objects}\n",
      "220631505   59.114    0.000   59.114    0.000 {method 'partition' of 'str' objects}\n",
      "   699839    0.812    0.000   57.453    0.000 constant_op.py:336(_constant_tensor_conversion_function)\n",
      "   999724    1.425    0.000   57.404    0.000 <__array_function__ internals>:2(pad)\n",
      "  2499425    5.915    0.000   57.366    0.000 base.py:142(tensor_size)\n",
      " 17672588   57.239    0.000   57.239    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
      "    99977    1.262    0.000   53.874    0.001 gen_array_ops.py:3392(fill_eager_fallback)\n",
      "   999724   10.660    0.000   53.519    0.000 arraypad.py:529(pad)\n",
      "  7198344   16.717    0.000   52.109    0.000 ops.py:1191(shape)\n",
      "    99977    0.647    0.000   51.778    0.001 adapter.py:69(gradient)\n",
      " 43226978   31.659    0.000   50.740    0.000 numerictypes.py:285(issubclass_)\n",
      " 35346176   14.917    0.000   50.242    0.000 _asarray.py:110(asanyarray)\n",
      "220731438   35.476    0.000   49.257    0.000 arrayprint.py:920(<genexpr>)\n",
      "438352673   48.777    0.000   48.777    0.000 preprocess.py:129(event_to_index)\n",
      " 29430152   39.560    0.000   47.987    0.000 getlimits.py:382(__new__)\n",
      "    99977    0.788    0.000   47.755    0.000 adapter.py:64(function)\n",
      "220731438   34.377    0.000   47.681    0.000 arrayprint.py:923(<genexpr>)\n",
      "201792027   33.613    0.000   47.004    0.000 arrayprint.py:949(<genexpr>)\n",
      "201792027   33.229    0.000   46.370    0.000 arrayprint.py:950(<genexpr>)\n",
      "220731438   33.100    0.000   45.874    0.000 arrayprint.py:930(<genexpr>)\n",
      "    99977    7.626    0.000   44.715    0.000 objective.py:251(gradient)\n",
      "   299931    0.483    0.000   44.572    0.000 array_ops.py:701(size_v2)\n",
      "   299931    0.511    0.000   43.822    0.000 array_ops.py:735(size)\n",
      "   299931    3.592    0.000   43.311    0.000 array_ops.py:767(size_internal)\n",
      "   999724   13.136    0.000   42.585    0.000 index_tricks.py:317(__getitem__)\n",
      "167500961   37.544    0.000   40.006    0.000 {built-in method builtins.isinstance}\n",
      "   299931   37.150    0.000   38.698    0.000 embedding_sgram.py:597(_extract_event_vectors)\n",
      "   699839   15.363    0.000   35.237    0.000 special_math_ops.py:1226(_einsum_v2_parse_and_resolve_equation)\n",
      " 19165774   35.064    0.000   35.064    0.000 {built-in method numpy.empty}\n",
      "   599862    3.208    0.000   34.646    0.000 constant_op.py:348(_tensor_shape_tensor_conversion_function)\n",
      "   299931    4.243    0.000   34.010    0.000 math_ops.py:1848(range)\n",
      "  2299471    3.105    0.000   33.243    0.000 base.py:149(tensor_dtype)\n",
      " 11097447   32.673    0.000   32.673    0.000 context.py:825(executing_eagerly)\n",
      " 14715076   12.391    0.000   32.366    0.000 numeric.py:424(count_nonzero)\n",
      "   299931    2.704    0.000   32.301    0.000 math_ops.py:1768(tensor_equals)\n",
      "  1199724    2.784    0.000   31.094    0.000 <__array_function__ internals>:2(all)\n",
      "  4099057   10.591    0.000   30.966    0.000 tensor_shape.py:755(__init__)\n",
      "   399908   13.845    0.000   29.859    0.000 embedding_sgram.py:322(dWs)\n",
      "  2299471    4.098    0.000   29.165    0.000 base.py:64(is_scalar)\n",
      "   299931    1.456    0.000   28.113    0.000 execute.py:280(<listcomp>)\n",
      "  1499655    1.700    0.000   27.984    0.000 tensor_conversion_registry.py:50(_default_conversion_function)\n",
      "   299931    0.437    0.000   27.822    0.000 base.py:165(to_tensor)\n",
      "   399908   12.673    0.000   27.670    0.000 embedding_sgram.py:283(dWc)\n",
      "   999724    3.142    0.000   27.192    0.000 text.py:93(standardize)\n",
      "  1199724    3.815    0.000   25.523    0.000 fromnumeric.py:2355(all)\n",
      "  4398988    7.094    0.000   25.438    0.000 context.py:1885(executing_eagerly)\n",
      "   399908   11.053    0.000   24.415    0.000 embedding_sgram.py:261(dWe)\n",
      "  2599402    5.158    0.000   24.003    0.000 ops.py:1006(__bool__)\n",
      " 69041521   23.602    0.000   23.602    0.000 {built-in method builtins.issubclass}\n",
      " 67291589   22.769    0.000   23.492    0.000 {built-in method builtins.getattr}\n",
      " 17389483   13.924    0.000   23.053    0.000 base.py:84(is_tensor)\n",
      "   999724    0.889    0.000   22.859    0.000 re.py:203(sub)\n",
      "  2499425    3.151    0.000   22.129    0.000 ops.py:1204(get_shape)\n",
      "  1899563    6.500    0.000   22.112    0.000 ops.py:6437(name_scope)\n",
      "   999724    9.216    0.000   20.754    0.000 text.py:252(<listcomp>)\n",
      "    99977    0.558    0.000   20.670    0.000 tf.py:169(add)\n",
      " 17672588   20.551    0.000   20.551    0.000 {method 'flatten' of 'numpy.ndarray' objects}\n",
      "    99977    8.670    0.000   20.510    0.000 function.py:752(sigmoid_cross_entropy_log_loss)\n",
      "    99977    2.023    0.000   20.111    0.000 gen_math_ops.py:299(add)\n",
      " 14715076   19.975    0.000   19.975    0.000 {built-in method numpy.core._multiarray_umath.count_nonzero}\n",
      "  2699379    2.926    0.000   19.967    0.000 ops.py:1058(_numpy)\n",
      "   499885    2.560    0.000   19.778    0.000 embedding_sgram.py:255(Be)\n",
      "  4099057    8.115    0.000   19.453    0.000 tensor_shape.py:765(<listcomp>)\n",
      "   999724   19.187    0.000   19.187    0.000 {method 'sub' of 're.Pattern' objects}\n",
      "    99977    2.649    0.000   19.067    0.000 embedding_sgram.py:1523(g)\n",
      "   699839    0.961    0.000   19.018    0.000 array_ops.py:1523(_autopacking_conversion_function)\n",
      "     1000    0.005    0.000   18.719    0.019 <__array_function__ internals>:2(mean)\n",
      "     1000    0.013    0.000   18.709    0.019 fromnumeric.py:3301(mean)\n",
      "     1000    0.049    0.000   18.696    0.019 _methods.py:161(_mean)\n",
      "  1399678    2.549    0.000   18.665    0.000 <__array_function__ internals>:2(reshape)\n",
      "  1999448    6.804    0.000   18.623    0.000 arraypad.py:454(_as_pairs)\n",
      "   699839    3.005    0.000   18.057    0.000 array_ops.py:1513(_should_not_autopack)\n",
      "  2699379   17.041    0.000   17.041    0.000 {method '_numpy_internal' of 'tensorflow.python.framework.ops.EagerTensor' objects}\n",
      "  2399448    5.076    0.000   15.981    0.000 base.py:93(is_float_tensor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16614639   15.623    0.000   15.623    0.000 fromnumeric.py:71(<dictcomp>)\n",
      "  1999540   14.584    0.000   14.584    0.000 {built-in method numpy.arange}\n",
      "   299931    0.379    0.000   14.571    0.000 array_ops.py:805(rank)\n",
      " 15814777   14.443    0.000   14.443    0.000 {method 'tolist' of 'numpy.ndarray' objects}\n",
      "   299931    1.111    0.000   14.361    0.000 gen_math_ops.py:3181(equal)\n",
      "   299931    2.986    0.000   14.191    0.000 array_ops.py:841(rank_internal)\n",
      "  3899103   13.593    0.000   14.068    0.000 dtypes.py:604(as_dtype)\n",
      "   199954    0.462    0.000   13.596    0.000 <__array_function__ internals>:2(isin)\n",
      "  4398988   13.331    0.000   13.331    0.000 {method '_shape_tuple' of 'tensorflow.python.framework.ops.EagerTensor' objects}\n",
      " 17672588    9.464    0.000   13.226    0.000 arraysetops.py:125(_unpack_tuple)\n",
      "  1399678    2.746    0.000   12.990    0.000 fromnumeric.py:199(reshape)\n",
      "   999724    2.205    0.000   12.967    0.000 numerictypes.py:599(find_common_type)\n",
      "   199954    1.035    0.000   12.792    0.000 arraysetops.py:615(isin)\n",
      "   299931    2.133    0.000   12.658    0.000 math_ops.py:1190(maybe_promote_tensors)\n",
      "  6798436   12.531    0.000   12.531    0.000 context.py:847(device_name)\n",
      "   699839    2.142    0.000   12.101    0.000 re.py:188(match)\n",
      "   699839    2.215    0.000   11.964    0.000 nest.py:319(flatten)\n",
      "  9197884   10.187    0.000   11.338    0.000 tensor_shape.py:190(__init__)\n",
      " 17114432   11.159    0.000   11.159    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
      "   199954    0.401    0.000   11.092    0.000 <__array_function__ internals>:2(in1d)\n",
      "  3099195    1.774    0.000   10.926    0.000 _asarray.py:23(asarray)\n",
      "  6498505    7.702    0.000   10.647    0.000 ops.py:1064(dtype)\n",
      "   999770    1.792    0.000   10.613    0.000 execute.py:284(<listcomp>)\n",
      "  1299701    2.758    0.000   10.206    0.000 tensor_util.py:1087(maybe_set_static_shape)\n",
      "   199954    6.798    0.000   10.179    0.000 arraysetops.py:498(in1d)\n",
      "   999724    6.372    0.000    9.961    0.000 arraypad.py:86(_pad_simple)\n",
      "    99977    0.817    0.000    9.879    0.000 function.py:734(check_binary_classification_X_T)\n",
      "   699839    9.745    0.000    9.749    0.000 {built-in method tensorflow.python.util._pywrap_utils.Flatten}\n",
      "  3599172    3.074    0.000    9.721    0.000 dtypes.py:83(base_dtype)\n",
      "  1999448    6.748    0.000    9.619    0.000 numerictypes.py:575(_can_coerce_all)\n",
      "  2199494    7.464    0.000    9.473    0.000 tensor_shape.py:865(__iter__)\n",
      "  1199724    4.716    0.000    9.052    0.000 dtypes.py:171(is_compatible_with)\n",
      "  1600632    3.920    0.000    9.042    0.000 {built-in method builtins.hasattr}\n",
      "  1299701    1.006    0.000    8.820    0.000 ops.py:6852(_is_keras_symbolic_tensor)\n",
      "    99977    0.903    0.000    8.637    0.000 base.py:99(T)\n",
      "  8098137    5.576    0.000    8.597    0.000 tensor_shape.py:871(<genexpr>)\n",
      " 44145228    8.559    0.000    8.559    0.000 preprocess.py:109(vocabulary)\n",
      "   999770    2.981    0.000    8.444    0.000 base.py:259(T)\n",
      "  1699609    8.296    0.000    8.296    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
      "   599862    0.551    0.000    8.277    0.000 base.py:76(is_float_scalar)\n",
      "   299931    0.868    0.000    8.199    0.000 gen_math_ops.py:7329(_range)\n",
      "   199954    5.682    0.000    7.967    0.000 embedding_sgram.py:270(Wc)\n",
      "  1599632    3.024    0.000    7.936    0.000 {built-in method builtins.all}\n",
      "    99977    1.495    0.000    7.923    0.000 embedding_sgram.py:1499(f)\n",
      "   999724    1.286    0.000    7.830    0.000 <__array_function__ internals>:2(concatenate)\n",
      "   199954    6.051    0.000    7.828    0.000 embedding_sgram.py:315(Ws)\n",
      "   999724    0.982    0.000    7.803    0.000 <__array_function__ internals>:2(round_)\n",
      "  3599080    7.793    0.000    7.793    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "   599862    1.284    0.000    7.726    0.000 base.py:33(is_np_float_scalar)\n",
      "  1699563    4.959    0.000    7.565    0.000 re.py:289(_compile)\n",
      "   899793    2.340    0.000    7.480    0.000 math_ops.py:925(cast)\n",
      "   899793    2.412    0.000    7.127    0.000 base.py:191(X)\n",
      "  3599172    6.647    0.000    6.647    0.000 dtypes.py:70(_is_ref_dtype)\n",
      "   999770    1.468    0.000    6.528    0.000 base.py:126(is_same_shape)\n",
      "   199954    0.730    0.000    6.483    0.000 composite.py:146(T)\n",
      "    99977    0.369    0.000    6.258    0.000 tf.py:162(concat)\n",
      " 31929509    6.234    0.000    6.234    0.000 {method 'rstrip' of 'str' objects}\n",
      "  2099517    6.105    0.000    6.105    0.000 dtypes.py:191(__eq__)\n",
      "  5798666    6.000    0.000    6.000    0.000 _internal.py:826(npy_ctypes_check)\n",
      " 14715076    5.836    0.000    5.836    0.000 embedding_sgram.py:201(dictionary)\n",
      "   499885    2.547    0.000    5.786    0.000 arrayprint.py:60(_make_options_dict)\n",
      "    99977    1.516    0.000    5.756    0.000 embedding_sgram.py:1456(_adapt_function_handle_Y)\n",
      "  2099471    2.349    0.000    5.701    0.000 base.py:130(tensor_rank)\n",
      "   999724    0.888    0.000    5.666    0.000 fromnumeric.py:3709(round_)\n",
      "   199954    4.222    0.000    5.611    0.000 embedding_sgram.py:216(negative_sample_indices)\n",
      "    99977    0.610    0.000    5.552    0.000 array_ops.py:1676(concat)\n",
      "   100010    0.102    0.000    5.534    0.000 {built-in method builtins.next}\n",
      "   100000    0.616    0.000    5.432    0.000 <ipython-input-33-0580c1f4e1b7>:1(sentences_generator)\n",
      " 17114525    5.404    0.000    5.404    0.000 {method 'items' of 'dict' objects}\n",
      "  2999172    2.129    0.000    5.279    0.000 <__array_function__ internals>:2(ndim)\n",
      "    99977    1.169    0.000    5.206    0.000 embedding_sgram.py:159(X)\n",
      "   699839    5.178    0.000    5.178    0.000 {method 'match' of 're.Pattern' objects}\n",
      "   599862    4.998    0.000    4.998    0.000 {built-in method numpy.zeros}\n",
      " 15015007    4.956    0.000    4.956    0.000 fromnumeric.py:2907(_prod_dispatcher)\n",
      "  6398528    4.937    0.000    4.937    0.000 base.py:233(N)\n",
      "    99977    0.578    0.000    4.881    0.000 gen_array_ops.py:1190(concat_v2)\n",
      "   999724    0.877    0.000    4.778    0.000 <__array_function__ internals>:2(around)\n",
      "   199954    0.414    0.000    4.690    0.000 <__array_function__ internals>:2(sum)\n",
      "   999724    3.649    0.000    4.616    0.000 arraypad.py:129(_set_pad_area)\n",
      " 17672588    4.418    0.000    4.418    0.000 arraysetops.py:133(_unique_dispatcher)\n",
      "   199954    3.308    0.000    4.382    0.000 embedding_sgram.py:190(context_indices)\n",
      "  6798436    4.348    0.000    4.348    0.000 context.py:503(ensure_initialized)\n",
      "  1399678    1.751    0.000    4.331    0.000 tensor_shape.py:1207(as_list)\n",
      "   599862    1.326    0.000    4.215    0.000 tensor_shape.py:1193(is_fully_defined)\n",
      "  2199494    2.438    0.000    4.209    0.000 tensor_conversion_registry.py:114(get)\n",
      " 17672588    4.157    0.000    4.157    0.000 fromnumeric.py:2442(_cumsum_dispatcher)\n",
      "   199954    0.755    0.000    4.115    0.000 composite.py:199(_set_label)\n",
      "  2299471    1.629    0.000    4.104    0.000 math_ops.py:1967(_may_reduce_to_scalar)\n",
      " 14715076    4.097    0.000    4.097    0.000 numeric.py:420(_count_nonzero_dispatcher)\n",
      " 11697309    4.080    0.000    4.080    0.000 tensor_shape.py:256(value)\n",
      " 16814593    4.054    0.000    4.054    0.000 embedding_sgram.py:211(SL)\n",
      "  7198344    4.043    0.000    4.043    0.000 {method '_datatype_enum' of 'tensorflow.python.framework.ops.EagerTensor' objects}\n",
      "  6798436    4.024    0.000    4.024    0.000 context.py:783(_handle)\n",
      "   199954    0.773    0.000    3.893    0.000 fromnumeric.py:2111(sum)\n",
      "  1899563    3.209    0.000    3.848    0.000 ops.py:392(__getattr__)\n",
      "  1799586    3.157    0.000    3.405    0.000 base.py:357(logger)\n",
      "   499885    3.374    0.000    3.374    0.000 arrayprint.py:358(_get_formatdict)\n",
      "   199954    2.363    0.000    3.206    0.000 embedding_sgram.py:182(target_indices)\n",
      "   699839    3.177    0.000    3.177    0.000 {method 'format' of 'str' objects}\n",
      " 27159920    3.147    0.000    3.147    0.000 {method 'lower' of 'str' objects}\n",
      "    99977    0.437    0.000    3.139    0.000 numeric.py:148(ones)\n",
      "   999770    1.488    0.000    3.009    0.000 backprop.py:167(_must_record_gradient)\n",
      "  4598966    2.955    0.000    2.955    0.000 base.py:185(D)\n",
      "   999724    0.935    0.000    2.908    0.000 fromnumeric.py:3199(around)\n",
      "   499885    2.639    0.000    2.639    0.000 {built-in method builtins.locals}\n",
      "  3799126    2.637    0.000    2.637    0.000 ops.py:1248(graph)\n",
      "  1399678    1.887    0.000    2.580    0.000 tensor_shape.py:1218(<listcomp>)\n",
      "  2899333    1.933    0.000    2.555    0.000 tensor_shape.py:828(rank)\n",
      " 14715076    2.541    0.000    2.541    0.000 preprocess.py:119(probabilities)\n",
      "  2299471    1.926    0.000    2.475    0.000 math_ops.py:1962(_has_fully_defined_shape)\n",
      "   599868    0.576    0.000    2.466    0.000 abc.py:96(__instancecheck__)\n",
      "   100000    0.564    0.000    2.466    0.000 utility_file.py:221(take)\n",
      "   299931    0.943    0.000    2.358    0.000 base.py:557(update)\n",
      "   899793    1.137    0.000    2.296    0.000 __init__.py:1424(debug)\n",
      "   699839    1.253    0.000    2.292    0.000 __init__.py:1448(warning)\n",
      "  1899563    2.291    0.000    2.291    0.000 __init__.py:1689(isEnabledFor)\n",
      "    99977    0.351    0.000    2.267    0.000 ops.py:1070(numpy)\n",
      "  2799356    2.235    0.000    2.235    0.000 {method 'group' of 're.Match' objects}\n",
      " 10797516    2.226    0.000    2.226    0.000 context.py:1860(context_safe)\n",
      "  2199494    1.484    0.000    2.216    0.000 tensor_shape.py:852(__len__)\n",
      "  5998344    2.170    0.000    2.170    0.000 numerictypes.py:584(<listcomp>)\n",
      "   199954    0.781    0.000    2.002    0.000 _ufunc_config.py:32(seterr)\n",
      "   599868    1.886    0.000    1.890    0.000 {built-in method _abc._abc_instancecheck}\n",
      "   999747    1.527    0.000    1.885    0.000 utility_file.py:280(file_line_stream)\n",
      "  2499425    1.864    0.000    1.864    0.000 array_ops.py:1519(<genexpr>)\n",
      "    99977    0.169    0.000    1.804    0.000 <__array_function__ internals>:2(amax)\n",
      "    99977    0.218    0.000    1.746    0.000 _ufunc_config.py:433(__enter__)\n",
      "  2399448    1.231    0.000    1.596    0.000 tensor_shape.py:1196(<genexpr>)\n",
      "  9697783    1.555    0.000    1.555    0.000 {method 'append' of 'list' objects}\n",
      "   999770    1.521    0.000    1.521    0.000 {built-in method tensorflow.python._pywrap_tfe.TFE_Py_TapeSetIsEmpty}\n",
      "   199954    1.462    0.000    1.462    0.000 base.py:122(L)\n",
      "  1199724    0.660    0.000    1.451    0.000 math_ops.py:1917(<genexpr>)\n",
      "  1899563    1.419    0.000    1.419    0.000 ops.py:176(__exit__)\n",
      "    99977    0.285    0.000    1.410    0.000 fromnumeric.py:2617(amax)\n",
      "   499885    0.573    0.000    1.380    0.000 base.py:102(is_integer_tensor)\n",
      "   299931    0.381    0.000    1.290    0.000 ops.py:5775(executing_eagerly_outside_functions)\n",
      "    99977    0.240    0.000    1.273    0.000 <__array_function__ internals>:2(copyto)\n",
      "   399908    0.808    0.000    1.245    0.000 objective.py:147(P)\n",
      "   999724    1.216    0.000    1.216    0.000 arraypad.py:58(_view_roi)\n",
      "    99977    0.660    0.000    1.197    0.000 function.py:412(transform_X_T)\n",
      "    99977    0.156    0.000    1.130    0.000 <__array_function__ internals>:2(amin)\n",
      "  1999448    1.129    0.000    1.129    0.000 arraypad.py:109(<genexpr>)\n",
      "  2599402    1.108    0.000    1.108    0.000 embedding_sgram.py:172(C)\n",
      "    99977    0.739    0.000    1.064    0.000 embedding_sgram.py:248(We)\n",
      "  2199494    1.044    0.000    1.044    0.000 tensor_shape.py:799(_v2_behavior)\n",
      "   999724    0.815    0.000    1.038    0.000 types.py:171(__get__)\n",
      "   999724    0.994    0.000    0.994    0.000 numerictypes.py:651(<listcomp>)\n",
      "  1999448    0.983    0.000    0.983    0.000 arraypad.py:120(<genexpr>)\n",
      "  1999448    0.967    0.000    0.967    0.000 arraypad.py:33(_slice_at_axis)\n",
      "  2199494    0.965    0.000    0.965    0.000 {built-in method builtins.iter}\n",
      "   299931    0.934    0.000    0.934    0.000 dtypes.py:102(as_numpy_dtype)\n",
      "    99977    0.233    0.000    0.893    0.000 base.py:202(to_list)\n",
      "   999724    0.887    0.000    0.887    0.000 {method 'round' of 'numpy.ndarray' objects}\n",
      "  2999172    0.851    0.000    0.851    0.000 fromnumeric.py:3106(ndim)\n",
      "    99977    0.222    0.000    0.810    0.000 fromnumeric.py:2742(amin)\n",
      "   699839    0.586    0.000    0.800    0.000 base.py:180(M)\n",
      "  1199724    0.798    0.000    0.798    0.000 base.py:253(T)\n",
      "   299931    0.388    0.000    0.789    0.000 math_ops.py:1918(<listcomp>)\n",
      "    99977    0.778    0.000    0.778    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
      "  1599632    0.769    0.000    0.769    0.000 embedding_sgram.py:167(E)\n",
      "    99977    0.210    0.000    0.728    0.000 <__array_function__ internals>:2(squeeze)\n",
      "       24    0.001    0.000    0.703    0.029 base.py:590(save)\n",
      "       24    0.398    0.017    0.702    0.029 utility_file.py:198(serialize)\n",
      "   199954    0.662    0.000    0.699    0.000 _ufunc_config.py:132(geterr)\n",
      "   199954    0.455    0.000    0.683    0.000 embedding_sgram.py:177(window_size)\n",
      "    99977    0.150    0.000    0.623    0.000 _ufunc_config.py:438(__exit__)\n",
      "   799840    0.612    0.000    0.612    0.000 base.py:170(name)\n",
      "    99977    0.162    0.000    0.610    0.000 embedding_sgram.py:154(X)\n",
      "  1899563    0.609    0.000    0.609    0.000 ops.py:170(__init__)\n",
      "   499885    0.580    0.000    0.580    0.000 {method 'discard' of 'set' objects}\n",
      "   699839    0.506    0.000    0.578    0.000 base.py:325(dS)\n",
      "  2999172    0.570    0.000    0.570    0.000 fromnumeric.py:3102(_ndim_dispatcher)\n",
      "  1399678    0.559    0.000    0.559    0.000 fromnumeric.py:194(_reshape_dispatcher)\n",
      "   499885    0.556    0.000    0.556    0.000 {built-in method _thread.get_ident}\n",
      "   599862    0.241    0.000    0.547    0.000 tensor_shape.py:847(ndims)\n",
      "   499885    0.533    0.000    0.533    0.000 {method 'update' of 'dict' objects}\n",
      "   499885    0.530    0.000    0.530    0.000 {method 'copy' of 'dict' objects}\n",
      "   499885    0.508    0.000    0.508    0.000 arrayprint.py:65(<dictcomp>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1199724    0.500    0.000    0.500    0.000 fromnumeric.py:2350(_all_dispatcher)\n",
      "   799816    0.476    0.000    0.476    0.000 tensor_shape.py:858(__bool__)\n",
      "  1399678    0.471    0.000    0.471    0.000 {method 'pop' of 'dict' objects}\n",
      "   699839    0.435    0.000    0.435    0.000 {method 'replace' of 'str' objects}\n",
      "  1999448    0.432    0.000    0.432    0.000 {method 'strip' of 'str' objects}\n",
      "  1999448    0.429    0.000    0.429    0.000 fromnumeric.py:3195(_around_dispatcher)\n",
      "   199954    0.255    0.000    0.424    0.000 composite.py:141(T)\n",
      "   100025    0.394    0.000    0.394    0.000 {method 'join' of 'str' objects}\n",
      "  1099701    0.384    0.000    0.384    0.000 preprocess.py:491(window_size)\n",
      "   299931    0.368    0.000    0.368    0.000 base.py:63(lr)\n",
      "  1099701    0.346    0.000    0.346    0.000 preprocess.py:503(event_size)\n",
      "    99977    0.163    0.000    0.337    0.000 fromnumeric.py:1439(squeeze)\n",
      "    99977    0.235    0.000    0.325    0.000 objective.py:156(J)\n",
      "    99977    0.208    0.000    0.325    0.000 base.py:143(layer_inference)\n",
      "   199954    0.314    0.000    0.314    0.000 {built-in method numpy.seterrobj}\n",
      "   499885    0.299    0.000    0.299    0.000 {built-in method builtins.id}\n",
      "    99977    0.280    0.000    0.296    0.000 base.py:157(layers_all)\n",
      "   299931    0.202    0.000    0.296    0.000 __init__.py:1436(info)\n",
      "   299931    0.287    0.000    0.287    0.000 embedding_sgram.py:355(optimizer)\n",
      "  1099701    0.285    0.000    0.285    0.000 {built-in method builtins.callable}\n",
      "  1899563    0.281    0.000    0.281    0.000 ops.py:173(__enter__)\n",
      "   499885    0.270    0.000    0.270    0.000 {method 'add' of 'set' objects}\n",
      "   499885    0.267    0.000    0.267    0.000 arrayprint.py:854(_none_or_positive_arg)\n",
      "   299931    0.258    0.000    0.258    0.000 composite.py:168(layers)\n",
      "       24    0.253    0.011    0.253    0.011 {built-in method _pickle.dump}\n",
      "   999724    0.252    0.000    0.252    0.000 multiarray.py:143(concatenate)\n",
      "   399908    0.244    0.000    0.244    0.000 {built-in method numpy.geterrobj}\n",
      "   999724    0.240    0.000    0.240    0.000 arraypad.py:521(_pad_dispatcher)\n",
      "   999724    0.223    0.000    0.223    0.000 enum.py:753(value)\n",
      "   699839    0.214    0.000    0.214    0.000 base.py:175(num_nodes)\n",
      "     1023    0.012    0.000    0.193    0.000 {built-in method builtins.print}\n",
      "    99977    0.104    0.000    0.182    0.000 base.py:335(objective)\n",
      "     2046    0.016    0.000    0.181    0.000 iostream.py:384(write)\n",
      "    99977    0.175    0.000    0.175    0.000 {method 'squeeze' of 'numpy.ndarray' objects}\n",
      "   299931    0.158    0.000    0.158    0.000 base.py:75(l2)\n",
      "     3069    0.019    0.000    0.153    0.000 iostream.py:195(schedule)\n",
      "   999724    0.148    0.000    0.148    0.000 numerictypes.py:652(<listcomp>)\n",
      "    14829    0.088    0.000    0.144    0.000 codecs.py:319(decode)\n",
      "    99977    0.119    0.000    0.140    0.000 function.py:620(transform_scalar_X_T)\n",
      "    99977    0.136    0.000    0.136    0.000 _ufunc_config.py:429(__init__)\n",
      "   199978    0.122    0.000    0.122    0.000 embedding_sgram.py:234(WO)\n",
      "    99977    0.074    0.000    0.115    0.000 base.py:182(assure_tensor)\n",
      "     3069    0.110    0.000    0.110    0.000 socket.py:438(send)\n",
      "   199978    0.103    0.000    0.103    0.000 embedding_sgram.py:229(W)\n",
      "   199954    0.089    0.000    0.089    0.000 arraysetops.py:494(_in1d_dispatcher)\n",
      "   199954    0.089    0.000    0.089    0.000 arraysetops.py:611(_isin_dispatcher)\n",
      "   100977    0.087    0.000    0.087    0.000 {built-in method time.time}\n",
      "    99977    0.086    0.000    0.086    0.000 preprocess.py:104(min_sequence_length)\n",
      "   199954    0.069    0.000    0.069    0.000 fromnumeric.py:2106(_sum_dispatcher)\n",
      "    14829    0.057    0.000    0.057    0.000 {built-in method _codecs.utf_8_decode}\n",
      "       48    0.054    0.001    0.056    0.001 {built-in method io.open}\n",
      "    99977    0.053    0.000    0.053    0.000 multiarray.py:1054(copyto)\n",
      "     1000    0.041    0.000    0.043    0.000 _methods.py:65(_count_reduce_items)\n",
      "    99977    0.041    0.000    0.041    0.000 fromnumeric.py:2612(_amax_dispatcher)\n",
      "    99977    0.037    0.000    0.037    0.000 fromnumeric.py:1435(_squeeze_dispatcher)\n",
      "    99977    0.032    0.000    0.032    0.000 fromnumeric.py:2737(_amin_dispatcher)\n",
      "     2046    0.003    0.000    0.027    0.000 iostream.py:321(_schedule_flush)\n",
      "        1    0.000    0.000    0.022    0.022 <ipython-input-33-0580c1f4e1b7>:59(<module>)\n",
      "     3069    0.008    0.000    0.017    0.000 threading.py:1071(is_alive)\n",
      "     2046    0.005    0.000    0.009    0.000 iostream.py:308(_is_master_process)\n",
      "     3069    0.004    0.000    0.008    0.000 threading.py:1017(_wait_for_tstate_lock)\n",
      "     3069    0.006    0.000    0.006    0.000 iostream.py:91(_event_pipe)\n",
      "       24    0.000    0.000    0.005    0.000 pathlib.py:1214(open)\n",
      "     3069    0.004    0.000    0.004    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "    153/9    0.001    0.000    0.004    0.000 abc.py:100(__subclasscheck__)\n",
      "    153/9    0.003    0.000    0.004    0.000 {built-in method _abc._abc_subclasscheck}\n",
      "     2046    0.004    0.000    0.004    0.000 {built-in method posix.getpid}\n",
      "        5    0.000    0.000    0.003    0.001 codeop.py:142(__call__)\n",
      "        5    0.003    0.001    0.003    0.001 {built-in method builtins.compile}\n",
      "       24    0.000    0.000    0.003    0.000 pathlib.py:1433(is_file)\n",
      "       24    0.000    0.000    0.003    0.000 pathlib.py:1193(stat)\n",
      "       24    0.003    0.000    0.003    0.000 {built-in method posix.stat}\n",
      "       48    0.000    0.000    0.003    0.000 pathlib.py:1039(__new__)\n",
      "     1000    0.002    0.000    0.003    0.000 base.py:129(history)\n",
      "       48    0.000    0.000    0.002    0.000 pathlib.py:678(_from_parts)\n",
      "     1000    0.002    0.000    0.002    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}\n",
      "       48    0.000    0.000    0.002    0.000 pathlib.py:658(_parse_args)\n",
      "       48    0.001    0.000    0.002    0.000 pathlib.py:64(parse_parts)\n",
      "       24    0.000    0.000    0.001    0.000 pathlib.py:1076(_opener)\n",
      "       24    0.001    0.000    0.001    0.000 {built-in method posix.open}\n",
      "       24    0.000    0.000    0.001    0.000 utility_file.py:108(is_path_creatable)\n",
      "     3069    0.001    0.000    0.001    0.000 {method 'append' of 'collections.deque' objects}\n",
      "     3069    0.001    0.000    0.001    0.000 threading.py:513(is_set)\n",
      "      240    0.001    0.000    0.001    0.000 {built-in method sys.intern}\n",
      "       24    0.001    0.000    0.001    0.000 {built-in method posix.access}\n",
      "       72    0.000    0.000    0.000    0.000 pathlib.py:728(__fspath__)\n",
      "       24    0.000    0.000    0.000    0.000 embedding_sgram.py:393(S)\n",
      "     1000    0.000    0.000    0.000    0.000 fromnumeric.py:3296(_mean_dispatcher)\n",
      "       72    0.000    0.000    0.000    0.000 pathlib.py:718(__str__)\n",
      "       24    0.000    0.000    0.000    0.000 _bootlocale.py:33(getpreferredencoding)\n",
      "       24    0.000    0.000    0.000    0.000 posixpath.py:150(dirname)\n",
      "      116    0.000    0.000    0.000    0.000 _collections_abc.py:392(__subclasshook__)\n",
      "       48    0.000    0.000    0.000    0.000 pathlib.py:293(splitroot)\n",
      "       24    0.000    0.000    0.000    0.000 {built-in method _locale.nl_langinfo}\n",
      "       24    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
      "       48    0.000    0.000    0.000    0.000 pathlib.py:701(_format_parsed_parts)\n",
      "       26    0.000    0.000    0.000    0.000 _collections_abc.py:302(__subclasshook__)\n",
      "       23    0.000    0.000    0.000    0.000 utility_file.py:35(__init__)\n",
      "       24    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
      "       24    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
      "       48    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x55e89e45f580}\n",
      "       48    0.000    0.000    0.000    0.000 pathlib.py:1049(_init)\n",
      "       24    0.000    0.000    0.000    0.000 __init__.py:145(_DType_reduce)\n",
      "        3    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
      "       24    0.000    0.000    0.000    0.000 posixpath.py:41(_get_sep)\n",
      "       48    0.000    0.000    0.000    0.000 {method 'lstrip' of 'str' objects}\n",
      "        6    0.000    0.000    0.000    0.000 tensor_conversion_registry.py:135(<genexpr>)\n",
      "        9    0.000    0.000    0.000    0.000 __init__.py:218(_acquireLock)\n",
      "       46    0.000    0.000    0.000    0.000 {method 'close' of 'generator' objects}\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:238(helper)\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:117(__exit__)\n",
      "       24    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}\n",
      "       72    0.000    0.000    0.000    0.000 {built-in method posix.fspath}\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:82(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:108(__enter__)\n",
      "       24    0.000    0.000    0.000    0.000 {built-in method _stat.S_ISREG}\n",
      "        5    0.000    0.000    0.000    0.000 hooks.py:103(__call__)\n",
      "        9    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}\n",
      "        5    0.000    0.000    0.000    0.000 traitlets.py:564(__get__)\n",
      "       10    0.000    0.000    0.000    0.000 compilerop.py:166(extra_flags)\n",
      "        9    0.000    0.000    0.000    0.000 __init__.py:227(_releaseLock)\n",
      "       24    0.000    0.000    0.000    0.000 embedding_sgram.py:381(state_elements)\n",
      "       48    0.000    0.000    0.000    0.000 {method 'reverse' of 'list' objects}\n",
      "        5    0.000    0.000    0.000    0.000 ipstruct.py:125(__getattr__)\n",
      "        5    0.000    0.000    0.000    0.000 interactiveshell.py:3338(compare)\n",
      "        9    0.000    0.000    0.000    0.000 __init__.py:1675(getEffectiveLevel)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
      "        5    0.000    0.000    0.000    0.000 traitlets.py:533(get)\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-33-0580c1f4e1b7>:61(<module>)\n",
      "        2    0.000    0.000    0.000    0.000 _collections_abc.py:367(__subclasshook__)\n",
      "        9    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}\n",
      "        5    0.000    0.000    0.000    0.000 interactiveshell.py:1278(user_global_ns)\n",
      "        9    0.000    0.000    0.000    0.000 __init__.py:1276(disable)\n",
      "        5    0.000    0.000    0.000    0.000 hooks.py:168(pre_run_code_hook)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-33-0580c1f4e1b7>:23(<module>)\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-33-0580c1f4e1b7>:22(<module>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sentences_generator(path_to_file, num_sentences):\n",
    "    stream = fileio.Function.file_line_stream(path_to_file)\n",
    "    try:\n",
    "        while True:\n",
    "            _lines = fileio.Function.take(num_sentences, stream)\n",
    "            yield np.array(_lines)\n",
    "    finally:\n",
    "        stream.close()\n",
    "\n",
    "# Sentences for the trainig\n",
    "source = sentences_generator(\n",
    "    path_to_file=path_to_corpus, num_sentences=NUM_SENTENCES\n",
    ")\n",
    "\n",
    "# Restore the state if exists.\n",
    "state = embedding.load(STATE_FILE)\n",
    "\n",
    "# Continue training\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "\n",
    "total_sentences = 0\n",
    "epochs = 0\n",
    "\n",
    "for i in range(MAX_ITERATIONS):\n",
    "    try:\n",
    "        sentences = next(source)\n",
    "        total_sentences += len(sentences)\n",
    "\n",
    "        start = time.time()\n",
    "        network.train(X=sentences, T=np.array([0]))\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                f\"Batch {i:05d} of {NUM_SENTENCES} sentences: \"\n",
    "                f\"Average Loss: {np.mean(network.history):10f} \"\n",
    "                f\"Duration {time.time() - start:3f}\"\n",
    "            )\n",
    "        if i % 1000 == 0:\n",
    "            # embedding.save(STATE_FILE)\n",
    "            pass\n",
    "        \n",
    "    except fileio.Function.GenearatorHasNoMore as e:\n",
    "        source.close()\n",
    "        embedding.save(STATE_FILE)\n",
    "\n",
    "        # Next epoch\n",
    "        print(f\"epoch {epochs} batches {i:05d} done\")\n",
    "        epochs += 1\n",
    "        source = sentences_generator(\n",
    "            path_to_file=path_to_corpus, num_sentences=NUM_SENTENCES\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        source.close()\n",
    "        raise e\n",
    "\n",
    "embedding.save(STATE_FILE)\n",
    "\n",
    "profiler.disable()\n",
    "profiler.print_stats(sort=\"cumtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluate the vector space\n",
    "\n",
    "Verify if the trained model, or the vector space W, has encoded the words in a way that **similar** words are close in the vector space.\n",
    "\n",
    "* [How to measure the similarity among vectors](https://math.stackexchange.com/questions/4132458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words ['cash']\n",
      "Word indices [413]\n",
      "prediction for ['cash']:\n",
      "[['dividends' 'amount' 'exceed' 'flow' 'unpaid' 'depreciation' 'emhart' 'ordinary' 'borrow' 'riskier']\n",
      " ['unpaid' 'emhart' 'flow' 'refunds' 'accumulated' 'earmarked' 'financings' 'exceed' 'depreciation' 'riskier']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oonisim/home/repository/git/oonisim/python_programs/nlp/src/layer/preprocessing/preprocess.py:216: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return self._vocabulary[list(iter(indices))]\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "context = \"cash\".split()\n",
    "word_indices = np.array(word_indexing.list_indices(context), dtype=TYPE_INT)\n",
    "\n",
    "print(f\"Words {context}\")\n",
    "print(f\"Word indices {word_indices}\")\n",
    "print(f\"prediction for {context}:\\n{word_indexing.list_events([embedding.predict(word_indices, n)])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Compare with [gensim word2vec](https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import (\n",
    "    Word2Vec\n",
    ")\n",
    "from gensim.models.word2vec import (\n",
    "    LineSentence    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = LineSentence(source=path_to_corpus)\n",
    "w2v = Word2Vec(\n",
    "    sentences=sentences, \n",
    "    sg=0,\n",
    "    window=5, \n",
    "    negative=5,\n",
    "    vector_size=100, \n",
    "    min_count=1, \n",
    "    workers=4\n",
    ")\n",
    "del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amount', 0.8938570618629456),\n",
       " ('debt', 0.8765637874603271),\n",
       " ('value', 0.8724021911621094),\n",
       " ('payment', 0.8634955286979675),\n",
       " ('proceeds', 0.856279730796814),\n",
       " ('assets', 0.8465060591697693),\n",
       " ('payments', 0.8203557133674622),\n",
       " ('dividends', 0.8193185925483704),\n",
       " ('dividend', 0.818557858467102),\n",
       " ('face', 0.8156279921531677)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(context, topn=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
