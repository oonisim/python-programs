{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec implementation\n",
    "\n",
    "## Original papers\n",
    "* [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "* [word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method](https://arxiv.org/abs/1402.3722)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The original paper proposed using:\n",
    "1. Matmul to extract word vectors from ```Win```.\n",
    "2. Softmax to calculate scores with all the word vectors from ```Wout```.\n",
    "\n",
    "<img src=\"image/word2vec_cbow_mechanism.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "### Motivation\n",
    "\n",
    "The original implementation was computationally expensive.\n",
    "\n",
    "1. Matmal to extract word vectors from ```Win```.\n",
    "2. Softmax can happen vocabulary size times with ```Wout```.\n",
    "\n",
    "<img src=\"image/word2vec_negative_sampling_motivation.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1. Use index to extract word vectors from Win.\n",
    "2. Instead of calculating softmax with all the word vectors from ```Wout```, sample small number (SL) of negative/false word vectors from ```Wout``` and calculate logistic log loss with each sample. \n",
    "\n",
    "<img src=\"image/wors2vec_neg_sample_backprop.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using only one Word vector space W\n",
    "\n",
    "There is no reasoning nor proof why two word vector space are required. In the end, we only use one word vector space, which appears to be ```Win```. \n",
    "\n",
    "However, if we use one vector space ```W``` for ```event```, ```context``` and ```negative samples```,then an event vector ```event=W[i]``` in a sentence can be used as a negative sample in another setence. Then the weight ```W[i]``` is updated for both positive and negative labels in the same gradient descent on ```W```. The actual [experiment of using only one vector space](./layer/embedding_single_vector_space.py) ```W``` did not work well.\n",
    "\n",
    "* [Why do we need 2 matrices for word2vec or GloVe](https://datascience.stackexchange.com/a/94422/68313)\n",
    "\n",
    "\n",
    "<img src=\"image/word2vec_why_not_one_W.png\" align=\"left\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from itertools import islice\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=400) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Google Colab environment\n",
    "\n",
    "Colab gets disconnected within approx 20 min. Hence not suitable for training (or need to upgrade to the pro version)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone github to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_GOOGLE_COLAB = True\n",
    "except:\n",
    "    IN_GOOGLE_COLAB = False\n",
    "    \n",
    "if IN_GOOGLE_COLAB:\n",
    "    !pip install line_profiler\n",
    "    !google.colab.drive.mount('/content/gdrive')\n",
    "    !rm -rf /content/drive/MyDrive/github\n",
    "    !mkdir -p /content/drive/MyDrive/github\n",
    "    !git clone https://github.com/oonisim/python-programs.git /content/drive/MyDrive/github\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone github to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_GOOGLE_COLAB = True\n",
    "except:\n",
    "    IN_GOOGLE_COLAB = False\n",
    "    \n",
    "if IN_GOOGLE_COLAB:\n",
    "    !pip install line_profiler\n",
    "    !google.colab.drive.mount('/content/gdrive')\n",
    "    !rm -rf /content/github\n",
    "    !mkdir -p /content/github\n",
    "    !git clone https://github.com/oonisim/python-programs.git /content/github\n",
    "        \n",
    "    import sys\n",
    "    sys.path.append('/content/github/nlp/src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook setups\n",
    "\n",
    "Auto reolaod causes an error in Jupyter notebooks. Restart the Jupyter kernel for the error:\n",
    "```TypeError: super(type, obj): obj must be an instance or subtype of type```\n",
    "See\n",
    "- https://stackoverflow.com/a/52927102/4281353\n",
    "- http://thomas-cokelaer.info/blog/2011/09/382/\n",
    "\n",
    "> The problem resides in the mechanism of reloading modules.\n",
    "> Reloading a module often changes the internal object in memory which\n",
    "> makes the isinstance test of super return False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "import function.fileio as fileio\n",
    "import function.text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.constant import (\n",
    "    TYPE_INT,\n",
    "    TYPE_FLOAT,\n",
    "    TYPE_LABEL,\n",
    "    TYPE_TENSOR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants\n",
    "\n",
    "Too large LR generates unusable event vector space.\n",
    "\n",
    "Uniform weight distribution does not work (Why?)\n",
    "Weights from the normal distribution sampling with small std (0.01) works (Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TEXT8 = False\n",
    "USE_PTB = not USE_TEXT8\n",
    "USE_CBOW = False\n",
    "USE_SGRAM = not USE_CBOW\n",
    "\n",
    "CORPUS_FILE = \"text8_256\" if USE_TEXT8 else \"ptb_train\"\n",
    "CORPUS_URL = \"https://data.deepai.org/text8.zip\" \\\n",
    "    if USE_TEXT8 else f'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt' \\\n",
    "\n",
    "TARGET_SIZE = TYPE_INT(1)       # Size of the target event (word)\n",
    "CONTEXT_SIZE = TYPE_INT(10)     # Size of the context in which the target event occurs.\n",
    "WINDOW_SIZE = TARGET_SIZE + CONTEXT_SIZE\n",
    "SAMPLE_SIZE = TYPE_INT(5)       # Size of the negative samples\n",
    "\n",
    "VECTOR_SIZE = TYPE_INT(100)     # Number of features in the event vector.\n",
    "WEIGHT_SCHEME = \"normal\"\n",
    "WEIGHT_PARAMS = {\n",
    "    \"std\": 0.01\n",
    "}\n",
    "\n",
    "LR = TYPE_FLOAT(20)\n",
    "NUM_SENTENCES = 10\n",
    "\n",
    "STATE_FILE = \"../models/word2vec_sgram_%s_E%s_C%s_S%s_W%s_%s_%s_V%s_LR%s_N%s.pkl\" % (\n",
    "    CORPUS_FILE,\n",
    "    TARGET_SIZE,\n",
    "    CONTEXT_SIZE,\n",
    "    SAMPLE_SIZE,\n",
    "    WEIGHT_SCHEME,\n",
    "    \"std\",\n",
    "    WEIGHT_PARAMS[\"std\"],\n",
    "    VECTOR_SIZE,\n",
    "    LR,\n",
    "    NUM_SENTENCES,\n",
    ")\n",
    "\n",
    "MAX_ITERATIONS = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oonisim/.keras/datasets/ptb_train\n"
     ]
    }
   ],
   "source": [
    "path_to_corpus = f\"~/.keras/datasets/{CORPUS_FILE}\"\n",
    "if fileio.Function.is_file(path_to_corpus):\n",
    "    pass\n",
    "else:\n",
    "    # text8, run \"cat text8 | xargs -n 512 > text8_512\" after download\n",
    "    path_to_corpus = tf.keras.utils.get_file(\n",
    "        fname=CORPUS_FILE,\n",
    "        origin=CORPUS_URL,\n",
    "        extract=True\n",
    "    )\n",
    "corpus = fileio.Function.read_file(path_to_corpus)\n",
    "print(path_to_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n"
     ]
    }
   ],
   "source": [
    "examples = corpus.split('\\n')[:1]\n",
    "for line in examples:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Event (word) indexing\n",
    "Index the events that have occurred in the event sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oonisim/conda/envs/python_programs/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "from layer.preprocessing import (\n",
    "    EventIndexing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexing = EventIndexing(\n",
    "    name=\"word_indexing\",\n",
    "    corpus=corpus,\n",
    "    min_sequence_length=WINDOW_SIZE\n",
    ")\n",
    "del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EventIndexing  for the corpus\n",
    "\n",
    "Adapt to the ```corpus``` and provides:\n",
    "* event_to_index dictionary\n",
    "* vocaburary of the corpus\n",
    "* word occurrence probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EventIndexing.vocabulary[10]:\n",
      "['<nil>' '<unk>' 'aer' 'banknote' 'berlitz' 'calloway' 'centrust' 'cluett' 'fromstein' 'gitano']\n",
      "\n",
      "EventIndexing.event_to_index[10]:\n",
      "('<nil>', 0)\n",
      "('<unk>', 1)\n",
      "('aer', 2)\n",
      "('banknote', 3)\n",
      "('berlitz', 4)\n",
      "('calloway', 5)\n",
      "('centrust', 6)\n",
      "('cluett', 7)\n",
      "('fromstein', 8)\n",
      "('gitano', 9)\n",
      "\n",
      "EventIndexing.probabilities[10]:\n",
      "<nil>                : 0.00000e+00\n",
      "<unk>                : 1.65308e-02\n",
      "aer                  : 5.34860e-06\n",
      "banknote             : 5.34860e-06\n",
      "berlitz              : 5.34860e-06\n",
      "calloway             : 5.34860e-06\n",
      "centrust             : 5.34860e-06\n",
      "cluett               : 5.34860e-06\n",
      "fromstein            : 5.34860e-06\n",
      "gitano               : 5.34860e-06\n"
     ]
    }
   ],
   "source": [
    "words = word_indexing.list_events(range(10))\n",
    "print(f\"EventIndexing.vocabulary[10]:\\n{words}\\n\")\n",
    "\n",
    "indices = word_indexing.list_indices(words)\n",
    "print(f\"EventIndexing.event_to_index[10]:\")\n",
    "for item in zip(words, indices):\n",
    "    print(item)\n",
    "\n",
    "probabilities = word_indexing.list_probabilities(words)\n",
    "print(f\"\\nEventIndexing.probabilities[10]:\")\n",
    "for word, p in zip(words, probabilities):\n",
    "    print(f\"{word:20s} : {p:.5e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling using the probability\n",
    "\n",
    "Sample events according to their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['addressing', 'abroad', 'carefully', 'maryland', 'co']\n"
     ]
    }
   ],
   "source": [
    "sample = word_indexing.sample(size=5)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "Sample events not including those events already sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_indices=[992, 36, 3110, 44, 237] \n",
      "events=['addresses' 'as' 'tissue' 'of' 'argue']\n"
     ]
    }
   ],
   "source": [
    "negative_indices = word_indexing.negative_sample_indices(\n",
    "    size=5, excludes=word_indexing.list_indices(sample)\n",
    ")\n",
    "print(f\"negative_indices={negative_indices} \\nevents={word_indexing.list_events(negative_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the             :    34\n",
      "asbestos        :    63\n",
      "fiber           :    86\n",
      "<unk>           :     1\n",
      "is              :    42\n",
      "unusually       :    87\n",
      "<unk>           :     1\n",
      "once            :    64\n",
      "it              :    80\n",
      "enters          :    88\n",
      "the             :    34\n",
      "<unk>           :     1\n"
     ]
    }
   ],
   "source": [
    "# sentences = \"\\n\".join(corpus.split('\\n')[5:6])\n",
    "sentences = \"\"\"\n",
    "the asbestos fiber <unk> is unusually <unk> once it enters the <unk> \n",
    "\"\"\"\n",
    "sequences = word_indexing.function(sentences)\n",
    "for pair in zip(sentences.strip().split(\" \"), sequences[0]):\n",
    "    print(f\"{pair[0]:15} : {pair[1]:5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EventContext\n",
    "\n",
    "EventContext layer generates ```(event, context)``` pairs from sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer.preprocessing import (\n",
    "    EventContext\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_context = EventContext(\n",
    "    name=\"ev\",\n",
    "    window_size=WINDOW_SIZE,\n",
    "    event_size=TARGET_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context of an event (word) in a sentence\n",
    "\n",
    "In the sentence ```\"a form of asbestos once used to make kent cigarette filters\"```, one of the context windows ```a form of asbestos once``` of size 5 and event size 1 has.\n",
    "* ```of``` as a target word.\n",
    "* ```(a, form) and (asbestos, once)``` as its context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence of the word indices for the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[37, 62, 44, 63, 64, 65, 66, 67, 68, 69, 70],\n",
       "       [29, 30, 31, 50, 51, 43, 44, 52, 53,  0,  0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \"\"\"\n",
    "a form of asbestos once used to make kent cigarette filters\n",
    "\n",
    "N years old and former chairman of consolidated gold \n",
    "\"\"\"\n",
    "\n",
    "sequence = word_indexing.function(sentences)\n",
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (event, context) pairs\n",
    "\n",
    "For each word (event) in the setence ```(of, asbestos, ... , kent)``` excludnig the ends of the sentence, create ```(target, context)``` as:\n",
    "\n",
    "```\n",
    "[\n",
    "  [of, a, form, asbestos, once],              # target is 'of', context is (a, form, asbestos, once)\n",
    "  ['asbestos', 'form', 'of', 'once', 'used'],\n",
    "  ['once', 'of', 'asbestos', 'used', 'to'],\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "### Format of the (event, context) pairs\n",
    "\n",
    "* **E** is the target event indices\n",
    "* **C** is the context indices\n",
    "\n",
    "<img src=\"image/event_context_format.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event context pairs. Shape (2, 11), Target event size 1, Window size 11.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[65, 37, 62, 44, 63, 64, 66, 67, 68, 69, 70],\n",
       "       [43, 29, 30, 31, 50, 51, 44, 52, 53,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_context_pairs = event_context.function(sequence)\n",
    "print(\n",
    "    f\"Event context pairs. Shape %s, Target event size %s, Window size %s.\" % \n",
    "    (event_context_pairs.shape, event_context.event_size, event_context.window_size)\n",
    ")\n",
    "event_context_pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (event, context) pairs in textual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['used',\n",
       "  'a',\n",
       "  'form',\n",
       "  'of',\n",
       "  'asbestos',\n",
       "  'once',\n",
       "  'to',\n",
       "  'make',\n",
       "  'kent',\n",
       "  'cigarette',\n",
       "  'filters'],\n",
       " ['chairman',\n",
       "  'n',\n",
       "  'years',\n",
       "  'old',\n",
       "  'and',\n",
       "  'former',\n",
       "  'of',\n",
       "  'consolidated',\n",
       "  'gold',\n",
       "  '<nil>',\n",
       "  '<nil>']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indexing.sequence_to_sentence(event_context_pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Word Embedding\n",
    "\n",
    "Embedding is to train the model to group similar events in a close proximity in the event vector space. If two events e.g. 'pencil' and 'pen' are similar concepts, then their event vectors resides in a close distance in the event space. \n",
    "\n",
    "* [Thought Vectors](https://wiki.pathmind.com/thought-vectors)\n",
    "\n",
    "## Training process\n",
    "\n",
    "1. Calculate ```Bc```, the BoW (Bag of Words) from context event vectors.\n",
    "2. Calculate ```Be```,  the BoW (Bag of Words) from target event vectors.\n",
    "3. The dot product ```Ye = dot(Bc, Be)``` is given the label 1 to get them closer.\n",
    "4. For each negative sample ```Ws(s)```, the dot product with ```Ys = dot(Be, Ws(s)``` is given the label 0 to get them apart. \n",
    "5. ```np.c_[Ye, Ys]``` is fowarded to the logistic log loss layer.\n",
    "\n",
    "<img src=\"image/word2vec_backprop_Be.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "if USE_CBOW:\n",
    "    from layer.embedding_cbow_dual_vector_spaces.py import (\n",
    "        Embedding\n",
    "    )\n",
    "else:\n",
    "    from layer.embedding_sgram import (\n",
    "        Embedding\n",
    "    )\n",
    "\n",
    "from optimizer import (\n",
    "    SGD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding: Embedding = Embedding(\n",
    "    name=\"embedding\",\n",
    "    num_nodes=WINDOW_SIZE,\n",
    "    target_size=TARGET_SIZE,\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    negative_sample_size=SAMPLE_SIZE,\n",
    "    event_vector_size=VECTOR_SIZE,\n",
    "    optimizer=SGD(lr=LR),\n",
    "    dictionary=word_indexing,\n",
    "    weight_initialization_scheme=WEIGHT_SCHEME,\n",
    "    weight_initialization_parameters=WEIGHT_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores ```np.c_[Ye, Ys]``` from the Embedding\n",
    "\n",
    "The 0th column is the scores for ```dot(Bc, Be``` for positive labels. The rest are the scores for ```dot(Bc, Ws)``` for negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "[[-9.1185665e-04 -2.4321437e-04  1.0157871e-04 -1.0421169e-03  5.6947256e-04  1.0440625e-03 -1.3973997e-03  1.0924268e-03  7.8901229e-04 -3.1144917e-04 -8.0015633e-04 -2.2692217e-04 -4.3101949e-04 -3.8034411e-04  1.9083207e-04]\n",
      " [ 5.4251461e-04  7.0913730e-04  2.2273404e-03  4.3803710e-04 -4.3168294e-04  1.2829594e-05  2.6357165e-04 -3.9755483e-05  0.0000000e+00  0.0000000e+00  4.5001745e-04 -6.9760508e-04  6.7859812e-04 -6.6164788e-04 -3.2094374e-04]]\n",
      "\n",
      "Scores for dot(Bc, Be): \n",
      "[[-0.00091186]\n",
      " [ 0.00054251]]\n"
     ]
    }
   ],
   "source": [
    "scores = embedding.function(event_context_pairs)\n",
    "print(f\"Scores:\\n{scores[:5]}\\n\")\n",
    "print(f\"Scores for dot(Bc, Be): \\n{scores[:5, :1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer.normalization import (\n",
    "    BatchNormalization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = BatchNormalization(\n",
    "    name=\"bn\",\n",
    "    num_nodes=1+SAMPLE_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Logistic Log Loss\n",
    "\n",
    "Train the model to get:\n",
    "1. BoW of the context event vectors close to the target event vector. Label 1\n",
    "2. BoW of the context event vectors away from each of the negative sample event vectors Label 0.\n",
    "\n",
    "This is a binary logistic classification, hence use Logistic Log Loss as the network objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.function import (\n",
    "    sigmoid_cross_entropy_log_loss,\n",
    "    sigmoid\n",
    ")\n",
    "from layer.objective import (\n",
    "    CrossEntropyLogLoss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CrossEntropyLogLoss(\n",
    "    name=\"loss\",\n",
    "    num_nodes=1,  # Logistic log loss\n",
    "    log_loss_function=sigmoid_cross_entropy_log_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Adapter\n",
    "\n",
    "The logistic log loss layer expects the input of shape ```(N,M=1)```, however Embedding outputs ```(N,(1+SL)``` where ```SL``` is SAMPLE_SIZE. The ```Adapter``` layer bridges between Embedding and the Loss layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer.adapter import (\n",
    "    Adapter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_function = embedding.adapt_function_to_logistic_log_loss(loss=loss)\n",
    "adapter_gradient = embedding.adapt_gradient_to_logistic_log_loss()\n",
    "\n",
    "adapter: Adapter = Adapter(\n",
    "    name=\"adapter\",\n",
    "    num_nodes=1,    # Number of output M=1 \n",
    "    function=adapter_function,\n",
    "    gradient=adapter_gradient\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Word2vec Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a sequential network\n",
    "\n",
    "$ \\text {Sentences} \\rightarrow EventIndexing \\rightarrow EventContext \\rightarrow  Embedding \\rightarrow Adapter \\rightarrow LogisticLogLoss$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.sequential import (\n",
    "    SequentialNetwork\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = SequentialNetwork(\n",
    "    name=\"word2vec\",\n",
    "    num_nodes=1,\n",
    "    inference_layers=[\n",
    "        word_indexing,\n",
    "        event_context,\n",
    "        embedding,\n",
    "        adapter\n",
    "    ],\n",
    "    objective_layers=[\n",
    "        loss\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved model file if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State file does not exist. Saving the initial model to ../models/word2vec_sgram_ptb_train_E1_C10_S5_Wnormal_std_0.01_V100_LR20.0_N10.pkl.\n"
     ]
    }
   ],
   "source": [
    "if fileio.Function.is_file(STATE_FILE):\n",
    "    print(\"Loading model...\\nSTATE_FILE: %s\" % STATE_FILE)\n",
    "    state = embedding.load(STATE_FILE)\n",
    "\n",
    "    fmt=\"\"\"Model loaded.\n",
    "    event_size %s\n",
    "    context_size: %s\n",
    "    event_vector_size: %s\n",
    "    \"\"\"\n",
    "    print(fmt % (\n",
    "        state[\"target_size\"], \n",
    "        state[\"context_size\"], \n",
    "        state[\"event_vector_size\"]\n",
    "    ))\n",
    "else:\n",
    "    print(\"State file does not exist. Saving the initial model to %s.\" % STATE_FILE)\n",
    "    embedding.save(STATE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 00000 of 10 sentences: Average Loss:   0.693137 Duration 0.211343\n",
      "Batch 00100 of 10 sentences: Average Loss:   0.668239 Duration 0.283479\n",
      "Batch 00200 of 10 sentences: Average Loss:   0.636050 Duration 0.192648\n",
      "Batch 00300 of 10 sentences: Average Loss:   0.612916 Duration 0.420230\n",
      "Batch 00400 of 10 sentences: Average Loss:   0.594639 Duration 0.183814\n",
      "Batch 00500 of 10 sentences: Average Loss:   0.581592 Duration 0.376362\n",
      "Batch 00600 of 10 sentences: Average Loss:   0.572798 Duration 0.172752\n",
      "Batch 00700 of 10 sentences: Average Loss:   0.563167 Duration 0.281529\n",
      "Batch 00800 of 10 sentences: Average Loss:   0.555399 Duration 0.259492\n",
      "Batch 00900 of 10 sentences: Average Loss:   0.549399 Duration 0.272705\n",
      "Batch 01000 of 10 sentences: Average Loss:   0.543852 Duration 0.407641\n",
      "Batch 01100 of 10 sentences: Average Loss:   0.539172 Duration 0.289123\n",
      "Batch 01200 of 10 sentences: Average Loss:   0.534918 Duration 0.254536\n",
      "Batch 01300 of 10 sentences: Average Loss:   0.530460 Duration 0.401049\n",
      "Batch 01400 of 10 sentences: Average Loss:   0.525776 Duration 0.244975\n",
      "Batch 01500 of 10 sentences: Average Loss:   0.522750 Duration 0.191103\n",
      "Batch 01600 of 10 sentences: Average Loss:   0.519390 Duration 0.249695\n",
      "Batch 01700 of 10 sentences: Average Loss:   0.516304 Duration 0.221259\n",
      "Batch 01800 of 10 sentences: Average Loss:   0.513565 Duration 0.265930\n",
      "Batch 01900 of 10 sentences: Average Loss:   0.510889 Duration 0.303209\n",
      "Batch 02000 of 10 sentences: Average Loss:   0.508037 Duration 0.273830\n",
      "Batch 02100 of 10 sentences: Average Loss:   0.505686 Duration 0.261772\n",
      "Batch 02200 of 10 sentences: Average Loss:   0.503053 Duration 0.365279\n",
      "Batch 02300 of 10 sentences: Average Loss:   0.500322 Duration 0.328828\n",
      "Batch 02400 of 10 sentences: Average Loss:   0.498484 Duration 0.244415\n",
      "Batch 02500 of 10 sentences: Average Loss:   0.496072 Duration 0.206160\n",
      "Batch 02600 of 10 sentences: Average Loss:   0.494158 Duration 0.238613\n",
      "Batch 02700 of 10 sentences: Average Loss:   0.492750 Duration 0.321376\n",
      "Batch 02800 of 10 sentences: Average Loss:   0.491098 Duration 0.434486\n",
      "Batch 02900 of 10 sentences: Average Loss:   0.489554 Duration 0.112507\n",
      "Batch 03000 of 10 sentences: Average Loss:   0.487946 Duration 0.195573\n",
      "Batch 03100 of 10 sentences: Average Loss:   0.486685 Duration 0.387284\n",
      "Batch 03200 of 10 sentences: Average Loss:   0.485237 Duration 0.302431\n",
      "Batch 03300 of 10 sentences: Average Loss:   0.483815 Duration 0.238338\n",
      "Batch 03400 of 10 sentences: Average Loss:   0.482549 Duration 0.195041\n",
      "Batch 03500 of 10 sentences: Average Loss:   0.481300 Duration 0.308671\n",
      "Batch 03600 of 10 sentences: Average Loss:   0.480020 Duration 0.358923\n",
      "Batch 03700 of 10 sentences: Average Loss:   0.479072 Duration 0.253361\n",
      "Batch 03800 of 10 sentences: Average Loss:   0.477959 Duration 0.244448\n",
      "Batch 03900 of 10 sentences: Average Loss:   0.476715 Duration 0.296314\n",
      "Batch 04000 of 10 sentences: Average Loss:   0.475446 Duration 0.198472\n",
      "Batch 04100 of 10 sentences: Average Loss:   0.474380 Duration 0.332412\n",
      "Batch 04200 of 10 sentences: Average Loss:   0.473499 Duration 0.169840\n",
      "epoch 0 batches 04207 done\n",
      "Batch 04300 of 10 sentences: Average Loss:   0.472741 Duration 0.379812\n",
      "Batch 04400 of 10 sentences: Average Loss:   0.471944 Duration 0.201968\n",
      "Batch 04500 of 10 sentences: Average Loss:   0.471023 Duration 0.248511\n",
      "Batch 04600 of 10 sentences: Average Loss:   0.469965 Duration 0.249342\n",
      "Batch 04700 of 10 sentences: Average Loss:   0.469188 Duration 0.312931\n",
      "Batch 04800 of 10 sentences: Average Loss:   0.468548 Duration 0.246416\n",
      "Batch 04900 of 10 sentences: Average Loss:   0.467699 Duration 0.205543\n",
      "Batch 05000 of 10 sentences: Average Loss:   0.466856 Duration 0.225995\n",
      "Batch 05100 of 10 sentences: Average Loss:   0.466225 Duration 0.409141\n",
      "Batch 05200 of 10 sentences: Average Loss:   0.465561 Duration 0.206477\n",
      "Batch 05300 of 10 sentences: Average Loss:   0.465026 Duration 0.165252\n",
      "Batch 05400 of 10 sentences: Average Loss:   0.464455 Duration 0.342313\n",
      "Batch 05500 of 10 sentences: Average Loss:   0.463829 Duration 0.241821\n",
      "Batch 05600 of 10 sentences: Average Loss:   0.462993 Duration 0.400646\n",
      "Batch 05700 of 10 sentences: Average Loss:   0.462433 Duration 0.169159\n",
      "Batch 05800 of 10 sentences: Average Loss:   0.461849 Duration 0.308530\n",
      "Batch 05900 of 10 sentences: Average Loss:   0.461257 Duration 0.291807\n",
      "Batch 06000 of 10 sentences: Average Loss:   0.460685 Duration 0.370427\n",
      "Batch 06100 of 10 sentences: Average Loss:   0.460139 Duration 0.229261\n",
      "Batch 06200 of 10 sentences: Average Loss:   0.459531 Duration 0.334804\n",
      "Batch 06300 of 10 sentences: Average Loss:   0.458971 Duration 0.259912\n",
      "Batch 06400 of 10 sentences: Average Loss:   0.458313 Duration 0.333048\n",
      "Batch 06500 of 10 sentences: Average Loss:   0.457542 Duration 0.186094\n",
      "Batch 06600 of 10 sentences: Average Loss:   0.457072 Duration 0.188459\n",
      "Batch 06700 of 10 sentences: Average Loss:   0.456468 Duration 0.245571\n",
      "Batch 06800 of 10 sentences: Average Loss:   0.455854 Duration 0.353215\n",
      "Batch 06900 of 10 sentences: Average Loss:   0.455441 Duration 0.269163\n",
      "Batch 07000 of 10 sentences: Average Loss:   0.454985 Duration 0.180767\n",
      "Batch 07100 of 10 sentences: Average Loss:   0.454530 Duration 0.393786\n",
      "Batch 07200 of 10 sentences: Average Loss:   0.453966 Duration 0.238593\n",
      "Batch 07300 of 10 sentences: Average Loss:   0.453600 Duration 0.276603\n",
      "Batch 07400 of 10 sentences: Average Loss:   0.453105 Duration 0.171775\n",
      "Batch 07500 of 10 sentences: Average Loss:   0.452631 Duration 0.237983\n",
      "Batch 07600 of 10 sentences: Average Loss:   0.452192 Duration 0.223228\n",
      "Batch 07700 of 10 sentences: Average Loss:   0.451723 Duration 0.376202\n",
      "Batch 07800 of 10 sentences: Average Loss:   0.451251 Duration 0.581702\n",
      "Batch 07900 of 10 sentences: Average Loss:   0.450935 Duration 0.352354\n",
      "Batch 08000 of 10 sentences: Average Loss:   0.450500 Duration 0.212490\n",
      "Batch 08100 of 10 sentences: Average Loss:   0.449998 Duration 0.195034\n",
      "Batch 08200 of 10 sentences: Average Loss:   0.449498 Duration 0.431189\n",
      "Batch 08300 of 10 sentences: Average Loss:   0.449050 Duration 0.347273\n",
      "Batch 08400 of 10 sentences: Average Loss:   0.448703 Duration 0.213637\n",
      "epoch 1 batches 08415 done\n",
      "Batch 08500 of 10 sentences: Average Loss:   0.448385 Duration 0.293060\n",
      "Batch 08600 of 10 sentences: Average Loss:   0.448070 Duration 0.453586\n",
      "Batch 08700 of 10 sentences: Average Loss:   0.447710 Duration 0.206971\n",
      "Batch 08800 of 10 sentences: Average Loss:   0.447251 Duration 0.177241\n",
      "Batch 08900 of 10 sentences: Average Loss:   0.446909 Duration 0.274546\n",
      "Batch 09000 of 10 sentences: Average Loss:   0.446652 Duration 0.291930\n",
      "Batch 09100 of 10 sentences: Average Loss:   0.446314 Duration 0.487090\n",
      "Batch 09200 of 10 sentences: Average Loss:   0.445913 Duration 0.471708\n",
      "Batch 09300 of 10 sentences: Average Loss:   0.445664 Duration 0.399354\n",
      "Batch 09400 of 10 sentences: Average Loss:   0.445365 Duration 0.263810\n",
      "Batch 09500 of 10 sentences: Average Loss:   0.445133 Duration 0.398913\n",
      "Batch 09600 of 10 sentences: Average Loss:   0.444835 Duration 0.244507\n",
      "Batch 09700 of 10 sentences: Average Loss:   0.444597 Duration 0.218755\n",
      "Batch 09800 of 10 sentences: Average Loss:   0.444178 Duration 0.258556\n",
      "Batch 09900 of 10 sentences: Average Loss:   0.443918 Duration 0.383352\n",
      "Batch 10000 of 10 sentences: Average Loss:   0.443673 Duration 0.186464\n",
      "Batch 10100 of 10 sentences: Average Loss:   0.443392 Duration 0.309949\n",
      "Batch 10200 of 10 sentences: Average Loss:   0.443138 Duration 0.232402\n",
      "Batch 10300 of 10 sentences: Average Loss:   0.442858 Duration 0.443259\n",
      "Batch 10400 of 10 sentences: Average Loss:   0.442566 Duration 0.321504\n",
      "Batch 10500 of 10 sentences: Average Loss:   0.442241 Duration 0.301989\n",
      "Batch 10600 of 10 sentences: Average Loss:   0.441876 Duration 0.232773\n",
      "Batch 10700 of 10 sentences: Average Loss:   0.441512 Duration 0.290576\n",
      "Batch 10800 of 10 sentences: Average Loss:   0.441230 Duration 0.255927\n",
      "Batch 10900 of 10 sentences: Average Loss:   0.440959 Duration 0.312921\n",
      "Batch 11000 of 10 sentences: Average Loss:   0.440609 Duration 0.174783\n",
      "Batch 11100 of 10 sentences: Average Loss:   0.440386 Duration 0.340395\n",
      "Batch 11200 of 10 sentences: Average Loss:   0.440152 Duration 0.300082\n",
      "Batch 11300 of 10 sentences: Average Loss:   0.439892 Duration 0.355455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11400 of 10 sentences: Average Loss:   0.439596 Duration 0.480985\n",
      "Batch 11500 of 10 sentences: Average Loss:   0.439370 Duration 0.218554\n",
      "Batch 11600 of 10 sentences: Average Loss:   0.439104 Duration 0.298814\n",
      "Batch 11700 of 10 sentences: Average Loss:   0.438841 Duration 0.171770\n",
      "Batch 11800 of 10 sentences: Average Loss:   0.438570 Duration 0.321132\n",
      "Batch 11900 of 10 sentences: Average Loss:   0.438357 Duration 0.389374\n",
      "Batch 12000 of 10 sentences: Average Loss:   0.438090 Duration 0.420473\n",
      "Batch 12100 of 10 sentences: Average Loss:   0.437911 Duration 0.277806\n",
      "Batch 12200 of 10 sentences: Average Loss:   0.437687 Duration 0.134432\n",
      "Batch 12300 of 10 sentences: Average Loss:   0.437359 Duration 0.167764\n",
      "Batch 12400 of 10 sentences: Average Loss:   0.437063 Duration 0.261726\n",
      "Batch 12500 of 10 sentences: Average Loss:   0.436828 Duration 0.289682\n",
      "Batch 12600 of 10 sentences: Average Loss:   0.436605 Duration 0.437866\n",
      "epoch 2 batches 12623 done\n",
      "Batch 12700 of 10 sentences: Average Loss:   0.436424 Duration 0.353098\n",
      "Batch 12800 of 10 sentences: Average Loss:   0.436243 Duration 0.339074\n",
      "Batch 12900 of 10 sentences: Average Loss:   0.436057 Duration 0.370731\n",
      "Batch 13000 of 10 sentences: Average Loss:   0.435763 Duration 0.284835\n",
      "Batch 13100 of 10 sentences: Average Loss:   0.435537 Duration 0.324223\n",
      "Batch 13200 of 10 sentences: Average Loss:   0.435398 Duration 0.250982\n",
      "Batch 13300 of 10 sentences: Average Loss:   0.435196 Duration 0.491314\n",
      "Batch 13400 of 10 sentences: Average Loss:   0.434924 Duration 0.213145\n",
      "Batch 13500 of 10 sentences: Average Loss:   0.434773 Duration 0.247119\n",
      "Batch 13600 of 10 sentences: Average Loss:   0.434603 Duration 0.451632\n",
      "Batch 13700 of 10 sentences: Average Loss:   0.434457 Duration 0.645679\n",
      "Batch 13800 of 10 sentences: Average Loss:   0.434306 Duration 0.223521\n",
      "Batch 13900 of 10 sentences: Average Loss:   0.434162 Duration 0.217689\n",
      "Batch 14000 of 10 sentences: Average Loss:   0.433896 Duration 0.322949\n",
      "Batch 14100 of 10 sentences: Average Loss:   0.433734 Duration 0.268606\n",
      "Batch 14200 of 10 sentences: Average Loss:   0.433605 Duration 0.290929\n",
      "Batch 14300 of 10 sentences: Average Loss:   0.433422 Duration 0.315752\n",
      "Batch 14400 of 10 sentences: Average Loss:   0.433262 Duration 0.256396\n",
      "Batch 14500 of 10 sentences: Average Loss:   0.433076 Duration 0.261133\n",
      "Batch 14600 of 10 sentences: Average Loss:   0.432896 Duration 0.331844\n",
      "Batch 14700 of 10 sentences: Average Loss:   0.432691 Duration 0.169292\n",
      "Batch 14800 of 10 sentences: Average Loss:   0.432499 Duration 0.375960\n",
      "Batch 14900 of 10 sentences: Average Loss:   0.432260 Duration 0.353755\n",
      "Batch 15000 of 10 sentences: Average Loss:   0.432029 Duration 0.246869\n",
      "Batch 15100 of 10 sentences: Average Loss:   0.431862 Duration 0.196038\n",
      "Batch 15200 of 10 sentences: Average Loss:   0.431655 Duration 0.271515\n",
      "Batch 15300 of 10 sentences: Average Loss:   0.431477 Duration 0.322824\n",
      "Batch 15400 of 10 sentences: Average Loss:   0.431324 Duration 0.265615\n",
      "Batch 15500 of 10 sentences: Average Loss:   0.431143 Duration 0.262270\n",
      "Batch 15600 of 10 sentences: Average Loss:   0.430959 Duration 0.316991\n",
      "Batch 15700 of 10 sentences: Average Loss:   0.430811 Duration 0.350712\n",
      "Batch 15800 of 10 sentences: Average Loss:   0.430631 Duration 0.297117\n",
      "Batch 15900 of 10 sentences: Average Loss:   0.430478 Duration 0.364999\n",
      "Batch 16000 of 10 sentences: Average Loss:   0.430270 Duration 0.344086\n",
      "Batch 16100 of 10 sentences: Average Loss:   0.430140 Duration 0.173605\n",
      "Batch 16200 of 10 sentences: Average Loss:   0.429954 Duration 0.236706\n",
      "Batch 16300 of 10 sentences: Average Loss:   0.429830 Duration 0.182566\n",
      "Batch 16400 of 10 sentences: Average Loss:   0.429656 Duration 0.263869\n",
      "Batch 16500 of 10 sentences: Average Loss:   0.429440 Duration 0.232830\n",
      "Batch 16600 of 10 sentences: Average Loss:   0.429225 Duration 0.326340\n",
      "Batch 16700 of 10 sentences: Average Loss:   0.429053 Duration 0.267708\n",
      "Batch 16800 of 10 sentences: Average Loss:   0.428904 Duration 0.248625\n",
      "epoch 3 batches 16831 done\n",
      "Batch 16900 of 10 sentences: Average Loss:   0.428760 Duration 0.175737\n",
      "Batch 17000 of 10 sentences: Average Loss:   0.428641 Duration 0.268104\n",
      "Batch 17100 of 10 sentences: Average Loss:   0.428526 Duration 0.329290\n",
      "Batch 17200 of 10 sentences: Average Loss:   0.428313 Duration 0.258742\n",
      "Batch 17300 of 10 sentences: Average Loss:   0.428149 Duration 0.258643\n",
      "Batch 17400 of 10 sentences: Average Loss:   0.428067 Duration 0.186368\n",
      "Batch 17500 of 10 sentences: Average Loss:   0.427947 Duration 0.184186\n",
      "Batch 17600 of 10 sentences: Average Loss:   0.427760 Duration 0.227871\n",
      "Batch 17700 of 10 sentences: Average Loss:   0.427666 Duration 0.355803\n",
      "Batch 17800 of 10 sentences: Average Loss:   0.427541 Duration 0.303504\n",
      "Batch 17900 of 10 sentences: Average Loss:   0.427434 Duration 0.357272\n",
      "Batch 18000 of 10 sentences: Average Loss:   0.427336 Duration 0.345440\n",
      "Batch 18100 of 10 sentences: Average Loss:   0.427237 Duration 0.414586\n",
      "Batch 18200 of 10 sentences: Average Loss:   0.427058 Duration 0.276645\n",
      "Batch 18300 of 10 sentences: Average Loss:   0.426910 Duration 0.321920\n",
      "Batch 18400 of 10 sentences: Average Loss:   0.426824 Duration 0.279194\n",
      "Batch 18500 of 10 sentences: Average Loss:   0.426678 Duration 0.268667\n",
      "Batch 18600 of 10 sentences: Average Loss:   0.426576 Duration 0.330466\n",
      "Batch 18700 of 10 sentences: Average Loss:   0.426434 Duration 0.372476\n",
      "Batch 18800 of 10 sentences: Average Loss:   0.426294 Duration 0.196198\n",
      "Batch 18900 of 10 sentences: Average Loss:   0.426163 Duration 0.162729\n",
      "Batch 19000 of 10 sentences: Average Loss:   0.426013 Duration 0.207481\n",
      "Batch 19100 of 10 sentences: Average Loss:   0.425840 Duration 0.398012\n",
      "Batch 19200 of 10 sentences: Average Loss:   0.425651 Duration 0.223932\n",
      "Batch 19300 of 10 sentences: Average Loss:   0.425533 Duration 0.164783\n",
      "Batch 19400 of 10 sentences: Average Loss:   0.425394 Duration 0.364202\n",
      "Batch 19500 of 10 sentences: Average Loss:   0.425252 Duration 0.253236\n",
      "Batch 19600 of 10 sentences: Average Loss:   0.425147 Duration 0.316031\n",
      "Batch 19700 of 10 sentences: Average Loss:   0.425025 Duration 0.453282\n",
      "Batch 19800 of 10 sentences: Average Loss:   0.424878 Duration 0.309481\n",
      "Batch 19900 of 10 sentences: Average Loss:   0.424769 Duration 0.222090\n",
      "Batch 20000 of 10 sentences: Average Loss:   0.424626 Duration 0.383839\n",
      "Batch 20100 of 10 sentences: Average Loss:   0.424530 Duration 0.327384\n",
      "Batch 20200 of 10 sentences: Average Loss:   0.424360 Duration 0.261170\n",
      "Batch 20300 of 10 sentences: Average Loss:   0.424277 Duration 0.449739\n",
      "Batch 20400 of 10 sentences: Average Loss:   0.424116 Duration 0.310145\n",
      "Batch 20500 of 10 sentences: Average Loss:   0.424027 Duration 0.258528\n",
      "Batch 20600 of 10 sentences: Average Loss:   0.423914 Duration 0.235014\n",
      "Batch 20700 of 10 sentences: Average Loss:   0.423737 Duration 0.302166\n",
      "Batch 20800 of 10 sentences: Average Loss:   0.423584 Duration 0.248026\n",
      "Batch 20900 of 10 sentences: Average Loss:   0.423435 Duration 0.254568\n",
      "Batch 21000 of 10 sentences: Average Loss:   0.423304 Duration 0.231882\n",
      "epoch 4 batches 21039 done\n",
      "Batch 21100 of 10 sentences: Average Loss:   0.423207 Duration 0.396387\n",
      "Batch 21200 of 10 sentences: Average Loss:   0.423127 Duration 0.351001\n",
      "Batch 21300 of 10 sentences: Average Loss:   0.423042 Duration 0.307743\n",
      "Batch 21400 of 10 sentences: Average Loss:   0.422871 Duration 0.270903\n",
      "Batch 21500 of 10 sentences: Average Loss:   0.422758 Duration 0.420431\n",
      "Batch 21600 of 10 sentences: Average Loss:   0.422692 Duration 0.358144\n",
      "Batch 21700 of 10 sentences: Average Loss:   0.422583 Duration 0.261984\n",
      "Batch 21800 of 10 sentences: Average Loss:   0.422438 Duration 0.241427\n",
      "Batch 21900 of 10 sentences: Average Loss:   0.422352 Duration 0.462462\n",
      "Batch 22000 of 10 sentences: Average Loss:   0.422276 Duration 0.242040\n",
      "Batch 22100 of 10 sentences: Average Loss:   0.422182 Duration 0.382664\n",
      "Batch 22200 of 10 sentences: Average Loss:   0.422114 Duration 0.330384\n",
      "Batch 22300 of 10 sentences: Average Loss:   0.422052 Duration 0.300099\n",
      "Batch 22400 of 10 sentences: Average Loss:   0.421928 Duration 0.403371\n",
      "Batch 22500 of 10 sentences: Average Loss:   0.421808 Duration 0.085313\n",
      "Batch 22600 of 10 sentences: Average Loss:   0.421744 Duration 0.221875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 22700 of 10 sentences: Average Loss:   0.421626 Duration 0.217629\n",
      "Batch 22800 of 10 sentences: Average Loss:   0.421561 Duration 0.479828\n",
      "Batch 22900 of 10 sentences: Average Loss:   0.421454 Duration 0.273519\n",
      "Batch 23000 of 10 sentences: Average Loss:   0.421329 Duration 0.290141\n",
      "Batch 23100 of 10 sentences: Average Loss:   0.421221 Duration 0.180151\n",
      "Batch 23200 of 10 sentences: Average Loss:   0.421106 Duration 0.665356\n",
      "Batch 23300 of 10 sentences: Average Loss:   0.420968 Duration 0.357016\n",
      "Batch 23400 of 10 sentences: Average Loss:   0.420815 Duration 0.172357\n",
      "Batch 23500 of 10 sentences: Average Loss:   0.420732 Duration 0.251593\n",
      "Batch 23600 of 10 sentences: Average Loss:   0.420615 Duration 0.288549\n",
      "Batch 23700 of 10 sentences: Average Loss:   0.420503 Duration 0.397693\n",
      "Batch 23800 of 10 sentences: Average Loss:   0.420421 Duration 0.261572\n",
      "Batch 23900 of 10 sentences: Average Loss:   0.420318 Duration 0.304776\n",
      "Batch 24000 of 10 sentences: Average Loss:   0.420210 Duration 0.336619\n",
      "Batch 24100 of 10 sentences: Average Loss:   0.420117 Duration 0.351768\n",
      "Batch 24200 of 10 sentences: Average Loss:   0.420011 Duration 0.331347\n",
      "Batch 24300 of 10 sentences: Average Loss:   0.419927 Duration 0.459817\n",
      "Batch 24400 of 10 sentences: Average Loss:   0.419794 Duration 0.280574\n",
      "Batch 24500 of 10 sentences: Average Loss:   0.419737 Duration 0.254047\n",
      "Batch 24600 of 10 sentences: Average Loss:   0.419581 Duration 0.443058\n",
      "Batch 24700 of 10 sentences: Average Loss:   0.419506 Duration 0.487797\n",
      "Batch 24800 of 10 sentences: Average Loss:   0.419426 Duration 0.293920\n",
      "Batch 24900 of 10 sentences: Average Loss:   0.419290 Duration 0.307979\n",
      "Batch 25000 of 10 sentences: Average Loss:   0.419191 Duration 0.362619\n",
      "Batch 25100 of 10 sentences: Average Loss:   0.419051 Duration 0.294655\n",
      "Batch 25200 of 10 sentences: Average Loss:   0.418965 Duration 0.316108\n",
      "epoch 5 batches 25247 done\n",
      "Batch 25300 of 10 sentences: Average Loss:   0.418881 Duration 0.312817\n",
      "Batch 25400 of 10 sentences: Average Loss:   0.418806 Duration 0.369367\n",
      "Batch 25500 of 10 sentences: Average Loss:   0.418746 Duration 0.350190\n",
      "Batch 25600 of 10 sentences: Average Loss:   0.418605 Duration 0.509605\n",
      "Batch 25700 of 10 sentences: Average Loss:   0.418509 Duration 0.257579\n",
      "Batch 25800 of 10 sentences: Average Loss:   0.418455 Duration 0.240569\n",
      "Batch 25900 of 10 sentences: Average Loss:   0.418371 Duration 0.404359\n",
      "Batch 26000 of 10 sentences: Average Loss:   0.418240 Duration 0.161827\n",
      "Batch 26100 of 10 sentences: Average Loss:   0.418168 Duration 0.261764\n",
      "Batch 26200 of 10 sentences: Average Loss:   0.418101 Duration 0.279984\n",
      "Batch 26300 of 10 sentences: Average Loss:   0.418026 Duration 0.304750\n",
      "Batch 26400 of 10 sentences: Average Loss:   0.417972 Duration 0.421345\n",
      "Batch 26500 of 10 sentences: Average Loss:   0.417917 Duration 0.303289\n",
      "Batch 26600 of 10 sentences: Average Loss:   0.417814 Duration 0.360124\n",
      "Batch 26700 of 10 sentences: Average Loss:   0.417710 Duration 0.215599\n",
      "Batch 26800 of 10 sentences: Average Loss:   0.417660 Duration 0.269225\n",
      "Batch 26900 of 10 sentences: Average Loss:   0.417561 Duration 0.273911\n",
      "Batch 27000 of 10 sentences: Average Loss:   0.417500 Duration 0.271042\n",
      "Batch 27100 of 10 sentences: Average Loss:   0.417414 Duration 0.294094\n",
      "Batch 27200 of 10 sentences: Average Loss:   0.417318 Duration 0.130673\n",
      "Batch 27300 of 10 sentences: Average Loss:   0.417225 Duration 0.314300\n",
      "Batch 27400 of 10 sentences: Average Loss:   0.417139 Duration 0.314925\n",
      "Batch 27500 of 10 sentences: Average Loss:   0.417036 Duration 0.348475\n",
      "Batch 27600 of 10 sentences: Average Loss:   0.416905 Duration 0.387579\n",
      "Batch 27700 of 10 sentences: Average Loss:   0.416840 Duration 0.381188\n",
      "Batch 27800 of 10 sentences: Average Loss:   0.416736 Duration 0.284829\n",
      "Batch 27900 of 10 sentences: Average Loss:   0.416645 Duration 0.409493\n",
      "Batch 28000 of 10 sentences: Average Loss:   0.416571 Duration 0.380115\n",
      "Batch 28100 of 10 sentences: Average Loss:   0.416489 Duration 0.370929\n",
      "Batch 28200 of 10 sentences: Average Loss:   0.416398 Duration 0.351206\n",
      "Batch 28300 of 10 sentences: Average Loss:   0.416316 Duration 0.208780\n",
      "Batch 28400 of 10 sentences: Average Loss:   0.416231 Duration 0.125571\n",
      "Batch 28500 of 10 sentences: Average Loss:   0.416159 Duration 0.171673\n",
      "Batch 28600 of 10 sentences: Average Loss:   0.416064 Duration 0.418858\n",
      "Batch 28700 of 10 sentences: Average Loss:   0.416017 Duration 0.173111\n",
      "Batch 28800 of 10 sentences: Average Loss:   0.415898 Duration 0.241063\n",
      "Batch 28900 of 10 sentences: Average Loss:   0.415836 Duration 0.152658\n",
      "Batch 29000 of 10 sentences: Average Loss:   0.415762 Duration 0.424482\n",
      "Batch 29100 of 10 sentences: Average Loss:   0.415646 Duration 0.227160\n",
      "Batch 29200 of 10 sentences: Average Loss:   0.415550 Duration 0.281050\n",
      "Batch 29300 of 10 sentences: Average Loss:   0.415431 Duration 0.236839\n",
      "Batch 29400 of 10 sentences: Average Loss:   0.415349 Duration 0.337782\n",
      "epoch 6 batches 29455 done\n",
      "Batch 29500 of 10 sentences: Average Loss:   0.415273 Duration 0.173292\n",
      "Batch 29600 of 10 sentences: Average Loss:   0.415211 Duration 0.136747\n",
      "Batch 29700 of 10 sentences: Average Loss:   0.415158 Duration 0.209019\n",
      "Batch 29800 of 10 sentences: Average Loss:   0.415045 Duration 0.197929\n",
      "Batch 29900 of 10 sentences: Average Loss:   0.414948 Duration 0.254488\n",
      "Batch 30000 of 10 sentences: Average Loss:   0.414900 Duration 0.245868\n",
      "Batch 30100 of 10 sentences: Average Loss:   0.414838 Duration 0.177978\n",
      "Batch 30200 of 10 sentences: Average Loss:   0.414735 Duration 0.336648\n",
      "Batch 30300 of 10 sentences: Average Loss:   0.414664 Duration 0.215074\n",
      "Batch 30400 of 10 sentences: Average Loss:   0.414609 Duration 0.268651\n",
      "Batch 30500 of 10 sentences: Average Loss:   0.414540 Duration 0.166873\n",
      "Batch 30600 of 10 sentences: Average Loss:   0.414500 Duration 0.172803\n",
      "Batch 30700 of 10 sentences: Average Loss:   0.414455 Duration 0.132920\n",
      "Batch 30800 of 10 sentences: Average Loss:   0.414368 Duration 0.208095\n",
      "Batch 30900 of 10 sentences: Average Loss:   0.414281 Duration 0.344134\n",
      "Batch 31000 of 10 sentences: Average Loss:   0.414233 Duration 0.254278\n",
      "Batch 31100 of 10 sentences: Average Loss:   0.414156 Duration 0.214707\n",
      "Batch 31200 of 10 sentences: Average Loss:   0.414105 Duration 0.164656\n",
      "Batch 31300 of 10 sentences: Average Loss:   0.414033 Duration 0.164355\n",
      "Batch 31400 of 10 sentences: Average Loss:   0.413957 Duration 0.316981\n",
      "Batch 31500 of 10 sentences: Average Loss:   0.413875 Duration 0.198810\n",
      "Batch 31600 of 10 sentences: Average Loss:   0.413794 Duration 0.159014\n",
      "Batch 31700 of 10 sentences: Average Loss:   0.413707 Duration 0.176036\n",
      "Batch 31800 of 10 sentences: Average Loss:   0.413582 Duration 0.224286\n",
      "Batch 31900 of 10 sentences: Average Loss:   0.413525 Duration 0.151234\n",
      "Batch 32000 of 10 sentences: Average Loss:   0.413441 Duration 0.274235\n",
      "Batch 32100 of 10 sentences: Average Loss:   0.413360 Duration 0.270045\n",
      "Batch 32200 of 10 sentences: Average Loss:   0.413305 Duration 0.289168\n",
      "Batch 32300 of 10 sentences: Average Loss:   0.413231 Duration 0.235460\n",
      "Batch 32400 of 10 sentences: Average Loss:   0.413153 Duration 0.247747\n",
      "Batch 32500 of 10 sentences: Average Loss:   0.413086 Duration 0.219853\n",
      "Batch 32600 of 10 sentences: Average Loss:   0.413017 Duration 0.178631\n",
      "Batch 32700 of 10 sentences: Average Loss:   0.412947 Duration 0.187430\n",
      "Batch 32800 of 10 sentences: Average Loss:   0.412866 Duration 0.162193\n",
      "Batch 32900 of 10 sentences: Average Loss:   0.412816 Duration 0.240204\n",
      "Batch 33000 of 10 sentences: Average Loss:   0.412718 Duration 0.185059\n",
      "Batch 33100 of 10 sentences: Average Loss:   0.412665 Duration 0.425472\n",
      "Batch 33200 of 10 sentences: Average Loss:   0.412608 Duration 0.173614\n",
      "Batch 33300 of 10 sentences: Average Loss:   0.412499 Duration 0.193204\n",
      "Batch 33400 of 10 sentences: Average Loss:   0.412418 Duration 0.219466\n",
      "Batch 33500 of 10 sentences: Average Loss:   0.412318 Duration 0.131088\n",
      "Batch 33600 of 10 sentences: Average Loss:   0.412242 Duration 0.290217\n",
      "epoch 7 batches 33663 done\n",
      "Batch 33700 of 10 sentences: Average Loss:   0.412188 Duration 0.368502\n",
      "Batch 33800 of 10 sentences: Average Loss:   0.412133 Duration 0.177492\n",
      "Batch 33900 of 10 sentences: Average Loss:   0.412081 Duration 0.138785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 34000 of 10 sentences: Average Loss:   0.411990 Duration 0.232713\n",
      "Batch 34100 of 10 sentences: Average Loss:   0.411904 Duration 0.231471\n",
      "Batch 34200 of 10 sentences: Average Loss:   0.411857 Duration 0.372552\n",
      "Batch 34300 of 10 sentences: Average Loss:   0.411814 Duration 0.211706\n",
      "Batch 34400 of 10 sentences: Average Loss:   0.411731 Duration 0.145076\n",
      "Batch 34500 of 10 sentences: Average Loss:   0.411666 Duration 0.336111\n",
      "Batch 34600 of 10 sentences: Average Loss:   0.411614 Duration 0.272889\n",
      "Batch 34700 of 10 sentences: Average Loss:   0.411558 Duration 0.234218\n",
      "Batch 34800 of 10 sentences: Average Loss:   0.411529 Duration 0.163997\n",
      "Batch 34900 of 10 sentences: Average Loss:   0.411485 Duration 0.169789\n",
      "Batch 35000 of 10 sentences: Average Loss:   0.411413 Duration 0.199089\n",
      "Batch 35100 of 10 sentences: Average Loss:   0.411337 Duration 0.256553\n",
      "Batch 35200 of 10 sentences: Average Loss:   0.411293 Duration 0.152184\n",
      "Batch 35300 of 10 sentences: Average Loss:   0.411230 Duration 0.153118\n",
      "Batch 35400 of 10 sentences: Average Loss:   0.411189 Duration 0.250784\n",
      "Batch 35500 of 10 sentences: Average Loss:   0.411125 Duration 0.200458\n",
      "Batch 35600 of 10 sentences: Average Loss:   0.411053 Duration 0.148759\n",
      "Batch 35700 of 10 sentences: Average Loss:   0.410982 Duration 0.199674\n",
      "Batch 35800 of 10 sentences: Average Loss:   0.410903 Duration 0.077965\n",
      "Batch 35900 of 10 sentences: Average Loss:   0.410829 Duration 0.220070\n",
      "Batch 36000 of 10 sentences: Average Loss:   0.410720 Duration 0.124969\n",
      "Batch 36100 of 10 sentences: Average Loss:   0.410677 Duration 0.169588\n",
      "Batch 36200 of 10 sentences: Average Loss:   0.410603 Duration 0.183789\n",
      "Batch 36300 of 10 sentences: Average Loss:   0.410522 Duration 0.160912\n",
      "Batch 36400 of 10 sentences: Average Loss:   0.410474 Duration 0.212546\n",
      "Batch 36500 of 10 sentences: Average Loss:   0.410411 Duration 0.221913\n",
      "Batch 36600 of 10 sentences: Average Loss:   0.410350 Duration 0.167230\n",
      "Batch 36700 of 10 sentences: Average Loss:   0.410295 Duration 0.188459\n",
      "Batch 36800 of 10 sentences: Average Loss:   0.410242 Duration 0.185610\n",
      "Batch 36900 of 10 sentences: Average Loss:   0.410180 Duration 0.203705\n",
      "Batch 37000 of 10 sentences: Average Loss:   0.410113 Duration 0.243279\n",
      "Batch 37100 of 10 sentences: Average Loss:   0.410065 Duration 0.235855\n",
      "Batch 37200 of 10 sentences: Average Loss:   0.409978 Duration 0.065647\n",
      "Batch 37300 of 10 sentences: Average Loss:   0.409920 Duration 0.172145\n",
      "Batch 37400 of 10 sentences: Average Loss:   0.409875 Duration 0.170219\n",
      "Batch 37500 of 10 sentences: Average Loss:   0.409780 Duration 0.199295\n",
      "Batch 37600 of 10 sentences: Average Loss:   0.409700 Duration 0.160885\n",
      "Batch 37700 of 10 sentences: Average Loss:   0.409609 Duration 0.121414\n",
      "Batch 37800 of 10 sentences: Average Loss:   0.409540 Duration 0.160867\n",
      "epoch 8 batches 37871 done\n",
      "Batch 37900 of 10 sentences: Average Loss:   0.409488 Duration 0.212317\n",
      "Batch 38000 of 10 sentences: Average Loss:   0.409447 Duration 0.188601\n",
      "Batch 38100 of 10 sentences: Average Loss:   0.409400 Duration 0.316155\n",
      "Batch 38200 of 10 sentences: Average Loss:   0.409318 Duration 0.185943\n",
      "Batch 38300 of 10 sentences: Average Loss:   0.409244 Duration 0.351670\n",
      "Batch 38400 of 10 sentences: Average Loss:   0.409195 Duration 0.147873\n",
      "Batch 38500 of 10 sentences: Average Loss:   0.409157 Duration 0.204007\n",
      "Batch 38600 of 10 sentences: Average Loss:   0.409089 Duration 0.199505\n",
      "Batch 38700 of 10 sentences: Average Loss:   0.409017 Duration 0.180654\n",
      "Batch 38800 of 10 sentences: Average Loss:   0.408977 Duration 0.232033\n",
      "Batch 38900 of 10 sentences: Average Loss:   0.408924 Duration 0.230428\n",
      "Batch 39000 of 10 sentences: Average Loss:   0.408901 Duration 0.217670\n",
      "Batch 39100 of 10 sentences: Average Loss:   0.408858 Duration 0.148431\n",
      "Batch 39200 of 10 sentences: Average Loss:   0.408792 Duration 0.204815\n",
      "Batch 39300 of 10 sentences: Average Loss:   0.408724 Duration 0.168644\n",
      "Batch 39400 of 10 sentences: Average Loss:   0.408691 Duration 0.205545\n",
      "Batch 39500 of 10 sentences: Average Loss:   0.408633 Duration 0.177476\n",
      "Batch 39600 of 10 sentences: Average Loss:   0.408588 Duration 0.167150\n",
      "Batch 39700 of 10 sentences: Average Loss:   0.408538 Duration 0.141181\n",
      "Batch 39800 of 10 sentences: Average Loss:   0.408471 Duration 0.106747\n",
      "Batch 39900 of 10 sentences: Average Loss:   0.408407 Duration 0.220306\n",
      "Batch 40000 of 10 sentences: Average Loss:   0.408338 Duration 0.063635\n",
      "Batch 40100 of 10 sentences: Average Loss:   0.408270 Duration 0.192704\n",
      "Batch 40200 of 10 sentences: Average Loss:   0.408177 Duration 0.196902\n",
      "Batch 40300 of 10 sentences: Average Loss:   0.408140 Duration 0.159810\n",
      "Batch 40400 of 10 sentences: Average Loss:   0.408071 Duration 0.167927\n",
      "Batch 40500 of 10 sentences: Average Loss:   0.408001 Duration 0.133949\n",
      "Batch 40600 of 10 sentences: Average Loss:   0.407956 Duration 0.179524\n",
      "Batch 40700 of 10 sentences: Average Loss:   0.407909 Duration 0.217240\n",
      "Batch 40800 of 10 sentences: Average Loss:   0.407851 Duration 0.219485\n",
      "Batch 40900 of 10 sentences: Average Loss:   0.407796 Duration 0.165023\n",
      "Batch 41000 of 10 sentences: Average Loss:   0.407754 Duration 0.184698\n",
      "Batch 41100 of 10 sentences: Average Loss:   0.407696 Duration 0.162444\n",
      "Batch 41200 of 10 sentences: Average Loss:   0.407639 Duration 0.142140\n",
      "Batch 41300 of 10 sentences: Average Loss:   0.407592 Duration 0.139743\n",
      "Batch 41400 of 10 sentences: Average Loss:   0.407523 Duration 0.154620\n",
      "Batch 41500 of 10 sentences: Average Loss:   0.407458 Duration 0.246761\n",
      "Batch 41600 of 10 sentences: Average Loss:   0.407423 Duration 0.189086\n",
      "Batch 41700 of 10 sentences: Average Loss:   0.407345 Duration 0.229595\n",
      "Batch 41800 of 10 sentences: Average Loss:   0.407277 Duration 0.232005\n",
      "Batch 41900 of 10 sentences: Average Loss:   0.407194 Duration 0.149840\n",
      "Batch 42000 of 10 sentences: Average Loss:   0.407130 Duration 0.187060\n",
      "epoch 9 batches 42079 done\n",
      "Batch 42100 of 10 sentences: Average Loss:   0.407078 Duration 0.133631\n",
      "Batch 42200 of 10 sentences: Average Loss:   0.407042 Duration 0.160405\n",
      "Batch 42300 of 10 sentences: Average Loss:   0.407002 Duration 0.177758\n",
      "Batch 42400 of 10 sentences: Average Loss:   0.406928 Duration 0.186796\n",
      "Batch 42500 of 10 sentences: Average Loss:   0.406865 Duration 0.154454\n",
      "Batch 42600 of 10 sentences: Average Loss:   0.406814 Duration 0.138002\n",
      "Batch 42700 of 10 sentences: Average Loss:   0.406783 Duration 0.184362\n",
      "Batch 42800 of 10 sentences: Average Loss:   0.406718 Duration 0.174518\n",
      "Batch 42900 of 10 sentences: Average Loss:   0.406650 Duration 0.195182\n",
      "Batch 43000 of 10 sentences: Average Loss:   0.406613 Duration 0.102122\n",
      "Batch 43100 of 10 sentences: Average Loss:   0.406574 Duration 0.189029\n",
      "Batch 43200 of 10 sentences: Average Loss:   0.406549 Duration 0.232934\n",
      "Batch 43300 of 10 sentences: Average Loss:   0.406511 Duration 0.193102\n",
      "Batch 43400 of 10 sentences: Average Loss:   0.406461 Duration 0.198300\n",
      "Batch 43500 of 10 sentences: Average Loss:   0.406395 Duration 0.168914\n",
      "Batch 43600 of 10 sentences: Average Loss:   0.406362 Duration 0.186597\n",
      "Batch 43700 of 10 sentences: Average Loss:   0.406316 Duration 0.171672\n",
      "Batch 43800 of 10 sentences: Average Loss:   0.406273 Duration 0.254367\n",
      "Batch 43900 of 10 sentences: Average Loss:   0.406233 Duration 0.245008\n",
      "Batch 44000 of 10 sentences: Average Loss:   0.406174 Duration 0.164803\n",
      "Batch 44100 of 10 sentences: Average Loss:   0.406112 Duration 0.212235\n",
      "Batch 44200 of 10 sentences: Average Loss:   0.406059 Duration 0.204234\n",
      "Batch 44300 of 10 sentences: Average Loss:   0.405985 Duration 0.215178\n",
      "Batch 44400 of 10 sentences: Average Loss:   0.405903 Duration 0.129381\n",
      "Batch 44500 of 10 sentences: Average Loss:   0.405865 Duration 0.177215\n",
      "Batch 44600 of 10 sentences: Average Loss:   0.405796 Duration 0.279383\n",
      "Batch 44700 of 10 sentences: Average Loss:   0.405729 Duration 0.122113\n",
      "Batch 44800 of 10 sentences: Average Loss:   0.405690 Duration 0.259768\n",
      "Batch 44900 of 10 sentences: Average Loss:   0.405641 Duration 0.156797\n",
      "Batch 45000 of 10 sentences: Average Loss:   0.405588 Duration 0.246334\n",
      "Batch 45100 of 10 sentences: Average Loss:   0.405541 Duration 0.241993\n",
      "Batch 45200 of 10 sentences: Average Loss:   0.405498 Duration 0.168687\n",
      "Batch 45300 of 10 sentences: Average Loss:   0.405442 Duration 0.209086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 45400 of 10 sentences: Average Loss:   0.405388 Duration 0.227461\n",
      "Batch 45500 of 10 sentences: Average Loss:   0.405346 Duration 0.197183\n",
      "Batch 45600 of 10 sentences: Average Loss:   0.405281 Duration 0.231802\n",
      "Batch 45700 of 10 sentences: Average Loss:   0.405220 Duration 0.231430\n",
      "Batch 45800 of 10 sentences: Average Loss:   0.405185 Duration 0.198672\n",
      "Batch 45900 of 10 sentences: Average Loss:   0.405112 Duration 0.209959\n",
      "Batch 46000 of 10 sentences: Average Loss:   0.405050 Duration 0.171040\n",
      "Batch 46100 of 10 sentences: Average Loss:   0.404975 Duration 0.201131\n",
      "Batch 46200 of 10 sentences: Average Loss:   0.404918 Duration 0.166828\n",
      "epoch 10 batches 46287 done\n",
      "Batch 46300 of 10 sentences: Average Loss:   0.404874 Duration 0.181416\n",
      "Batch 46400 of 10 sentences: Average Loss:   0.404840 Duration 0.195611\n",
      "Batch 46500 of 10 sentences: Average Loss:   0.404803 Duration 0.124634\n",
      "Batch 46600 of 10 sentences: Average Loss:   0.404744 Duration 0.251456\n",
      "Batch 46700 of 10 sentences: Average Loss:   0.404686 Duration 0.204163\n",
      "Batch 46800 of 10 sentences: Average Loss:   0.404633 Duration 0.253350\n",
      "Batch 46900 of 10 sentences: Average Loss:   0.404605 Duration 0.188618\n",
      "Batch 47000 of 10 sentences: Average Loss:   0.404543 Duration 0.218926\n",
      "Batch 47100 of 10 sentences: Average Loss:   0.404487 Duration 0.136214\n",
      "Batch 47200 of 10 sentences: Average Loss:   0.404449 Duration 0.109002\n",
      "Batch 47300 of 10 sentences: Average Loss:   0.404411 Duration 0.264505\n",
      "Batch 47400 of 10 sentences: Average Loss:   0.404392 Duration 0.229535\n",
      "Batch 47500 of 10 sentences: Average Loss:   0.404352 Duration 0.236313\n",
      "Batch 47600 of 10 sentences: Average Loss:   0.404314 Duration 0.123178\n",
      "Batch 47700 of 10 sentences: Average Loss:   0.404251 Duration 0.198565\n",
      "Batch 47800 of 10 sentences: Average Loss:   0.404217 Duration 0.195512\n",
      "Batch 47900 of 10 sentences: Average Loss:   0.404179 Duration 0.301190\n",
      "Batch 48000 of 10 sentences: Average Loss:   0.404138 Duration 0.179718\n",
      "Batch 48100 of 10 sentences: Average Loss:   0.404101 Duration 0.145605\n",
      "Batch 48200 of 10 sentences: Average Loss:   0.404048 Duration 0.300759\n",
      "Batch 48300 of 10 sentences: Average Loss:   0.403997 Duration 0.219570\n",
      "Batch 48400 of 10 sentences: Average Loss:   0.403946 Duration 0.372850\n",
      "Batch 48500 of 10 sentences: Average Loss:   0.403877 Duration 0.165100\n",
      "Batch 48600 of 10 sentences: Average Loss:   0.403804 Duration 0.171986\n",
      "Batch 48700 of 10 sentences: Average Loss:   0.403762 Duration 0.189335\n",
      "Batch 48800 of 10 sentences: Average Loss:   0.403695 Duration 0.208512\n",
      "Batch 48900 of 10 sentences: Average Loss:   0.403632 Duration 0.202754\n",
      "Batch 49000 of 10 sentences: Average Loss:   0.403590 Duration 0.215350\n",
      "Batch 49100 of 10 sentences: Average Loss:   0.403554 Duration 0.186237\n",
      "Batch 49200 of 10 sentences: Average Loss:   0.403507 Duration 0.185347\n",
      "Batch 49300 of 10 sentences: Average Loss:   0.403461 Duration 0.332006\n",
      "Batch 49400 of 10 sentences: Average Loss:   0.403422 Duration 0.203778\n",
      "Batch 49500 of 10 sentences: Average Loss:   0.403372 Duration 0.141513\n",
      "Batch 49600 of 10 sentences: Average Loss:   0.403322 Duration 0.278301\n",
      "Batch 49700 of 10 sentences: Average Loss:   0.403282 Duration 0.138294\n",
      "Batch 49800 of 10 sentences: Average Loss:   0.403223 Duration 0.170940\n",
      "Batch 49900 of 10 sentences: Average Loss:   0.403167 Duration 0.187320\n",
      "Batch 50000 of 10 sentences: Average Loss:   0.403135 Duration 0.241568\n",
      "Batch 50100 of 10 sentences: Average Loss:   0.403072 Duration 0.154139\n",
      "Batch 50200 of 10 sentences: Average Loss:   0.403012 Duration 0.250215\n",
      "Batch 50300 of 10 sentences: Average Loss:   0.402938 Duration 0.172023\n",
      "Batch 50400 of 10 sentences: Average Loss:   0.402888 Duration 0.253046\n",
      "epoch 11 batches 50495 done\n",
      "Batch 50500 of 10 sentences: Average Loss:   0.402849 Duration 0.201961\n",
      "Batch 50600 of 10 sentences: Average Loss:   0.402816 Duration 0.129593\n",
      "Batch 50700 of 10 sentences: Average Loss:   0.402780 Duration 0.175459\n",
      "Batch 50800 of 10 sentences: Average Loss:   0.402728 Duration 0.123078\n",
      "Batch 50900 of 10 sentences: Average Loss:   0.402668 Duration 0.190635\n",
      "Batch 51000 of 10 sentences: Average Loss:   0.402618 Duration 0.284022\n",
      "Batch 51100 of 10 sentences: Average Loss:   0.402594 Duration 0.227961\n",
      "Batch 51200 of 10 sentences: Average Loss:   0.402543 Duration 0.250490\n",
      "Batch 51300 of 10 sentences: Average Loss:   0.402487 Duration 0.177326\n",
      "Batch 51400 of 10 sentences: Average Loss:   0.402451 Duration 0.139061\n",
      "Batch 51500 of 10 sentences: Average Loss:   0.402414 Duration 0.188802\n",
      "Batch 51600 of 10 sentences: Average Loss:   0.402390 Duration 0.204556\n",
      "Batch 51700 of 10 sentences: Average Loss:   0.402353 Duration 0.207830\n",
      "Batch 51800 of 10 sentences: Average Loss:   0.402318 Duration 0.203913\n",
      "Batch 51900 of 10 sentences: Average Loss:   0.402260 Duration 0.140043\n",
      "Batch 52000 of 10 sentences: Average Loss:   0.402234 Duration 0.171975\n",
      "Batch 52100 of 10 sentences: Average Loss:   0.402195 Duration 0.213181\n",
      "Batch 52200 of 10 sentences: Average Loss:   0.402155 Duration 0.268693\n",
      "Batch 52300 of 10 sentences: Average Loss:   0.402118 Duration 0.201494\n",
      "Batch 52400 of 10 sentences: Average Loss:   0.402073 Duration 0.253495\n",
      "Batch 52500 of 10 sentences: Average Loss:   0.402021 Duration 0.154794\n",
      "Batch 52600 of 10 sentences: Average Loss:   0.401977 Duration 0.159760\n",
      "Batch 52700 of 10 sentences: Average Loss:   0.401915 Duration 0.207055\n",
      "Batch 52800 of 10 sentences: Average Loss:   0.401851 Duration 0.252791\n",
      "Batch 52900 of 10 sentences: Average Loss:   0.401814 Duration 0.401966\n",
      "Batch 53000 of 10 sentences: Average Loss:   0.401755 Duration 0.196023\n",
      "Batch 53100 of 10 sentences: Average Loss:   0.401695 Duration 0.110776\n",
      "Batch 53200 of 10 sentences: Average Loss:   0.401658 Duration 0.156007\n",
      "Batch 53300 of 10 sentences: Average Loss:   0.401614 Duration 0.144884\n",
      "Batch 53400 of 10 sentences: Average Loss:   0.401570 Duration 0.086673\n",
      "Batch 53500 of 10 sentences: Average Loss:   0.401523 Duration 0.156114\n",
      "Batch 53600 of 10 sentences: Average Loss:   0.401489 Duration 0.238965\n",
      "Batch 53700 of 10 sentences: Average Loss:   0.401446 Duration 0.114158\n",
      "Batch 53800 of 10 sentences: Average Loss:   0.401404 Duration 0.231467\n",
      "Batch 53900 of 10 sentences: Average Loss:   0.401357 Duration 0.480226\n"
     ]
    }
   ],
   "source": [
    "def sentences_generator(path_to_file, num_sentences):\n",
    "    stream = fileio.Function.file_line_stream(path_to_file)\n",
    "    try:\n",
    "        while True:\n",
    "            _lines = fileio.Function.take(num_sentences, stream)\n",
    "            yield np.array(_lines)\n",
    "    finally:\n",
    "        stream.close()\n",
    "\n",
    "# Sentences for the trainig\n",
    "source = sentences_generator(\n",
    "    path_to_file=path_to_corpus, num_sentences=NUM_SENTENCES\n",
    ")\n",
    "\n",
    "# Restore the state if exists.\n",
    "state = embedding.load(STATE_FILE)\n",
    "\n",
    "# Continue training\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "\n",
    "total_sentences = 0\n",
    "epochs = 0\n",
    "\n",
    "for i in range(MAX_ITERATIONS):\n",
    "    try:\n",
    "        sentences = next(source)\n",
    "        total_sentences += len(sentences)\n",
    "\n",
    "        start = time.time()\n",
    "        network.train(X=sentences, T=np.array([0]))\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                f\"Batch {i:05d} of {NUM_SENTENCES} sentences: \"\n",
    "                f\"Average Loss: {np.mean(network.history):10f} \"\n",
    "                f\"Duration {time.time() - start:3f}\"\n",
    "            )\n",
    "        if i % 1000 == 0:\n",
    "            # embedding.save(STATE_FILE)\n",
    "            pass\n",
    "        \n",
    "    except fileio.Function.GenearatorHasNoMore as e:\n",
    "        source.close()\n",
    "        embedding.save(STATE_FILE)\n",
    "\n",
    "        # Next epoch\n",
    "        print(f\"epoch {epochs} batches {i:05d} done\")\n",
    "        epochs += 1\n",
    "        source = sentences_generator(\n",
    "            path_to_file=path_to_corpus, num_sentences=NUM_SENTENCES\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        source.close()\n",
    "        raise e\n",
    "\n",
    "embedding.save(STATE_FILE)\n",
    "\n",
    "profiler.disable()\n",
    "profiler.print_stats(sort=\"cumtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluate the vector space\n",
    "\n",
    "Verify if the trained model, or the vector space W, has encoded the words in a way that **similar** words are close in the vector space.\n",
    "\n",
    "* [How to measure the similarity among vectors](https://math.stackexchange.com/questions/4132458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "context = \"cash\".split()\n",
    "word_indices = np.array(word_indexing.list_indices(context), dtype=TYPE_INT)\n",
    "\n",
    "print(f\"Words {context}\")\n",
    "print(f\"Word indices {word_indices}\")\n",
    "print(f\"prediction for {context}:\\n{word_indexing.list_events([embedding.predict(word_indices, n)])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Compare with [gensim word2vec](https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import (\n",
    "    Word2Vec\n",
    ")\n",
    "from gensim.models.word2vec import (\n",
    "    LineSentence    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = LineSentence(source=path_to_corpus)\n",
    "w2v = Word2Vec(\n",
    "    sentences=sentences, \n",
    "    sg=0,\n",
    "    window=5, \n",
    "    negative=5,\n",
    "    vector_size=100, \n",
    "    min_count=1, \n",
    "    workers=4\n",
    ")\n",
    "del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(context, topn=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
