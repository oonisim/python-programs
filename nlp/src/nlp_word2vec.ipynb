{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec implementation\n",
    "\n",
    "* [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "* [word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method](https://arxiv.org/abs/1402.3722)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The original paper proposed using:\n",
    "1. Matmul to extract word vectors from ```Win```.\n",
    "2. Softmax to calculate scores with all the word vectors from ```Wout```.\n",
    "\n",
    "<img src=\"image/word2vec_cbow_mechanism.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "### Motivation\n",
    "\n",
    "The original implementation was computationally expensive.\n",
    "\n",
    "1. Matmal to extract word vectors from ```Win```.\n",
    "2. Softmax can happen vocabulary size times with ```Wout```.\n",
    "\n",
    "<img src=\"image/word2vec_negative_sampling_motivation.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1. Use index to extract word vectors from Win.\n",
    "2. Instead of calculating softmax with all the word vectors from ```Wout```, sample small number (SL) of negative/false word vectors from ```Wout``` and calculate logistic log loss with each sample. \n",
    "\n",
    "<img src=\"image/wors2vec_neg_sample_backprop.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using only one Word vector space W\n",
    "\n",
    "There is no reasoning nor proof why two word vector space are required. In the end, we only use one word vector space, which appears to be ```Win```. Use only one word vector space ```W``` to test if it works.\n",
    "\n",
    "<img src=\"image/word2vec_backprop_Be.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from itertools import islice\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=400) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Google Colab environment\n",
    "\n",
    "Colab gets disconnected within approx 20 min. Hence not suitable for training (or need to upgrade to the pro version)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone github to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_GOOGLE_COLAB = True\n",
    "except:\n",
    "    IN_GOOGLE_COLAB = False\n",
    "    \n",
    "if IN_GOOGLE_COLAB:\n",
    "    !pip install line_profiler\n",
    "    !google.colab.drive.mount('/content/gdrive')\n",
    "    !rm -rf /content/drive/MyDrive/github\n",
    "    !mkdir -p /content/drive/MyDrive/github\n",
    "    !git clone https://github.com/oonisim/python-programs.git /content/drive/MyDrive/github\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone github to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_GOOGLE_COLAB = True\n",
    "except:\n",
    "    IN_GOOGLE_COLAB = False\n",
    "    \n",
    "if IN_GOOGLE_COLAB:\n",
    "    !pip install line_profiler\n",
    "    !google.colab.drive.mount('/content/gdrive')\n",
    "    !rm -rf /content/github\n",
    "    !mkdir -p /content/github\n",
    "    !git clone https://github.com/oonisim/python-programs.git /content/github\n",
    "        \n",
    "    import sys\n",
    "    sys.path.append('/content/github/nlp/src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook setups\n",
    "\n",
    "Auto reolaod causes an error in Jupyter notebooks. Restart the Jupyter kernel for the error:\n",
    "```TypeError: super(type, obj): obj must be an instance or subtype of type```\n",
    "See\n",
    "- https://stackoverflow.com/a/52927102/4281353\n",
    "- http://thomas-cokelaer.info/blog/2011/09/382/\n",
    "\n",
    "> The problem resides in the mechanism of reloading modules.\n",
    "> Reloading a module often changes the internal object in memory which\n",
    "> makes the isinstance test of super return False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "import function.fileio as fileio\n",
    "import function.text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.constant import (\n",
    "    TYPE_INT,\n",
    "    TYPE_FLOAT,\n",
    "    TYPE_LABEL,\n",
    "    TYPE_TENSOR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PTB = True\n",
    "DEBUG = False\n",
    "VALIDATION = True\n",
    "\n",
    "TARGET_SIZE = 1   # Size of the target event (word)\n",
    "CONTEXT_SIZE = 6  # Size of the context.\n",
    "WINDOW_SIZE = TARGET_SIZE + CONTEXT_SIZE\n",
    "SAMPLE_SIZE = 10   # Size of the negative samples\n",
    "VECTOR_SIZE = 100  # Number of features in the event vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"To be, or not to be, that is the question that matters\"\n",
    "_file = \"ptb.train.txt\"\n",
    "if USE_PTB:\n",
    "    if not fileio.Function.is_file(f\"~/.keras/datasets/{_file}\"):\n",
    "        path_to_ptb = tf.keras.utils.get_file(\n",
    "            _file, \n",
    "            f'https://raw.githubusercontent.com/tomsercu/lstm/master/data/{_file}'\n",
    "        )\n",
    "    corpus = fileio.Function.read_file(path_to_ptb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n",
      " pierre <unk> N years old will join the board as a nonexecutive director nov. N \n",
      " mr. <unk> is chairman of <unk> n.v. the dutch publishing group \n",
      " rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate \n",
      " a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported \n"
     ]
    }
   ],
   "source": [
    "examples = corpus.split('\\n')[:5]\n",
    "for line in examples:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Event (word) indexing\n",
    "Index the events that have occurred in the event sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer.preprocessing import (\n",
    "    EventIndexing, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexing = EventIndexing(\n",
    "    name=\"word_indexing_on_ptb\",\n",
    "    corpus=corpus\n",
    ")\n",
    "del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EventIndexing  for the corpus\n",
    "\n",
    "Adapt to the ```corpus``` and provides:\n",
    "* event_to_index dictionary\n",
    "* vocaburary of the corpus\n",
    "* word occurrence probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EventIndexing.vocabulary[10]:\n",
      "['<nil>' '<unk>' 'aer' 'banknote' 'berlitz' 'calloway' 'centrust' 'cluett' 'fromstein' 'gitano']\n",
      "\n",
      "EventIndexing.event_to_index[10]:\n",
      "('<nil>', 0)\n",
      "('<unk>', 1)\n",
      "('aer', 2)\n",
      "('banknote', 3)\n",
      "('berlitz', 4)\n",
      "('calloway', 5)\n",
      "('centrust', 6)\n",
      "('cluett', 7)\n",
      "('fromstein', 8)\n",
      "('gitano', 9)\n",
      "\n",
      "EventIndexing.probabilities[10]:\n",
      "<nil>                : 0.00000e+00\n",
      "<unk>                : 1.65308e-02\n",
      "aer                  : 5.34860e-06\n",
      "banknote             : 5.34860e-06\n",
      "berlitz              : 5.34860e-06\n",
      "calloway             : 5.34860e-06\n",
      "centrust             : 5.34860e-06\n",
      "cluett               : 5.34860e-06\n",
      "fromstein            : 5.34860e-06\n",
      "gitano               : 5.34860e-06\n"
     ]
    }
   ],
   "source": [
    "words = word_indexing.list_events(range(10))\n",
    "print(f\"EventIndexing.vocabulary[10]:\\n{words}\\n\")\n",
    "\n",
    "indices = word_indexing.list_indices(words)\n",
    "print(f\"EventIndexing.event_to_index[10]:\")\n",
    "for item in zip(words, indices):\n",
    "    print(item)\n",
    "\n",
    "probabilities = word_indexing.list_probabilities(words)\n",
    "print(f\"\\nEventIndexing.probabilities[10]:\")\n",
    "for word, p in zip(words, probabilities):\n",
    "    print(f\"{word:20s} : {p:.5e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling using the probability\n",
    "\n",
    "Sample events according to their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subordinate', 'share', 'into', 't', 'sentences']\n"
     ]
    }
   ],
   "source": [
    "sample = word_indexing.sample(size=5)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "Sample events not including those events already sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_indices=[1024, 6433, 9445, 268, 44] \n",
      "events=['i' 'listening' 'fazio' 'environmental' 'of']\n"
     ]
    }
   ],
   "source": [
    "negative_indices = word_indexing.negative_sample_indices(\n",
    "    size=5, excludes=word_indexing.list_indices(sample)\n",
    ")\n",
    "print(f\"negative_indices={negative_indices} \\nevents={word_indexing.list_events(negative_indices)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the             :    34\n",
      "asbestos        :    63\n",
      "fiber           :    86\n",
      "<unk>           :     1\n",
      "is              :    42\n",
      "unusually       :    87\n",
      "<unk>           :     1\n",
      "once            :    64\n",
      "it              :    80\n",
      "enters          :    88\n",
      "the             :    34\n",
      "<unk>           :     1\n",
      "\n",
      "with           :     0\n",
      "even            :     0\n",
      "brief           :     0\n"
     ]
    }
   ],
   "source": [
    "# sentences = \"\\n\".join(corpus.split('\\n')[5:6])\n",
    "sentences = \"\"\"\n",
    "the asbestos fiber <unk> is unusually <unk> once it enters the <unk> \n",
    "with even brief exposures to it causing symptoms that show up decades later researchers said\n",
    "\"\"\"\n",
    "sequences = word_indexing.function(sentences)\n",
    "for pair in zip(sentences.strip().split(\" \"), sequences[0]):\n",
    "    print(f\"{pair[0]:15} : {pair[1]:5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EventContext\n",
    "\n",
    "EventContext layer generates ```(event, context)``` pairs from sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer.preprocessing import (\n",
    "    EventContext\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_context = EventContext(\n",
    "    name=\"ev\",\n",
    "    window_size=WINDOW_SIZE,\n",
    "    event_size=TARGET_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context of an event (word) in a sentence\n",
    "\n",
    "In the sentence ```\"a form of asbestos once used to make kent cigarette filters\"```, one of the context windows ```a form of asbestos once``` of size 5 and event size 1 has.\n",
    "* ```of``` as a target word.\n",
    "* ```(a, form) and (asbestos, once)``` as its context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence of the word indices for the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[37, 62, 44, 63, 64, 65, 66, 67, 68, 69, 70,  0,  0,  0,  0,  0],\n",
       "       [29, 30, 31, 50, 51, 43, 44, 52, 53, 54, 55, 56, 57, 37, 38, 39]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \"\"\"\n",
    "a form of asbestos once used to make kent cigarette filters\n",
    "\n",
    "N years old and former chairman of consolidated gold fields plc was named a nonexecutive director\n",
    "\"\"\"\n",
    "\n",
    "sequence = word_indexing.function(sentences)\n",
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (event, context) pairs\n",
    "\n",
    "For each word (event) in the setence ```(of, asbestos, ... , kent)``` excludnig the ends of the sentence, create ```(target, context)``` as:\n",
    "\n",
    "```\n",
    "[\n",
    "  [of, a, form, asbestos, once],              # target is 'of', context is (a, form, asbestos, once)\n",
    "  ['asbestos', 'form', 'of', 'once', 'used'],\n",
    "  ['once', 'of', 'asbestos', 'used', 'to'],\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "### Format of the (event, context) pairs\n",
    "\n",
    "* **E** is the target event indices\n",
    "* **C** is the context indices\n",
    "\n",
    "<img src=\"image/event_context_format.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event context pairs. Shape (18, 7), Target event size 1, Window size 7.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[63, 37, 62, 44, 64, 65, 66],\n",
       "       [64, 62, 44, 63, 65, 66, 67],\n",
       "       [65, 44, 63, 64, 66, 67, 68],\n",
       "       [66, 63, 64, 65, 67, 68, 69],\n",
       "       [67, 64, 65, 66, 68, 69, 70],\n",
       "       [68, 65, 66, 67, 69, 70,  0],\n",
       "       [69, 66, 67, 68, 70,  0,  0],\n",
       "       [70, 67, 68, 69,  0,  0,  0],\n",
       "       [50, 29, 30, 31, 51, 43, 44],\n",
       "       [51, 30, 31, 50, 43, 44, 52]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_context_pairs = event_context.function(sequence)\n",
    "print(\n",
    "    f\"Event context pairs. Shape %s, Target event size %s, Window size %s.\" % \n",
    "    (event_context_pairs.shape, event_context.event_size, event_context.window_size)\n",
    ")\n",
    "event_context_pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (event, context) pairs in textual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['asbestos', 'a', 'form', 'of', 'once', 'used', 'to'],\n",
       " ['once', 'form', 'of', 'asbestos', 'used', 'to', 'make'],\n",
       " ['used', 'of', 'asbestos', 'once', 'to', 'make', 'kent'],\n",
       " ['to', 'asbestos', 'once', 'used', 'make', 'kent', 'cigarette'],\n",
       " ['make', 'once', 'used', 'to', 'kent', 'cigarette', 'filters'],\n",
       " ['kent', 'used', 'to', 'make', 'cigarette', 'filters', '<nil>'],\n",
       " ['cigarette', 'to', 'make', 'kent', 'filters', '<nil>', '<nil>'],\n",
       " ['filters', 'make', 'kent', 'cigarette', '<nil>', '<nil>', '<nil>'],\n",
       " ['and', 'n', 'years', 'old', 'former', 'chairman', 'of'],\n",
       " ['former', 'years', 'old', 'and', 'chairman', 'of', 'consolidated']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indexing.sequence_to_sentence(event_context_pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Word Embedding\n",
    "\n",
    "Embedding is to train the model to group similar events in a close proximity in the event vector space. If two events e.g. 'pencil' and 'pen' are similar concepts, then their event vectors resides in a close distance in the event space. \n",
    "\n",
    "* [Thought Vectors](https://wiki.pathmind.com/thought-vectors)\n",
    "\n",
    "## Training process\n",
    "\n",
    "1. Calculate ```Bc```, the BoW (Bag of Words) from context event vectors.\n",
    "2. Calculate ```Be```,  the BoW (Bag of Words) from target event vectors.\n",
    "3. The dot product ```Ye = dot(Bc, Be)``` is given the label 1 to get them closer.\n",
    "4. For each negative sample ```Ws(s)```, the dot product with ```Ys = dot(Be, Ws(s)``` is given the label 0 to get them apart. \n",
    "5. ```np.c_[Ye, Ys]``` is fowarded to the logistic log loss layer.\n",
    "\n",
    "<img src=\"image/word2vec_backprop_Be.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer import (\n",
    "    Embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding: Embedding = Embedding(\n",
    "    name=\"embedding\",\n",
    "    num_nodes=WINDOW_SIZE,\n",
    "    target_size=TARGET_SIZE,\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    negative_sample_size=SAMPLE_SIZE,\n",
    "    event_vector_size=VECTOR_SIZE,\n",
    "    dictionary=word_indexing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores ```np.c_[Ye, Ys]``` from the Embedding\n",
    "\n",
    "The 0th column is the scores for ```dot(Bc, Be``` for positive labels. The rest are the scores for ```dot(Bc, Ws)``` for negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "[[-11.342428    -2.555201    -3.0864296   -6.2725124    4.2037983   -2.0689092  -18.332573    -2.5910058   -4.976053    -1.3367822   -1.303802  ]\n",
      " [ -4.102366     5.4036145    9.27371     -2.9248075   17.596584    -6.2422657   -0.93544555   7.6472836    5.7966843   -6.730744   -12.388437  ]\n",
      " [ -2.2152784   10.308252    -0.31621504  -4.530749    -7.2113247   12.009228     8.913546    16.463419    -5.450465    -8.169324    -2.016971  ]\n",
      " [  7.261647    -5.6301117    3.9049284    4.096468    -3.1283627    0.78723     -0.6988249   -3.1793845   -1.8274341   -0.26620245   4.6407547 ]\n",
      " [ -4.615188    -0.40471506  17.433493    -7.4154363   -3.6721926   14.468176    11.101589     6.032888    -2.7721272    7.5408087    2.2634978 ]]\n",
      "\n",
      "Scores for dot(Bc, Be): \n",
      "[[-11.342428 ]\n",
      " [ -4.102366 ]\n",
      " [ -2.2152784]\n",
      " [  7.261647 ]\n",
      " [ -4.615188 ]]\n"
     ]
    }
   ],
   "source": [
    "scores = embedding.function(event_context_pairs)\n",
    "print(f\"Scores:\\n{scores[:5]}\\n\")\n",
    "print(f\"Scores for dot(Bc, Be): \\n{scores[:5, :1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Logistic Log Loss\n",
    "\n",
    "Train the model to get:\n",
    "1. BoW of the context event vectors close to the target event vector. Label 1\n",
    "2. BoW of the context event vectors away from each of the negative sample event vectors Label 0.\n",
    "\n",
    "This is a binary logistic classification, hence use Logistic Log Loss as the network objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.function import (\n",
    "    sigmoid_cross_entropy_log_loss,\n",
    "    sigmoid\n",
    ")\n",
    "from layer.objective import (\n",
    "    CrossEntropyLogLoss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CrossEntropyLogLoss(\n",
    "    name=\"loss\",\n",
    "    num_nodes=1,  # Logistic log loss\n",
    "    log_loss_function=sigmoid_cross_entropy_log_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Adapter\n",
    "\n",
    "The logistic log loss layer expects the input of shape ```(N,M=1)```, however Embedding outputs ```(N,(1+SL)``` where ```SL``` is SAMPLE_SIZE. The ```Adapter``` layer bridges between Embedding and the Loss layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer.adapter import (\n",
    "    Adapter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_function = embedding.adapt_function_to_logistic_log_loss(loss=loss)\n",
    "adapter_gradient = embedding.adapt_gradient_to_logistic_log_loss()\n",
    "\n",
    "adapter: Adapter = Adapter(\n",
    "    name=\"adapter\",\n",
    "    num_nodes=1,    # Number of output M=1 \n",
    "    function=adapter_function,\n",
    "    gradient=adapter_gradient\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Word2vec Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a sequential network\n",
    "\n",
    "$ \\text {Sentences} \\rightarrow EventIndexing \\rightarrow EventContext \\rightarrow  Embedding \\rightarrow Adapter \\rightarrow LogisticLogLoss$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.sequential import (\n",
    "    SequentialNetwork\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = SequentialNetwork(\n",
    "    name=\"word2vec\",\n",
    "    num_nodes=1,\n",
    "    inference_layers=[\n",
    "        word_indexing,\n",
    "        event_context,\n",
    "        embedding,\n",
    "        adapter\n",
    "    ],\n",
    "    objective_layers=[\n",
    "        loss\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STATE_FILE = \"wor2vec_embedding_10MAY2021_Jupyter.pkl\"\n",
    "#embedding.save(STATE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 00000: Loss:        3.112713\n",
      "Batch 00100: Loss:        3.133286\n",
      "Batch 00200: Loss:        3.085930\n",
      "Batch 00300: Loss:        3.159912\n",
      "Batch 00400: Loss:        3.081688\n",
      "Batch 00500: Loss:        3.074373\n",
      "Batch 00600: Loss:        3.082561\n",
      "Batch 00700: Loss:        2.947149\n",
      "Batch 00800: Loss:        3.010769\n",
      "epoch 0 done\n",
      "Batch 00900: Loss:        3.096384\n",
      "Batch 01000: Loss:        2.915789\n",
      "Batch 01100: Loss:        3.053586\n",
      "Batch 01200: Loss:        3.210108\n",
      "Batch 01300: Loss:        2.984506\n",
      "Batch 01400: Loss:        3.017680\n",
      "Batch 01500: Loss:        2.910682\n",
      "Batch 01600: Loss:        3.035512\n",
      "epoch 1 done\n",
      "Batch 01700: Loss:        2.962959\n",
      "Batch 01800: Loss:        2.983829\n",
      "Batch 01900: Loss:        2.914031\n",
      "Batch 02000: Loss:        2.976863\n",
      "Batch 02100: Loss:        2.835485\n",
      "Batch 02200: Loss:        2.943722\n",
      "Batch 02300: Loss:        2.959889\n",
      "Batch 02400: Loss:        2.929062\n",
      "Batch 02500: Loss:        2.870975\n",
      "epoch 2 done\n",
      "Batch 02600: Loss:        2.839388\n",
      "Batch 02700: Loss:        2.910233\n",
      "Batch 02800: Loss:        2.819096\n",
      "Batch 02900: Loss:        2.916199\n",
      "Batch 03000: Loss:        2.854874\n",
      "Batch 03100: Loss:        2.916801\n",
      "Batch 03200: Loss:        2.826679\n",
      "Batch 03300: Loss:        2.779388\n",
      "epoch 3 done\n",
      "Batch 03400: Loss:        2.917031\n",
      "Batch 03500: Loss:        2.833414\n",
      "Batch 03600: Loss:        2.861039\n",
      "Batch 03700: Loss:        2.752903\n",
      "Batch 03800: Loss:        2.863806\n",
      "Batch 03900: Loss:        2.906388\n",
      "Batch 04000: Loss:        2.818362\n",
      "Batch 04100: Loss:        2.892906\n",
      "Batch 04200: Loss:        2.781549\n",
      "epoch 4 done\n",
      "Batch 04300: Loss:        2.872695\n",
      "Batch 04400: Loss:        2.893197\n",
      "Batch 04500: Loss:        2.715761\n",
      "Batch 04600: Loss:        2.818646\n",
      "Batch 04700: Loss:        2.740713\n",
      "Batch 04800: Loss:        2.829294\n",
      "Batch 04900: Loss:        2.618681\n",
      "Batch 05000: Loss:        2.828678\n",
      "epoch 5 done\n",
      "Batch 05100: Loss:        2.818059\n",
      "Batch 05200: Loss:        2.764162\n",
      "Batch 05300: Loss:        2.762959\n",
      "Batch 05400: Loss:        2.800384\n",
      "Batch 05500: Loss:        2.658688\n",
      "Batch 05600: Loss:        2.707185\n",
      "Batch 05700: Loss:        2.733071\n",
      "Batch 05800: Loss:        2.772146\n",
      "epoch 6 done\n",
      "Batch 06000: Loss:        2.828618\n",
      "Batch 06100: Loss:        2.754337\n",
      "Batch 06200: Loss:        2.723292\n",
      "Batch 06300: Loss:        2.693427\n",
      "Batch 06400: Loss:        2.702215\n",
      "Batch 06500: Loss:        2.752496\n",
      "Batch 06600: Loss:        2.693768\n",
      "Batch 06700: Loss:        2.654542\n",
      "epoch 7 done\n",
      "Batch 06800: Loss:        2.568832\n",
      "Batch 06900: Loss:        2.680684\n",
      "Batch 07000: Loss:        2.697104\n",
      "Batch 07100: Loss:        2.817827\n",
      "Batch 07200: Loss:        2.715146\n",
      "Batch 07300: Loss:        2.763320\n",
      "Batch 07400: Loss:        2.618500\n",
      "Batch 07500: Loss:        2.593877\n",
      "epoch 8 done\n",
      "Batch 07600: Loss:        2.756381\n",
      "Batch 07700: Loss:        2.732677\n",
      "Batch 07800: Loss:        2.567536\n",
      "Batch 07900: Loss:        2.694665\n",
      "Batch 08000: Loss:        2.691128\n",
      "Batch 08100: Loss:        2.641381\n",
      "Batch 08200: Loss:        2.638136\n",
      "Batch 08300: Loss:        2.621550\n",
      "Batch 08400: Loss:        2.727657\n",
      "epoch 9 done\n",
      "Batch 08500: Loss:        2.671848\n",
      "Batch 08600: Loss:        2.653926\n",
      "Batch 08700: Loss:        2.614926\n",
      "Batch 08800: Loss:        2.652777\n",
      "Batch 08900: Loss:        2.645990\n",
      "Batch 09000: Loss:        2.617540\n",
      "Batch 09100: Loss:        2.578330\n",
      "Batch 09200: Loss:        2.599698\n",
      "epoch 10 done\n",
      "Batch 09300: Loss:        2.634583\n",
      "Batch 09400: Loss:        2.641776\n",
      "Batch 09500: Loss:        2.571682\n",
      "Batch 09600: Loss:        2.650912\n",
      "Batch 09700: Loss:        2.580099\n",
      "Batch 09800: Loss:        2.625246\n",
      "Batch 09900: Loss:        2.672834\n",
      "Batch 10000: Loss:        2.641314\n",
      "Batch 10100: Loss:        2.525256\n",
      "epoch 11 done\n",
      "Batch 10200: Loss:        2.643153\n",
      "Batch 10300: Loss:        2.648669\n",
      "Batch 10400: Loss:        2.595408\n",
      "Batch 10500: Loss:        2.566305\n",
      "Batch 10600: Loss:        2.687830\n",
      "Batch 10700: Loss:        2.557534\n",
      "Batch 10800: Loss:        2.649833\n",
      "Batch 10900: Loss:        2.537958\n",
      "epoch 12 done\n",
      "Batch 11000: Loss:        2.664802\n",
      "Batch 11100: Loss:        2.691224\n",
      "Batch 11200: Loss:        2.604437\n",
      "Batch 11300: Loss:        2.596736\n",
      "Batch 11400: Loss:        2.631989\n",
      "Batch 11500: Loss:        2.525471\n",
      "Batch 11600: Loss:        2.675766\n",
      "Batch 11700: Loss:        2.667495\n",
      "Batch 11800: Loss:        2.546775\n",
      "epoch 13 done\n",
      "Batch 11900: Loss:        2.549045\n",
      "Batch 12000: Loss:        2.543977\n",
      "Batch 12100: Loss:        2.644307\n",
      "Batch 12200: Loss:        2.507536\n",
      "Batch 12300: Loss:        2.501803\n",
      "Batch 12400: Loss:        2.470198\n",
      "Batch 12500: Loss:        2.543545\n",
      "Batch 12600: Loss:        2.615236\n",
      "epoch 14 done\n",
      "Batch 12700: Loss:        2.639206\n",
      "Batch 12800: Loss:        2.453350\n",
      "Batch 12900: Loss:        2.502396\n",
      "Batch 13000: Loss:        2.470468\n",
      "Batch 13100: Loss:        2.461168\n",
      "Batch 13200: Loss:        2.565476\n",
      "Batch 13300: Loss:        2.585497\n",
      "Batch 13400: Loss:        2.600076\n",
      "epoch 15 done\n",
      "Batch 13500: Loss:        2.640025\n",
      "Batch 13600: Loss:        2.471463\n",
      "Batch 13700: Loss:        2.552864\n",
      "Batch 13800: Loss:        2.494492\n",
      "Batch 13900: Loss:        2.522558\n",
      "Batch 14000: Loss:        2.631966\n",
      "Batch 14100: Loss:        2.573047\n",
      "Batch 14200: Loss:        2.404263\n",
      "Batch 14300: Loss:        2.547389\n",
      "epoch 16 done\n",
      "Batch 14400: Loss:        2.493191\n",
      "Batch 14500: Loss:        2.517738\n",
      "Batch 14600: Loss:        2.524308\n",
      "Batch 14700: Loss:        2.538969\n",
      "Batch 14800: Loss:        2.493494\n",
      "Batch 14900: Loss:        2.558341\n",
      "Batch 15000: Loss:        2.500499\n",
      "Batch 15100: Loss:        2.574013\n",
      "epoch 17 done\n",
      "Batch 15200: Loss:        2.550865\n",
      "Batch 15300: Loss:        2.502709\n",
      "Batch 15400: Loss:        2.585976\n",
      "Batch 15500: Loss:        2.504028\n",
      "Batch 15600: Loss:        2.431451\n",
      "Batch 15700: Loss:        2.526809\n",
      "Batch 15800: Loss:        2.477949\n",
      "Batch 15900: Loss:        2.564719\n",
      "Batch 16000: Loss:        2.556349\n",
      "epoch 18 done\n",
      "Batch 16100: Loss:        2.450346\n",
      "Batch 16200: Loss:        2.470207\n",
      "Batch 16300: Loss:        2.467003\n",
      "Batch 16400: Loss:        2.557137\n",
      "Batch 16500: Loss:        2.374484\n",
      "Batch 16600: Loss:        2.558002\n",
      "Batch 16700: Loss:        2.427593\n",
      "Batch 16800: Loss:        2.379133\n",
      "epoch 19 done\n",
      "Batch 16900: Loss:        2.501481\n",
      "Batch 17000: Loss:        2.514872\n",
      "Batch 17100: Loss:        2.506669\n",
      "Batch 17200: Loss:        2.426113\n",
      "Batch 17300: Loss:        2.493958\n",
      "Batch 17400: Loss:        2.446757\n",
      "Batch 17500: Loss:        2.517544\n",
      "Batch 17600: Loss:        2.457668\n",
      "Batch 17700: Loss:        2.543619\n",
      "epoch 20 done\n",
      "Batch 17800: Loss:        2.471460\n",
      "Batch 17900: Loss:        2.549780\n",
      "Batch 18000: Loss:        2.487200\n",
      "Batch 18100: Loss:        2.497797\n",
      "Batch 18200: Loss:        2.392730\n",
      "Batch 18300: Loss:        2.494966\n",
      "Batch 18400: Loss:        2.280510\n",
      "Batch 18500: Loss:        2.501462\n",
      "epoch 21 done\n",
      "Batch 18600: Loss:        2.472734\n",
      "Batch 18700: Loss:        2.376235\n",
      "Batch 18800: Loss:        2.516581\n",
      "Batch 18900: Loss:        2.451216\n",
      "Batch 19000: Loss:        2.408637\n",
      "Batch 19100: Loss:        2.429826\n",
      "Batch 19200: Loss:        2.372046\n",
      "Batch 19300: Loss:        2.464490\n",
      "epoch 22 done\n",
      "Batch 19400: Loss:        2.536669\n",
      "Batch 19500: Loss:        2.440324\n",
      "Batch 19600: Loss:        2.388553\n",
      "Batch 19700: Loss:        2.240594\n",
      "Batch 19800: Loss:        2.375087\n",
      "Batch 19900: Loss:        2.431610\n",
      "Batch 20000: Loss:        2.353841\n",
      "Batch 20100: Loss:        2.439294\n",
      "Batch 20200: Loss:        2.420398\n",
      "epoch 23 done\n",
      "Batch 20300: Loss:        2.314103\n",
      "Batch 20400: Loss:        2.509907\n",
      "Batch 20500: Loss:        2.504276\n",
      "Batch 20600: Loss:        2.507354\n",
      "Batch 20700: Loss:        2.265198\n",
      "Batch 20800: Loss:        2.498098\n",
      "Batch 20900: Loss:        2.499382\n",
      "Batch 21000: Loss:        2.530904\n",
      "epoch 24 done\n",
      "Batch 21100: Loss:        2.311210\n",
      "Batch 21200: Loss:        2.499302\n",
      "Batch 21300: Loss:        2.470991\n",
      "Batch 21400: Loss:        2.343549\n",
      "Batch 21500: Loss:        2.303310\n",
      "Batch 21600: Loss:        2.409000\n",
      "Batch 21700: Loss:        2.341295\n",
      "Batch 21800: Loss:        2.369555\n",
      "Batch 21900: Loss:        2.414818\n",
      "epoch 25 done\n",
      "Batch 22000: Loss:        2.311523\n",
      "Batch 22100: Loss:        2.436116\n",
      "Batch 22200: Loss:        2.395569\n",
      "Batch 22300: Loss:        2.346766\n",
      "Batch 22400: Loss:        2.326314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 22500: Loss:        2.415982\n",
      "Batch 22600: Loss:        2.408726\n",
      "Batch 22700: Loss:        2.412830\n",
      "epoch 26 done\n",
      "Batch 22800: Loss:        2.446558\n",
      "Batch 22900: Loss:        2.340935\n",
      "Batch 23000: Loss:        2.503056\n",
      "Batch 23100: Loss:        2.298322\n",
      "Batch 23200: Loss:        2.245181\n",
      "Batch 23300: Loss:        2.398801\n",
      "Batch 23400: Loss:        2.474218\n",
      "Batch 23500: Loss:        2.309206\n",
      "Batch 23600: Loss:        2.333836\n",
      "epoch 27 done\n",
      "Batch 23700: Loss:        2.380915\n",
      "Batch 23800: Loss:        2.342965\n",
      "Batch 23900: Loss:        2.463509\n",
      "Batch 24000: Loss:        2.451466\n",
      "Batch 24100: Loss:        2.429160\n",
      "Batch 24200: Loss:        2.361946\n",
      "Batch 24300: Loss:        2.310337\n",
      "Batch 24400: Loss:        2.314754\n",
      "epoch 28 done\n",
      "Batch 24500: Loss:        2.312159\n",
      "Batch 24600: Loss:        2.323457\n",
      "Batch 24700: Loss:        2.436345\n",
      "Batch 24800: Loss:        2.447668\n",
      "Batch 24900: Loss:        2.407589\n",
      "Batch 25000: Loss:        2.460996\n",
      "Batch 25100: Loss:        2.404357\n",
      "Batch 25200: Loss:        2.431618\n",
      "epoch 29 done\n",
      "Batch 25300: Loss:        2.299810\n",
      "Batch 25400: Loss:        2.478804\n",
      "Batch 25500: Loss:        2.421980\n",
      "Batch 25600: Loss:        2.384767\n",
      "Batch 25700: Loss:        2.355901\n",
      "Batch 25800: Loss:        2.339430\n",
      "Batch 25900: Loss:        2.349327\n",
      "Batch 26000: Loss:        2.323691\n",
      "Batch 26100: Loss:        2.323966\n",
      "epoch 30 done\n",
      "Batch 26200: Loss:        2.348723\n",
      "Batch 26300: Loss:        2.405164\n",
      "Batch 26400: Loss:        2.406855\n",
      "Batch 26500: Loss:        2.025423\n",
      "Batch 26600: Loss:        2.444819\n",
      "Batch 26700: Loss:        2.300116\n",
      "Batch 26800: Loss:        2.324786\n",
      "Batch 26900: Loss:        2.386353\n",
      "epoch 31 done\n",
      "Batch 27000: Loss:        2.317967\n",
      "Batch 27100: Loss:        2.393918\n",
      "Batch 27200: Loss:        2.396115\n",
      "Batch 27300: Loss:        2.313479\n",
      "Batch 27400: Loss:        2.300315\n",
      "Batch 27500: Loss:        2.349170\n",
      "Batch 27600: Loss:        2.356655\n",
      "Batch 27700: Loss:        2.282444\n",
      "Batch 27800: Loss:        2.418397\n",
      "epoch 32 done\n",
      "Batch 27900: Loss:        2.356473\n",
      "Batch 28000: Loss:        2.401703\n",
      "Batch 28100: Loss:        2.416176\n",
      "Batch 28200: Loss:        2.218317\n",
      "Batch 28300: Loss:        2.290780\n",
      "Batch 28400: Loss:        2.314034\n",
      "Batch 28500: Loss:        2.345051\n",
      "Batch 28600: Loss:        2.353388\n",
      "epoch 33 done\n",
      "Batch 28700: Loss:        2.341586\n",
      "Batch 28800: Loss:        2.330739\n",
      "Batch 28900: Loss:        2.374473\n",
      "Batch 29000: Loss:        2.390363\n",
      "Batch 29100: Loss:        2.253758\n",
      "Batch 29200: Loss:        2.294354\n",
      "Batch 29300: Loss:        2.322134\n",
      "Batch 29400: Loss:        2.325843\n",
      "Batch 29500: Loss:        2.403152\n",
      "epoch 34 done\n",
      "Batch 29600: Loss:        2.311260\n",
      "Batch 29700: Loss:        2.283707\n",
      "Batch 29800: Loss:        2.374638\n",
      "Batch 29900: Loss:        2.321517\n",
      "Batch 30000: Loss:        2.377017\n",
      "Batch 30100: Loss:        2.328337\n",
      "Batch 30200: Loss:        2.244089\n",
      "Batch 30300: Loss:        2.290824\n",
      "epoch 35 done\n",
      "Batch 30400: Loss:        2.454980\n",
      "Batch 30500: Loss:        2.339809\n",
      "Batch 30600: Loss:        2.332906\n",
      "Batch 30700: Loss:        2.318304\n",
      "Batch 30800: Loss:        2.351481\n",
      "Batch 30900: Loss:        2.254705\n",
      "Batch 31000: Loss:        2.468754\n",
      "Batch 31100: Loss:        2.329865\n",
      "epoch 36 done\n",
      "Batch 31200: Loss:        2.304836\n",
      "Batch 31300: Loss:        2.309637\n",
      "Batch 31400: Loss:        2.424209\n",
      "Batch 31500: Loss:        2.346906\n",
      "Batch 31600: Loss:        2.295077\n",
      "Batch 31700: Loss:        2.321282\n",
      "Batch 31800: Loss:        2.296359\n",
      "Batch 31900: Loss:        2.335539\n",
      "Batch 32000: Loss:        2.182209\n",
      "epoch 37 done\n",
      "Batch 32100: Loss:        2.310906\n",
      "Batch 32200: Loss:        2.397006\n",
      "Batch 32300: Loss:        2.351869\n",
      "Batch 32400: Loss:        2.408560\n",
      "Batch 32500: Loss:        2.367348\n",
      "Batch 32600: Loss:        2.240838\n",
      "Batch 32700: Loss:        2.228181\n",
      "Batch 32800: Loss:        2.220175\n",
      "epoch 38 done\n",
      "Batch 32900: Loss:        2.276224\n",
      "Batch 33000: Loss:        2.347684\n",
      "Batch 33100: Loss:        2.357764\n",
      "Batch 33200: Loss:        2.225907\n",
      "Batch 33300: Loss:        2.255277\n",
      "Batch 33400: Loss:        2.253114\n",
      "Batch 33500: Loss:        2.355318\n",
      "Batch 33600: Loss:        2.320109\n",
      "Batch 33700: Loss:        2.387410\n",
      "epoch 39 done\n",
      "Batch 33800: Loss:        2.214488\n",
      "Batch 33900: Loss:        2.277183\n",
      "Batch 34000: Loss:        2.337161\n",
      "Batch 34100: Loss:        2.242684\n",
      "Batch 34200: Loss:        2.354663\n",
      "Batch 34300: Loss:        2.281456\n",
      "Batch 34400: Loss:        2.379955\n",
      "Batch 34500: Loss:        2.223214\n",
      "epoch 40 done\n",
      "Batch 34600: Loss:        2.272479\n",
      "Batch 34700: Loss:        2.336093\n",
      "Batch 34800: Loss:        2.340363\n",
      "Batch 34900: Loss:        2.295793\n",
      "Batch 35000: Loss:        2.341603\n",
      "Batch 35100: Loss:        2.246616\n",
      "Batch 35200: Loss:        2.258602\n",
      "Batch 35300: Loss:        2.293279\n",
      "Batch 35400: Loss:        2.180348\n",
      "epoch 41 done\n",
      "Batch 35500: Loss:        2.350073\n",
      "Batch 35600: Loss:        2.221574\n",
      "Batch 35700: Loss:        2.239655\n",
      "Batch 35800: Loss:        2.188344\n",
      "Batch 35900: Loss:        2.333854\n",
      "Batch 36000: Loss:        2.166818\n",
      "Batch 36100: Loss:        2.157410\n",
      "Batch 36200: Loss:        2.255371\n",
      "epoch 42 done\n",
      "Batch 36300: Loss:        2.313853\n",
      "Batch 36400: Loss:        2.264465\n",
      "Batch 36500: Loss:        2.190254\n",
      "Batch 36600: Loss:        2.256300\n",
      "Batch 36700: Loss:        2.273310\n",
      "Batch 36800: Loss:        2.179264\n",
      "Batch 36900: Loss:        2.349646\n",
      "Batch 37000: Loss:        2.309386\n",
      "epoch 43 done\n",
      "Batch 37100: Loss:        2.218411\n",
      "Batch 37200: Loss:        2.175500\n",
      "Batch 37300: Loss:        2.232092\n",
      "Batch 37400: Loss:        2.260133\n",
      "Batch 37500: Loss:        2.218196\n",
      "Batch 37600: Loss:        2.008961\n",
      "Batch 37700: Loss:        2.286305\n",
      "Batch 37800: Loss:        2.277882\n",
      "Batch 37900: Loss:        2.256167\n",
      "epoch 44 done\n",
      "Batch 38000: Loss:        2.117261\n",
      "Batch 38100: Loss:        2.255010\n",
      "Batch 38200: Loss:        2.321980\n",
      "Batch 38300: Loss:        2.252887\n",
      "Batch 38400: Loss:        2.235610\n",
      "Batch 38500: Loss:        2.304855\n",
      "Batch 38600: Loss:        2.273205\n",
      "Batch 38700: Loss:        2.226645\n",
      "epoch 45 done\n",
      "Batch 38800: Loss:        2.252989\n",
      "Batch 38900: Loss:        2.203316\n",
      "Batch 39000: Loss:        2.298347\n",
      "Batch 39100: Loss:        2.181002\n",
      "Batch 39200: Loss:        2.158747\n",
      "Batch 39300: Loss:        2.291782\n",
      "Batch 39400: Loss:        2.260011\n",
      "Batch 39500: Loss:        2.222893\n",
      "Batch 39600: Loss:        2.340683\n",
      "epoch 46 done\n",
      "Batch 39700: Loss:        2.237932\n",
      "Batch 39800: Loss:        2.226777\n",
      "Batch 39900: Loss:        2.220508\n",
      "Batch 40000: Loss:        2.261180\n",
      "Batch 40100: Loss:        2.151957\n",
      "Batch 40200: Loss:        2.196174\n",
      "Batch 40300: Loss:        2.284056\n",
      "Batch 40400: Loss:        2.173470\n",
      "epoch 47 done\n",
      "Batch 40500: Loss:        2.198924\n",
      "Batch 40600: Loss:        2.276571\n",
      "Batch 40700: Loss:        2.093833\n",
      "Batch 40800: Loss:        2.282805\n",
      "Batch 40900: Loss:        2.008032\n",
      "Batch 41000: Loss:        2.169790\n",
      "Batch 41100: Loss:        2.260656\n",
      "Batch 41200: Loss:        2.215690\n",
      "Batch 41300: Loss:        2.234083\n",
      "epoch 48 done\n",
      "Batch 41400: Loss:        2.113320\n",
      "Batch 41500: Loss:        2.202603\n",
      "Batch 41600: Loss:        2.170324\n",
      "Batch 41700: Loss:        2.169287\n",
      "Batch 41800: Loss:        2.144070\n",
      "Batch 41900: Loss:        2.202472\n",
      "Batch 42000: Loss:        2.222841\n",
      "Batch 42100: Loss:        1.971606\n",
      "epoch 49 done\n",
      "Batch 42200: Loss:        2.138217\n",
      "Batch 42300: Loss:        2.192593\n",
      "Batch 42400: Loss:        2.215711\n",
      "Batch 42500: Loss:        2.202122\n",
      "Batch 42600: Loss:        2.283098\n",
      "Batch 42700: Loss:        2.279695\n",
      "Batch 42800: Loss:        2.186913\n",
      "Batch 42900: Loss:        2.140957\n",
      "epoch 50 done\n",
      "Batch 43000: Loss:        2.217259\n",
      "Batch 43100: Loss:        2.268982\n",
      "Batch 43200: Loss:        2.164444\n",
      "Batch 43300: Loss:        2.115556\n",
      "Batch 43400: Loss:        2.245554\n",
      "Batch 43500: Loss:        2.136095\n",
      "Batch 43600: Loss:        2.215338\n",
      "Batch 43700: Loss:        2.186629\n",
      "Batch 43800: Loss:        2.236567\n",
      "epoch 51 done\n",
      "Batch 43900: Loss:        2.298321\n",
      "Batch 44000: Loss:        2.193797\n",
      "Batch 44100: Loss:        2.103615\n",
      "Batch 44200: Loss:        2.203839\n",
      "Batch 44300: Loss:        2.114738\n",
      "Batch 44400: Loss:        2.209623\n",
      "Batch 44500: Loss:        2.147668\n",
      "Batch 44600: Loss:        2.225509\n",
      "epoch 52 done\n",
      "Batch 44700: Loss:        2.174222\n",
      "Batch 44800: Loss:        2.134565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 44900: Loss:        2.185711\n",
      "Batch 45000: Loss:        2.164877\n",
      "Batch 45100: Loss:        2.165250\n",
      "Batch 45200: Loss:        2.272878\n",
      "Batch 45300: Loss:        2.218867\n",
      "Batch 45400: Loss:        2.154819\n",
      "Batch 45500: Loss:        2.150480\n",
      "epoch 53 done\n",
      "Batch 45600: Loss:        2.220457\n",
      "Batch 45700: Loss:        2.219050\n",
      "Batch 45800: Loss:        2.192997\n",
      "Batch 45900: Loss:        2.215337\n",
      "Batch 46000: Loss:        2.193352\n",
      "Batch 46100: Loss:        2.241782\n",
      "Batch 46200: Loss:        2.277177\n",
      "Batch 46300: Loss:        2.261205\n",
      "epoch 54 done\n",
      "Batch 46400: Loss:        2.237511\n",
      "Batch 46500: Loss:        2.078870\n",
      "Batch 46600: Loss:        2.263288\n",
      "Batch 46700: Loss:        2.199450\n",
      "Batch 46800: Loss:        2.106005\n",
      "Batch 46900: Loss:        2.232303\n",
      "Batch 47000: Loss:        2.189767\n",
      "Batch 47100: Loss:        2.189201\n",
      "Batch 47200: Loss:        2.168115\n",
      "epoch 55 done\n",
      "Batch 47300: Loss:        2.154792\n",
      "Batch 47400: Loss:        2.158733\n",
      "Batch 47500: Loss:        2.179071\n",
      "Batch 47600: Loss:        2.162982\n",
      "Batch 47700: Loss:        2.185221\n",
      "Batch 47800: Loss:        2.083683\n",
      "Batch 47900: Loss:        2.133787\n",
      "Batch 48000: Loss:        2.156272\n",
      "epoch 56 done\n",
      "Batch 48100: Loss:        2.259079\n",
      "Batch 48200: Loss:        2.134933\n",
      "Batch 48300: Loss:        2.126383\n",
      "Batch 48400: Loss:        2.109262\n",
      "Batch 48500: Loss:        2.137274\n",
      "Batch 48600: Loss:        2.223843\n",
      "Batch 48700: Loss:        2.211661\n",
      "Batch 48800: Loss:        2.150258\n",
      "epoch 57 done\n",
      "Batch 48900: Loss:        2.250360\n",
      "Batch 49000: Loss:        2.127467\n",
      "Batch 49100: Loss:        2.188135\n",
      "Batch 49200: Loss:        2.206587\n",
      "Batch 49300: Loss:        2.149861\n",
      "Batch 49400: Loss:        2.180303\n",
      "Batch 49500: Loss:        2.224749\n",
      "Batch 49600: Loss:        2.128486\n",
      "Batch 49700: Loss:        2.236650\n",
      "epoch 58 done\n",
      "Batch 49800: Loss:        2.103472\n",
      "Batch 49900: Loss:        2.199117\n",
      "Batch 50000: Loss:        2.190580\n",
      "Batch 50100: Loss:        2.160004\n",
      "Batch 50200: Loss:        2.073928\n",
      "Batch 50300: Loss:        2.097193\n",
      "Batch 50400: Loss:        2.088141\n",
      "Batch 50500: Loss:        2.109204\n",
      "epoch 59 done\n",
      "Batch 50600: Loss:        2.132893\n",
      "Batch 50700: Loss:        2.204786\n",
      "Batch 50800: Loss:        2.122546\n",
      "Batch 50900: Loss:        2.153115\n",
      "Batch 51000: Loss:        2.094433\n",
      "Batch 51100: Loss:        2.033994\n",
      "Batch 51200: Loss:        2.011206\n",
      "Batch 51300: Loss:        2.205197\n",
      "Batch 51400: Loss:        2.098344\n",
      "epoch 60 done\n",
      "Batch 51500: Loss:        2.169661\n",
      "Batch 51600: Loss:        2.062963\n",
      "Batch 51700: Loss:        2.139023\n",
      "Batch 51800: Loss:        2.202953\n",
      "Batch 51900: Loss:        2.107551\n",
      "Batch 52000: Loss:        2.162067\n",
      "Batch 52100: Loss:        2.209116\n",
      "Batch 52200: Loss:        2.147749\n",
      "epoch 61 done\n",
      "Batch 52300: Loss:        2.117072\n",
      "Batch 52400: Loss:        2.201686\n",
      "Batch 52500: Loss:        2.119832\n",
      "Batch 52600: Loss:        2.239595\n",
      "Batch 52700: Loss:        2.146386\n",
      "Batch 52800: Loss:        2.190304\n",
      "Batch 52900: Loss:        2.100299\n",
      "Batch 53000: Loss:        2.113940\n",
      "Batch 53100: Loss:        2.260565\n",
      "epoch 62 done\n",
      "Batch 53200: Loss:        2.085693\n",
      "Batch 53300: Loss:        2.178662\n",
      "Batch 53400: Loss:        2.049130\n",
      "Batch 53500: Loss:        2.185039\n",
      "Batch 53600: Loss:        2.177246\n",
      "Batch 53700: Loss:        2.083931\n",
      "Batch 53800: Loss:        2.119094\n",
      "Batch 53900: Loss:        2.268923\n",
      "epoch 63 done\n",
      "Batch 54000: Loss:        2.248678\n",
      "Batch 54100: Loss:        2.047938\n",
      "Batch 54200: Loss:        2.158171\n",
      "Batch 54300: Loss:        2.158975\n",
      "Batch 54400: Loss:        2.126005\n",
      "Batch 54500: Loss:        2.137616\n",
      "Batch 54600: Loss:        2.186670\n",
      "Batch 54700: Loss:        2.159195\n",
      "epoch 64 done\n",
      "Batch 54800: Loss:        2.068200\n",
      "Batch 54900: Loss:        2.051743\n",
      "Batch 55000: Loss:        2.157231\n",
      "Batch 55100: Loss:        2.235308\n",
      "Batch 55200: Loss:        2.211073\n",
      "Batch 55300: Loss:        2.177038\n",
      "Batch 55400: Loss:        2.226443\n",
      "Batch 55500: Loss:        2.166004\n",
      "Batch 55600: Loss:        2.148025\n",
      "epoch 65 done\n",
      "Batch 55700: Loss:        2.107051\n",
      "Batch 55800: Loss:        2.181964\n",
      "Batch 55900: Loss:        2.216626\n",
      "Batch 56000: Loss:        2.146090\n",
      "Batch 56100: Loss:        2.137305\n",
      "Batch 56200: Loss:        2.130679\n",
      "Batch 56300: Loss:        2.115428\n",
      "Batch 56400: Loss:        1.999137\n",
      "epoch 66 done\n",
      "Batch 56500: Loss:        2.069815\n",
      "Batch 56600: Loss:        2.170735\n",
      "Batch 56700: Loss:        2.097296\n",
      "Batch 56800: Loss:        2.127141\n",
      "Batch 56900: Loss:        2.182133\n",
      "Batch 57000: Loss:        2.119584\n",
      "Batch 57100: Loss:        2.125225\n",
      "Batch 57200: Loss:        2.206162\n",
      "Batch 57300: Loss:        2.155298\n",
      "epoch 67 done\n",
      "Batch 57400: Loss:        2.126805\n",
      "Batch 57500: Loss:        2.188859\n",
      "Batch 57600: Loss:        2.179439\n",
      "Batch 57700: Loss:        2.192612\n",
      "Batch 57800: Loss:        2.069031\n",
      "Batch 57900: Loss:        2.094802\n",
      "Batch 58000: Loss:        2.207660\n",
      "Batch 58100: Loss:        2.148502\n",
      "epoch 68 done\n",
      "Batch 58200: Loss:        2.153313\n",
      "Batch 58300: Loss:        2.162745\n",
      "Batch 58400: Loss:        2.034489\n",
      "Batch 58500: Loss:        2.130036\n",
      "Batch 58600: Loss:        2.089174\n",
      "Batch 58700: Loss:        2.106591\n",
      "Batch 58800: Loss:        2.014410\n",
      "Batch 58900: Loss:        2.110368\n",
      "Batch 59000: Loss:        2.189428\n",
      "epoch 69 done\n",
      "Batch 59100: Loss:        2.152683\n",
      "Batch 59200: Loss:        2.132159\n",
      "Batch 59300: Loss:        2.184777\n",
      "Batch 59400: Loss:        1.908182\n",
      "Batch 59500: Loss:        2.128186\n",
      "Batch 59600: Loss:        2.159676\n",
      "Batch 59700: Loss:        2.136248\n",
      "Batch 59800: Loss:        2.164703\n",
      "epoch 70 done\n",
      "Batch 59900: Loss:        2.131918\n",
      "Batch 60000: Loss:        2.142353\n",
      "Batch 60100: Loss:        2.150065\n",
      "Batch 60200: Loss:        2.141358\n",
      "Batch 60300: Loss:        2.144923\n",
      "Batch 60400: Loss:        2.134191\n",
      "Batch 60500: Loss:        2.110210\n",
      "Batch 60600: Loss:        2.104441\n",
      "epoch 71 done\n",
      "Batch 60700: Loss:        2.129421\n",
      "Batch 60800: Loss:        2.064306\n",
      "Batch 60900: Loss:        2.055623\n",
      "Batch 61000: Loss:        2.178530\n",
      "Batch 61100: Loss:        2.182409\n",
      "Batch 61200: Loss:        2.236846\n",
      "Batch 61300: Loss:        2.226080\n",
      "Batch 61400: Loss:        2.048052\n",
      "Batch 61500: Loss:        2.125694\n",
      "epoch 72 done\n",
      "Batch 61600: Loss:        2.045234\n",
      "Batch 61700: Loss:        2.197652\n",
      "Batch 61800: Loss:        2.096285\n",
      "Batch 61900: Loss:        2.030099\n",
      "Batch 62000: Loss:        2.075267\n",
      "Batch 62100: Loss:        2.128942\n",
      "Batch 62200: Loss:        1.953608\n",
      "Batch 62300: Loss:        2.034719\n",
      "epoch 73 done\n",
      "Batch 62400: Loss:        2.144142\n",
      "Batch 62500: Loss:        2.107125\n",
      "Batch 62600: Loss:        2.128125\n",
      "Batch 62700: Loss:        2.118725\n",
      "Batch 62800: Loss:        2.198609\n",
      "Batch 62900: Loss:        2.158838\n",
      "Batch 63000: Loss:        2.162627\n",
      "Batch 63100: Loss:        2.108990\n",
      "Batch 63200: Loss:        2.019735\n",
      "epoch 74 done\n",
      "Batch 63300: Loss:        2.081402\n",
      "Batch 63400: Loss:        2.103290\n",
      "Batch 63500: Loss:        2.007804\n",
      "Batch 63600: Loss:        2.043929\n",
      "Batch 63700: Loss:        2.210973\n",
      "Batch 63800: Loss:        2.142009\n",
      "Batch 63900: Loss:        2.148582\n",
      "Batch 64000: Loss:        2.154716\n",
      "epoch 75 done\n",
      "Batch 64100: Loss:        2.046282\n",
      "Batch 64200: Loss:        2.196333\n",
      "Batch 64300: Loss:        2.217287\n",
      "Batch 64400: Loss:        2.080769\n",
      "Batch 64500: Loss:        2.013265\n",
      "Batch 64600: Loss:        2.123163\n",
      "Batch 64700: Loss:        2.046428\n",
      "Batch 64800: Loss:        2.131285\n",
      "Batch 64900: Loss:        1.971430\n",
      "epoch 76 done\n",
      "Batch 65000: Loss:        2.067404\n",
      "Batch 65100: Loss:        2.083140\n",
      "Batch 65200: Loss:        2.061537\n",
      "Batch 65300: Loss:        2.109007\n",
      "Batch 65400: Loss:        2.039201\n",
      "Batch 65500: Loss:        2.073942\n",
      "Batch 65600: Loss:        2.095383\n",
      "Batch 65700: Loss:        2.067636\n",
      "epoch 77 done\n",
      "Batch 65800: Loss:        2.050804\n",
      "Batch 65900: Loss:        2.023329\n",
      "Batch 66000: Loss:        2.036792\n",
      "Batch 66100: Loss:        2.024339\n",
      "Batch 66200: Loss:        2.204094\n",
      "Batch 66300: Loss:        2.029797\n",
      "Batch 66400: Loss:        2.072595\n",
      "Batch 66500: Loss:        2.120072\n",
      "epoch 78 done\n",
      "Batch 66600: Loss:        2.053378\n",
      "Batch 66700: Loss:        2.157390\n",
      "Batch 66800: Loss:        2.109858\n",
      "Batch 66900: Loss:        2.071413\n",
      "Batch 67000: Loss:        2.081813\n",
      "Batch 67100: Loss:        2.104507\n",
      "Batch 67200: Loss:        2.152178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 67300: Loss:        2.028180\n",
      "Batch 67400: Loss:        2.029108\n",
      "epoch 79 done\n",
      "Batch 67500: Loss:        2.052614\n",
      "Batch 67600: Loss:        2.070246\n",
      "Batch 67700: Loss:        2.023265\n",
      "Batch 67800: Loss:        2.072086\n",
      "Batch 67900: Loss:        2.071011\n",
      "Batch 68000: Loss:        2.035030\n",
      "Batch 68100: Loss:        2.078843\n",
      "Batch 68200: Loss:        2.051834\n",
      "epoch 80 done\n",
      "Batch 68300: Loss:        2.060919\n",
      "Batch 68400: Loss:        2.073912\n",
      "Batch 68500: Loss:        1.996692\n",
      "Batch 68600: Loss:        2.012480\n",
      "Batch 68700: Loss:        2.199805\n",
      "Batch 68800: Loss:        1.978501\n",
      "Batch 68900: Loss:        2.168346\n",
      "Batch 69000: Loss:        1.978974\n",
      "Batch 69100: Loss:        2.050477\n",
      "epoch 81 done\n",
      "Batch 69200: Loss:        2.057402\n",
      "Batch 69300: Loss:        2.032913\n",
      "Batch 69400: Loss:        2.007004\n"
     ]
    }
   ],
   "source": [
    "NUM_SENTENCES = 50\n",
    "MAX_ITERATIONS = 100000\n",
    "\n",
    "\n",
    "def sentences_generator(path_to_file, num_sentences):\n",
    "    stream = fileio.Function.file_line_stream(path_to_file)\n",
    "    try:\n",
    "        while True:\n",
    "            _lines = fileio.Function.take(num_sentences, stream)\n",
    "            yield np.array(_lines)\n",
    "    finally:\n",
    "        stream.close()\n",
    "\n",
    "# Sentences for the trainig\n",
    "path_to_input = path_to_ptb\n",
    "source = sentences_generator(\n",
    "    path_to_file=path_to_input, num_sentences=NUM_SENTENCES\n",
    ")\n",
    "\n",
    "# Restore the state if exists.\n",
    "state = embedding.load(STATE_FILE)\n",
    "\n",
    "# Continue training\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "\n",
    "total_sentences = 0\n",
    "epochs = 0\n",
    "\n",
    "for i in range(MAX_ITERATIONS):\n",
    "    try:\n",
    "        sentences = next(source)\n",
    "        total_sentences += len(sentences)\n",
    "        network.train(X=sentences, T=np.array([0]))\n",
    "        if i % 100 == 0:\n",
    "            # print(f\"Batch {i:05d} of {NUM_SENTENCES} sentences: Loss: {network.history[-1]:15f}\")\n",
    "            print(f\"Batch {i:05d}: Loss: {network.history[-1]:15f}\")\n",
    "        if i % 1000 == 0:\n",
    "            embedding.save(STATE_FILE)\n",
    "            # print(\"State saved.\")\n",
    "\n",
    "    except fileio.Function.GenearatorHasNoMore as e:\n",
    "        # Next epoch\n",
    "        print(f\"epoch {epochs} done\")\n",
    "        epochs += 1\n",
    "        source.close()\n",
    "        source = sentences_generator(\n",
    "            path_to_file=path_to_input, num_sentences=NUM_SENTENCES\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        source.close()\n",
    "        raise e\n",
    "\n",
    "embedding.save(STATE_FILE)\n",
    "\n",
    "profiler.disable()\n",
    "profiler.print_stats(sort=\"cumtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluate the vector space\n",
    "\n",
    "Verify if the trained model, or the vector space W, has encoded the words in a way that **similar** words are close in the vector space.\n",
    "\n",
    "* [How to measure the similarity among vectors](https://math.stackexchange.com/questions/4132458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "context = \"Verify if the trained model, or the vector space W, has encoded the words in a way that similar words are close in the vector space.\".split()\n",
    "word_indices = np.array(word_indexing.list_indices(context), dtype=TYPE_INT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([5841, 6830, 8028], dtype=int32)>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.predict(word_indices, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['milan', 'speeds', 'customs'], dtype='<U19')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indexing.list_events([embedding.predict(word_indices, n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
