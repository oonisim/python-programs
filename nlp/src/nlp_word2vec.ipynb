{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec implementation\n",
    "\n",
    "* [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "* [word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method](https://arxiv.org/abs/1402.3722)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The original paper proposed using:\n",
    "1. Matmul to extract word vectors from ```Win```.\n",
    "2. Softmax to calculate scores with all the word vectors from ```Wout```.\n",
    "\n",
    "<img src=\"image/word2vec_cbow_mechanism.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "### Motivation\n",
    "\n",
    "The original implementation was computationally expensive.\n",
    "\n",
    "1. Matmal to extract word vectors from ```Win```.\n",
    "2. Softmax can happen vocabulary size times with ```Wout```.\n",
    "\n",
    "<img src=\"image/word2vec_negative_sampling_motivation.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1. Use index to extract word vectors from Win.\n",
    "2. Instead of calculating softmax with all the word vectors from ```Wout```, sample small number (SL) of negative/false word vectors from ```Wout``` and calculate logistic log loss with each sample. \n",
    "\n",
    "<img src=\"image/wors2vec_neg_sample_backprop.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using only one Word vector space W\n",
    "\n",
    "There is no reasoning nor proof why two word vector space are required. In the end, we only use one word vector space, which appears to be ```Win```. Use only one word vector space ```W``` to test if it works.\n",
    "\n",
    "<img src=\"image/word2vec_backprop_Be.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from itertools import islice\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=400) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Google Colab environment\n",
    "\n",
    "Colab gets disconnected within approx 20 min. Hence not suitable for training (or need to upgrade to the pro version)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone github to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_GOOGLE_COLAB = True\n",
    "except:\n",
    "    IN_GOOGLE_COLAB = False\n",
    "    \n",
    "if IN_GOOGLE_COLAB:\n",
    "    !pip install line_profiler\n",
    "    !google.colab.drive.mount('/content/gdrive')\n",
    "    !rm -rf /content/drive/MyDrive/github\n",
    "    !mkdir -p /content/drive/MyDrive/github\n",
    "    !git clone https://github.com/oonisim/python-programs.git /content/drive/MyDrive/github\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone github to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_GOOGLE_COLAB = True\n",
    "except:\n",
    "    IN_GOOGLE_COLAB = False\n",
    "    \n",
    "if IN_GOOGLE_COLAB:\n",
    "    !pip install line_profiler\n",
    "    !google.colab.drive.mount('/content/gdrive')\n",
    "    !rm -rf /content/github\n",
    "    !mkdir -p /content/github\n",
    "    !git clone https://github.com/oonisim/python-programs.git /content/github\n",
    "        \n",
    "    import sys\n",
    "    sys.path.append('/content/github/nlp/src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook setups\n",
    "\n",
    "Auto reolaod causes an error in Jupyter notebooks. Restart the Jupyter kernel for the error:\n",
    "```TypeError: super(type, obj): obj must be an instance or subtype of type```\n",
    "See\n",
    "- https://stackoverflow.com/a/52927102/4281353\n",
    "- http://thomas-cokelaer.info/blog/2011/09/382/\n",
    "\n",
    "> The problem resides in the mechanism of reloading modules.\n",
    "> Reloading a module often changes the internal object in memory which\n",
    "> makes the isinstance test of super return False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "import function.fileio as fileio\n",
    "import function.text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.constant import (\n",
    "    TYPE_INT,\n",
    "    TYPE_FLOAT,\n",
    "    TYPE_LABEL,\n",
    "    TYPE_TENSOR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PTB = True\n",
    "DEBUG = False\n",
    "VALIDATION = True\n",
    "\n",
    "TARGET_SIZE = 1   # Size of the target event (word)\n",
    "CONTEXT_SIZE = 10  # Size of the context.\n",
    "WINDOW_SIZE = TARGET_SIZE + CONTEXT_SIZE\n",
    "SAMPLE_SIZE = 5   # Size of the negative samples\n",
    "VECTOR_SIZE = 100  # Number of features in the event vector.\n",
    "STATE_FILE = \"/home/oonisim/home/repository/git/oonisim/python_programs/nlp/models/word2vec_vecsize_100.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"To be, or not to be, that is the question that matters\"\n",
    "_file = \"ptb.train.txt\"\n",
    "if USE_PTB:\n",
    "    if not fileio.Function.is_file(f\"~/.keras/datasets/{_file}\"):\n",
    "        path_to_ptb = tf.keras.utils.get_file(\n",
    "            _file, \n",
    "            f'https://raw.githubusercontent.com/tomsercu/lstm/master/data/{_file}'\n",
    "        )\n",
    "    corpus = fileio.Function.read_file(path_to_ptb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n",
      " pierre <unk> N years old will join the board as a nonexecutive director nov. N \n",
      " mr. <unk> is chairman of <unk> n.v. the dutch publishing group \n",
      " rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate \n",
      " a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported \n"
     ]
    }
   ],
   "source": [
    "examples = corpus.split('\\n')[:5]\n",
    "for line in examples:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Event (word) indexing\n",
    "Index the events that have occurred in the event sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer.preprocessing import (\n",
    "    EventIndexing, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexing = EventIndexing(\n",
    "    name=\"word_indexing_on_ptb\",\n",
    "    corpus=corpus,\n",
    "    min_sequence_length=WINDOW_SIZE\n",
    ")\n",
    "del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EventIndexing  for the corpus\n",
    "\n",
    "Adapt to the ```corpus``` and provides:\n",
    "* event_to_index dictionary\n",
    "* vocaburary of the corpus\n",
    "* word occurrence probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EventIndexing.vocabulary[10]:\n",
      "['<nil>' '<unk>' 'aer' 'banknote' 'berlitz' 'calloway' 'centrust' 'cluett' 'fromstein' 'gitano']\n",
      "\n",
      "EventIndexing.event_to_index[10]:\n",
      "('<nil>', 0)\n",
      "('<unk>', 1)\n",
      "('aer', 2)\n",
      "('banknote', 3)\n",
      "('berlitz', 4)\n",
      "('calloway', 5)\n",
      "('centrust', 6)\n",
      "('cluett', 7)\n",
      "('fromstein', 8)\n",
      "('gitano', 9)\n",
      "\n",
      "EventIndexing.probabilities[10]:\n",
      "<nil>                : 0.00000e+00\n",
      "<unk>                : 1.65308e-02\n",
      "aer                  : 5.34860e-06\n",
      "banknote             : 5.34860e-06\n",
      "berlitz              : 5.34860e-06\n",
      "calloway             : 5.34860e-06\n",
      "centrust             : 5.34860e-06\n",
      "cluett               : 5.34860e-06\n",
      "fromstein            : 5.34860e-06\n",
      "gitano               : 5.34860e-06\n"
     ]
    }
   ],
   "source": [
    "words = word_indexing.list_events(range(10))\n",
    "print(f\"EventIndexing.vocabulary[10]:\\n{words}\\n\")\n",
    "\n",
    "indices = word_indexing.list_indices(words)\n",
    "print(f\"EventIndexing.event_to_index[10]:\")\n",
    "for item in zip(words, indices):\n",
    "    print(item)\n",
    "\n",
    "probabilities = word_indexing.list_probabilities(words)\n",
    "print(f\"\\nEventIndexing.probabilities[10]:\")\n",
    "for word, p in zip(words, probabilities):\n",
    "    print(f\"{word:20s} : {p:.5e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling using the probability\n",
    "\n",
    "Sample events according to their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['leaders' 'taipei' 'stunning' 'grim' 'potatoes']\n"
     ]
    }
   ],
   "source": [
    "sample = word_indexing.sample(size=5)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "Sample events not including those events already sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_indices=[9348, 6949, 2629, 2318, 2289] \n",
      "events=['wonderful' 'charitable' 'heating' 'feeling' 'film']\n"
     ]
    }
   ],
   "source": [
    "negative_indices = word_indexing.negative_sample_indices(\n",
    "    size=5, excludes=word_indexing.list_indices(sample)\n",
    ")\n",
    "print(f\"negative_indices={negative_indices} \\nevents={word_indexing.list_events(negative_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the             :    34\n",
      "asbestos        :    63\n",
      "fiber           :    86\n",
      "<unk>           :     1\n",
      "is              :    42\n",
      "unusually       :    87\n",
      "<unk>           :     1\n",
      "once            :    64\n",
      "it              :    80\n",
      "enters          :    88\n",
      "the             :    34\n",
      "<unk>           :     1\n",
      "\n",
      "with           :     0\n",
      "even            :     0\n",
      "brief           :     0\n"
     ]
    }
   ],
   "source": [
    "# sentences = \"\\n\".join(corpus.split('\\n')[5:6])\n",
    "sentences = \"\"\"\n",
    "the asbestos fiber <unk> is unusually <unk> once it enters the <unk> \n",
    "with even brief exposures to it causing symptoms that show up decades later researchers said\n",
    "\"\"\"\n",
    "sequences = word_indexing.function(sentences)\n",
    "for pair in zip(sentences.strip().split(\" \"), sequences[0]):\n",
    "    print(f\"{pair[0]:15} : {pair[1]:5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EventContext\n",
    "\n",
    "EventContext layer generates ```(event, context)``` pairs from sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer.preprocessing import (\n",
    "    EventContext\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_context = EventContext(\n",
    "    name=\"ev\",\n",
    "    window_size=WINDOW_SIZE,\n",
    "    event_size=TARGET_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context of an event (word) in a sentence\n",
    "\n",
    "In the sentence ```\"a form of asbestos once used to make kent cigarette filters\"```, one of the context windows ```a form of asbestos once``` of size 5 and event size 1 has.\n",
    "* ```of``` as a target word.\n",
    "* ```(a, form) and (asbestos, once)``` as its context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence of the word indices for the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[37, 62, 44, 63, 64, 65, 66, 67, 68, 69, 70,  0,  0,  0,  0,  0],\n",
       "       [29, 30, 31, 50, 51, 43, 44, 52, 53, 54, 55, 56, 57, 37, 38, 39]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \"\"\"\n",
    "a form of asbestos once used to make kent cigarette filters\n",
    "\n",
    "N years old and former chairman of consolidated gold fields plc was named a nonexecutive director\n",
    "\"\"\"\n",
    "\n",
    "sequence = word_indexing.function(sentences)\n",
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (event, context) pairs\n",
    "\n",
    "For each word (event) in the setence ```(of, asbestos, ... , kent)``` excludnig the ends of the sentence, create ```(target, context)``` as:\n",
    "\n",
    "```\n",
    "[\n",
    "  [of, a, form, asbestos, once],              # target is 'of', context is (a, form, asbestos, once)\n",
    "  ['asbestos', 'form', 'of', 'once', 'used'],\n",
    "  ['once', 'of', 'asbestos', 'used', 'to'],\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "### Format of the (event, context) pairs\n",
    "\n",
    "* **E** is the target event indices\n",
    "* **C** is the context indices\n",
    "\n",
    "<img src=\"image/event_context_format.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event context pairs. Shape (12, 11), Target event size 1, Window size 11.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[65, 37, 62, 44, 63, 64, 66, 67, 68, 69, 70],\n",
       "       [66, 62, 44, 63, 64, 65, 67, 68, 69, 70,  0],\n",
       "       [67, 44, 63, 64, 65, 66, 68, 69, 70,  0,  0],\n",
       "       [68, 63, 64, 65, 66, 67, 69, 70,  0,  0,  0],\n",
       "       [69, 64, 65, 66, 67, 68, 70,  0,  0,  0,  0],\n",
       "       [70, 65, 66, 67, 68, 69,  0,  0,  0,  0,  0],\n",
       "       [43, 29, 30, 31, 50, 51, 44, 52, 53, 54, 55],\n",
       "       [44, 30, 31, 50, 51, 43, 52, 53, 54, 55, 56],\n",
       "       [52, 31, 50, 51, 43, 44, 53, 54, 55, 56, 57],\n",
       "       [53, 50, 51, 43, 44, 52, 54, 55, 56, 57, 37]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_context_pairs = event_context.function(sequence)\n",
    "print(\n",
    "    f\"Event context pairs. Shape %s, Target event size %s, Window size %s.\" % \n",
    "    (event_context_pairs.shape, event_context.event_size, event_context.window_size)\n",
    ")\n",
    "event_context_pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (event, context) pairs in textual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['used',\n",
       "  'a',\n",
       "  'form',\n",
       "  'of',\n",
       "  'asbestos',\n",
       "  'once',\n",
       "  'to',\n",
       "  'make',\n",
       "  'kent',\n",
       "  'cigarette',\n",
       "  'filters'],\n",
       " ['to',\n",
       "  'form',\n",
       "  'of',\n",
       "  'asbestos',\n",
       "  'once',\n",
       "  'used',\n",
       "  'make',\n",
       "  'kent',\n",
       "  'cigarette',\n",
       "  'filters',\n",
       "  '<nil>'],\n",
       " ['make',\n",
       "  'of',\n",
       "  'asbestos',\n",
       "  'once',\n",
       "  'used',\n",
       "  'to',\n",
       "  'kent',\n",
       "  'cigarette',\n",
       "  'filters',\n",
       "  '<nil>',\n",
       "  '<nil>'],\n",
       " ['kent',\n",
       "  'asbestos',\n",
       "  'once',\n",
       "  'used',\n",
       "  'to',\n",
       "  'make',\n",
       "  'cigarette',\n",
       "  'filters',\n",
       "  '<nil>',\n",
       "  '<nil>',\n",
       "  '<nil>'],\n",
       " ['cigarette',\n",
       "  'once',\n",
       "  'used',\n",
       "  'to',\n",
       "  'make',\n",
       "  'kent',\n",
       "  'filters',\n",
       "  '<nil>',\n",
       "  '<nil>',\n",
       "  '<nil>',\n",
       "  '<nil>'],\n",
       " ['filters',\n",
       "  'used',\n",
       "  'to',\n",
       "  'make',\n",
       "  'kent',\n",
       "  'cigarette',\n",
       "  '<nil>',\n",
       "  '<nil>',\n",
       "  '<nil>',\n",
       "  '<nil>',\n",
       "  '<nil>'],\n",
       " ['chairman',\n",
       "  'n',\n",
       "  'years',\n",
       "  'old',\n",
       "  'and',\n",
       "  'former',\n",
       "  'of',\n",
       "  'consolidated',\n",
       "  'gold',\n",
       "  'fields',\n",
       "  'plc'],\n",
       " ['of',\n",
       "  'years',\n",
       "  'old',\n",
       "  'and',\n",
       "  'former',\n",
       "  'chairman',\n",
       "  'consolidated',\n",
       "  'gold',\n",
       "  'fields',\n",
       "  'plc',\n",
       "  'was'],\n",
       " ['consolidated',\n",
       "  'old',\n",
       "  'and',\n",
       "  'former',\n",
       "  'chairman',\n",
       "  'of',\n",
       "  'gold',\n",
       "  'fields',\n",
       "  'plc',\n",
       "  'was',\n",
       "  'named'],\n",
       " ['gold',\n",
       "  'and',\n",
       "  'former',\n",
       "  'chairman',\n",
       "  'of',\n",
       "  'consolidated',\n",
       "  'fields',\n",
       "  'plc',\n",
       "  'was',\n",
       "  'named',\n",
       "  'a']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indexing.sequence_to_sentence(event_context_pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Word Embedding\n",
    "\n",
    "Embedding is to train the model to group similar events in a close proximity in the event vector space. If two events e.g. 'pencil' and 'pen' are similar concepts, then their event vectors resides in a close distance in the event space. \n",
    "\n",
    "* [Thought Vectors](https://wiki.pathmind.com/thought-vectors)\n",
    "\n",
    "## Training process\n",
    "\n",
    "1. Calculate ```Bc```, the BoW (Bag of Words) from context event vectors.\n",
    "2. Calculate ```Be```,  the BoW (Bag of Words) from target event vectors.\n",
    "3. The dot product ```Ye = dot(Bc, Be)``` is given the label 1 to get them closer.\n",
    "4. For each negative sample ```Ws(s)```, the dot product with ```Ys = dot(Be, Ws(s)``` is given the label 0 to get them apart. \n",
    "5. ```np.c_[Ye, Ys]``` is fowarded to the logistic log loss layer.\n",
    "\n",
    "<img src=\"image/word2vec_backprop_Be.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer import (\n",
    "    Embedding\n",
    ")\n",
    "from optimizer import (\n",
    "    SGD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding: Embedding = Embedding(\n",
    "    name=\"embedding\",\n",
    "    num_nodes=WINDOW_SIZE,\n",
    "    target_size=TARGET_SIZE,\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    negative_sample_size=SAMPLE_SIZE,\n",
    "    event_vector_size=VECTOR_SIZE,\n",
    "    optimizer=SGD(lr=TYPE_FLOAT(0.5)),\n",
    "    dictionary=word_indexing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores ```np.c_[Ye, Ys]``` from the Embedding\n",
    "\n",
    "The 0th column is the scores for ```dot(Bc, Be``` for positive labels. The rest are the scores for ```dot(Bc, Ws)``` for negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "[[-4.7106897e-03 -4.8856679e-03 -1.2798817e-04  1.1684952e-03 -1.6764110e-03 -1.0835825e-03]\n",
      " [-4.5479019e-03 -2.2900996e-03 -4.6602665e-03 -5.1962025e-04 -1.5752653e-03  1.7776270e-05]\n",
      " [ 6.1073392e-03  5.2285870e-04  4.1223299e-03 -1.7459705e-03 -1.3933808e-04 -5.5324985e-05]\n",
      " [-2.0977100e-03  7.4077584e-04 -1.8932857e-04 -1.4614742e-03  1.9989037e-03 -2.5731474e-03]\n",
      " [-3.3790807e-03 -5.3184214e-03 -4.6287337e-04 -2.9225873e-03 -5.5602994e-03 -1.5333435e-03]]\n",
      "\n",
      "Scores for dot(Bc, Be): \n",
      "[[-0.00471069]\n",
      " [-0.0045479 ]\n",
      " [ 0.00610734]\n",
      " [-0.00209771]\n",
      " [-0.00337908]]\n"
     ]
    }
   ],
   "source": [
    "scores = embedding.function(event_context_pairs)\n",
    "print(f\"Scores:\\n{scores[:5]}\\n\")\n",
    "print(f\"Scores for dot(Bc, Be): \\n{scores[:5, :1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([1,2,3]).numpy().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Logistic Log Loss\n",
    "\n",
    "Train the model to get:\n",
    "1. BoW of the context event vectors close to the target event vector. Label 1\n",
    "2. BoW of the context event vectors away from each of the negative sample event vectors Label 0.\n",
    "\n",
    "This is a binary logistic classification, hence use Logistic Log Loss as the network objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.function import (\n",
    "    sigmoid_cross_entropy_log_loss,\n",
    "    sigmoid\n",
    ")\n",
    "from layer.objective import (\n",
    "    CrossEntropyLogLoss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CrossEntropyLogLoss(\n",
    "    name=\"loss\",\n",
    "    num_nodes=1,  # Logistic log loss\n",
    "    log_loss_function=sigmoid_cross_entropy_log_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Adapter\n",
    "\n",
    "The logistic log loss layer expects the input of shape ```(N,M=1)```, however Embedding outputs ```(N,(1+SL)``` where ```SL``` is SAMPLE_SIZE. The ```Adapter``` layer bridges between Embedding and the Loss layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer.adapter import (\n",
    "    Adapter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_function = embedding.adapt_function_to_logistic_log_loss(loss=loss)\n",
    "adapter_gradient = embedding.adapt_gradient_to_logistic_log_loss()\n",
    "\n",
    "adapter: Adapter = Adapter(\n",
    "    name=\"adapter\",\n",
    "    num_nodes=1,    # Number of output M=1 \n",
    "    function=adapter_function,\n",
    "    gradient=adapter_gradient\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Word2vec Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a sequential network\n",
    "\n",
    "$ \\text {Sentences} \\rightarrow EventIndexing \\rightarrow EventContext \\rightarrow  Embedding \\rightarrow Adapter \\rightarrow LogisticLogLoss$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.sequential import (\n",
    "    SequentialNetwork\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = SequentialNetwork(\n",
    "    name=\"word2vec\",\n",
    "    num_nodes=1,\n",
    "    inference_layers=[\n",
    "        word_indexing,\n",
    "        event_context,\n",
    "        embedding,\n",
    "        adapter\n",
    "    ],\n",
    "    objective_layers=[\n",
    "        loss\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved model file if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "STATE_FILE: /home/oonisim/home/repository/git/oonisim/python_programs/nlp/models/word2vec_vecsize_100.pkl\n",
      "Model loaded.\n",
      "    event_size 1\n",
      "    context_size: 10\n",
      "    event_vector_size: 100\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "if fileio.Function.is_file(STATE_FILE):\n",
    "    print(\"Loading model...\\nSTATE_FILE: %s\" % STATE_FILE)\n",
    "    state = embedding.load(STATE_FILE)\n",
    "\n",
    "    fmt=\"\"\"Model loaded.\n",
    "    event_size %s\n",
    "    context_size: %s\n",
    "    event_vector_size: %s\n",
    "    \"\"\"\n",
    "    print(fmt % (\n",
    "        state[\"target_size\"], \n",
    "        state[\"context_size\"], \n",
    "        state[\"event_vector_size\"]\n",
    "    ))\n",
    "else:\n",
    "    print(\"State file does not exist. Saving the initial model.\")\n",
    "    embedding.save(STATE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 00000 of 50 sentences: Average Loss:   0.176311 Duration 1.399338\n",
      "Batch 00100 of 50 sentences: Average Loss:   0.038964 Duration 1.436963\n",
      "Batch 00200 of 50 sentences: Average Loss:   0.039094 Duration 1.287315\n",
      "Batch 00300 of 50 sentences: Average Loss:   0.038099 Duration 1.330058\n"
     ]
    }
   ],
   "source": [
    "NUM_SENTENCES = 50\n",
    "MAX_ITERATIONS = 1000000\n",
    "\n",
    "def sentences_generator(path_to_file, num_sentences):\n",
    "    stream = fileio.Function.file_line_stream(path_to_file)\n",
    "    try:\n",
    "        while True:\n",
    "            _lines = fileio.Function.take(num_sentences, stream)\n",
    "            yield np.array(_lines)\n",
    "    finally:\n",
    "        stream.close()\n",
    "\n",
    "# Sentences for the trainig\n",
    "path_to_input = path_to_ptb\n",
    "source = sentences_generator(\n",
    "    path_to_file=path_to_input, num_sentences=NUM_SENTENCES\n",
    ")\n",
    "\n",
    "# Restore the state if exists.\n",
    "state = embedding.load(STATE_FILE)\n",
    "\n",
    "# Continue training\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "\n",
    "total_sentences = 0\n",
    "epochs = 0\n",
    "\n",
    "for i in range(MAX_ITERATIONS):\n",
    "    try:\n",
    "        sentences = next(source)\n",
    "        total_sentences += len(sentences)\n",
    "\n",
    "        start = time.time()\n",
    "        network.train(X=sentences, T=np.array([0]))\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                f\"Batch {i:05d} of {NUM_SENTENCES} sentences: \"\n",
    "                f\"Average Loss: {np.mean(network.history[-1]):10f} \"\n",
    "                f\"Duration {time.time() - start:3f}\"\n",
    "            )\n",
    "        if i % 10 == 0:\n",
    "            embedding.save(STATE_FILE)\n",
    "        \n",
    "    except fileio.Function.GenearatorHasNoMore as e:\n",
    "        # Next epoch\n",
    "        print(f\"epoch {epochs} batches {i:05d} done\")\n",
    "        print(f\"Average loss: {np.mean(network.history):15f}\")\n",
    "        epochs += 1\n",
    "        source.close()\n",
    "        source = sentences_generator(\n",
    "            path_to_file=path_to_input, num_sentences=NUM_SENTENCES\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        source.close()\n",
    "        raise e\n",
    "\n",
    "embedding.save(STATE_FILE)\n",
    "\n",
    "profiler.disable()\n",
    "profiler.print_stats(sort=\"cumtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluate the vector space\n",
    "\n",
    "Verify if the trained model, or the vector space W, has encoded the words in a way that **similar** words are close in the vector space.\n",
    "\n",
    "* [How to measure the similarity among vectors](https://math.stackexchange.com/questions/4132458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "context = \"smoking\".split()\n",
    "word_indices = np.array(word_indexing.list_indices(context), dtype=TYPE_INT)\n",
    "\n",
    "print(f\"Words {context}\")\n",
    "print(f\"Word indices {word_indices}\")\n",
    "print(f\"prediction for {context}:\\n{word_indexing.list_events([embedding.predict(word_indices, n)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(shape=(141, 11))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
