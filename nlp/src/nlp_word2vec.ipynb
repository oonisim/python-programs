{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec implementation\n",
    "\n",
    "## Original papers\n",
    "* [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "* [word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method](https://arxiv.org/abs/1402.3722)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The original paper proposed using:\n",
    "1. Matmul to extract word vectors from ```Win```.\n",
    "2. Softmax to calculate scores with all the word vectors from ```Wout```.\n",
    "\n",
    "<img src=\"image/word2vec_cbow_mechanism.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "### Motivation\n",
    "\n",
    "The original implementation was computationally expensive.\n",
    "\n",
    "1. Matmal to extract word vectors from ```Win```.\n",
    "2. Softmax can happen vocabulary size times with ```Wout```.\n",
    "\n",
    "<img src=\"image/word2vec_negative_sampling_motivation.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1. Use index to extract word vectors from Win.\n",
    "2. Instead of calculating softmax with all the word vectors from ```Wout```, sample small number (SL) of negative/false word vectors from ```Wout``` and calculate logistic log loss with each sample. \n",
    "\n",
    "<img src=\"image/wors2vec_neg_sample_backprop.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using only one Word vector space W\n",
    "\n",
    "There is no reasoning nor proof why two word vector space are required. In the end, we only use one word vector space, which appears to be ```Win```. \n",
    "\n",
    "However, if we use one vector space ```W``` for ```event```, ```context``` and ```negative samples```,then an event vector ```event=W[i]``` in a sentence can be used as a negative sample in another setence. Then the weight ```W[i]``` is updated for both positive and negative labels in the same gradient descent on ```W```. The actual [experiment of using only one vector space](./layer/embedding_single_vector_space.py) ```W``` did not work well.\n",
    "\n",
    "* [Why do we need 2 matrices for word2vec or GloVe](https://datascience.stackexchange.com/a/94422/68313)\n",
    "\n",
    "\n",
    "<img src=\"image/word2vec_why_not_one_W.png\" align=\"left\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from itertools import islice\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=400) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Google Colab environment\n",
    "\n",
    "Colab gets disconnected within approx 20 min. Hence not suitable for training (or need to upgrade to the pro version)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone github to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_GOOGLE_COLAB = True\n",
    "except:\n",
    "    IN_GOOGLE_COLAB = False\n",
    "    \n",
    "if IN_GOOGLE_COLAB:\n",
    "    !pip install line_profiler\n",
    "    !google.colab.drive.mount('/content/gdrive')\n",
    "    !rm -rf /content/drive/MyDrive/github\n",
    "    !mkdir -p /content/drive/MyDrive/github\n",
    "    !git clone https://github.com/oonisim/python-programs.git /content/drive/MyDrive/github\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone github to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_GOOGLE_COLAB = True\n",
    "except:\n",
    "    IN_GOOGLE_COLAB = False\n",
    "    \n",
    "if IN_GOOGLE_COLAB:\n",
    "    !pip install line_profiler\n",
    "    !google.colab.drive.mount('/content/gdrive')\n",
    "    !rm -rf /content/github\n",
    "    !mkdir -p /content/github\n",
    "    !git clone https://github.com/oonisim/python-programs.git /content/github\n",
    "        \n",
    "    import sys\n",
    "    sys.path.append('/content/github/nlp/src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook setups\n",
    "\n",
    "Auto reolaod causes an error in Jupyter notebooks. Restart the Jupyter kernel for the error:\n",
    "```TypeError: super(type, obj): obj must be an instance or subtype of type```\n",
    "See\n",
    "- https://stackoverflow.com/a/52927102/4281353\n",
    "- http://thomas-cokelaer.info/blog/2011/09/382/\n",
    "\n",
    "> The problem resides in the mechanism of reloading modules.\n",
    "> Reloading a module often changes the internal object in memory which\n",
    "> makes the isinstance test of super return False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "import function.fileio as fileio\n",
    "import function.text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.constant import (\n",
    "    TYPE_INT,\n",
    "    TYPE_FLOAT,\n",
    "    TYPE_LABEL,\n",
    "    TYPE_TENSOR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PTB = True\n",
    "DEBUG = False\n",
    "VALIDATION = True\n",
    "\n",
    "TARGET_SIZE = 1   # Size of the target event (word)\n",
    "CONTEXT_SIZE = 10  # Size of the context.\n",
    "WINDOW_SIZE = TARGET_SIZE + CONTEXT_SIZE\n",
    "SAMPLE_SIZE = 5   # Size of the negative samples\n",
    "VECTOR_SIZE = 100  # Number of features in the event vector.\n",
    "STATE_FILE = \"/home/oonisim/home/repository/git/oonisim/python_programs/nlp/models/word2vec_vecsize_100.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"To be, or not to be, that is the question that matters\"\n",
    "_file = \"ptb.train.txt\"\n",
    "if USE_PTB:\n",
    "    if not fileio.Function.is_file(f\"~/.keras/datasets/{_file}\"):\n",
    "        path_to_ptb = tf.keras.utils.get_file(\n",
    "            _file, \n",
    "            f'https://raw.githubusercontent.com/tomsercu/lstm/master/data/{_file}'\n",
    "        )\n",
    "    corpus = fileio.Function.read_file(path_to_ptb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n",
      " pierre <unk> N years old will join the board as a nonexecutive director nov. N \n",
      " mr. <unk> is chairman of <unk> n.v. the dutch publishing group \n",
      " rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate \n",
      " a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported \n"
     ]
    }
   ],
   "source": [
    "examples = corpus.split('\\n')[:5]\n",
    "for line in examples:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Event (word) indexing\n",
    "Index the events that have occurred in the event sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer.preprocessing import (\n",
    "    EventIndexing, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexing = EventIndexing(\n",
    "    name=\"word_indexing_on_ptb\",\n",
    "    corpus=corpus,\n",
    "    min_sequence_length=WINDOW_SIZE\n",
    ")\n",
    "del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EventIndexing  for the corpus\n",
    "\n",
    "Adapt to the ```corpus``` and provides:\n",
    "* event_to_index dictionary\n",
    "* vocaburary of the corpus\n",
    "* word occurrence probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EventIndexing.vocabulary[10]:\n",
      "['<nil>' '<unk>' 'aer' 'banknote' 'berlitz' 'calloway' 'centrust' 'cluett' 'fromstein' 'gitano']\n",
      "\n",
      "EventIndexing.event_to_index[10]:\n",
      "('<nil>', 0)\n",
      "('<unk>', 1)\n",
      "('aer', 2)\n",
      "('banknote', 3)\n",
      "('berlitz', 4)\n",
      "('calloway', 5)\n",
      "('centrust', 6)\n",
      "('cluett', 7)\n",
      "('fromstein', 8)\n",
      "('gitano', 9)\n",
      "\n",
      "EventIndexing.probabilities[10]:\n",
      "<nil>                : 0.00000e+00\n",
      "<unk>                : 1.65308e-02\n",
      "aer                  : 5.34860e-06\n",
      "banknote             : 5.34860e-06\n",
      "berlitz              : 5.34860e-06\n",
      "calloway             : 5.34860e-06\n",
      "centrust             : 5.34860e-06\n",
      "cluett               : 5.34860e-06\n",
      "fromstein            : 5.34860e-06\n",
      "gitano               : 5.34860e-06\n"
     ]
    }
   ],
   "source": [
    "words = word_indexing.list_events(range(10))\n",
    "print(f\"EventIndexing.vocabulary[10]:\\n{words}\\n\")\n",
    "\n",
    "indices = word_indexing.list_indices(words)\n",
    "print(f\"EventIndexing.event_to_index[10]:\")\n",
    "for item in zip(words, indices):\n",
    "    print(item)\n",
    "\n",
    "probabilities = word_indexing.list_probabilities(words)\n",
    "print(f\"\\nEventIndexing.probabilities[10]:\")\n",
    "for word, p in zip(words, probabilities):\n",
    "    print(f\"{word:20s} : {p:.5e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling using the probability\n",
    "\n",
    "Sample events according to their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['on', 'surgery', 'educational', 'full', 'worried']\n"
     ]
    }
   ],
   "source": [
    "sample = word_indexing.sample(size=5)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "Sample events not including those events already sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_indices=[2112, 66, 1989, 779, 3788] \n",
      "events=['sony' 'to' 'contributed' 'shore' 'actually']\n"
     ]
    }
   ],
   "source": [
    "negative_indices = word_indexing.negative_sample_indices(\n",
    "    size=5, excludes=word_indexing.list_indices(sample)\n",
    ")\n",
    "print(f\"negative_indices={negative_indices} \\nevents={word_indexing.list_events(negative_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the             :    34\n",
      "asbestos        :    63\n",
      "fiber           :    86\n",
      "<unk>           :     1\n",
      "is              :    42\n",
      "unusually       :    87\n",
      "<unk>           :     1\n",
      "once            :    64\n",
      "it              :    80\n",
      "enters          :    88\n",
      "the             :    34\n",
      "<unk>           :     1\n",
      "\n",
      "with           :     0\n",
      "even            :     0\n",
      "brief           :     0\n"
     ]
    }
   ],
   "source": [
    "# sentences = \"\\n\".join(corpus.split('\\n')[5:6])\n",
    "sentences = \"\"\"\n",
    "the asbestos fiber <unk> is unusually <unk> once it enters the <unk> \n",
    "with even brief exposures to it causing symptoms that show up decades later researchers said\n",
    "\"\"\"\n",
    "sequences = word_indexing.function(sentences)\n",
    "for pair in zip(sentences.strip().split(\" \"), sequences[0]):\n",
    "    print(f\"{pair[0]:15} : {pair[1]:5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EventContext\n",
    "\n",
    "EventContext layer generates ```(event, context)``` pairs from sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer.preprocessing import (\n",
    "    EventContext\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_context = EventContext(\n",
    "    name=\"ev\",\n",
    "    window_size=WINDOW_SIZE,\n",
    "    event_size=TARGET_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context of an event (word) in a sentence\n",
    "\n",
    "In the sentence ```\"a form of asbestos once used to make kent cigarette filters\"```, one of the context windows ```a form of asbestos once``` of size 5 and event size 1 has.\n",
    "* ```of``` as a target word.\n",
    "* ```(a, form) and (asbestos, once)``` as its context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence of the word indices for the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[37, 62, 44, 63, 64, 65, 66, 67, 68, 69, 70,  0,  0,  0,  0,  0],\n",
       "       [29, 30, 31, 50, 51, 43, 44, 52, 53, 54, 55, 56, 57, 37, 38, 39]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \"\"\"\n",
    "a form of asbestos once used to make kent cigarette filters\n",
    "\n",
    "N years old and former chairman of consolidated gold fields plc was named a nonexecutive director\n",
    "\"\"\"\n",
    "\n",
    "sequence = word_indexing.function(sentences)\n",
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (event, context) pairs\n",
    "\n",
    "For each word (event) in the setence ```(of, asbestos, ... , kent)``` excludnig the ends of the sentence, create ```(target, context)``` as:\n",
    "\n",
    "```\n",
    "[\n",
    "  [of, a, form, asbestos, once],              # target is 'of', context is (a, form, asbestos, once)\n",
    "  ['asbestos', 'form', 'of', 'once', 'used'],\n",
    "  ['once', 'of', 'asbestos', 'used', 'to'],\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "### Format of the (event, context) pairs\n",
    "\n",
    "* **E** is the target event indices\n",
    "* **C** is the context indices\n",
    "\n",
    "<img src=\"image/event_context_format.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event context pairs. Shape (12, 11), Target event size 1, Window size 11.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[65, 37, 62, 44, 63, 64, 66, 67, 68, 69, 70],\n",
       "       [66, 62, 44, 63, 64, 65, 67, 68, 69, 70,  0],\n",
       "       [67, 44, 63, 64, 65, 66, 68, 69, 70,  0,  0],\n",
       "       [68, 63, 64, 65, 66, 67, 69, 70,  0,  0,  0],\n",
       "       [69, 64, 65, 66, 67, 68, 70,  0,  0,  0,  0],\n",
       "       [70, 65, 66, 67, 68, 69,  0,  0,  0,  0,  0],\n",
       "       [43, 29, 30, 31, 50, 51, 44, 52, 53, 54, 55],\n",
       "       [44, 30, 31, 50, 51, 43, 52, 53, 54, 55, 56],\n",
       "       [52, 31, 50, 51, 43, 44, 53, 54, 55, 56, 57],\n",
       "       [53, 50, 51, 43, 44, 52, 54, 55, 56, 57, 37]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_context_pairs = event_context.function(sequence)\n",
    "print(\n",
    "    f\"Event context pairs. Shape %s, Target event size %s, Window size %s.\" % \n",
    "    (event_context_pairs.shape, event_context.event_size, event_context.window_size)\n",
    ")\n",
    "event_context_pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (event, context) pairs in textual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['used',\n",
       "  'a',\n",
       "  'form',\n",
       "  'of',\n",
       "  'asbestos',\n",
       "  'once',\n",
       "  'to',\n",
       "  'make',\n",
       "  'kent',\n",
       "  'cigarette',\n",
       "  'filters'],\n",
       " ['to',\n",
       "  'form',\n",
       "  'of',\n",
       "  'asbestos',\n",
       "  'once',\n",
       "  'used',\n",
       "  'make',\n",
       "  'kent',\n",
       "  'cigarette',\n",
       "  'filters',\n",
       "  '<nil>'],\n",
       " ['make',\n",
       "  'of',\n",
       "  'asbestos',\n",
       "  'once',\n",
       "  'used',\n",
       "  'to',\n",
       "  'kent',\n",
       "  'cigarette',\n",
       "  'filters',\n",
       "  '<nil>',\n",
       "  '<nil>'],\n",
       " ['kent',\n",
       "  'asbestos',\n",
       "  'once',\n",
       "  'used',\n",
       "  'to',\n",
       "  'make',\n",
       "  'cigarette',\n",
       "  'filters',\n",
       "  '<nil>',\n",
       "  '<nil>',\n",
       "  '<nil>'],\n",
       " ['cigarette',\n",
       "  'once',\n",
       "  'used',\n",
       "  'to',\n",
       "  'make',\n",
       "  'kent',\n",
       "  'filters',\n",
       "  '<nil>',\n",
       "  '<nil>',\n",
       "  '<nil>',\n",
       "  '<nil>'],\n",
       " ['filters',\n",
       "  'used',\n",
       "  'to',\n",
       "  'make',\n",
       "  'kent',\n",
       "  'cigarette',\n",
       "  '<nil>',\n",
       "  '<nil>',\n",
       "  '<nil>',\n",
       "  '<nil>',\n",
       "  '<nil>'],\n",
       " ['chairman',\n",
       "  'n',\n",
       "  'years',\n",
       "  'old',\n",
       "  'and',\n",
       "  'former',\n",
       "  'of',\n",
       "  'consolidated',\n",
       "  'gold',\n",
       "  'fields',\n",
       "  'plc'],\n",
       " ['of',\n",
       "  'years',\n",
       "  'old',\n",
       "  'and',\n",
       "  'former',\n",
       "  'chairman',\n",
       "  'consolidated',\n",
       "  'gold',\n",
       "  'fields',\n",
       "  'plc',\n",
       "  'was'],\n",
       " ['consolidated',\n",
       "  'old',\n",
       "  'and',\n",
       "  'former',\n",
       "  'chairman',\n",
       "  'of',\n",
       "  'gold',\n",
       "  'fields',\n",
       "  'plc',\n",
       "  'was',\n",
       "  'named'],\n",
       " ['gold',\n",
       "  'and',\n",
       "  'former',\n",
       "  'chairman',\n",
       "  'of',\n",
       "  'consolidated',\n",
       "  'fields',\n",
       "  'plc',\n",
       "  'was',\n",
       "  'named',\n",
       "  'a']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indexing.sequence_to_sentence(event_context_pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Word Embedding\n",
    "\n",
    "Embedding is to train the model to group similar events in a close proximity in the event vector space. If two events e.g. 'pencil' and 'pen' are similar concepts, then their event vectors resides in a close distance in the event space. \n",
    "\n",
    "* [Thought Vectors](https://wiki.pathmind.com/thought-vectors)\n",
    "\n",
    "## Training process\n",
    "\n",
    "1. Calculate ```Bc```, the BoW (Bag of Words) from context event vectors.\n",
    "2. Calculate ```Be```,  the BoW (Bag of Words) from target event vectors.\n",
    "3. The dot product ```Ye = dot(Bc, Be)``` is given the label 1 to get them closer.\n",
    "4. For each negative sample ```Ws(s)```, the dot product with ```Ys = dot(Be, Ws(s)``` is given the label 0 to get them apart. \n",
    "5. ```np.c_[Ye, Ys]``` is fowarded to the logistic log loss layer.\n",
    "\n",
    "<img src=\"image/word2vec_backprop_Be.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer import (\n",
    "    Embedding\n",
    ")\n",
    "from optimizer import (\n",
    "    SGD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding: Embedding = Embedding(\n",
    "    name=\"embedding\",\n",
    "    num_nodes=WINDOW_SIZE,\n",
    "    target_size=TARGET_SIZE,\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    negative_sample_size=SAMPLE_SIZE,\n",
    "    event_vector_size=VECTOR_SIZE,\n",
    "    optimizer=SGD(lr=TYPE_FLOAT(0.3)),\n",
    "    dictionary=word_indexing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores ```np.c_[Ye, Ys]``` from the Embedding\n",
    "\n",
    "The 0th column is the scores for ```dot(Bc, Be``` for positive labels. The rest are the scores for ```dot(Bc, Ws)``` for negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "[[ 2.8448356e-03 -4.6626781e-04 -1.0513982e-03 -3.1637019e-03 -7.7288365e-05  9.3030103e-05]\n",
      " [-1.8796960e-03 -7.1030618e-03 -1.3254826e-03  2.5461549e-03 -1.7686044e-03 -1.0504745e-03]\n",
      " [ 6.2393253e-03 -1.6674981e-03  3.5888143e-04  4.0004007e-04  1.3552639e-03  1.2281188e-03]\n",
      " [ 1.1894747e-04  3.4915728e-03  1.0206593e-03 -6.8337061e-03  2.8054942e-03  2.2942876e-04]\n",
      " [-4.2786673e-03 -3.6547363e-03 -4.2858580e-04 -4.0217284e-03 -1.5611844e-03 -5.2395929e-03]]\n",
      "\n",
      "Scores for dot(Bc, Be): \n",
      "[[ 0.00284484]\n",
      " [-0.0018797 ]\n",
      " [ 0.00623933]\n",
      " [ 0.00011895]\n",
      " [-0.00427867]]\n"
     ]
    }
   ],
   "source": [
    "scores = embedding.function(event_context_pairs)\n",
    "print(f\"Scores:\\n{scores[:5]}\\n\")\n",
    "print(f\"Scores for dot(Bc, Be): \\n{scores[:5, :1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([1,2,3]).numpy().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Logistic Log Loss\n",
    "\n",
    "Train the model to get:\n",
    "1. BoW of the context event vectors close to the target event vector. Label 1\n",
    "2. BoW of the context event vectors away from each of the negative sample event vectors Label 0.\n",
    "\n",
    "This is a binary logistic classification, hence use Logistic Log Loss as the network objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.function import (\n",
    "    sigmoid_cross_entropy_log_loss,\n",
    "    sigmoid\n",
    ")\n",
    "from layer.objective import (\n",
    "    CrossEntropyLogLoss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CrossEntropyLogLoss(\n",
    "    name=\"loss\",\n",
    "    num_nodes=1,  # Logistic log loss\n",
    "    log_loss_function=sigmoid_cross_entropy_log_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Adapter\n",
    "\n",
    "The logistic log loss layer expects the input of shape ```(N,M=1)```, however Embedding outputs ```(N,(1+SL)``` where ```SL``` is SAMPLE_SIZE. The ```Adapter``` layer bridges between Embedding and the Loss layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer.adapter import (\n",
    "    Adapter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_function = embedding.adapt_function_to_logistic_log_loss(loss=loss)\n",
    "adapter_gradient = embedding.adapt_gradient_to_logistic_log_loss()\n",
    "\n",
    "adapter: Adapter = Adapter(\n",
    "    name=\"adapter\",\n",
    "    num_nodes=1,    # Number of output M=1 \n",
    "    function=adapter_function,\n",
    "    gradient=adapter_gradient\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Word2vec Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a sequential network\n",
    "\n",
    "$ \\text {Sentences} \\rightarrow EventIndexing \\rightarrow EventContext \\rightarrow  Embedding \\rightarrow Adapter \\rightarrow LogisticLogLoss$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.sequential import (\n",
    "    SequentialNetwork\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = SequentialNetwork(\n",
    "    name=\"word2vec\",\n",
    "    num_nodes=1,\n",
    "    inference_layers=[\n",
    "        word_indexing,\n",
    "        event_context,\n",
    "        embedding,\n",
    "        adapter\n",
    "    ],\n",
    "    objective_layers=[\n",
    "        loss\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved model file if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "STATE_FILE: /home/oonisim/home/repository/git/oonisim/python_programs/nlp/models/word2vec_vecsize_100.pkl\n",
      "Model loaded.\n",
      "    event_size 1\n",
      "    context_size: 10\n",
      "    event_vector_size: 100\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "if fileio.Function.is_file(STATE_FILE):\n",
    "    print(\"Loading model...\\nSTATE_FILE: %s\" % STATE_FILE)\n",
    "    state = embedding.load(STATE_FILE)\n",
    "\n",
    "    fmt=\"\"\"Model loaded.\n",
    "    event_size %s\n",
    "    context_size: %s\n",
    "    event_vector_size: %s\n",
    "    \"\"\"\n",
    "    print(fmt % (\n",
    "        state[\"target_size\"], \n",
    "        state[\"context_size\"], \n",
    "        state[\"event_vector_size\"]\n",
    "    ))\n",
    "else:\n",
    "    print(\"State file does not exist. Saving the initial model.\")\n",
    "    embedding.save(STATE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 00000 of 50 sentences: Average Loss:   0.288738 Duration 0.785595\n",
      "Batch 00100 of 50 sentences: Average Loss:   0.263785 Duration 0.821840\n",
      "Batch 00200 of 50 sentences: Average Loss:   0.261323 Duration 0.686028\n",
      "Batch 00300 of 50 sentences: Average Loss:   0.259980 Duration 0.873889\n",
      "Batch 00400 of 50 sentences: Average Loss:   0.258004 Duration 0.696118\n",
      "Batch 00500 of 50 sentences: Average Loss:   0.255848 Duration 0.872351\n",
      "Batch 00600 of 50 sentences: Average Loss:   0.255005 Duration 0.538219\n",
      "Batch 00700 of 50 sentences: Average Loss:   0.254163 Duration 0.502189\n",
      "Batch 00800 of 50 sentences: Average Loss:   0.253061 Duration 0.586886\n",
      "epoch 0 batches 00842 done\n",
      "Average loss:        0.252672\n",
      "Batch 00900 of 50 sentences: Average Loss:   0.252699 Duration 0.659787\n",
      "Batch 01000 of 50 sentences: Average Loss:   0.252112 Duration 0.754639\n",
      "Batch 01100 of 50 sentences: Average Loss:   0.252105 Duration 0.731786\n",
      "Batch 01200 of 50 sentences: Average Loss:   0.251873 Duration 0.536669\n",
      "Batch 01300 of 50 sentences: Average Loss:   0.251105 Duration 0.508901\n",
      "Batch 01400 of 50 sentences: Average Loss:   0.250786 Duration 0.504055\n",
      "Batch 01500 of 50 sentences: Average Loss:   0.250500 Duration 0.444503\n",
      "Batch 01600 of 50 sentences: Average Loss:   0.250171 Duration 0.503255\n",
      "epoch 1 batches 01685 done\n",
      "Average loss:        0.249636\n",
      "Batch 01700 of 50 sentences: Average Loss:   0.249662 Duration 0.587815\n",
      "Batch 01800 of 50 sentences: Average Loss:   0.249460 Duration 0.472108\n",
      "Batch 01900 of 50 sentences: Average Loss:   0.249310 Duration 0.577142\n",
      "Batch 02000 of 50 sentences: Average Loss:   0.249163 Duration 0.501357\n",
      "Batch 02100 of 50 sentences: Average Loss:   0.248947 Duration 0.539595\n",
      "Batch 02200 of 50 sentences: Average Loss:   0.248562 Duration 0.608583\n",
      "Batch 02300 of 50 sentences: Average Loss:   0.248476 Duration 0.854804\n",
      "Batch 02400 of 50 sentences: Average Loss:   0.248217 Duration 0.476812\n",
      "Batch 02500 of 50 sentences: Average Loss:   0.247930 Duration 0.548493\n",
      "epoch 2 batches 02528 done\n",
      "Average loss:        0.247855\n",
      "Batch 02600 of 50 sentences: Average Loss:   0.247771 Duration 0.647666\n",
      "Batch 02700 of 50 sentences: Average Loss:   0.247699 Duration 0.887697\n",
      "Batch 02800 of 50 sentences: Average Loss:   0.247659 Duration 0.516068\n",
      "Batch 02900 of 50 sentences: Average Loss:   0.247598 Duration 0.767305\n",
      "Batch 03000 of 50 sentences: Average Loss:   0.247263 Duration 0.515797\n",
      "Batch 03100 of 50 sentences: Average Loss:   0.247132 Duration 0.645007\n",
      "Batch 03200 of 50 sentences: Average Loss:   0.246987 Duration 0.616144\n",
      "Batch 03300 of 50 sentences: Average Loss:   0.246865 Duration 0.468342\n",
      "epoch 3 batches 03371 done\n",
      "Average loss:        0.246662\n",
      "Batch 03400 of 50 sentences: Average Loss:   0.246660 Duration 0.730341\n",
      "Batch 03500 of 50 sentences: Average Loss:   0.246593 Duration 0.464482\n",
      "Batch 03600 of 50 sentences: Average Loss:   0.246535 Duration 0.649485\n",
      "Batch 03700 of 50 sentences: Average Loss:   0.246475 Duration 0.566320\n",
      "Batch 03800 of 50 sentences: Average Loss:   0.246362 Duration 0.803493\n",
      "Batch 03900 of 50 sentences: Average Loss:   0.246115 Duration 0.714741\n",
      "Batch 04000 of 50 sentences: Average Loss:   0.246081 Duration 0.582129\n",
      "Batch 04100 of 50 sentences: Average Loss:   0.245962 Duration 0.695073\n",
      "Batch 04200 of 50 sentences: Average Loss:   0.245774 Duration 0.486105\n",
      "epoch 4 batches 04214 done\n",
      "Average loss:        0.245757\n",
      "Batch 04300 of 50 sentences: Average Loss:   0.245690 Duration 0.685099\n",
      "Batch 04400 of 50 sentences: Average Loss:   0.245654 Duration 0.690432\n",
      "Batch 04500 of 50 sentences: Average Loss:   0.245616 Duration 0.572482\n",
      "Batch 04600 of 50 sentences: Average Loss:   0.245582 Duration 0.494514\n",
      "Batch 04700 of 50 sentences: Average Loss:   0.245431 Duration 0.475112\n",
      "Batch 04800 of 50 sentences: Average Loss:   0.245347 Duration 0.561887\n",
      "Batch 04900 of 50 sentences: Average Loss:   0.245294 Duration 0.405502\n",
      "Batch 05000 of 50 sentences: Average Loss:   0.245161 Duration 0.701629\n",
      "epoch 5 batches 05057 done\n",
      "Average loss:        0.245051\n",
      "Batch 05100 of 50 sentences: Average Loss:   0.245079 Duration 0.591808\n",
      "Batch 05200 of 50 sentences: Average Loss:   0.244981 Duration 0.556081\n",
      "Batch 05300 of 50 sentences: Average Loss:   0.244993 Duration 0.699347\n",
      "Batch 05400 of 50 sentences: Average Loss:   0.244937 Duration 0.595534\n",
      "Batch 05500 of 50 sentences: Average Loss:   0.244838 Duration 0.484494\n",
      "Batch 05600 of 50 sentences: Average Loss:   0.244735 Duration 0.885929\n",
      "Batch 05700 of 50 sentences: Average Loss:   0.244703 Duration 0.582193\n",
      "Batch 05800 of 50 sentences: Average Loss:   0.244634 Duration 0.653964\n",
      "epoch 6 batches 05900 done\n",
      "Average loss:        0.244472\n",
      "Batch 06000 of 50 sentences: Average Loss:   0.244421 Duration 0.482140\n",
      "Batch 06100 of 50 sentences: Average Loss:   0.244383 Duration 0.421187\n",
      "Batch 06200 of 50 sentences: Average Loss:   0.244381 Duration 0.572266\n",
      "Batch 06300 of 50 sentences: Average Loss:   0.244311 Duration 0.598770\n",
      "Batch 06400 of 50 sentences: Average Loss:   0.244201 Duration 0.483625\n",
      "Batch 06500 of 50 sentences: Average Loss:   0.244148 Duration 0.504225\n",
      "Batch 06600 of 50 sentences: Average Loss:   0.244093 Duration 0.497442\n",
      "Batch 06700 of 50 sentences: Average Loss:   0.243990 Duration 0.553755\n",
      "epoch 7 batches 06743 done\n",
      "Average loss:        0.243938\n",
      "Batch 06800 of 50 sentences: Average Loss:   0.243940 Duration 0.405622\n",
      "Batch 06900 of 50 sentences: Average Loss:   0.243864 Duration 0.542583\n",
      "Batch 07000 of 50 sentences: Average Loss:   0.243882 Duration 0.522771\n",
      "Batch 07100 of 50 sentences: Average Loss:   0.243867 Duration 0.568784\n",
      "Batch 07200 of 50 sentences: Average Loss:   0.243743 Duration 0.611316\n",
      "Batch 07300 of 50 sentences: Average Loss:   0.243693 Duration 0.570234\n",
      "Batch 07400 of 50 sentences: Average Loss:   0.243644 Duration 0.454574\n",
      "Batch 07500 of 50 sentences: Average Loss:   0.243589 Duration 0.586803\n",
      "epoch 8 batches 07586 done\n",
      "Average loss:        0.243477\n",
      "Batch 07600 of 50 sentences: Average Loss:   0.243482 Duration 0.542189\n",
      "Batch 07700 of 50 sentences: Average Loss:   0.243461 Duration 0.592809\n",
      "Batch 07800 of 50 sentences: Average Loss:   0.243429 Duration 0.658674\n",
      "Batch 07900 of 50 sentences: Average Loss:   0.243422 Duration 0.659668\n",
      "Batch 08000 of 50 sentences: Average Loss:   0.243369 Duration 0.502786\n",
      "Batch 08100 of 50 sentences: Average Loss:   0.243266 Duration 0.711143\n",
      "Batch 08200 of 50 sentences: Average Loss:   0.243242 Duration 0.443664\n",
      "Batch 08300 of 50 sentences: Average Loss:   0.243164 Duration 0.587759\n",
      "Batch 08400 of 50 sentences: Average Loss:   0.243089 Duration 0.650703\n",
      "epoch 9 batches 08429 done\n",
      "Average loss:        0.243071\n",
      "Batch 08500 of 50 sentences: Average Loss:   0.243046 Duration 0.554351\n",
      "Batch 08600 of 50 sentences: Average Loss:   0.243015 Duration 0.577858\n",
      "Batch 08700 of 50 sentences: Average Loss:   0.243022 Duration 0.793581\n",
      "Batch 08800 of 50 sentences: Average Loss:   0.243005 Duration 0.555329\n",
      "Batch 08900 of 50 sentences: Average Loss:   0.242904 Duration 0.739539\n",
      "Batch 09000 of 50 sentences: Average Loss:   0.242875 Duration 0.756027\n",
      "Batch 09100 of 50 sentences: Average Loss:   0.242831 Duration 0.618762\n",
      "Batch 09200 of 50 sentences: Average Loss:   0.242782 Duration 0.769401\n",
      "epoch 10 batches 09272 done\n",
      "Average loss:        0.242718\n",
      "Batch 09300 of 50 sentences: Average Loss:   0.242716 Duration 0.601488\n",
      "Batch 09400 of 50 sentences: Average Loss:   0.242693 Duration 0.484190\n",
      "Batch 09500 of 50 sentences: Average Loss:   0.242669 Duration 0.543923\n",
      "Batch 09600 of 50 sentences: Average Loss:   0.242654 Duration 0.594694\n",
      "Batch 09700 of 50 sentences: Average Loss:   0.242612 Duration 0.355768\n",
      "Batch 09800 of 50 sentences: Average Loss:   0.242527 Duration 0.557261\n",
      "Batch 09900 of 50 sentences: Average Loss:   0.242510 Duration 0.476539\n",
      "Batch 10000 of 50 sentences: Average Loss:   0.242459 Duration 0.898474\n",
      "Batch 10100 of 50 sentences: Average Loss:   0.242378 Duration 0.557456\n",
      "epoch 11 batches 10115 done\n",
      "Average loss:        0.242368\n",
      "Batch 10200 of 50 sentences: Average Loss:   0.242337 Duration 0.499332\n",
      "Batch 10300 of 50 sentences: Average Loss:   0.242324 Duration 0.699781\n",
      "Batch 10400 of 50 sentences: Average Loss:   0.242303 Duration 0.495397\n",
      "Batch 10500 of 50 sentences: Average Loss:   0.242290 Duration 0.509935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10600 of 50 sentences: Average Loss:   0.242228 Duration 0.560842\n",
      "Batch 10700 of 50 sentences: Average Loss:   0.242183 Duration 0.653175\n",
      "Batch 10800 of 50 sentences: Average Loss:   0.242163 Duration 0.545556\n",
      "Batch 10900 of 50 sentences: Average Loss:   0.242107 Duration 0.495770\n",
      "epoch 12 batches 10958 done\n",
      "Average loss:        0.242058\n",
      "Batch 11000 of 50 sentences: Average Loss:   0.242065 Duration 0.680079\n",
      "Batch 11100 of 50 sentences: Average Loss:   0.242017 Duration 0.492269\n",
      "Batch 11200 of 50 sentences: Average Loss:   0.242026 Duration 0.565234\n",
      "Batch 11300 of 50 sentences: Average Loss:   0.241996 Duration 0.861460\n",
      "Batch 11400 of 50 sentences: Average Loss:   0.241951 Duration 0.655205\n",
      "Batch 11500 of 50 sentences: Average Loss:   0.241890 Duration 0.427369\n",
      "Batch 11600 of 50 sentences: Average Loss:   0.241871 Duration 0.551649\n",
      "Batch 11700 of 50 sentences: Average Loss:   0.241832 Duration 0.852637\n",
      "Batch 11800 of 50 sentences: Average Loss:   0.241748 Duration 0.249130\n",
      "epoch 13 batches 11801 done\n",
      "Average loss:        0.241748\n",
      "Batch 11900 of 50 sentences: Average Loss:   0.241724 Duration 0.676095\n",
      "Batch 12000 of 50 sentences: Average Loss:   0.241710 Duration 0.814785\n",
      "Batch 12100 of 50 sentences: Average Loss:   0.241708 Duration 0.598592\n",
      "Batch 12200 of 50 sentences: Average Loss:   0.241681 Duration 0.537169\n",
      "Batch 12300 of 50 sentences: Average Loss:   0.241621 Duration 0.537420\n",
      "Batch 12400 of 50 sentences: Average Loss:   0.241581 Duration 0.566353\n",
      "Batch 12500 of 50 sentences: Average Loss:   0.241545 Duration 0.484935\n",
      "Batch 12600 of 50 sentences: Average Loss:   0.241492 Duration 0.679394\n",
      "epoch 14 batches 12644 done\n",
      "Average loss:        0.241465\n",
      "Batch 12700 of 50 sentences: Average Loss:   0.241473 Duration 0.519012\n",
      "Batch 12800 of 50 sentences: Average Loss:   0.241422 Duration 0.321880\n",
      "Batch 12900 of 50 sentences: Average Loss:   0.241423 Duration 0.628751\n",
      "Batch 13000 of 50 sentences: Average Loss:   0.241411 Duration 0.462932\n",
      "Batch 13100 of 50 sentences: Average Loss:   0.241345 Duration 0.499006\n",
      "Batch 13200 of 50 sentences: Average Loss:   0.241312 Duration 0.441228\n",
      "Batch 13300 of 50 sentences: Average Loss:   0.241282 Duration 0.607817\n",
      "Batch 13400 of 50 sentences: Average Loss:   0.241245 Duration 0.528351\n",
      "epoch 15 batches 13487 done\n",
      "Average loss:        0.241181\n",
      "Batch 13500 of 50 sentences: Average Loss:   0.241181 Duration 0.516631\n",
      "Batch 13600 of 50 sentences: Average Loss:   0.241163 Duration 0.527596\n",
      "Batch 13700 of 50 sentences: Average Loss:   0.241147 Duration 0.596595\n",
      "Batch 13800 of 50 sentences: Average Loss:   0.241141 Duration 0.479220\n",
      "Batch 13900 of 50 sentences: Average Loss:   0.241104 Duration 0.529765\n",
      "Batch 14000 of 50 sentences: Average Loss:   0.241043 Duration 0.561280\n",
      "Batch 14100 of 50 sentences: Average Loss:   0.241028 Duration 0.439596\n",
      "Batch 14200 of 50 sentences: Average Loss:   0.240981 Duration 0.505585\n",
      "Batch 14300 of 50 sentences: Average Loss:   0.240929 Duration 0.587307\n",
      "epoch 16 batches 14330 done\n",
      "Average loss:        0.240918\n",
      "Batch 14400 of 50 sentences: Average Loss:   0.240900 Duration 0.579897\n",
      "Batch 14500 of 50 sentences: Average Loss:   0.240880 Duration 0.533192\n",
      "Batch 14600 of 50 sentences: Average Loss:   0.240877 Duration 0.533931\n",
      "Batch 14700 of 50 sentences: Average Loss:   0.240860 Duration 0.544967\n",
      "Batch 14800 of 50 sentences: Average Loss:   0.240800 Duration 0.513642\n",
      "Batch 14900 of 50 sentences: Average Loss:   0.240774 Duration 0.461308\n",
      "Batch 15000 of 50 sentences: Average Loss:   0.240746 Duration 0.413712\n",
      "Batch 15100 of 50 sentences: Average Loss:   0.240708 Duration 0.767935\n",
      "epoch 17 batches 15173 done\n",
      "Average loss:        0.240666\n",
      "Batch 15200 of 50 sentences: Average Loss:   0.240667 Duration 0.562663\n",
      "Batch 15300 of 50 sentences: Average Loss:   0.240648 Duration 0.515715\n",
      "Batch 15400 of 50 sentences: Average Loss:   0.240630 Duration 0.583541\n",
      "Batch 15500 of 50 sentences: Average Loss:   0.240611 Duration 0.369605\n",
      "Batch 15600 of 50 sentences: Average Loss:   0.240585 Duration 0.257617\n",
      "Batch 15700 of 50 sentences: Average Loss:   0.240525 Duration 0.633645\n",
      "Batch 15800 of 50 sentences: Average Loss:   0.240515 Duration 0.671984\n",
      "Batch 15900 of 50 sentences: Average Loss:   0.240477 Duration 0.606841\n",
      "Batch 16000 of 50 sentences: Average Loss:   0.240427 Duration 0.692323\n",
      "epoch 18 batches 16016 done\n",
      "Average loss:        0.240421\n",
      "Batch 16100 of 50 sentences: Average Loss:   0.240404 Duration 0.348514\n",
      "Batch 16200 of 50 sentences: Average Loss:   0.240383 Duration 0.650522\n",
      "Batch 16300 of 50 sentences: Average Loss:   0.240370 Duration 0.739132\n",
      "Batch 16400 of 50 sentences: Average Loss:   0.240359 Duration 0.755819\n",
      "Batch 16500 of 50 sentences: Average Loss:   0.240315 Duration 0.711435\n",
      "Batch 16600 of 50 sentences: Average Loss:   0.240282 Duration 0.578409\n",
      "Batch 16700 of 50 sentences: Average Loss:   0.240264 Duration 0.636616\n",
      "Batch 16800 of 50 sentences: Average Loss:   0.240220 Duration 0.633513\n",
      "epoch 19 batches 16859 done\n",
      "Average loss:        0.240184\n",
      "Batch 16900 of 50 sentences: Average Loss:   0.240187 Duration 0.462400\n",
      "Batch 17000 of 50 sentences: Average Loss:   0.240154 Duration 0.608166\n",
      "Batch 17100 of 50 sentences: Average Loss:   0.240153 Duration 0.705634\n",
      "Batch 17200 of 50 sentences: Average Loss:   0.240136 Duration 0.811003\n",
      "Batch 17300 of 50 sentences: Average Loss:   0.240104 Duration 0.710551\n",
      "Batch 17400 of 50 sentences: Average Loss:   0.240064 Duration 0.469311\n",
      "Batch 17500 of 50 sentences: Average Loss:   0.240047 Duration 0.676790\n",
      "Batch 17600 of 50 sentences: Average Loss:   0.240019 Duration 0.642652\n",
      "Batch 17700 of 50 sentences: Average Loss:   0.239963 Duration 0.504166\n",
      "epoch 20 batches 17702 done\n",
      "Average loss:        0.239962\n",
      "Batch 17800 of 50 sentences: Average Loss:   0.239941 Duration 0.638057\n",
      "Batch 17900 of 50 sentences: Average Loss:   0.239928 Duration 0.610486\n",
      "Batch 18000 of 50 sentences: Average Loss:   0.239920 Duration 0.730241\n",
      "Batch 18100 of 50 sentences: Average Loss:   0.239901 Duration 0.583092\n",
      "Batch 18200 of 50 sentences: Average Loss:   0.239855 Duration 0.528123\n",
      "Batch 18300 of 50 sentences: Average Loss:   0.239825 Duration 0.716176\n",
      "Batch 18400 of 50 sentences: Average Loss:   0.239803 Duration 0.585001\n",
      "Batch 18500 of 50 sentences: Average Loss:   0.239765 Duration 0.561155\n",
      "epoch 21 batches 18545 done\n",
      "Average loss:        0.239746\n",
      "Batch 18600 of 50 sentences: Average Loss:   0.239748 Duration 0.449681\n",
      "Batch 18700 of 50 sentences: Average Loss:   0.239712 Duration 0.501381\n",
      "Batch 18800 of 50 sentences: Average Loss:   0.239716 Duration 0.708937\n",
      "Batch 18900 of 50 sentences: Average Loss:   0.239703 Duration 0.686226\n",
      "Batch 19000 of 50 sentences: Average Loss:   0.239657 Duration 0.475915\n",
      "Batch 19100 of 50 sentences: Average Loss:   0.239626 Duration 0.685298\n",
      "Batch 19200 of 50 sentences: Average Loss:   0.239604 Duration 0.552882\n",
      "Batch 19300 of 50 sentences: Average Loss:   0.239575 Duration 0.483592\n",
      "epoch 22 batches 19388 done\n",
      "Average loss:        0.239526\n",
      "Batch 19400 of 50 sentences: Average Loss:   0.239527 Duration 0.675017\n",
      "Batch 19500 of 50 sentences: Average Loss:   0.239511 Duration 0.519759\n",
      "Batch 19600 of 50 sentences: Average Loss:   0.239492 Duration 0.710236\n",
      "Batch 19700 of 50 sentences: Average Loss:   0.239488 Duration 0.439216\n",
      "Batch 19800 of 50 sentences: Average Loss:   0.239458 Duration 0.454447\n",
      "Batch 19900 of 50 sentences: Average Loss:   0.239417 Duration 0.617102\n",
      "Batch 20000 of 50 sentences: Average Loss:   0.239400 Duration 0.478521\n",
      "Batch 20100 of 50 sentences: Average Loss:   0.239368 Duration 0.519673\n",
      "Batch 20200 of 50 sentences: Average Loss:   0.239328 Duration 0.496999\n",
      "epoch 23 batches 20231 done\n",
      "Average loss:        0.239318\n",
      "Batch 20300 of 50 sentences: Average Loss:   0.239308 Duration 0.674778\n",
      "Batch 20400 of 50 sentences: Average Loss:   0.239288 Duration 0.602997\n",
      "Batch 20500 of 50 sentences: Average Loss:   0.239285 Duration 0.784812\n",
      "Batch 20600 of 50 sentences: Average Loss:   0.239275 Duration 1.703571\n",
      "Batch 20700 of 50 sentences: Average Loss:   0.239226 Duration 0.794937\n",
      "Batch 20800 of 50 sentences: Average Loss:   0.239208 Duration 0.934258\n",
      "Batch 20900 of 50 sentences: Average Loss:   0.239185 Duration 0.823902\n",
      "Batch 21000 of 50 sentences: Average Loss:   0.239151 Duration 0.851315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24 batches 21074 done\n",
      "Average loss:        0.239115\n",
      "Batch 21100 of 50 sentences: Average Loss:   0.239117 Duration 0.706414\n",
      "Batch 21200 of 50 sentences: Average Loss:   0.239099 Duration 0.722414\n",
      "Batch 21300 of 50 sentences: Average Loss:   0.239084 Duration 0.895724\n",
      "Batch 21400 of 50 sentences: Average Loss:   0.239066 Duration 0.599506\n",
      "Batch 21500 of 50 sentences: Average Loss:   0.239044 Duration 0.274326\n",
      "Batch 21600 of 50 sentences: Average Loss:   0.238998 Duration 0.535940\n",
      "Batch 21700 of 50 sentences: Average Loss:   0.238987 Duration 0.587002\n",
      "Batch 21800 of 50 sentences: Average Loss:   0.238958 Duration 0.693025\n",
      "Batch 21900 of 50 sentences: Average Loss:   0.238919 Duration 0.604217\n",
      "epoch 25 batches 21917 done\n",
      "Average loss:        0.238913\n",
      "Batch 22000 of 50 sentences: Average Loss:   0.238899 Duration 0.524585\n",
      "Batch 22100 of 50 sentences: Average Loss:   0.238884 Duration 0.511303\n",
      "Batch 22200 of 50 sentences: Average Loss:   0.238872 Duration 0.710792\n",
      "Batch 22300 of 50 sentences: Average Loss:   0.238860 Duration 0.518064\n",
      "Batch 22400 of 50 sentences: Average Loss:   0.238826 Duration 0.570116\n",
      "Batch 22500 of 50 sentences: Average Loss:   0.238794 Duration 0.520833\n",
      "Batch 22600 of 50 sentences: Average Loss:   0.238779 Duration 0.536446\n",
      "Batch 22700 of 50 sentences: Average Loss:   0.238747 Duration 0.552871\n",
      "epoch 26 batches 22760 done\n",
      "Average loss:        0.238719\n",
      "Batch 22800 of 50 sentences: Average Loss:   0.238720 Duration 0.730306\n",
      "Batch 22900 of 50 sentences: Average Loss:   0.238691 Duration 0.522091\n",
      "Batch 23000 of 50 sentences: Average Loss:   0.238688 Duration 0.684748\n",
      "Batch 23100 of 50 sentences: Average Loss:   0.238671 Duration 0.964371\n",
      "Batch 23200 of 50 sentences: Average Loss:   0.238643 Duration 0.675629\n",
      "Batch 23300 of 50 sentences: Average Loss:   0.238608 Duration 0.580713\n",
      "Batch 23400 of 50 sentences: Average Loss:   0.238592 Duration 1.123646\n",
      "Batch 23500 of 50 sentences: Average Loss:   0.238567 Duration 0.500753\n",
      "Batch 23600 of 50 sentences: Average Loss:   0.238522 Duration 0.547463\n",
      "epoch 27 batches 23603 done\n",
      "Average loss:        0.238521\n",
      "Batch 23700 of 50 sentences: Average Loss:   0.238502 Duration 0.676546\n",
      "Batch 23800 of 50 sentences: Average Loss:   0.238489 Duration 0.546657\n",
      "Batch 23900 of 50 sentences: Average Loss:   0.238480 Duration 0.840671\n",
      "Batch 24000 of 50 sentences: Average Loss:   0.238465 Duration 0.777265\n",
      "Batch 24100 of 50 sentences: Average Loss:   0.238428 Duration 0.596607\n",
      "Batch 24200 of 50 sentences: Average Loss:   0.238406 Duration 0.639554\n",
      "Batch 24300 of 50 sentences: Average Loss:   0.238386 Duration 0.522334\n",
      "Batch 24400 of 50 sentences: Average Loss:   0.238350 Duration 0.545582\n",
      "epoch 28 batches 24446 done\n",
      "Average loss:        0.238333\n",
      "Batch 24500 of 50 sentences: Average Loss:   0.238334 Duration 0.481168\n",
      "Batch 24600 of 50 sentences: Average Loss:   0.238303 Duration 0.426793\n",
      "Batch 24700 of 50 sentences: Average Loss:   0.238303 Duration 0.755331\n",
      "Batch 24800 of 50 sentences: Average Loss:   0.238292 Duration 1.194753\n",
      "Batch 24900 of 50 sentences: Average Loss:   0.238257 Duration 0.664526\n",
      "Batch 25000 of 50 sentences: Average Loss:   0.238234 Duration 0.683759\n",
      "Batch 25100 of 50 sentences: Average Loss:   0.238215 Duration 0.560636\n",
      "Batch 25200 of 50 sentences: Average Loss:   0.238190 Duration 0.518284\n",
      "epoch 29 batches 25289 done\n",
      "Average loss:        0.238151\n",
      "Batch 25300 of 50 sentences: Average Loss:   0.238150 Duration 0.561386\n",
      "Batch 25400 of 50 sentences: Average Loss:   0.238139 Duration 1.094551\n",
      "Batch 25500 of 50 sentences: Average Loss:   0.238122 Duration 0.571014\n",
      "Batch 25600 of 50 sentences: Average Loss:   0.238116 Duration 0.759242\n",
      "Batch 25700 of 50 sentences: Average Loss:   0.238092 Duration 0.591712\n",
      "Batch 25800 of 50 sentences: Average Loss:   0.238056 Duration 0.515318\n",
      "Batch 25900 of 50 sentences: Average Loss:   0.238041 Duration 0.671442\n",
      "Batch 26000 of 50 sentences: Average Loss:   0.238013 Duration 0.817376\n",
      "Batch 26100 of 50 sentences: Average Loss:   0.237979 Duration 0.560744\n",
      "epoch 30 batches 26132 done\n",
      "Average loss:        0.237970\n",
      "Batch 26200 of 50 sentences: Average Loss:   0.237958 Duration 0.846119\n",
      "Batch 26300 of 50 sentences: Average Loss:   0.237938 Duration 0.599287\n",
      "Batch 26400 of 50 sentences: Average Loss:   0.237934 Duration 0.551116\n",
      "Batch 26500 of 50 sentences: Average Loss:   0.237927 Duration 0.559453\n",
      "Batch 26600 of 50 sentences: Average Loss:   0.237885 Duration 0.591278\n",
      "Batch 26700 of 50 sentences: Average Loss:   0.237871 Duration 0.416441\n",
      "Batch 26800 of 50 sentences: Average Loss:   0.237853 Duration 1.026864\n",
      "Batch 26900 of 50 sentences: Average Loss:   0.237828 Duration 0.786024\n",
      "epoch 31 batches 26975 done\n",
      "Average loss:        0.237801\n",
      "Batch 27000 of 50 sentences: Average Loss:   0.237802 Duration 0.646462\n",
      "Batch 27100 of 50 sentences: Average Loss:   0.237789 Duration 0.612349\n",
      "Batch 27200 of 50 sentences: Average Loss:   0.237774 Duration 0.779139\n",
      "Batch 27300 of 50 sentences: Average Loss:   0.237759 Duration 0.539119\n",
      "Batch 27400 of 50 sentences: Average Loss:   0.237739 Duration 0.768657\n",
      "Batch 27500 of 50 sentences: Average Loss:   0.237701 Duration 0.526138\n",
      "Batch 27600 of 50 sentences: Average Loss:   0.237693 Duration 0.706088\n",
      "Batch 27700 of 50 sentences: Average Loss:   0.237664 Duration 0.617749\n",
      "Batch 27800 of 50 sentences: Average Loss:   0.237631 Duration 0.536832\n",
      "epoch 32 batches 27818 done\n",
      "Average loss:        0.237625\n",
      "Batch 27900 of 50 sentences: Average Loss:   0.237612 Duration 0.738716\n",
      "Batch 28000 of 50 sentences: Average Loss:   0.237595 Duration 0.618795\n",
      "Batch 28100 of 50 sentences: Average Loss:   0.237582 Duration 0.486652\n",
      "Batch 28200 of 50 sentences: Average Loss:   0.237571 Duration 0.726252\n",
      "Batch 28300 of 50 sentences: Average Loss:   0.237540 Duration 0.864992\n",
      "Batch 28400 of 50 sentences: Average Loss:   0.237516 Duration 0.891271\n",
      "Batch 28500 of 50 sentences: Average Loss:   0.237502 Duration 0.893200\n",
      "Batch 28600 of 50 sentences: Average Loss:   0.237475 Duration 0.509885\n",
      "epoch 33 batches 28661 done\n",
      "Average loss:        0.237451\n",
      "Batch 28700 of 50 sentences: Average Loss:   0.237450 Duration 0.572863\n",
      "Batch 28800 of 50 sentences: Average Loss:   0.237429 Duration 0.572238\n",
      "Batch 28900 of 50 sentences: Average Loss:   0.237421 Duration 0.545360\n",
      "Batch 29000 of 50 sentences: Average Loss:   0.237404 Duration 0.576516\n",
      "Batch 29100 of 50 sentences: Average Loss:   0.237383 Duration 0.514501\n",
      "Batch 29200 of 50 sentences: Average Loss:   0.237353 Duration 0.444921\n",
      "Batch 29300 of 50 sentences: Average Loss:   0.237340 Duration 0.473104\n",
      "Batch 29400 of 50 sentences: Average Loss:   0.237319 Duration 0.616944\n",
      "Batch 29500 of 50 sentences: Average Loss:   0.237285 Duration 0.619505\n",
      "epoch 34 batches 29504 done\n",
      "Average loss:        0.237283\n",
      "Batch 29600 of 50 sentences: Average Loss:   0.237264 Duration 0.615891\n",
      "Batch 29700 of 50 sentences: Average Loss:   0.237253 Duration 0.592826\n",
      "Batch 29800 of 50 sentences: Average Loss:   0.237242 Duration 0.652790\n",
      "Batch 29900 of 50 sentences: Average Loss:   0.237226 Duration 0.608440\n",
      "Batch 30000 of 50 sentences: Average Loss:   0.237195 Duration 0.657409\n",
      "Batch 30100 of 50 sentences: Average Loss:   0.237172 Duration 0.753750\n",
      "Batch 30200 of 50 sentences: Average Loss:   0.237153 Duration 0.518691\n",
      "Batch 30300 of 50 sentences: Average Loss:   0.237125 Duration 0.676664\n",
      "epoch 35 batches 30347 done\n",
      "Average loss:        0.237112\n",
      "Batch 30400 of 50 sentences: Average Loss:   0.237113 Duration 0.540582\n",
      "Batch 30500 of 50 sentences: Average Loss:   0.237087 Duration 0.521510\n",
      "Batch 30600 of 50 sentences: Average Loss:   0.237087 Duration 0.670760\n",
      "Batch 30700 of 50 sentences: Average Loss:   0.237074 Duration 0.579246\n",
      "Batch 30800 of 50 sentences: Average Loss:   0.237043 Duration 0.755187\n",
      "Batch 30900 of 50 sentences: Average Loss:   0.237018 Duration 0.542672\n",
      "Batch 31000 of 50 sentences: Average Loss:   0.237003 Duration 0.528825\n",
      "Batch 31100 of 50 sentences: Average Loss:   0.236979 Duration 0.664195\n",
      "epoch 36 batches 31190 done\n",
      "Average loss:        0.236946\n",
      "Batch 31200 of 50 sentences: Average Loss:   0.236946 Duration 0.647838\n",
      "Batch 31300 of 50 sentences: Average Loss:   0.236930 Duration 0.540041\n",
      "Batch 31400 of 50 sentences: Average Loss:   0.236916 Duration 0.571493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 31500 of 50 sentences: Average Loss:   0.236908 Duration 0.690712\n",
      "Batch 31600 of 50 sentences: Average Loss:   0.236886 Duration 0.478841\n",
      "Batch 31700 of 50 sentences: Average Loss:   0.236855 Duration 0.578388\n",
      "Batch 31800 of 50 sentences: Average Loss:   0.236840 Duration 0.593733\n",
      "Batch 31900 of 50 sentences: Average Loss:   0.236817 Duration 0.360845\n",
      "Batch 32000 of 50 sentences: Average Loss:   0.236787 Duration 0.387591\n",
      "epoch 37 batches 32033 done\n",
      "Average loss:        0.236778\n",
      "Batch 32100 of 50 sentences: Average Loss:   0.236769 Duration 0.597708\n",
      "Batch 32200 of 50 sentences: Average Loss:   0.236752 Duration 0.686003\n",
      "Batch 32300 of 50 sentences: Average Loss:   0.236745 Duration 0.589600\n",
      "Batch 32400 of 50 sentences: Average Loss:   0.236736 Duration 0.561581\n",
      "Batch 32500 of 50 sentences: Average Loss:   0.236694 Duration 0.566597\n",
      "Batch 32600 of 50 sentences: Average Loss:   0.236676 Duration 0.514654\n",
      "Batch 32700 of 50 sentences: Average Loss:   0.236660 Duration 0.609902\n",
      "Batch 32800 of 50 sentences: Average Loss:   0.236636 Duration 0.650310\n",
      "epoch 38 batches 32876 done\n",
      "Average loss:        0.236610\n",
      "Batch 32900 of 50 sentences: Average Loss:   0.236611 Duration 0.464600\n",
      "Batch 33000 of 50 sentences: Average Loss:   0.236594 Duration 0.514352\n",
      "Batch 33100 of 50 sentences: Average Loss:   0.236582 Duration 0.671721\n",
      "Batch 33200 of 50 sentences: Average Loss:   0.236567 Duration 0.782439\n",
      "Batch 33300 of 50 sentences: Average Loss:   0.236547 Duration 0.669245\n",
      "Batch 33400 of 50 sentences: Average Loss:   0.236513 Duration 0.462476\n",
      "Batch 33500 of 50 sentences: Average Loss:   0.236504 Duration 0.427759\n",
      "Batch 33600 of 50 sentences: Average Loss:   0.236479 Duration 0.442034\n",
      "Batch 33700 of 50 sentences: Average Loss:   0.236449 Duration 0.439704\n",
      "epoch 39 batches 33719 done\n",
      "Average loss:        0.236446\n",
      "Batch 33800 of 50 sentences: Average Loss:   0.236432 Duration 0.592602\n",
      "Batch 33900 of 50 sentences: Average Loss:   0.236418 Duration 0.518289\n",
      "Batch 34000 of 50 sentences: Average Loss:   0.236406 Duration 0.641717\n",
      "Batch 34100 of 50 sentences: Average Loss:   0.236397 Duration 0.724692\n",
      "Batch 34200 of 50 sentences: Average Loss:   0.236369 Duration 0.639414\n",
      "Batch 34300 of 50 sentences: Average Loss:   0.236346 Duration 0.413211\n",
      "Batch 34400 of 50 sentences: Average Loss:   0.236332 Duration 0.832673\n",
      "Batch 34500 of 50 sentences: Average Loss:   0.236309 Duration 0.654187\n",
      "epoch 40 batches 34562 done\n",
      "Average loss:        0.236288\n",
      "Batch 34600 of 50 sentences: Average Loss:   0.236287 Duration 0.712371\n",
      "Batch 34700 of 50 sentences: Average Loss:   0.236269 Duration 0.524158\n",
      "Batch 34800 of 50 sentences: Average Loss:   0.236262 Duration 0.654890\n",
      "Batch 34900 of 50 sentences: Average Loss:   0.236247 Duration 0.667069\n",
      "Batch 35000 of 50 sentences: Average Loss:   0.236225 Duration 0.632323\n",
      "Batch 35100 of 50 sentences: Average Loss:   0.236197 Duration 0.590430\n",
      "Batch 35200 of 50 sentences: Average Loss:   0.236185 Duration 0.489032\n",
      "Batch 35300 of 50 sentences: Average Loss:   0.236165 Duration 0.541438\n",
      "Batch 35400 of 50 sentences: Average Loss:   0.236132 Duration 0.654937\n",
      "epoch 41 batches 35405 done\n",
      "Average loss:        0.236129\n",
      "Batch 35500 of 50 sentences: Average Loss:   0.236113 Duration 0.637416\n",
      "Batch 35600 of 50 sentences: Average Loss:   0.236101 Duration 0.516407\n",
      "Batch 35700 of 50 sentences: Average Loss:   0.236091 Duration 0.557825\n",
      "Batch 35800 of 50 sentences: Average Loss:   0.236078 Duration 0.466907\n",
      "Batch 35900 of 50 sentences: Average Loss:   0.236049 Duration 0.502476\n",
      "Batch 36000 of 50 sentences: Average Loss:   0.236028 Duration 0.479590\n",
      "Batch 36100 of 50 sentences: Average Loss:   0.236012 Duration 0.706670\n",
      "Batch 36200 of 50 sentences: Average Loss:   0.235982 Duration 0.705387\n",
      "epoch 42 batches 36248 done\n",
      "Average loss:        0.235968\n",
      "Batch 36300 of 50 sentences: Average Loss:   0.235967 Duration 0.559368\n",
      "Batch 36400 of 50 sentences: Average Loss:   0.235943 Duration 0.601380\n",
      "Batch 36500 of 50 sentences: Average Loss:   0.235941 Duration 0.627716\n",
      "Batch 36600 of 50 sentences: Average Loss:   0.235928 Duration 0.578706\n",
      "Batch 36700 of 50 sentences: Average Loss:   0.235899 Duration 0.573338\n",
      "Batch 36800 of 50 sentences: Average Loss:   0.235879 Duration 0.691103\n",
      "Batch 36900 of 50 sentences: Average Loss:   0.235865 Duration 0.580726\n",
      "Batch 37000 of 50 sentences: Average Loss:   0.235844 Duration 0.544180\n",
      "epoch 43 batches 37091 done\n",
      "Average loss:        0.235814\n",
      "Batch 37100 of 50 sentences: Average Loss:   0.235813 Duration 0.469004\n",
      "Batch 37200 of 50 sentences: Average Loss:   0.235799 Duration 0.450766\n",
      "Batch 37300 of 50 sentences: Average Loss:   0.235782 Duration 0.644684\n",
      "Batch 37400 of 50 sentences: Average Loss:   0.235774 Duration 0.580810\n",
      "Batch 37500 of 50 sentences: Average Loss:   0.235754 Duration 0.654597\n",
      "Batch 37600 of 50 sentences: Average Loss:   0.235726 Duration 0.352091\n",
      "Batch 37700 of 50 sentences: Average Loss:   0.235713 Duration 0.549733\n",
      "Batch 37800 of 50 sentences: Average Loss:   0.235692 Duration 0.283985\n",
      "Batch 37900 of 50 sentences: Average Loss:   0.235668 Duration 0.539350\n",
      "epoch 44 batches 37934 done\n",
      "Average loss:        0.235659\n",
      "Batch 38000 of 50 sentences: Average Loss:   0.235651 Duration 0.538982\n",
      "Batch 38100 of 50 sentences: Average Loss:   0.235633 Duration 0.841619\n",
      "Batch 38200 of 50 sentences: Average Loss:   0.235624 Duration 0.827172\n",
      "Batch 38300 of 50 sentences: Average Loss:   0.235617 Duration 0.428831\n",
      "Batch 38400 of 50 sentences: Average Loss:   0.235581 Duration 0.587716\n",
      "Batch 38500 of 50 sentences: Average Loss:   0.235565 Duration 0.506042\n",
      "Batch 38600 of 50 sentences: Average Loss:   0.235550 Duration 0.630667\n",
      "Batch 38700 of 50 sentences: Average Loss:   0.235527 Duration 0.633154\n",
      "epoch 45 batches 38777 done\n",
      "Average loss:        0.235502\n",
      "Batch 38800 of 50 sentences: Average Loss:   0.235502 Duration 0.534055\n",
      "Batch 38900 of 50 sentences: Average Loss:   0.235488 Duration 0.466109\n",
      "Batch 39000 of 50 sentences: Average Loss:   0.235475 Duration 0.746782\n",
      "Batch 39100 of 50 sentences: Average Loss:   0.235460 Duration 0.673643\n",
      "Batch 39200 of 50 sentences: Average Loss:   0.235442 Duration 0.626390\n",
      "Batch 39300 of 50 sentences: Average Loss:   0.235409 Duration 0.618731\n",
      "Batch 39400 of 50 sentences: Average Loss:   0.235402 Duration 0.454505\n",
      "Batch 39500 of 50 sentences: Average Loss:   0.235380 Duration 0.570725\n",
      "Batch 39600 of 50 sentences: Average Loss:   0.235353 Duration 0.821887\n",
      "epoch 46 batches 39620 done\n",
      "Average loss:        0.235349\n",
      "Batch 39700 of 50 sentences: Average Loss:   0.235337 Duration 0.721056\n",
      "Batch 39800 of 50 sentences: Average Loss:   0.235324 Duration 1.201750\n",
      "Batch 39900 of 50 sentences: Average Loss:   0.235310 Duration 0.428427\n",
      "Batch 40000 of 50 sentences: Average Loss:   0.235301 Duration 0.613406\n",
      "Batch 40100 of 50 sentences: Average Loss:   0.235275 Duration 0.467464\n",
      "Batch 40200 of 50 sentences: Average Loss:   0.235255 Duration 0.552544\n",
      "Batch 40300 of 50 sentences: Average Loss:   0.235240 Duration 0.641230\n",
      "Batch 40400 of 50 sentences: Average Loss:   0.235219 Duration 0.550588\n",
      "epoch 47 batches 40463 done\n",
      "Average loss:        0.235199\n",
      "Batch 40500 of 50 sentences: Average Loss:   0.235196 Duration 0.510810\n",
      "Batch 40600 of 50 sentences: Average Loss:   0.235179 Duration 0.602811\n",
      "Batch 40700 of 50 sentences: Average Loss:   0.235168 Duration 0.508862\n",
      "Batch 40800 of 50 sentences: Average Loss:   0.235153 Duration 0.725921\n",
      "Batch 40900 of 50 sentences: Average Loss:   0.235132 Duration 0.591996\n",
      "Batch 41000 of 50 sentences: Average Loss:   0.235107 Duration 0.600382\n",
      "Batch 41100 of 50 sentences: Average Loss:   0.235093 Duration 0.571904\n",
      "Batch 41200 of 50 sentences: Average Loss:   0.235074 Duration 0.617548\n",
      "Batch 41300 of 50 sentences: Average Loss:   0.235047 Duration 0.540209\n",
      "epoch 48 batches 41306 done\n",
      "Average loss:        0.235045\n",
      "Batch 41400 of 50 sentences: Average Loss:   0.235030 Duration 0.490706\n",
      "Batch 41500 of 50 sentences: Average Loss:   0.235019 Duration 0.508132\n",
      "Batch 41600 of 50 sentences: Average Loss:   0.235007 Duration 0.617990\n",
      "Batch 41700 of 50 sentences: Average Loss:   0.234994 Duration 0.473056\n",
      "Batch 41800 of 50 sentences: Average Loss:   0.234969 Duration 0.384724\n",
      "Batch 41900 of 50 sentences: Average Loss:   0.234948 Duration 0.465832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 42000 of 50 sentences: Average Loss:   0.234933 Duration 0.423516\n",
      "Batch 42100 of 50 sentences: Average Loss:   0.234907 Duration 0.492759\n",
      "epoch 49 batches 42149 done\n",
      "Average loss:        0.234894\n",
      "Batch 42200 of 50 sentences: Average Loss:   0.234893 Duration 0.643545\n",
      "Batch 42300 of 50 sentences: Average Loss:   0.234869 Duration 0.643559\n",
      "Batch 42400 of 50 sentences: Average Loss:   0.234866 Duration 0.571243\n",
      "Batch 42500 of 50 sentences: Average Loss:   0.234854 Duration 0.512357\n",
      "Batch 42600 of 50 sentences: Average Loss:   0.234829 Duration 0.530386\n",
      "Batch 42700 of 50 sentences: Average Loss:   0.234809 Duration 0.671252\n",
      "Batch 42800 of 50 sentences: Average Loss:   0.234795 Duration 0.553913\n",
      "Batch 42900 of 50 sentences: Average Loss:   0.234773 Duration 0.624893\n",
      "epoch 50 batches 42992 done\n",
      "Average loss:        0.234747\n",
      "Batch 43000 of 50 sentences: Average Loss:   0.234746 Duration 0.567007\n",
      "Batch 43100 of 50 sentences: Average Loss:   0.234730 Duration 0.569697\n",
      "Batch 43200 of 50 sentences: Average Loss:   0.234716 Duration 0.520807\n",
      "Batch 43300 of 50 sentences: Average Loss:   0.234707 Duration 0.571155\n",
      "Batch 43400 of 50 sentences: Average Loss:   0.234690 Duration 0.622033\n",
      "Batch 43500 of 50 sentences: Average Loss:   0.234661 Duration 0.493387\n",
      "Batch 43600 of 50 sentences: Average Loss:   0.234646 Duration 0.665194\n",
      "Batch 43700 of 50 sentences: Average Loss:   0.234628 Duration 0.223742\n",
      "Batch 43800 of 50 sentences: Average Loss:   0.234605 Duration 0.485255\n",
      "epoch 51 batches 43835 done\n",
      "Average loss:        0.234596\n",
      "Batch 43900 of 50 sentences: Average Loss:   0.234587 Duration 0.709598\n",
      "Batch 44000 of 50 sentences: Average Loss:   0.234569 Duration 0.544290\n",
      "Batch 44100 of 50 sentences: Average Loss:   0.234561 Duration 0.519089\n",
      "Batch 44200 of 50 sentences: Average Loss:   0.234554 Duration 0.749243\n",
      "Batch 44300 of 50 sentences: Average Loss:   0.234522 Duration 0.519805\n",
      "Batch 44400 of 50 sentences: Average Loss:   0.234507 Duration 0.686905\n",
      "Batch 44500 of 50 sentences: Average Loss:   0.234493 Duration 0.669441\n",
      "Batch 44600 of 50 sentences: Average Loss:   0.234472 Duration 0.585520\n",
      "epoch 52 batches 44678 done\n",
      "Average loss:        0.234450\n",
      "Batch 44700 of 50 sentences: Average Loss:   0.234450 Duration 0.765230\n",
      "Batch 44800 of 50 sentences: Average Loss:   0.234436 Duration 0.862335\n",
      "Batch 44900 of 50 sentences: Average Loss:   0.234423 Duration 0.787067\n",
      "Batch 45000 of 50 sentences: Average Loss:   0.234410 Duration 0.562602\n",
      "Batch 45100 of 50 sentences: Average Loss:   0.234393 Duration 0.556079\n",
      "Batch 45200 of 50 sentences: Average Loss:   0.234362 Duration 0.735485\n",
      "Batch 45300 of 50 sentences: Average Loss:   0.234354 Duration 0.584536\n",
      "Batch 45400 of 50 sentences: Average Loss:   0.234333 Duration 0.574975\n",
      "Batch 45500 of 50 sentences: Average Loss:   0.234308 Duration 0.682131\n",
      "epoch 53 batches 45521 done\n",
      "Average loss:        0.234303\n",
      "Batch 45600 of 50 sentences: Average Loss:   0.234291 Duration 0.693882\n",
      "Batch 45700 of 50 sentences: Average Loss:   0.234279 Duration 0.628272\n",
      "Batch 45800 of 50 sentences: Average Loss:   0.234267 Duration 0.467722\n",
      "Batch 45900 of 50 sentences: Average Loss:   0.234258 Duration 0.967239\n",
      "Batch 46000 of 50 sentences: Average Loss:   0.234232 Duration 0.752979\n",
      "Batch 46100 of 50 sentences: Average Loss:   0.234215 Duration 1.063255\n",
      "Batch 46200 of 50 sentences: Average Loss:   0.234199 Duration 0.583420\n",
      "Batch 46300 of 50 sentences: Average Loss:   0.234179 Duration 0.425158\n",
      "epoch 54 batches 46364 done\n",
      "Average loss:        0.234160\n",
      "Batch 46400 of 50 sentences: Average Loss:   0.234158 Duration 0.499017\n",
      "Batch 46500 of 50 sentences: Average Loss:   0.234142 Duration 0.809038\n",
      "Batch 46600 of 50 sentences: Average Loss:   0.234133 Duration 0.604748\n",
      "Batch 46700 of 50 sentences: Average Loss:   0.234120 Duration 0.778975\n",
      "Batch 46800 of 50 sentences: Average Loss:   0.234103 Duration 0.602823\n",
      "Batch 46900 of 50 sentences: Average Loss:   0.234078 Duration 0.638864\n",
      "Batch 47000 of 50 sentences: Average Loss:   0.234065 Duration 0.646946\n",
      "Batch 47100 of 50 sentences: Average Loss:   0.234048 Duration 0.613149\n",
      "Batch 47200 of 50 sentences: Average Loss:   0.234023 Duration 0.680360\n",
      "epoch 55 batches 47207 done\n",
      "Average loss:        0.234021\n",
      "Batch 47300 of 50 sentences: Average Loss:   0.234005 Duration 0.714910\n",
      "Batch 47400 of 50 sentences: Average Loss:   0.233993 Duration 0.628412\n",
      "Batch 47500 of 50 sentences: Average Loss:   0.233982 Duration 0.504966\n",
      "Batch 47600 of 50 sentences: Average Loss:   0.233969 Duration 0.604121\n",
      "Batch 47700 of 50 sentences: Average Loss:   0.233945 Duration 0.584121\n",
      "Batch 47800 of 50 sentences: Average Loss:   0.233928 Duration 0.501287\n",
      "Batch 47900 of 50 sentences: Average Loss:   0.233914 Duration 0.600208\n",
      "Batch 48000 of 50 sentences: Average Loss:   0.233889 Duration 0.599161\n",
      "epoch 56 batches 48050 done\n",
      "Average loss:        0.233875\n",
      "Batch 48100 of 50 sentences: Average Loss:   0.233873 Duration 0.909967\n",
      "Batch 48200 of 50 sentences: Average Loss:   0.233850 Duration 0.631398\n",
      "Batch 48300 of 50 sentences: Average Loss:   0.233845 Duration 0.647931\n",
      "Batch 48400 of 50 sentences: Average Loss:   0.233831 Duration 0.511714\n",
      "Batch 48500 of 50 sentences: Average Loss:   0.233809 Duration 0.890751\n",
      "Batch 48600 of 50 sentences: Average Loss:   0.233789 Duration 0.868232\n",
      "Batch 48700 of 50 sentences: Average Loss:   0.233775 Duration 0.611926\n",
      "Batch 48800 of 50 sentences: Average Loss:   0.233757 Duration 0.467100\n",
      "epoch 57 batches 48893 done\n",
      "Average loss:        0.233732\n",
      "Batch 48900 of 50 sentences: Average Loss:   0.233731 Duration 0.638124\n",
      "Batch 49000 of 50 sentences: Average Loss:   0.233715 Duration 0.734133\n",
      "Batch 49100 of 50 sentences: Average Loss:   0.233702 Duration 0.528548\n",
      "Batch 49200 of 50 sentences: Average Loss:   0.233695 Duration 0.656064\n",
      "Batch 49300 of 50 sentences: Average Loss:   0.233678 Duration 0.687305\n",
      "Batch 49400 of 50 sentences: Average Loss:   0.233653 Duration 0.532470\n",
      "Batch 49500 of 50 sentences: Average Loss:   0.233639 Duration 0.632737\n",
      "Batch 49600 of 50 sentences: Average Loss:   0.233622 Duration 0.477774\n",
      "Batch 49700 of 50 sentences: Average Loss:   0.233600 Duration 0.701417\n",
      "epoch 58 batches 49736 done\n",
      "Average loss:        0.233592\n",
      "Batch 49800 of 50 sentences: Average Loss:   0.233585 Duration 1.028687\n",
      "Batch 49900 of 50 sentences: Average Loss:   0.233569 Duration 0.569185\n",
      "Batch 50000 of 50 sentences: Average Loss:   0.233563 Duration 0.598519\n",
      "Batch 50100 of 50 sentences: Average Loss:   0.233554 Duration 0.650609\n",
      "Batch 50200 of 50 sentences: Average Loss:   0.233525 Duration 1.212537\n",
      "Batch 50300 of 50 sentences: Average Loss:   0.233507 Duration 0.614459\n",
      "Batch 50400 of 50 sentences: Average Loss:   0.233493 Duration 0.568688\n",
      "Batch 50500 of 50 sentences: Average Loss:   0.233473 Duration 0.593923\n",
      "epoch 59 batches 50579 done\n",
      "Average loss:        0.233453\n",
      "Batch 50600 of 50 sentences: Average Loss:   0.233453 Duration 0.598406\n",
      "Batch 50700 of 50 sentences: Average Loss:   0.233438 Duration 0.739985\n",
      "Batch 50800 of 50 sentences: Average Loss:   0.233426 Duration 0.497360\n",
      "Batch 50900 of 50 sentences: Average Loss:   0.233414 Duration 0.645188\n",
      "Batch 51000 of 50 sentences: Average Loss:   0.233400 Duration 0.706614\n",
      "Batch 51100 of 50 sentences: Average Loss:   0.233370 Duration 0.646733\n",
      "Batch 51200 of 50 sentences: Average Loss:   0.233362 Duration 0.708399\n",
      "Batch 51300 of 50 sentences: Average Loss:   0.233343 Duration 0.628539\n",
      "Batch 51400 of 50 sentences: Average Loss:   0.233319 Duration 0.552153\n",
      "epoch 60 batches 51422 done\n",
      "Average loss:        0.233314\n",
      "Batch 51500 of 50 sentences: Average Loss:   0.233302 Duration 0.628649\n",
      "Batch 51600 of 50 sentences: Average Loss:   0.233291 Duration 0.705334\n",
      "Batch 51700 of 50 sentences: Average Loss:   0.233279 Duration 0.577025\n",
      "Batch 51800 of 50 sentences: Average Loss:   0.233269 Duration 0.669131\n",
      "Batch 51900 of 50 sentences: Average Loss:   0.233246 Duration 0.627156\n",
      "Batch 52000 of 50 sentences: Average Loss:   0.233230 Duration 0.615952\n",
      "Batch 52100 of 50 sentences: Average Loss:   0.233214 Duration 0.627956\n",
      "Batch 52200 of 50 sentences: Average Loss:   0.233195 Duration 0.631112\n",
      "epoch 61 batches 52265 done\n",
      "Average loss:        0.233178\n",
      "Batch 52300 of 50 sentences: Average Loss:   0.233175 Duration 0.713147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 52400 of 50 sentences: Average Loss:   0.233160 Duration 0.817258\n",
      "Batch 52500 of 50 sentences: Average Loss:   0.233149 Duration 0.747409\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1cdc9925f8fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/home/repository/git/oonisim/python_programs/nlp/src/network/base.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, T, run_validations)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;31m# --------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTYPE_FLOAT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_L\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTYPE_FLOAT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/home/repository/git/oonisim/python_programs/nlp/src/network/base.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/home/repository/git/oonisim/python_programs/nlp/src/common/function.py\u001b[0m in \u001b[0;36m_\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/home/repository/git/oonisim/python_programs/nlp/src/layer/embedding.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;31m# Xj may include v. Then rewarding and penalizing on v happen at the same time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m         \u001b[0;31m# set() cannot take a list as it is mutable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m         \u001b[0;31m# set([1,2,3]) is a Python runtime hack. It should be set((1,2,3))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;31m# --------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/home/repository/git/oonisim/python_programs/nlp/src/layer/embedding.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;31m# set() cannot take a list as it is mutable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m         \u001b[0;31m# set([1,2,3]) is a Python runtime hack. It should be set((1,2,3))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m         \u001b[0;31m# --------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         self._negative_sample_indices = self.reshape(X=self.to_tensor([\n",
      "\u001b[0;32m~/home/repository/git/oonisim/python_programs/nlp/src/layer/preprocessing/preprocess.py\u001b[0m in \u001b[0;36mnegative_sample_indices\u001b[0;34m(self, size, excludes)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;34m\"availability %s > sample size %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mavailability\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mexcludes_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexcludes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/home/repository/git/oonisim/python_programs/nlp/src/layer/preprocessing/preprocess.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mUSE_PROBABILITY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mUSE_PROBABILITY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             sampled: List[str] = list(np.random.choice(\n\u001b[0m\u001b[1;32m    266\u001b[0m                 \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mEVENT_META_ENTITIES_COUNT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python_programs/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unique_dispatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m def unique(ar, return_index=False, return_inverse=False,\n\u001b[1;32m    140\u001b[0m            return_counts=False, axis=None):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_SENTENCES = 50\n",
    "MAX_ITERATIONS = 100000\n",
    "\n",
    "def sentences_generator(path_to_file, num_sentences):\n",
    "    stream = fileio.Function.file_line_stream(path_to_file)\n",
    "    try:\n",
    "        while True:\n",
    "            _lines = fileio.Function.take(num_sentences, stream)\n",
    "            yield np.array(_lines)\n",
    "    finally:\n",
    "        stream.close()\n",
    "\n",
    "# Sentences for the trainig\n",
    "path_to_input = path_to_ptb\n",
    "source = sentences_generator(\n",
    "    path_to_file=path_to_input, num_sentences=NUM_SENTENCES\n",
    ")\n",
    "\n",
    "# Restore the state if exists.\n",
    "state = embedding.load(STATE_FILE)\n",
    "\n",
    "# Continue training\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "\n",
    "total_sentences = 0\n",
    "epochs = 0\n",
    "\n",
    "for i in range(MAX_ITERATIONS):\n",
    "    try:\n",
    "        sentences = next(source)\n",
    "        total_sentences += len(sentences)\n",
    "\n",
    "        start = time.time()\n",
    "        network.train(X=sentences, T=np.array([0]))\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                f\"Batch {i:05d} of {NUM_SENTENCES} sentences: \"\n",
    "                f\"Average Loss: {np.mean(network.history):10f} \"\n",
    "                f\"Duration {time.time() - start:3f}\"\n",
    "            )\n",
    "        if i % 10 == 0:\n",
    "            embedding.save(STATE_FILE)\n",
    "        \n",
    "    except fileio.Function.GenearatorHasNoMore as e:\n",
    "        # Next epoch\n",
    "        print(f\"epoch {epochs} batches {i:05d} done\")\n",
    "        print(f\"Average loss: {np.mean(network.history):15f}\")\n",
    "        epochs += 1\n",
    "        source.close()\n",
    "        source = sentences_generator(\n",
    "            path_to_file=path_to_input, num_sentences=NUM_SENTENCES\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        source.close()\n",
    "        raise e\n",
    "\n",
    "embedding.save(STATE_FILE)\n",
    "\n",
    "profiler.disable()\n",
    "profiler.print_stats(sort=\"cumtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluate the vector space\n",
    "\n",
    "Verify if the trained model, or the vector space W, has encoded the words in a way that **similar** words are close in the vector space.\n",
    "\n",
    "* [How to measure the similarity among vectors](https://math.stackexchange.com/questions/4132458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "context = \"cash\".split()\n",
    "word_indices = np.array(word_indexing.list_indices(context), dtype=TYPE_INT)\n",
    "\n",
    "print(f\"Words {context}\")\n",
    "print(f\"Word indices {word_indices}\")\n",
    "print(f\"prediction for {context}:\\n{word_indexing.list_events([embedding.predict(word_indices, n)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(shape=(141, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "## Compare with [gensim word2vec](https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import (\n",
    "    Word2Vec\n",
    ")\n",
    "from gensim.models.word2vec import (\n",
    "    LineSentence    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = LineSentence(source=path_to_ptb)\n",
    "w2v = Word2Vec(\n",
    "    sentences=sentences, \n",
    "    sg=0,\n",
    "    window=5, \n",
    "    negative=5,\n",
    "    vector_size=100, \n",
    "    min_count=1, \n",
    "    workers=4\n",
    ")\n",
    "del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(context, topn=n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
