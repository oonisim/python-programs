{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from itertools import islice\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=400) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook setups\n",
    "\n",
    "Auto reolaod causes an error in Jupyter notebooks. Restart the Jupyter kernel for the error:\n",
    "```TypeError: super(type, obj): obj must be an instance or subtype of type```\n",
    "See\n",
    "- https://stackoverflow.com/a/52927102/4281353\n",
    "- http://thomas-cokelaer.info/blog/2011/09/382/\n",
    "\n",
    "> The problem resides in the mechanism of reloading modules.\n",
    "> Reloading a module often changes the internal object in memory which\n",
    "> makes the isinstance test of super return False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "import function.fileio as fileio\n",
    "import function.text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.constant import (\n",
    "    TYPE_INT,\n",
    "    TYPE_FLOAT,\n",
    "    TYPE_LABEL,\n",
    "    TYPE_TENSOR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PTB = True\n",
    "DEBUG = False\n",
    "VALIDATION = True\n",
    "\n",
    "TARGET_SIZE = 1   # Size of the target event (word)\n",
    "CONTEXT_SIZE = 10  # Size of the context.\n",
    "WINDOW_SIZE = TARGET_SIZE + CONTEXT_SIZE\n",
    "SAMPLE_SIZE = 5   # Size of the negative samples\n",
    "VECTOR_SIZE = 100  # Number of features in the event vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"To be, or not to be, that is the question that matters\"\n",
    "_file = \"ptb.train.txt\"\n",
    "if USE_PTB:\n",
    "    if not fileio.Function.is_file(f\"~/.keras/datasets/{_file}\"):\n",
    "        path_to_ptb = tf.keras.utils.get_file(\n",
    "            _file, \n",
    "            f'https://raw.githubusercontent.com/tomsercu/lstm/master/data/{_file}'\n",
    "        )\n",
    "    corpus = fileio.Function.read_file(path_to_ptb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n",
      " pierre <unk> N years old will join the board as a nonexecutive director nov. N \n",
      " mr. <unk> is chairman of <unk> n.v. the dutch publishing group \n",
      " rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate \n",
      " a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported \n"
     ]
    }
   ],
   "source": [
    "examples = corpus.split('\\n')[:5]\n",
    "for line in examples:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Event (word) indexing\n",
    "Index the events that have occurred in the event sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer.preprocessing import (\n",
    "    EventIndexing, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexing = EventIndexing(\n",
    "    name=\"word_indexing_on_ptb\",\n",
    "    corpus=corpus\n",
    ")\n",
    "del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Word Embedding\n",
    "\n",
    "Embedding is to train the model to group similar events in a close proximity in the event vector space. If two events e.g. 'pencil' and 'pen' are similar concepts, then their event vectors resides in a close distance in the event space. \n",
    "\n",
    "* [Thought Vectors](https://wiki.pathmind.com/thought-vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer import (\n",
    "    Embedding\n",
    ")\n",
    "from optimizer import (\n",
    "    SGD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding: Embedding = Embedding(\n",
    "    name=\"embedding\",\n",
    "    num_nodes=WINDOW_SIZE,\n",
    "    target_size=TARGET_SIZE,\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    negative_sample_size=SAMPLE_SIZE,\n",
    "    event_vector_size=VECTOR_SIZE,\n",
    "    optimizer=SGD(lr=TYPE_FLOAT(0.3)),\n",
    "    dictionary=word_indexing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STATE_FILE_20 = \"../models/word2vec_vecsize_20.pkl\"\n",
    "STATE_FILE_50 = \"../models/word2vec_vecsize_50.pkl\"\n",
    "STATE_FILE_100 = \"../models/word2vec_vecsize_100.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluate the vector space\n",
    "\n",
    "Verify if the trained model, or the vector space W, has encoded the words in a way that **similar** words are close in the vector space.\n",
    "\n",
    "* [How to measure the similarity among vectors](https://math.stackexchange.com/questions/4132458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words ['laundering']\n",
      "Word indices [8239]\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "context = \"laundering\".split()\n",
    "word_indices = np.array(word_indexing.list_indices(context), dtype=TYPE_INT)\n",
    "\n",
    "print(f\"Words {context}\")\n",
    "print(f\"Word indices {word_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector size 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "event_size 1\n",
      "context_size: 10\n",
      "event_vector_size: 50\n",
      "\n",
      "[['opinions' 'drama' 'knowledge' 'gates' 'tenants' 'excess' 'shipments' 'irresponsible' 'minivans' 'details']\n",
      " ['opinions' 'railway' 'indexation' 'tips' 'teagan' 'jayark' 'supermarkets' 'wilfred' 'contrasts' 'blank']]\n"
     ]
    }
   ],
   "source": [
    "state = embedding.load(STATE_FILE_50)\n",
    "\n",
    "fmt=\"\"\"Model loaded.\n",
    "event_size %s\n",
    "context_size: %s\n",
    "event_vector_size: %s\n",
    "\"\"\"\n",
    "print(fmt % (\n",
    "    state[\"target_size\"], \n",
    "    state[\"context_size\"], \n",
    "    state[\"event_vector_size\"]\n",
    "))\n",
    "\n",
    "print(word_indexing.list_events([embedding.predict(word_indices, n)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector size 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "event_size 1\n",
      "context_size: 10\n",
      "event_vector_size: 100\n",
      "\n",
      "[['biscuits' 'nomura' 'glazer' 'faded' 'should' 'tumbling' 'apart' 'credited' 'unique' 'guterman']\n",
      " ['unique' 'investigations' 'nomura' 'church' 'epo' 'apart' 'necessary' 'biscuits' 'incorrect' 'bleak']]\n"
     ]
    }
   ],
   "source": [
    "state = embedding.load(STATE_FILE_100)\n",
    "\n",
    "fmt=\"\"\"Model loaded.\n",
    "event_size %s\n",
    "context_size: %s\n",
    "event_vector_size: %s\n",
    "\"\"\"\n",
    "print(fmt % (\n",
    "    state[\"target_size\"], \n",
    "    state[\"context_size\"], \n",
    "    state[\"event_vector_size\"]\n",
    "))\n",
    "\n",
    "print(word_indexing.list_events([embedding.predict(word_indices, n)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Compare with [gensim word2vec](https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import (\n",
    "    Word2Vec\n",
    ")\n",
    "from gensim.models.word2vec import (\n",
    "    LineSentence    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = LineSentence(source=\"~/.keras/datasets/ptb.train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(\n",
    "    sentences=sentences, \n",
    "    sg=0,\n",
    "    window=5, \n",
    "    negative=5,\n",
    "    vector_size=100, \n",
    "    min_count=1, \n",
    "    workers=4\n",
    ")\n",
    "del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('procedures', 0.9436922073364258),\n",
       " ('execute', 0.941146969795227),\n",
       " ('tasks', 0.9382684826850891),\n",
       " ('opportunities', 0.93809974193573),\n",
       " ('tougher', 0.9370934367179871),\n",
       " ('jurors', 0.9356532692909241),\n",
       " ('harm', 0.9328603148460388),\n",
       " ('overly', 0.9327043890953064),\n",
       " ('ideas', 0.9326834082603455),\n",
       " ('safer', 0.9326456785202026)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(context, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
