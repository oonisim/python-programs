{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec implementation\n",
    "\n",
    "* [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "* [word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method](https://arxiv.org/abs/1402.3722)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from itertools import islice\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=400) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook setups\n",
    "\n",
    "Auto reolaod causes an error in Jupyter notebooks. Restart the Jupyter kernel for the error:\n",
    "```TypeError: super(type, obj): obj must be an instance or subtype of type```\n",
    "See\n",
    "- https://stackoverflow.com/a/52927102/4281353\n",
    "- http://thomas-cokelaer.info/blog/2011/09/382/\n",
    "\n",
    "> The problem resides in the mechanism of reloading modules.\n",
    "> Reloading a module often changes the internal object in memory which\n",
    "> makes the isinstance test of super return False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "import function.fileio as fileio\n",
    "import function.text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.constant import (\n",
    "    TYPE_INT,\n",
    "    TYPE_FLOAT,\n",
    "    TYPE_LABEL,\n",
    "    TYPE_TENSOR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PTB = True\n",
    "DEBUG = False\n",
    "VALIDATION = True\n",
    "\n",
    "TARGET_SIZE = 1   # Size of the target event (word)\n",
    "CONTEXT_SIZE = 10  # Size of the context.\n",
    "WINDOW_SIZE = TARGET_SIZE + CONTEXT_SIZE\n",
    "SAMPLE_SIZE = 5   # Size of the negative samples\n",
    "VECTOR_SIZE = 100  # Number of features in the event vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"To be, or not to be, that is the question that matters\"\n",
    "_file = \"ptb.train.txt\"\n",
    "if USE_PTB:\n",
    "    if not fileio.Function.is_file(f\"~/.keras/datasets/{_file}\"):\n",
    "        path_to_ptb = tf.keras.utils.get_file(\n",
    "            _file, \n",
    "            f'https://raw.githubusercontent.com/tomsercu/lstm/master/data/{_file}'\n",
    "        )\n",
    "    corpus = fileio.Function.read_file(path_to_ptb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n",
      " pierre <unk> N years old will join the board as a nonexecutive director nov. N \n",
      " mr. <unk> is chairman of <unk> n.v. the dutch publishing group \n",
      " rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate \n",
      " a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported \n"
     ]
    }
   ],
   "source": [
    "examples = corpus.split('\\n')[:5]\n",
    "for line in examples:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Event (word) indexing\n",
    "Index the events that have occurred in the event sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer.preprocessing import (\n",
    "    EventIndexing, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexing = EventIndexing(\n",
    "    name=\"word_indexing_on_ptb\",\n",
    "    corpus=corpus\n",
    ")\n",
    "del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EventIndexing  for the corpus\n",
    "\n",
    "Adapt to the ```corpus``` and provides:\n",
    "* event_to_index dictionary\n",
    "* vocaburary of the corpus\n",
    "* word occurrence probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EventIndexing.vocabulary[10]:\n",
      "['<nil>' '<unk>' 'aer' 'banknote' 'berlitz' 'calloway' 'centrust' 'cluett' 'fromstein' 'gitano']\n",
      "\n",
      "EventIndexing.event_to_index[10]:\n",
      "('<nil>', 0)\n",
      "('<unk>', 1)\n",
      "('aer', 2)\n",
      "('banknote', 3)\n",
      "('berlitz', 4)\n",
      "('calloway', 5)\n",
      "('centrust', 6)\n",
      "('cluett', 7)\n",
      "('fromstein', 8)\n",
      "('gitano', 9)\n",
      "\n",
      "EventIndexing.probabilities[10]:\n",
      "<nil>                : 0.00000e+00\n",
      "<unk>                : 1.65308e-02\n",
      "aer                  : 5.34860e-06\n",
      "banknote             : 5.34860e-06\n",
      "berlitz              : 5.34860e-06\n",
      "calloway             : 5.34860e-06\n",
      "centrust             : 5.34860e-06\n",
      "cluett               : 5.34860e-06\n",
      "fromstein            : 5.34860e-06\n",
      "gitano               : 5.34860e-06\n"
     ]
    }
   ],
   "source": [
    "words = word_indexing.list_events(range(10))\n",
    "print(f\"EventIndexing.vocabulary[10]:\\n{words}\\n\")\n",
    "\n",
    "indices = word_indexing.list_indices(words)\n",
    "print(f\"EventIndexing.event_to_index[10]:\")\n",
    "for item in zip(words, indices):\n",
    "    print(item)\n",
    "\n",
    "probabilities = word_indexing.list_probabilities(words)\n",
    "print(f\"\\nEventIndexing.probabilities[10]:\")\n",
    "for word, p in zip(words, probabilities):\n",
    "    print(f\"{word:20s} : {p:.5e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling using the probability\n",
    "\n",
    "Sample events according to their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['permanent' 'audience' 'predict' 'f' 'productions']\n"
     ]
    }
   ],
   "source": [
    "sample = word_indexing.sample(size=5)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "Sample events not including those events already sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_indices=[8642, 8479, 6278, 2343, 5385] \n",
      "events=['doubling' 'gang' 'puerto' 'calls' 'guard']\n"
     ]
    }
   ],
   "source": [
    "negative_indices = word_indexing.negative_sample_indices(\n",
    "    size=5, excludes=word_indexing.list_indices(sample)\n",
    ")\n",
    "print(f\"negative_indices={negative_indices} \\nevents={word_indexing.list_events(negative_indices)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the             :    34\n",
      "asbestos        :    63\n",
      "fiber           :    86\n",
      "<unk>           :     1\n",
      "is              :    42\n",
      "unusually       :    87\n",
      "<unk>           :     1\n",
      "once            :    64\n",
      "it              :    80\n",
      "enters          :    88\n",
      "the             :    34\n",
      "<unk>           :     1\n",
      "\n",
      "with           :     0\n",
      "even            :     0\n",
      "brief           :     0\n"
     ]
    }
   ],
   "source": [
    "# sentences = \"\\n\".join(corpus.split('\\n')[5:6])\n",
    "sentences = \"\"\"\n",
    "the asbestos fiber <unk> is unusually <unk> once it enters the <unk> \n",
    "with even brief exposures to it causing symptoms that show up decades later researchers said\n",
    "\"\"\"\n",
    "sequences = word_indexing.function(sentences)\n",
    "for pair in zip(sentences.strip().split(\" \"), sequences[0]):\n",
    "    print(f\"{pair[0]:15} : {pair[1]:5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EventContext\n",
    "\n",
    "EventContext layer generates ```(event, context)``` pairs from sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer.preprocessing import (\n",
    "    EventContext\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_context = EventContext(\n",
    "    name=\"ev\",\n",
    "    window_size=WINDOW_SIZE,\n",
    "    event_size=TARGET_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context of an event (word) in a sentence\n",
    "\n",
    "In the sentence ```\"a form of asbestos once used to make kent cigarette filters\"```, one of the context windows ```a form of asbestos once``` of size 5 and event size 1 has.\n",
    "* ```of``` as a target word.\n",
    "* ```(a, form) and (asbestos, once)``` as its context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence of the word indices for the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n",
      "Sentence is empty. Skipping...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[37, 62, 44, 63, 64, 65, 66, 67, 68, 69, 70,  0,  0,  0,  0,  0],\n",
       "       [29, 30, 31, 50, 51, 43, 44, 52, 53, 54, 55, 56, 57, 37, 38, 39]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \"\"\"\n",
    "a form of asbestos once used to make kent cigarette filters\n",
    "\n",
    "N years old and former chairman of consolidated gold fields plc was named a nonexecutive director\n",
    "\"\"\"\n",
    "\n",
    "sequence = word_indexing.function(sentences)\n",
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (event, context) pairs\n",
    "\n",
    "For each word (event) in the setence ```(of, asbestos, ... , kent)``` excludnig the ends of the sentence, create ```(target, context)``` as:\n",
    "\n",
    "```\n",
    "[\n",
    "  [of, a, form, asbestos, once],              # target is 'of', context is (a, form, asbestos, once)\n",
    "  ['asbestos', 'form', 'of', 'once', 'used'],\n",
    "  ['once', 'of', 'asbestos', 'used', 'to'],\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "### Format of the (event, context) pairs\n",
    "\n",
    "* **E** is the target event indices\n",
    "* **C** is the context indices\n",
    "\n",
    "<img src=\"image/event_context_format.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event context pairs. Shape (21, 5), Target event size 1, Window size 5.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[44, 37, 62, 63, 64],\n",
       "       [63, 62, 44, 64, 65],\n",
       "       [64, 44, 63, 65, 66],\n",
       "       [65, 63, 64, 66, 67],\n",
       "       [66, 64, 65, 67, 68],\n",
       "       [67, 65, 66, 68, 69],\n",
       "       [68, 66, 67, 69, 70],\n",
       "       [69, 67, 68, 70,  0],\n",
       "       [70, 68, 69,  0,  0],\n",
       "       [31, 29, 30, 50, 51]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_context_pairs = event_context.function(sequence)\n",
    "print(\n",
    "    f\"Event context pairs. Shape %s, Target event size %s, Window size %s.\" % \n",
    "    (event_context_pairs.shape, event_context.event_size, event_context.window_size)\n",
    ")\n",
    "event_context_pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (event, context) pairs in textual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['of', 'a', 'form', 'asbestos', 'once'],\n",
       " ['asbestos', 'form', 'of', 'once', 'used'],\n",
       " ['once', 'of', 'asbestos', 'used', 'to'],\n",
       " ['used', 'asbestos', 'once', 'to', 'make'],\n",
       " ['to', 'once', 'used', 'make', 'kent'],\n",
       " ['make', 'used', 'to', 'kent', 'cigarette'],\n",
       " ['kent', 'to', 'make', 'cigarette', 'filters'],\n",
       " ['cigarette', 'make', 'kent', 'filters', '<nil>'],\n",
       " ['filters', 'kent', 'cigarette', '<nil>', '<nil>'],\n",
       " ['old', 'n', 'years', 'and', 'former']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indexing.sequence_to_sentence(event_context_pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Word Embedding\n",
    "\n",
    "Embedding is to train the model to group similar events in a close proximity in the event vector space. If two events e.g. 'pencil' and 'pen' are similar concepts, then their event vectors resides in a close distance in the event space. \n",
    "\n",
    "* [Thought Vectors](https://wiki.pathmind.com/thought-vectors)\n",
    "\n",
    "## Training process\n",
    "\n",
    "1. Calculate ```Bc```, the BoW (Bag of Words) from context event vectors.\n",
    "2. Calculate ```Be```,  the BoW (Bag of Words) from target event vectors.\n",
    "3. The dot product ```Ye = dot(Bc, Be)``` is given the label 1 to get them closer.\n",
    "4. For each negative sample ```Ws(s)```, the dot product with ```Ys = dot(Be, Ws(s)``` is given the label 0 to get them apart. \n",
    "5. ```np.c_[Ye, Ys]``` is fowarded to the logistic log loss layer.\n",
    "\n",
    "<img src=\"image/word2vec_backprop_Be.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer import (\n",
    "    Embedding\n",
    ")\n",
    "from optimizer import (\n",
    "    SGD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding: Embedding = Embedding(\n",
    "    name=\"embedding\",\n",
    "    num_nodes=WINDOW_SIZE,\n",
    "    target_size=TARGET_SIZE,\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    negative_sample_size=SAMPLE_SIZE,\n",
    "    event_vector_size=VECTOR_SIZE,\n",
    "    optimizer=SGD(lr=TYPE_FLOAT(0.3)),\n",
    "    dictionary=word_indexing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Word2vec Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a sequential network\n",
    "\n",
    "$ \\text {Sentences} \\rightarrow EventIndexing \\rightarrow EventContext \\rightarrow  Embedding \\rightarrow Adapter \\rightarrow LogisticLogLoss$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STATE_FILE_20 = \"../models/word2vec_vecsize_20.pkl\"\n",
    "STATE_FILE_50 = \"../models/word2vec_vecsize_50.pkl\"\n",
    "STATE_FILE_100 = \"../models/word2vec_vecsize_100.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluate the vector space\n",
    "\n",
    "Verify if the trained model, or the vector space W, has encoded the words in a way that **similar** words are close in the vector space.\n",
    "\n",
    "* [How to measure the similarity among vectors](https://math.stackexchange.com/questions/4132458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words ['terrorism']\n",
      "Word indices [7228]\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "context = \"terrorism\".split()\n",
    "word_indices = np.array(word_indexing.list_indices(context), dtype=TYPE_INT)\n",
    "\n",
    "print(f\"Words {context}\")\n",
    "print(f\"Word indices {word_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector size 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "event_size 1\n",
      "context_size: 4\n",
      "event_vector_size: 50\n",
      "\n",
      "[['colorado' 'observers' 'warrants' 'intended' 'chain' 'hostile' 'drawn' 'newspapers' 'insurers' 'gop']\n",
      " ['colorado' 'ivy' 'observers' 'viability' 'diaper' 'drawn' 'rubbermaid' 'warrants' 'priorities' 'raiders']]\n"
     ]
    }
   ],
   "source": [
    "state = embedding.load(STATE_FILE_50)\n",
    "\n",
    "fmt=\"\"\"Model loaded.\n",
    "event_size %s\n",
    "context_size: %s\n",
    "event_vector_size: %s\n",
    "\"\"\"\n",
    "print(fmt % (\n",
    "    state[\"target_size\"], \n",
    "    state[\"context_size\"], \n",
    "    state[\"event_vector_size\"]\n",
    "))\n",
    "\n",
    "print(word_indexing.list_events([embedding.predict(word_indices, n)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector size 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "event_size 1\n",
      "context_size: 10\n",
      "event_vector_size: 100\n",
      "\n",
      "[['crash' 'children' 'morning' 'following' 'tax' 'cold' 'lead' 'share' 'home' 'p']\n",
      " ['grumman' 'watching' 'challenges' 'container' 'fired' 'slashing' 'ward' 'decades' 'bates' 'lawrence']]\n"
     ]
    }
   ],
   "source": [
    "state = embedding.load(STATE_FILE_100)\n",
    "\n",
    "fmt=\"\"\"Model loaded.\n",
    "event_size %s\n",
    "context_size: %s\n",
    "event_vector_size: %s\n",
    "\"\"\"\n",
    "print(fmt % (\n",
    "    state[\"target_size\"], \n",
    "    state[\"context_size\"], \n",
    "    state[\"event_vector_size\"]\n",
    "))\n",
    "\n",
    "print(word_indexing.list_events([embedding.predict(word_indices, n)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
