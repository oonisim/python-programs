{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from itertools import islice\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=400) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook setups\n",
    "\n",
    "Auto reolaod causes an error in Jupyter notebooks. Restart the Jupyter kernel for the error:\n",
    "```TypeError: super(type, obj): obj must be an instance or subtype of type```\n",
    "See\n",
    "- https://stackoverflow.com/a/52927102/4281353\n",
    "- http://thomas-cokelaer.info/blog/2011/09/382/\n",
    "\n",
    "> The problem resides in the mechanism of reloading modules.\n",
    "> Reloading a module often changes the internal object in memory which\n",
    "> makes the isinstance test of super return False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "import function.fileio as fileio\n",
    "import function.text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.constant import (\n",
    "    TYPE_INT,\n",
    "    TYPE_FLOAT,\n",
    "    TYPE_LABEL,\n",
    "    TYPE_TENSOR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TEXT8 = True\n",
    "USE_PTB = not USE_TEXT8\n",
    "USE_CBOW = False\n",
    "USE_SGRAM = not USE_CBOW\n",
    "\n",
    "CORPUS_FILE = \"text8_512\" if USE_TEXT8 else \"ptb_train\"\n",
    "CORPUS_URL = \"https://data.deepai.org/text8.zip\" \\\n",
    "    if USE_TEXT8 else f'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt'\n",
    "\n",
    "TARGET_SIZE = 1   # Size of the target event (word)\n",
    "CONTEXT_SIZE = 10  # Size of the context.\n",
    "WINDOW_SIZE = TARGET_SIZE + CONTEXT_SIZE\n",
    "SAMPLE_SIZE = 5   # Size of the negative samples\n",
    "VECTOR_SIZE = 100  # Number of features in the event vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_FILE_PTB_C10_S5_WND_V100_LR5 = \"../models/word2vec_sgram_ptb_train_E1_C10_S5_Wnormal_std_0.01_V100_LR5.0_N10.pkl\"\n",
    "STATE_FILE_PTB_C10_S5_WND_V100_LR20 = \"../models/word2vec_sgram_ptb_train_E1_C10_S5_Wnormal_std_0.01_V100_LR20.0_N10.pkl\"\n",
    "STATE_FILE_TEXT8_C10_S5_WND_v100_LR20=\"../models/word2vec_sgram_text8_512_E1_C10_S5_Wnormal_std_0.01_V100_LR20.0_N1.pkl\"\n",
    "STATE_FILE = STATE_FILE_TEXT8_C10_S5_WND_v100_LR20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_corpus = f\"~/.keras/datasets/{CORPUS_FILE}\"\n",
    "if fileio.Function.is_file(path_to_corpus):\n",
    "    pass\n",
    "else:\n",
    "    # text8, run \"cat text8 | xargs -n 512 > text8_512\" after download\n",
    "    path_to_corpus = tf.keras.utils.get_file(\n",
    "        fname=CORPUS_FILE,\n",
    "        origin=CORPUS_URL,\n",
    "        extract=True\n",
    "    )\n",
    "corpus = fileio.Function.read_file(path_to_corpus)\n",
    "print(path_to_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = corpus.split('\\n')[:1]\n",
    "for line in examples:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Event (word) indexing\n",
    "Index the events that have occurred in the event sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from layer.preprocessing import (\n",
    "    EventIndexing, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexing = EventIndexing(\n",
    "    name=\"word_indexing_on_ptb\",\n",
    "    corpus=corpus\n",
    ")\n",
    "del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Word Embedding\n",
    "\n",
    "Embedding is to train the model to group similar events in a close proximity in the event vector space. If two events e.g. 'pencil' and 'pen' are similar concepts, then their event vectors resides in a close distance in the event space. \n",
    "\n",
    "* [Thought Vectors](https://wiki.pathmind.com/thought-vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "if USE_CBOW:\n",
    "    from layer.embedding_cbow_dual_vector_spaces.py import (\n",
    "        Embedding\n",
    "    )\n",
    "else:\n",
    "    from layer.embedding_sgram import (\n",
    "        Embedding\n",
    "    )\n",
    "\n",
    "from optimizer import (\n",
    "    SGD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding: Embedding = Embedding(\n",
    "    name=\"embedding\",\n",
    "    num_nodes=WINDOW_SIZE,\n",
    "    target_size=TARGET_SIZE,\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    negative_sample_size=SAMPLE_SIZE,\n",
    "    event_vector_size=VECTOR_SIZE,\n",
    "    dictionary=word_indexing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluate the vector space\n",
    "\n",
    "Verify if the trained model, or the vector space W, has encoded the words in a way that **similar** words are close in the vector space.\n",
    "\n",
    "* [How to measure the similarity among vectors](https://math.stackexchange.com/questions/4132458)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark \n",
    "\n",
    "Use [gensim word2vec](https://radimrehurek.com/gensim/models/word2vec.html) as the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import (\n",
    "    Word2Vec\n",
    ")\n",
    "from gensim.models.word2vec import (\n",
    "    LineSentence    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = LineSentence(source=path_to_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(\n",
    "    sentences=sentences, \n",
    "    sg=0,\n",
    "    window=5, \n",
    "    negative=5,\n",
    "    vector_size=100, \n",
    "    min_count=1, \n",
    "    workers=4\n",
    ")\n",
    "del sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "context = \"king\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text8 based trained model\n",
    "\n",
    "Split text8 into lines where each line has N words (e.g. 512)\n",
    "```\n",
    "N=512\n",
    "cat text8 | xargs -n $N > text8_$N\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT8_C10_S5_WND_v100_LR20\n",
    "\n",
    "```\n",
    "USE_TEXT8 = True\n",
    "\n",
    "CORPUS_FILE = \"text8_512\" if USE_TEXT8 else \"ptb_train\"\n",
    "CORPUS_URL = \"https://data.deepai.org/text8.zip\" \\\n",
    "    if USE_TEXT8 else 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt'\n",
    "\n",
    "TARGET_SIZE = TYPE_INT(1)       # Size of the target event (word)\n",
    "CONTEXT_SIZE = TYPE_INT(10)     # Size of the context in which the target event occurs.\n",
    "WINDOW_SIZE = TARGET_SIZE + CONTEXT_SIZE\n",
    "SAMPLE_SIZE = TYPE_INT(5)      # Size of the negative samples\n",
    "VECTOR_SIZE = TYPE_INT(100)     # Number of features in the event vector.\n",
    "\n",
    "WEIGHT_SCHEME = \"normal\"\n",
    "WEIGHT_PARAMS = {\n",
    "    \"std\": 0.01\n",
    "}\n",
    "LR = TYPE_FLOAT(20.0)\n",
    "\n",
    "NUM_SENTENCES = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = embedding.load(STATE_FILE_TEXT8_C10_S5_WND_v100_LR20)\n",
    "\n",
    "fmt=\"\"\"Model loaded.\n",
    "event_size %s\n",
    "context_size: %s\n",
    "event_vector_size: %s\n",
    "\"\"\"\n",
    "print(fmt % (\n",
    "    state[\"target_size\"], \n",
    "    state[\"context_size\"], \n",
    "    state[\"event_vector_size\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision between text8-based model and gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text8 model predictions (cosine evaluation) for ['king']:\n",
      "2971:son\n",
      "4962:henry\n",
      "1229:mary\n",
      "5569:queen\n",
      "8165:emperor\n",
      "4906:england\n",
      "6156:prince\n",
      "5657:charles\n",
      "4998:james\n",
      "4567:iii\n",
      "\n",
      "Text8 model predictions (distance evaluation) for ['king']:\n",
      "2971:son\n",
      "4962:henry\n",
      "4906:england\n",
      "8165:emperor\n",
      "4998:james\n",
      "5657:charles\n",
      "3371:president\n",
      "1229:mary\n",
      "2768:minister\n",
      "173:william\n",
      "\n",
      "Gensim predictions for ['king']:\n",
      "('prince', 0.7555170059204102)\n",
      "('queen', 0.7447685599327087)\n",
      "('throne', 0.7199389338493347)\n",
      "('emperor', 0.7015615105628967)\n",
      "('kings', 0.6873074173927307)\n",
      "('vii', 0.6840494871139526)\n",
      "('regent', 0.6827466487884521)\n",
      "('pope', 0.6799136400222778)\n",
      "('elector', 0.6754884123802185)\n",
      "('sultan', 0.6695173978805542)\n"
     ]
    }
   ],
   "source": [
    "cosines, distances = embedding.predict(word_indices, n)\n",
    "print(\"Text8 model predictions (cosine evaluation) for %s:\" % context)\n",
    "for word_index in cosines:\n",
    "    print(f\"{word_index}:{word_indexing.list_events([word_index])}\")\n",
    "    \n",
    "print(\"\\nText8 model predictions (distance evaluation) for %s:\" % context)\n",
    "for word_index in distances:\n",
    "    print(f\"{word_index}:{word_indexing.list_events([word_index])}\")\n",
    "    \n",
    "print(\"\\nGensim predictions for %s:\" % context)\n",
    "for word in w2v.wv.most_similar(context, topn=n):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PTB based training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C10_S5_WND_V100_LR5\n",
    "\n",
    "```\n",
    "TARGET_SIZE = TYPE_INT(1)       # Size of the target event (word)\n",
    "CONTEXT_SIZE = TYPE_INT(10)     # Size of the context in which the target event occurs.\n",
    "WINDOW_SIZE = TARGET_SIZE + CONTEXT_SIZE\n",
    "SAMPLE_SIZE = TYPE_INT(5)       # Size of the negative samples\n",
    "\n",
    "VECTOR_SIZE = TYPE_INT(100)     # Number of features in the event vector.\n",
    "WEIGHT_SCHEME = \"normal\"\n",
    "WEIGHT_PARAMS = {\n",
    "    \"std\": 0.01\n",
    "}\n",
    "\n",
    "LR = TYPE_FLOAT(5)\n",
    "NUM_SENTENCES = 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = embedding.load(STATE_PTB_FILE_C10_S5_WND_V100_LR5)\n",
    "\n",
    "fmt=\"\"\"Model loaded.\n",
    "event_size %s\n",
    "context_size: %s\n",
    "event_vector_size: %s\n",
    "\"\"\"\n",
    "print(fmt % (\n",
    "    state[\"target_size\"], \n",
    "    state[\"context_size\"], \n",
    "    state[\"event_vector_size\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indices = np.array(word_indexing.list_indices(context), dtype=TYPE_INT)\n",
    "\n",
    "print(f\"Words {context}\")\n",
    "print(f\"Word indices {word_indices}\")\n",
    "print(word_indexing.list_events([embedding.predict(word_indices, n)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C10_S5_WND_V100_LR20\n",
    "\n",
    "```\n",
    "TARGET_SIZE = TYPE_INT(1)       # Size of the target event (word)\n",
    "CONTEXT_SIZE = TYPE_INT(10)     # Size of the context in which the target event occurs.\n",
    "WINDOW_SIZE = TARGET_SIZE + CONTEXT_SIZE\n",
    "SAMPLE_SIZE = TYPE_INT(5)       # Size of the negative samples\n",
    "\n",
    "VECTOR_SIZE = TYPE_INT(100)     # Number of features in the event vector.\n",
    "WEIGHT_SCHEME = \"normal\"\n",
    "WEIGHT_PARAMS = {\n",
    "    \"std\": 0.01\n",
    "}\n",
    "\n",
    "LR = TYPE_FLOAT(20)\n",
    "NUM_SENTENCES = 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = embedding.load(STATE_FILE_PTB_C10_S5_WND_V100_LR20)\n",
    "\n",
    "fmt=\"\"\"Model loaded.\n",
    "event_size %s\n",
    "context_size: %s\n",
    "event_vector_size: %s\n",
    "\"\"\"\n",
    "print(fmt % (\n",
    "    state[\"target_size\"], \n",
    "    state[\"context_size\"], \n",
    "    state[\"event_vector_size\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indices = np.array(word_indexing.list_indices(context), dtype=TYPE_INT)\n",
    "\n",
    "print(f\"Words {context}\")\n",
    "print(f\"Word indices {word_indices}\")\n",
    "print(word_indexing.list_events([embedding.predict(word_indices, n)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
