{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "Implement a neural network.\n",
    "\n",
    "\n",
    "### Function implementations\n",
    "\n",
    "\n",
    "* sigmoid\n",
    "* softmax\n",
    "* cross entropy log loss (binary and categorical)\n",
    "\n",
    "### Layer implementations\n",
    "\n",
    "\n",
    "* Standardization\n",
    "* Matmul\n",
    "* Activation (sigmoid, softmax)\n",
    "* Log loss output\n",
    "* Batch normalization (TBD)\n",
    "\n",
    "Mathjax formulas get corrupted in github.\n",
    "\n",
    "### Classifier Implementations\n",
    "\n",
    "* Binary classifiation with (matmul-loss) layers.\n",
    "* Categorical classifiation with (matmul-ReLu-matmul-Relu-loss) layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* [CS231n: Convolutional Neural Networks for Visual Recognition 2017](http://cs231n.stanford.edu/2017/syllabus)\n",
    "    - [cs231n 2017 assignment #1 kNN, SVM, SoftMax, two-layer network](https://cs231n.github.io/assignments2017/assignment1/)\n",
    "    - [Training a Softmax Linear Classifier](https://cs231n.github.io/neural-networks-case-study)\n",
    "* [ゼロから作る Deep Learning](https://github.com/oreilly-japan/deep-learning-from-scratch)\n",
    "* [Mathematics for Machine Learning](https://mml-book.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network overview\n",
    "\n",
    "Structure of the network and how forward and backward propagations work.\n",
    "\n",
    "<img src=\"image/nn_diagram.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts \n",
    "\n",
    "## Objective function\n",
    "\n",
    "The network trains the layers so as to minimize the objective function ```L``` which calculates the loss. Each layer at ```i``` is a function $f_i$ which takes an input $X_i$ from a previous layer and outputs $Y_i = f(X_i)$. The post layers of the form an objective function $L_i$ for the layer: $L = L_i(Y_i)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/nn_functions.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward path\n",
    "\n",
    "The process where each layer ```i``` calculate its output $Y_i = f(X_i)$ and forward it to the next layer(s) as their input $X_{i+1}$.\n",
    "\n",
    "## Backward path\n",
    "\n",
    "The process of automatic differentication, or *back-propagation* where each layer calculates its gradient $\\frac {\\partial L_i(Y_i)}{\\partial Y_i}$ , that is, the impact $Y_i$ will make on the objective ```L``` when it changes. With the gradient, we can apply the gradient descent $X_i = X_i - \\lambda  \\frac {\\partial L_i(Y_i)}{\\partial Y_i} \\frac {\\partial Y_i }{\\partial X_i}$ to update $X_i$ that would reduce the objective ```L```.\n",
    "\n",
    "## Cycle\n",
    "\n",
    "A round-trip of a forward path and a backward path with a batch data set $(X, T)$. How many cycles to happen with each batch is an implementation decision. \n",
    "\n",
    "## Epoch\n",
    "\n",
    "Total cycles to consume the entire training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminologies\n",
    "\n",
    "## X\n",
    "A batch input to a layer. Matrix shape is ```(N, D)```.\n",
    "\n",
    "* ```N``` : Number of rows in a batch X, or batch size\n",
    "* ```D``` : Number of features in a data in X.\n",
    "\n",
    "\n",
    "## T\n",
    "Labels for X. There are two formats available for the label.\n",
    "\n",
    "#### One Hot Encoding (OHE) labels\n",
    "\n",
    "When a neural network predicts a class out of ```3``` classes for an input ```x``` and the correct class is ```2```, then the label ```t``` is specified as ```t = [0, 1, 0]```.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ T_{_{OHE}} } &= ( \n",
    "    \\overset{ (M,) }{ T_{(n=0)} }, \\dots , \\overset{ (M,) }{ T_{(n=N-1)} } \n",
    ") \n",
    "\\\\\n",
    "\\overset{ (M,) }{ T_{ _{OHE} (n)} } &= ( \\overset{ () }{ t_{(n)(m=0)} }, \\; \\dots \\;, \\overset{ () }{ t_{(n)(m=M-1)} })\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "#### Index labels\n",
    "\n",
    "The label ```t``` is specified as ```t = 2```. \n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,) }{ T_{_{IDX}} } &= (\\overset{ () }{ t_{(n=0)} }, \\; \\dots \\;, \\overset{ () }{ t_{(n=N-1)} }) \\end{align*}\n",
    "$\n",
    "\n",
    "## W\n",
    "A set of weight parameters of a node in a Matmul layer. Shape is ```(M, D)```.\n",
    "\n",
    "* ```M``` : Number of nodes in a layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Matrix order\n",
    "\n",
    "Use the row-order matrix. For instance, the weight matrix ```W``` of a Matmul layer has a shape ```(M, D)``` where each row in ```W``` represents a node in the layer. It will be efficient to use the column order matrix of shape ```(D, M)``` for ```W``` so that the matrix multiplication at a Matmul layer can be executed as ```X@W```  which is a ```shape:(N,D) @ shape:(D,M)``` operation without transpose. \n",
    "\n",
    "However, for the purpose of consistency and clarity, use the shape ```W:(M, D)``` although it will cause transposes ```W.T``` at the Matmul operations, and revese transposing ```dL/dW.T``` to ```dL/dW``` when updating ```W``` at the gradient descents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python & Jupyter setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Optional,\n",
    "    Union,\n",
    "    List,\n",
    "    Dict,\n",
    "    Tuple,\n",
    "    Callable\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python path\n",
    "Python path setup to avoid the relative imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from functools import partial\n",
    "import copy\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install line_profile memory_profiler\n",
    "%load_ext memory_profiler\n",
    "%load_ext line_profiler\n",
    "\n",
    "# Logging is enabled by calling logging.basicConfig\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.ERROR)\n",
    "# Logger = logging.getLogger(\"neural_network\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.style as mplstyle\n",
    "mplstyle.use('fast')\n",
    "plt.ion()\n",
    "\n",
    "# Note: with notebook backend from the top, updating the plot line does not work...\n",
    "%matplotlib notebook\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=80) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization Layer\n",
    "Apply normalization or use batch normaliation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matmul layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import (\n",
    "    xavier,\n",
    "    he,\n",
    "    uniform\n",
    ")\n",
    "from layer.matmul import Matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward path\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ Y } \n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "{ Y_{(n=0)} } \\\\\n",
    "\\vdots \\\\\n",
    "{ Y_{(n)} } \\\\\n",
    "\\vdots \\\\\n",
    "{ Y_{(n=N-1)} }\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\overset{ (N,D) }{ X } \\; @ \\; \\overset{ (D,M) }{ W^T }\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (M,) }{ Y_{(n)} } &= (y_{(n)(m=0)}, \\; \\dots, \\; y_{(n)(m)},  \\; \\dots, \\; y_{(n)(m=M-1)})\n",
    "\\\\\n",
    "\\overset{ () }{ y_{(n)(m)} }\n",
    "&= \\overset{ (D,) }{ X_{(n)} } \\cdot \\overset{ (D,) }{ W_{(m)}^T }\n",
    "= \\sum\\limits ^{D}_{d=0}  \\overset{ () }{ x_{(n)(d)} } * \\overset{ () }{ w_{(m)(d)} }\n",
    "\\\\\n",
    "_{(0 \\le d \\le D, \\; 0 \\le n \\lt N, \\; 0 \\le m \\lt M)}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward path\n",
    "### Gradient dL/dX\n",
    "\n",
    "Impact on L by $dX$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,D) }{ \\frac {\\partial L }{ \\partial X } }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial Y} }\n",
    "@ \\overset { (M,D) }{ W } \n",
    "\\end{align*}\n",
    "$\n",
    "<img src=\"image/nn_back_propagation_dL_dX.png\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient dL/dW.T\n",
    "Impact on L by $dW^T$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{ \\partial W^T } }\n",
    "= \\overset { (D,N) }{ X^T } \n",
    "@ \n",
    "\\overset { (N,M) }{ \\frac {\\partial L}{\\partial Y} }\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "<img src=\"image/nn_back_propagation_dL_dWT.png\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization Layer\n",
    "\n",
    "* [Understanding the backward pass through Batch Normalization Layer](http://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)\n",
    "<img src=\"image/bn_back_propagation.png\" align=\"left\" />\n",
    "<img src=\"image/batch_normalization_steps_small.jpg\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward path\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ A } &= \n",
    "activation \\left( \n",
    "    \\overset{ (N,M) }{ Y }  = \n",
    "    \\begin{bmatrix}\n",
    "    { Y_{(n=0)} } \\\\\n",
    "    \\vdots \\\\\n",
    "    { Y_{(n)} } \\\\\n",
    "    \\vdots \\\\\n",
    "    { Y_{(n=N-1)} }\n",
    "    \\end{bmatrix}\n",
    "\\right)\n",
    "\\\\\n",
    "\\overset{ (M,) }{ A_{(n)} } \n",
    "&= activation \\left( \\overset{ (M,) }{ Y_{(n) }} \\right)  \\\\\n",
    "&= (a_{(n)(m=0)}, \\; \\dots, \\; a_{(n)(m)},  \\; \\dots, \\; a_{(n)(m=M-1)})\n",
    "\\\\\n",
    "\\overset{ () }{ a_{(n)(m)} } &= activation \\left( \\overset{ () }{ y_{(n)(m)} } \\right)\n",
    "\\quad _{(0 \\le n \\lt N, \\; 0 \\le m \\lt M)}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward path\n",
    "### Gradient dL/dY\n",
    "\n",
    "Impact on L by dY from the matmul layer.\n",
    "\n",
    "$\n",
    "\\begin {align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{ \\partial Y } }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial A} } \n",
    "* \n",
    "\\overset { (N,M) }{ \\frac {\\partial A}{\\partial Y} }\n",
    "\\end {align*}\n",
    "$\n",
    "\n",
    "#### For sigmoid activation\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset {(N,M)}{\\frac { \\partial L }{ \\partial Y} }\n",
    "&= \\frac { \\partial A }{ \\partial Y} * A * (1 - A)\n",
    "\\\\\n",
    "\\frac { \\partial y_{(n)(m)} } { \\partial a_{(n)(m)} }\n",
    "&= a_{(n)(m)} * (1 - a_{(n)(m)} )  \\\\ \n",
    "y_{(n)(m)} = sigmoid(a_{(n)(m)} )&=  \\frac {1}{ 1 + exp(y_{(n)(m)})}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "#### For ReLU activation\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac { \\partial a_{(n)(m)} }{ \\partial y_{(n)(m)} }\n",
    "&= 1 \\quad y_{(n)(m)}  \\gt 0 \\\\\n",
    "&= 0 \\quad y_{(n)(m)}  \\le 0 \\\\\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax layer\n",
    "$C_n$ is to prevent the overflow at $np.exp()$.\n",
    "\n",
    "<img src=\"image/softmax.png\" align=\"left\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exp(x) can take all x values and produces a positive, which is required for log(y) that needs y > 0, hence fit-for-purpose to build a probability function.\n",
    "\n",
    "<img src=\"image/exp.gif\" align=\"left\" width=250/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax and Cross Entropy Log Loss are combined as the gradient results in a simple form $P - T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def softmax(X: np.ndarray, out=None) -> np.ndarray:\n",
      "    \"\"\"Softmax P = exp(X) / sum(exp(X))\n",
      "    Args:\n",
      "        X: batch input data of shape (N,M).\n",
      "            N: Batch size\n",
      "            M: Number of nodes\n",
      "        out: A location into which the result is stored\n",
      "    Returns:\n",
      "        P: Probability of shape (N,M)\n",
      "    \"\"\"\n",
      "    name = \"softmax\"\n",
      "    assert isinstance(X, float) or (isinstance(X, np.ndarray) and X.dtype == TYPE_FLOAT), \\\n",
      "        \"X must be float or ndarray(dtype=TYPE_FLOAT)\"\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # exp(x-c) to prevent the infinite exp(x) for a large value x, with c = max(x).\n",
      "    # keepdims=True to be able to broadcast.\n",
      "    # --------------------------------------------------------------------------------\n",
      "    C = np.max(X, axis=-1, keepdims=True)\n",
      "    exp = np.exp(X - C)\n",
      "    P = np.divide(exp, np.sum(exp, axis=-1, keepdims=True), out=out)\n",
      "    Logger.debug(\"%s: X %s exp %s P %s\", name, X, exp, P)\n",
      "\n",
      "    return P\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from layer import CrossEntropyLogLoss\n",
    "from common import softmax\n",
    "\n",
    "lines = inspect.getsource(softmax)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foward path\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,1) }{ C } &= np.max\\left( \n",
    "    \\overset{ (N,M) }{ A }, \\; axis=-1,  \\; keepdim=True \n",
    "\\right) \\\\\n",
    "&=  \n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n=0  )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n    )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n=N-1)} } \\end{bmatrix}\n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\overset{ (N,M) }{ EXP } &= np.exp \\left( \\overset{ (N,M) }{ A } - \\overset{ (N,1) }{ C } \\right)\n",
    "= np.exp \\left(\n",
    "    \\begin{bmatrix}\n",
    "    { A_{(n=0)} } - { C_{(n=0)} }\\\\\n",
    "    \\vdots \\\\\n",
    "    { A_{(n)} }   - { C_{(n)} }\\\\\n",
    "    \\vdots \\\\\n",
    "    { A_{(n=N-1)} } - { C_{(n=N-1)} }\\\\\n",
    "    \\end{bmatrix}\n",
    "\\right) \n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "    e_{(n=0)(m=0)}   & \\dots      & e_{(n=0)(m=M-1)}   \\\\  \n",
    "    \\vdots           & e_{(n)(m)} & \\vdots             \\\\\n",
    "    e_{(n=N-1)(m=0)} & \\dots      & e_{(n=N-1)(m=M-1)} \n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,1) }{ S } &= \\overset{ (N,1) }{ sum(EXP) } = np.sum \\left( \n",
    "    \\overset{ (N,M) }{ EXP }, \\; axis=-1,  \\; keepdim=True \n",
    "\\right)\n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n=0  )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n    )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n=N-1)} } \\end{bmatrix}\n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\overset{ () }{ s_{(n)} } &= \\sum\\limits ^{M-1}_{m=0} np.exp(\\; a_{(n)(m)} - c_{(n)} \\; )\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,M) }{ P } &= \\overset{ (N,M) }{ EXP }  \\;\\; / \\;\\; \\overset{ (N,1) }{ sum(EXP) } \n",
    "\\\\\n",
    "\\overset{ (N,) }{ P_{(n)} } &= (p_{(n)(m=0)}, \\; \\dots, \\; p_{(n)(m)} , \\; \\dots, \\; p_{(n)(m=M-1)})\n",
    "\\\\\n",
    "{ p_{(n)(m)} } \n",
    "&= \\frac {np.exp \\left( \n",
    "    { a_{(n)(m) } } - { c_{(n)} }) \\right) \n",
    "}\n",
    "{  \n",
    "np.sum \\left( \n",
    "    np.exp \\left( \n",
    "        a_{(n)(m) } - c_{(n)}\n",
    "    \\right)\n",
    "\\right) \n",
    "}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward path\n",
    "\n",
    "### Gradient dL/dA\n",
    "\n",
    "Impact on L by dA from the activation layer.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{\\partial A} }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial P} }\n",
    "* \n",
    "\\overset { (N,M) }{ \\frac {\\partial P }{\\partial A} } \n",
    "= \n",
    "\\frac {1}{N} (P - T)\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "$\n",
    "Jacobian \\; : \\; f \\circ g \\rightarrow Jf \\circ Jg\n",
    "\\\\\n",
    "\\begin{align*}\n",
    "\\\\\n",
    "L &= f(\\; p_{(n)(m=0)} \\;) = f( \\; g(\\;  a_{(n)(m=0)} \\; ) \\; ) \\quad : p = g(a) = softmax(a)\n",
    "\\\\\n",
    "\\frac {\\partial L} { \\partial a_{(n)(m=0)} }\n",
    "&= Jf(p) \\circ Jg(a) \n",
    "=  \\frac {\\partial L} { \\partial p_{(n)(m=0)} } * \\frac {\\partial  p_{(n)(m=0)}} { \\partial a_{(n)(m=0)} }\n",
    "\\\\\n",
    "&= \\frac {1}{N} \\left(\n",
    " p_{(n)(m=0)} -t_{(n)(m=0)}\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula is calculated by chaing the gradient from ***cross-entropy-log-loss***, and the gradients of the steps in ***softmax***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient from the cross entropy log loss\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac { \\partial L }{ \\partial  p_{(n)(m=0)} }\n",
    "&= \\frac{-1}{N} t_{(n)(m=0)} * \\frac {s_{(n)}}{e_{(n)(m=0)}}\n",
    "\\\\\n",
    "\\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "&= \\frac{-1}{N} t_{(n)(m=1)} * \\frac {s_{(n)}}{e_{(n)(m=1)}}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "####  Gradient $\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "p_{(n)(m=0)} &= \\frac {e_{(n)(m=0)} }{ s_{(n)} } \\\\\n",
    "p_{(n)(m=0)} &= f \\circ g_{(m=0)} = { s^{-1}_{(n)} } \\; * \\; { e_{(n)(m=0)} }\n",
    "\\rightarrow \\frac { \\partial  p_{(n)(m=0)} } { \\partial s^{-1}_{(n)} } = e_{(n)(m=0)}\n",
    "\\\\\n",
    "p_{(n)(m=1)} &= \\frac {e_{(n)(m=1)} }{ s_{(n)} } \\\\\n",
    "p_{(n)(m=1)} &= f \\circ g_{(m=1)} = { s^{-1}_{(n)} } \\; * \\; { e_{(n)(m=1)} }\n",
    "\\rightarrow \\frac { \\partial  p_{(n)(m=1)} } { \\partial s^{-1}_{(n)} } = e_{(n)(m=1)}\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "\\left[\n",
    "    \\frac { \\partial  p_{(n)(m=0)} } { \\partial s^{-1}_{(n)} } * \n",
    "    \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \n",
    "    +\n",
    "    \\frac { \\partial  p_{(n)(m=1)} } { \\partial s^{-1}_{(n)} } * \n",
    "    \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "\\right]\n",
    "\\\\\n",
    "&= \\sum\\limits^{M-1}_{m=0} \n",
    "    e_{(n)(m)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m)} } \n",
    "\\\\\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "    \\begin{bmatrix}\n",
    "    e_{(n)(m=0)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \\\\\n",
    "    + \\\\\n",
    "    e_{(n)(m=1)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "    \\end{bmatrix}\n",
    "\\\\\n",
    "&= -s_{(n)}(\\; t_{(n)(m=0)} + t_{(n)(m=1)} \\;) \\\\\n",
    "&= -s_{(n)}\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "\\left[\n",
    "    e_{(n)(m=0)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \n",
    "    + \n",
    "    e_{(n)(m=1)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "\\right]\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient $\\frac {\\partial L }{ \\partial { s_{(n)} } } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac {1} { s_{(n)} } &= s^{-1}_{(n)} \\rightarrow\n",
    "\\frac { \\partial { s^{-1}_{(n)} } } {\\partial s_{(n)}} = \\frac {-1}{s^{2}_{(n)}}\n",
    "\\\\\n",
    "\\frac {\\partial L }{ \\partial { s_{(n)} } } \n",
    "&=\n",
    "\\frac {-1}{s^{2}_{(n)}} * \n",
    "\\frac {\\partial L}{ \\partial s^{-1}_{(n)} } \\\\\n",
    "&= \\frac {1}{s_n}\n",
    "\\end{align*} \\\\\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Gradient $\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } $\n",
    "$\n",
    "\\begin{align*}\n",
    "s_{(n)} &= \\sum\\limits ^{M-1}_{m=0} e_{(n)(m)} \\rightarrow \n",
    "\\frac { \\partial { s_{(n)} } } {\\partial e_{(n)(m=0)}} = 1\n",
    "\\\\\n",
    "p_{(n)(m=0)} &= \\frac {e_{(n)(m=0)} }{ s_{(n)} }\\rightarrow \n",
    "\\frac { \\partial { p_{(n)(m=0)} } } {\\partial e_{(n)(m=0)}} = \\frac {1}{s_{(n)}}\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } \n",
    "&= \\begin{bmatrix}  \n",
    "    \\frac { \\partial { s_{(n)} } } {\\partial e_{(n)(m=0)}} *  \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \\\\\n",
    "    + \\\\\n",
    "    \\frac { \\partial { p_{(n)(m=0)} } } {\\partial e_{(n)(m=0)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}  \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \\\\\n",
    "    + \\\\\n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } &= \n",
    "\\left[\n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "    + \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \n",
    "\\right]\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } &= \n",
    "\\begin{bmatrix}  \n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} }  \\\\\n",
    "    +  \\\\\n",
    "    \\frac {\\partial L }{ \\partial s_{(n)} } \n",
    "\\end{bmatrix} \\\\\n",
    "&= \\frac {-t_{(n)(m=0)}}{e_{(n)(m=0)} } + \\frac {1}{s_{n}}\n",
    "\\end{align*}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Gardient $\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "e_{(n)(m)} &= exp(\\; a_{(n)(m)} \\; ) \\rightarrow \\frac { \\partial e_{(n)(m)} }{ \\partial a_{(n)(m)} } = e_{(n)(m)} \n",
    "\\\\\n",
    "e_{(n)(m=0)} &= exp(a_{(n)(m=0)}) \\rightarrow \\frac { \\partial e_{(n)(m=0)} }{ \\partial a_{(n)(m=0)} } = e_{(n)(m=0)} \n",
    "\\\\\n",
    "e_{(n)(m=1)} &= exp(a_{(n)(m=1)}) \\\\\n",
    "\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } \n",
    "&=   \\frac { \\partial e_{(n)(m=0)} }{ \\partial a_{(n)(m=0)} } * \n",
    "    \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \n",
    "= e_{(n)(m=0)} * \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } \n",
    "&= e_{(n)(m=0)} * \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \\\\\n",
    "&= -t_{(n)(m=0)} + \\frac { e_{(n)(m=0)} }{ s_{n} } \\\\\n",
    "&= p_{(n)(m=0)} -t_{(n)(m=0)} \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Log Loss\n",
    "\n",
    "A probability distribution $P(x)$ can be represented with its entropy $E(x) = \\sum\\limits_{x}  \\frac {p(x)}{log(p(x)} = - \\sum\\limits_{x} p(x) log(p(x))$. In the diagram, x: (0:dog, 1:cat, 2:fish, 3:bird) are labels and p(dog) is 0.5. When  a NN predicts an input x as a probability distribution $P(x)$, then the $E(x) = 1.75$. \n",
    "\n",
    "0. $p(dog)=\\frac {1}{2}$\n",
    "1. $p(cat)=\\frac {1}{4}$\n",
    "2. $p(fish)=\\frac {1}{8}$\n",
    "3. $p(bird)=\\frac {1}{8}$\n",
    "\n",
    "When the truth is that x is a dog, then the probability distribution of the truth $P(t)$ has the entropy $E(t) = 0$.\n",
    "\n",
    "0. $p(dog)=1$\n",
    "1. $p(cat)=0$\n",
    "2. $p(fish)=0$\n",
    "3. $p(bird)=0$\n",
    "\n",
    "The difference E(x) - E(t) = E(x) = 1.75 can be used as the distance or the error of the prediction from the truth. Need to understand further but  the actuall loss function is $E(x) = -tlog(p(x)) = -log(p(x))$ where p(x) is the probability from the softmax for the correct label.\n",
    "\n",
    "\n",
    "<img src=\"image/entropy.png\" align=\"left\" width=600/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.log() is ln based on the mathematical constant $e$ and its derivative $\\frac {\\partial log(x)}{\\partial x} = \\frac {1}{x}$.\n",
    "\n",
    "* [Logarithm](https://en.wikipedia.org/wiki/Logarithm)\n",
    "\n",
    "\n",
    "<img src=\"image/logarithm_plots.png\" align=\"left\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [ML Grossary - Loss Functions](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)\n",
    "\n",
    "<img src=\"image/cross_entropy_log_loss.png\" align=\"left\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cross_entropy_log_loss_input_combinations.xlsx](./common/cross_entropy_log_loss_input_combinations.xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cross_entropy_log_loss(\n",
      "        P: Union[np.ndarray, float],\n",
      "        T: Union[np.ndarray, int],\n",
      "        f: Callable = categorical_log_loss,\n",
      "        offset: float = OFFSET_LOG\n",
      ") -> np.ndarray:\n",
      "    \"\"\"Cross entropy log loss [ -t(n)(m) * log(p(n)(m)) ] for multi labels.\n",
      "    Args:\n",
      "        P: activation or probabilities from an activation function.\n",
      "        T: labels\n",
      "        f: Cross entropy log loss function f(P, T) where P is activation, T is label\n",
      "        offset: small number to avoid np.inf by log(0) by log(0+offset)\n",
      "\n",
      "    Returns:\n",
      "        J: Loss value of shape (N,), a loss value per batch.\n",
      "\n",
      "    NOTE:\n",
      "        Handle only the label whose value is True. The reason not to use non-labels to\n",
      "        calculate the loss is TBD.\n",
      "\n",
      "        See transform_X_T for the format and shape of P and T.\n",
      "    \"\"\"\n",
      "    name = \"cross_entropy_log_loss\"\n",
      "    P, T = transform_X_T(P, T)\n",
      "    if P.ndim == 0:\n",
      "        assert False, \"P.ndim needs (N,M) after transform_X_T(P, T)\"\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # P is scalar, T is a scalar binary OHE label. Return -t * log(p).\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # assert T.ndim == 0, \"P.ndim==0 requires T.ndim==0 but %s\" % T.shape\n",
      "        # return f(P, T, offset)\n",
      "\n",
      "    if (1 < P.ndim == T.ndim) and (P.shape[1] == T.shape[1] == 1):\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # This condition X:(N,1), T(N,1) tells T is the 2D binary OHE labels.\n",
      "        # T is 2D binary OHE labels e.g. T[[0],[1],[0]], P[[0.9],[0.1],[0.3]].\n",
      "        # Return -T * log(P)\n",
      "        # --------------------------------------------------------------------------------\n",
      "        return np.squeeze(f(P=P, T=T, offset=offset), axis=-1)    # Shape from (N,M) to (N,)\n",
      "        # return f(P=P, T=T, offset=offset).reshape(-1)\n",
      "\n",
      "    # ================================================================================\n",
      "    # Calculate Cross entropy log loss -t * log(p).\n",
      "    # Select an element P[n][t] at each row n which corresponds to the true label t.\n",
      "    # Use the Numpy tuple indexing. e.g. P[n=0][t=2] and P[n=3][t=4].\n",
      "    # P[\n",
      "    #   (0, 3),\n",
      "    #   (2, 4)     # The tuple sizes must be the same at all axes\n",
      "    # ]\n",
      "    #\n",
      "    # Tuple indexing selects only one element per row.\n",
      "    # Beware the numpy behavior difference between P[(n),(m)] and P[[n],[m]].\n",
      "    # https://stackoverflow.com/questions/66269684\n",
      "    # P[1,1]  and P[(0)(0)] results in a scalar value, HOWEVER, P[[1],[1]] in array.\n",
      "    #\n",
      "    # P shape can be (1,1), (1, M), (N, 1), (N, M), hence P[rows, cols] are:\n",
      "    # P (1,1) -> P[rows, cols] results in a 1D of  (1,).\n",
      "    # P (1,M) -> P[rows, cols] results in a 1D of  (1,)\n",
      "    # P (N,1) -> P[rows, cols] results in a 1D of  (N,)\n",
      "    # P (N,M) -> P[rows, cols] results in a 1D of  (N,)\n",
      "    #\n",
      "    # J shape matches with the P[rows, cols] shape.\n",
      "    # ================================================================================\n",
      "    N = batch_size = P.shape[0]\n",
      "    rows = np.arange(N)     # (N,)\n",
      "    cols = T                # Same shape (N,) with rows\n",
      "    assert rows.shape == cols.shape, \\\n",
      "        f\"np P indices need the same shape but rows {rows.shape} cols {cols.shape}.\"\n",
      "\n",
      "    _P = P[rows, cols]\n",
      "    Logger.debug(\"%s: N is [%s]\", name, N)\n",
      "    Logger.debug(\"%s: P.shape %s\", name, P.shape)\n",
      "    Logger.debug(\"%s: P[rows, cols].shape %s\", name, _P.shape)\n",
      "    Logger.debug(\"%s: P[rows, cols] is %s\", name, _P)\n",
      "\n",
      "    J = f(P=_P, T=int(1), offset=offset)\n",
      "\n",
      "    assert not np.all(np.isnan(J)), f\"log(x) caused nan for P \\n{P}.\"\n",
      "    Logger.debug(\"%s: J is [%s]\", name, J)\n",
      "    Logger.debug(\"%s: J.shape %s\\n\", name, J.shape)\n",
      "\n",
      "    assert (J.ndim > 0) and (0 < N == J.shape[0]), \\\n",
      "        f\"Loss J.shape is expected to be ({N},) but {J.shape}\"\n",
      "    return J\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from common import (\n",
    "    cross_entropy_log_loss,\n",
    "    OFFSET_LOG\n",
    ")\n",
    "lines = inspect.getsource(cross_entropy_log_loss)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For label using One Hot Encoding (OHE)\n",
    "For instance, if multi labels are (0,1,2,3,4) and each label is OHE, then the label for 2 is (0,0,1,0,0).   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product of matrix rows\n",
    "\n",
    "There is no formal operation to calculate the dot products of the rows from two matrices, but to calculate the diagonal of the matlix multiplication that also calculate non-diagonals. To avoid calculating non-diagonals, use [einsum](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html).\n",
    "\n",
    "* [Name of matrix operation of ```[A[0] dot B[0], A[1] dot B[1] ]``` from 2x2 matrices A, B](https://math.stackexchange.com/questions/4010721/name-of-matrix-operation-of-a0-dot-b0-a1-dot-b1-from-2x2-matrices-a)\n",
    "\n",
    "<img src=\"image/dot_products_of_matrix_rows.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is \n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "b.T is \n",
      "[[ 0 -3]\n",
      " [-1 -4]\n",
      " [-2 -5]]\n",
      "\n",
      "c[\n",
      "    np.inner(a[0], b[0]),\n",
      "    np.inner(a[1], b[1]),    \n",
      "] is [-5, -50]\n",
      "\n",
      "\n",
      "np.einsum('ij,ji->i', a, b.T) is [ -5 -50]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(6).reshape(2,3)\n",
    "b = np.arange(0,-6,-1).reshape(2,3)\n",
    "c = [\n",
    "    np.inner(a[0], b[0]),\n",
    "    np.inner(a[1], b[1]),    \n",
    "]\n",
    "print(f\"a is \\n{a}\")\n",
    "print(f\"b.T is \\n{b.T}\\n\")\n",
    "fmt=f\"\"\"c[\n",
    "    np.inner(a[0], b[0]),\n",
    "    np.inner(a[1], b[1]),    \n",
    "] is {c}\\n\n",
    "\"\"\"\n",
    "print(fmt)\n",
    "\n",
    "# Use einsum\n",
    "e = np.einsum('ij,ji->i', a, b.T)\n",
    "fmt=\"np.einsum('ij,ji->i', a, b.T)\"\n",
    "print(f\"{fmt} is {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foward path (OHE)\n",
    "$\n",
    "\\text{ for one hot encoding labels }\n",
    "\\\\\n",
    "\\begin{align*}\n",
    "\\overset{ (N,) }{ J } &= - \\sum\\limits^{M-1}_{m=0} \n",
    "    \\left[ \\; \\;  \n",
    "        t_{(n)(m)} \\;  * \\;  np.log(p_{(n)(m)}) \\;\\;  \n",
    "    \\right]\n",
    "\\\\\n",
    "\\overset{ () }{ j_{(n)} } &= \\overset{ (M,) }{ T_{(n)} } \\cdot \\overset{ (M,) }{ P_{(n)} } \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient dL/dP\n",
    "\n",
    "Impact on L by the $dP$ from the softmax layer for one hot encoding labels.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac { \\partial L }{ \\partial P} }\n",
    "&= \\overset { (N,) }{ \\frac { \\partial L }{ \\partial J} } * \n",
    "\\overset { (N,M) }{ \n",
    "\\left(\n",
    " - \\frac { \\partial T } { \\partial P }\n",
    " \\right) \n",
    "} \n",
    "= - \\frac {1}{N }  \\frac { \\partial T } { \\partial P }\n",
    "\\\\\n",
    "\\frac {\\partial L }{\\partial p_{(n)(m=0)}} \n",
    "&= \\frac {\\partial L}{\\partial j_{(n)}} * \\frac {\\partial j_{(n)}} {\\partial p_{(n)(m=0)}} \n",
    "= \\frac {1}{N} \\frac { -t_{(n)(m=0)}}{ p_{(n)(m=0)} } \n",
    "=  \\frac {1}{N} \\left(\n",
    " -t_{(n)(m=0)} * \\frac { s_{(n)} }{ e_{(n)(m=0)} }\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For label using indexing \n",
    "For instance, if the multi labels are (0,1,2,3,4) then the index is 2 for the label 2. If the labels are (2,4,6,8,9), then the index is 3 for the label 8.  \n",
    "\n",
    "Use LP to select the probabilities from P for the corresponding labels. For instance, if the label is 2 (hence the index is 2) for X(n=0), and 4 for X(n=3), then the numpy tuple indexing selects ```P[n=0][m=2]``` and ```P[n=3][m=4] ```.\n",
    "\n",
    "```\n",
    "P[\n",
    "   (0, 3),\n",
    "   (2, 4)\n",
    "]\n",
    "```\n",
    "\n",
    "$\n",
    "\\text{ for index labels e.g. (5, 2, 0, 9, ...)}\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,) }{ J } = - np.sum(\\; np.log(LP), \\; axis = -1 \\;) \\\\\n",
    "LP = label\\_probability = P \\left[ \\\\\n",
    "\\quad ( \\; 0, \\; \\dots, \\;  {N-1}) , \\\\\n",
    "\\quad ( \\; t_{(n=0)} \\; , \\dots , \\; t_{(n=N-1)}) \\\\\n",
    "\\right]\n",
    "\\\\\n",
    "\\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward path\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ () }{ L } = \\frac {1}{N} \\sum\\limits^{N-1}_{n=0} \\overset{ () }{ j_{{(n)}} }\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gardient dL/dJ\n",
    "\n",
    "Impact on L by $dJ$ from the cross entropy log loss layer.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,) }{ \\frac {\\partial L}{\\partial J} }  &= \\frac {1}{N} \\overset{(N,)}{ones}\n",
    "\\\\\n",
    "\\frac {\\partial L}{\\partial j_{(n)} } &= \\frac {1}{N} \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.33333333, 0.33333333])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 3\n",
    "dJ = np.ones(N) / N\n",
    "dJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical gradient\n",
    "\n",
    "The objective of back-propagation is to analytically calculate the gradient of the objective function $g(X_i) = \\frac {\\partial L_i(Y_i)}{\\partial Y_i} \\frac {\\partial Y_i}{\\partial X_i}$ at each layer. Suppose the shape of $X_i$ is ```(N, M)```. We can take an element of X at an index ```(n,m)``` and add a small change ```h```, then see what impact ```h``` makes by calculating the numerical gradient ```gn``` as: $\n",
    "\\begin {align*}\n",
    "gn(X_i) = \\frac {L_i(f_i(X_i+h)) - L_i(f_i(X_i-h))} {2h }\n",
    "\\end {align*}\n",
    "$. Then $gn(X_i) \\approx \\; $gn(X_i)$ would assure the gradient calculation should be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def numerical_jacobian(\n",
      "        f: Callable[[np.ndarray], np.ndarray],\n",
      "        X: Union[np.ndarray, float],\n",
      "        delta: Optional[TYPE_FLOAT] = OFFSET_DELTA\n",
      ") -> np.ndarray:\n",
      "    \"\"\"Calculate Jacobian matrix J numerically with (f(X+h) - f(X-h)) / 2h\n",
      "    Jacobian matrix element Jpq = df/dXpq, the impact on J by the\n",
      "    small difference to Xpq where p is row index and q is col index of J.\n",
      "\n",
      "    Note:\n",
      "        Beware limitations by the float storage size, e.g. loss of significance.\n",
      "        https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html\n",
      "        https://ece.uwaterloo.ca/~dwharder/NumericalAnalysis/Contents/\n",
      "        https://ece.uwaterloo.ca/~dwharder/NumericalAnalysis/02Numerics/Weaknesses/\n",
      "        https://www.cise.ufl.edu/~mssz/CompOrg/CDA-arith.html\n",
      "\n",
      "    Args:\n",
      "        f: Y=f(X) where Y is a scalar or shape() array.\n",
      "        X: input of shame (N, M), or (N,) or ()\n",
      "        delta: small delta value to calculate the f value for X+/-h\n",
      "    Returns:\n",
      "        J: Jacobian matrix that has the same shape of X.\n",
      "    \"\"\"\n",
      "    name = \"numerical_jacobian\"\n",
      "    X = np.array(X, dtype=TYPE_FLOAT) if isinstance(X, (float, int)) else X\n",
      "    J = np.zeros_like(X, dtype=TYPE_FLOAT)\n",
      "    delta = OFFSET_DELTA if (delta is None or delta <= 0.0) else delta\n",
      "    divider = 2 * delta\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # (x+h) or (x-h) may cause an invalid value area for the function f.\n",
      "    # e.g log loss tries to offset x=0 by adding a small value k as log(0+k).\n",
      "    # However because k=1e-7 << h=1e-5, f(x-h) causes nan due to log(x < 0)\n",
      "    # as x needs to be > 0 for log.\n",
      "    #\n",
      "    # X and tmp must be float, or it will be int causing float calculation fail.\n",
      "    # e.g. f(1-h) = log(1-h) causes log(0) instead of log(1-h).\n",
      "    # --------------------------------------------------------------------------------\n",
      "    assert (X.dtype == TYPE_FLOAT), \"X must be float type\"\n",
      "    assert delta > 0.0 and isinstance(delta, TYPE_FLOAT)\n",
      "\n",
      "    it = np.nditer(X, flags=['multi_index'], op_flags=['readwrite'])\n",
      "    while not it.finished:\n",
      "        idx = it.multi_index\n",
      "        tmp: TYPE_FLOAT = X[idx]\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # f(x+h)\n",
      "        # --------------------------------------------------------------------------------\n",
      "        X[idx] = tmp + delta\n",
      "        fx1: Union[np.ndarray, float] = f(X)  # f(x+h)\n",
      "        Logger.debug(\n",
      "            \"%s: idx[%s] x[%s] (x+h)[%s] fx1=[%s]\",\n",
      "            name, idx, tmp, tmp+delta, fx1\n",
      "        )\n",
      "\n",
      "        assert \\\n",
      "            ((isinstance(fx1, np.ndarray) and fx1.size == 1) or isinstance(fx1, float)), \\\n",
      "            f\"The f function needs to return scalar or shape () but {fx1}\"\n",
      "        assert np.isfinite(fx1), \\\n",
      "            \"f(x+h) caused nan for f %s for X %s\" % (f, (tmp + delta))\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # f(x-h)\n",
      "        # --------------------------------------------------------------------------------\n",
      "        X[idx] = tmp - delta\n",
      "        fx2: Union[np.ndarray, float] = f(X)\n",
      "        Logger.debug(\n",
      "            \"%s: idx[%s] x[%s] (x-h)[%s] fx2=[%s]\",\n",
      "            name, idx, tmp, tmp-delta, fx2\n",
      "        )\n",
      "        assert \\\n",
      "            ((isinstance(fx2, np.ndarray) and fx2.size == 1) or isinstance(fx2, float)), \\\n",
      "            f\"The f function needs to return scalar or shape () but {fx2}\"\n",
      "        assert np.isfinite(fx2), \\\n",
      "            \"f(x-h) caused nan for f %s for X %s\" % (f, (tmp - delta))\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # When f(x+k) and f(x-k) are relatively too close, subtract between them can ben\n",
      "        # too small, and the precision error 1/f(x) relative to f(x+k) and f(x-k) is much\n",
      "        # larger relative to that of f(x+k) or f(x-k), hence the result can be unstable.\n",
      "        # Prevent the subtract df(x) from being too small to f(x) by assuring df(x)/dx is\n",
      "        # greater than GN_DIFF_ACCEPTANCE_RATIO.\n",
      "        #\n",
      "        # e.g. For logistic log loss function f(x) with log(+1e-7) to avoid log(0)/inf.\n",
      "        # x[14.708627877981929] (x+h)[14.708627878981929] fx1=[14.708628288297405]\n",
      "        # x[14.708627877981929] (x-h)[14.708627876981929] fx2=[14.708628286670217]\n",
      "        # (fx1-fx2)=[1.6271872738116144e-09]\n",
      "        # (fx1-fx2) / fxn < 1e-10. The difference is relatively too small to f(x).\n",
      "        #\n",
      "        #\n",
      "        # If the gradient of f(x) at x is nearly zero, or saturation, then reconsider\n",
      "        # if using the numerical gradient is fit for the purpose. Prevent the gradient\n",
      "        # from being too close to zero by f(x+k)-f(x-k) > GN_DIFF_ACCEPTANCE_VALUE\n",
      "        # --------------------------------------------------------------------------------\n",
      "        difference = (fx1 - fx2)\n",
      "        Logger.debug(\"%s: (fx1-fx2)=[%s]\", name, difference)\n",
      "\n",
      "        derivative_saturation_condition = (difference == 0.0)\n",
      "        if derivative_saturation_condition:\n",
      "            fmt = \"%s: derivative saturation fx1=fx2=%s detected.\\n\"\n",
      "            args = tuple([name, fx1])\n",
      "            Logger.warning(fmt, *args)\n",
      "            assert ENFORCE_STRICT_ASSERT, fmt % args\n",
      "\n",
      "        # subtract_cancellation_condition = (fx1 != fx2) and (\n",
      "        #         (difference < (fx1 * GN_DIFF_ACCEPTANCE_RATIO)) or\n",
      "        #         (difference < (fx2 * GN_DIFF_ACCEPTANCE_RATIO))\n",
      "        # )\n",
      "\n",
      "        # if subtract_cancellation_condition:\n",
      "        #     fmt = \"%s: potential subtract cancellation (fx1-fx2)/fx < %s detected.\\n\"\\\n",
      "        #           \"(fx1:%s - fx2:%s) is %s, gn %s.\"\n",
      "        #     args = tuple([\n",
      "        #         name,\n",
      "        #         GN_DIFF_ACCEPTANCE_RATIO,\n",
      "        #         fx1,\n",
      "        #         fx2,\n",
      "        #         difference,\n",
      "        #         (fx1-fx2) / (2 * delta)\n",
      "        #     ])\n",
      "        #     Logger.warning(fmt, *args)\n",
      "        #     assert ENFORCE_STRICT_ASSERT, fmt % args\n",
      "\n",
      "        J[idx] = difference / divider\n",
      "        X[idx] = tmp\n",
      "        it.iternext()\n",
      "\n",
      "    if not np.all(np.isfinite(J)):\n",
      "        raise ValueError(f\"{name} caused Nan or Inf\")\n",
      "\n",
      "    gradient_saturation_condition = (abs(J) < GRADIENT_SATURATION_THRESHOLD)\n",
      "    if np.all(gradient_saturation_condition):\n",
      "        __J = J[gradient_saturation_condition]\n",
      "        msg = \"%s: The gradient [%s] should be saturated.\"\n",
      "        Logger.warning(msg, name, __J)\n",
      "        assert ENFORCE_STRICT_ASSERT, msg % (name, __J)\n",
      "\n",
      "    return J\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from common import (\n",
    "    numerical_jacobian,\n",
    "    OFFSET_DELTA\n",
    ")\n",
    "lines = inspect.getsource(numerical_jacobian)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Consideration\n",
    "\n",
    "#### Instability of numerical calculations\n",
    "\n",
    "A float number can have infinite length e.g. ```1/3``` in the real world, but a computer needs to approximate it by rounding it to fit into a limited storage. Need to assure numerical errors are prevented or detected while calculating gradient numerically. See [Numerical errors](numerical_errors.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example \n",
    "\n",
    "Comparing the analytical gradient and the numerical gradient of a logistic log loss ```L = -(1-T) * log(Z)``` where ```T = 0``` and ```Z=sigmoid(X)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The analytical and numerical gradients of the logistic log loss for X:\n",
      "[[ 2.78964315  0.64176548]\n",
      " [-2.48800383  0.12515952]]\n",
      "\n",
      "Analytical gradient:\n",
      "[[0.94211359 0.65515244]\n",
      " [0.07670345 0.5312491 ]]\n",
      "\n",
      "Numerical gradient:\n",
      "[[0.94211305 0.65515593]\n",
      " [0.07670753 0.5312506 ]]\n"
     ]
    }
   ],
   "source": [
    "def t_0_logistic_log_loss(X):\n",
    "    \"\"\"Logistic log loss function\"\"\"\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # For numerical stability, re-forumulate the logistic log loss -(1-T)log(1-Z) as\n",
    "    # X + log(1+exp(-X)).\n",
    "    \n",
    "    # By Reza.B\n",
    "    # Let z=1/(1+p), p= e^(-x), then log(1-z)=log(p)-log(1+p), which is more stable\n",
    "    # in terms of rounding errors (we got rid of division, which is the main issue \n",
    "    # in numerical instabilities). \n",
    "    # --------------------------------------------------------------------------------\n",
    "    L = np.sum(X + np.log(1 + np.exp(-X)))\n",
    "    return L.tolist()\n",
    "\n",
    "def gradient_t_0_loss(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "X = np.random.uniform(-5, 5, (2,2))\n",
    "print(f\"The analytical and numerical gradients of the logistic log loss for X:\\n{X}\\n\")\n",
    "\n",
    "analytical_gradient = gradient_t_0_loss(X)\n",
    "numerical_gradient = numerical_jacobian(t_0_logistic_log_loss, X)\n",
    "\n",
    "print(f\"Analytical gradient:\\n{analytical_gradient}\\n\")\n",
    "print(f\"Numerical gradient:\\n{numerical_gradient}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification\n",
    "\n",
    "Use Matmul and CrossEntropyLogLoss layers to build a binary classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from common import (\n",
    "    weights,\n",
    "    transform_X_T,\n",
    "    sigmoid_cross_entropy_log_loss,\n",
    "    softmax_cross_entropy_log_loss\n",
    ")\n",
    "from data import (\n",
    "    linear_separable\n",
    ")\n",
    "from optimizer import (\n",
    "    Optimizer,\n",
    "    SGD\n",
    ")\n",
    "from network import (\n",
    "    train_binary_classifier\n",
    ")\n",
    "from drawing import (\n",
    "    COLOR_LABELS,   # labels to classify outside/0/red or inside/1/green.\n",
    "    plot_categorical_predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X\n",
    "\n",
    "Training data is two dimensional plots that can be linearly separable with a line whose normal is $(w1, w2)$ and point is $b=-w0/w2$. The line is written as $X \\cdot W = 0$ where $W = (w0,w1,w2)$ and $X = (x0, x1, x2)$. $T$ are binary labels that tells if each plot is classfied as 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500    # Number of plots\n",
    "D = 2      # Number of features\n",
    "from data import (\n",
    "    linear_separable\n",
    ")\n",
    "X, T, V = linear_separable(d=D, n=N)\n",
    "_X = np.c_[\n",
    "    np.ones(N),     # Bias\n",
    "    X\n",
    "]\n",
    "#print(f\"X.shape {X.shape} T.shape {T.shape} W {V}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "/* global mpl */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function () {\n",
       "    if (typeof WebSocket !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof MozWebSocket !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert(\n",
       "            'Your browser does not have WebSocket support. ' +\n",
       "                'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "                'Firefox 4 and 5 are also supported but you ' +\n",
       "                'have to enable WebSockets in about:config.'\n",
       "        );\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = this.ws.binaryType !== undefined;\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById('mpl-warnings');\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent =\n",
       "                'This browser does not support binary websocket messages. ' +\n",
       "                'Performance may be slow.';\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = document.createElement('div');\n",
       "    this.root.setAttribute('style', 'display: inline-block');\n",
       "    this._root_extra_style(this.root);\n",
       "\n",
       "    parent_element.appendChild(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen = function () {\n",
       "        fig.send_message('supports_binary', { value: fig.supports_binary });\n",
       "        fig.send_message('send_image_mode', {});\n",
       "        if (fig.ratio !== 1) {\n",
       "            fig.send_message('set_dpi_ratio', { dpi_ratio: fig.ratio });\n",
       "        }\n",
       "        fig.send_message('refresh', {});\n",
       "    };\n",
       "\n",
       "    this.imageObj.onload = function () {\n",
       "        if (fig.image_mode === 'full') {\n",
       "            // Full images could contain transparency (where diff images\n",
       "            // almost always do), so we need to clear the canvas so that\n",
       "            // there is no ghosting.\n",
       "            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "        }\n",
       "        fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "    };\n",
       "\n",
       "    this.imageObj.onunload = function () {\n",
       "        fig.ws.close();\n",
       "    };\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_header = function () {\n",
       "    var titlebar = document.createElement('div');\n",
       "    titlebar.classList =\n",
       "        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n",
       "    var titletext = document.createElement('div');\n",
       "    titletext.classList = 'ui-dialog-title';\n",
       "    titletext.setAttribute(\n",
       "        'style',\n",
       "        'width: 100%; text-align: center; padding: 3px;'\n",
       "    );\n",
       "    titlebar.appendChild(titletext);\n",
       "    this.root.appendChild(titlebar);\n",
       "    this.header = titletext;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = (this.canvas_div = document.createElement('div'));\n",
       "    canvas_div.setAttribute(\n",
       "        'style',\n",
       "        'border: 1px solid #ddd;' +\n",
       "            'box-sizing: content-box;' +\n",
       "            'clear: both;' +\n",
       "            'min-height: 1px;' +\n",
       "            'min-width: 1px;' +\n",
       "            'outline: 0;' +\n",
       "            'overflow: hidden;' +\n",
       "            'position: relative;' +\n",
       "            'resize: both;'\n",
       "    );\n",
       "\n",
       "    function on_keyboard_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.key_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    canvas_div.addEventListener(\n",
       "        'keydown',\n",
       "        on_keyboard_event_closure('key_press')\n",
       "    );\n",
       "    canvas_div.addEventListener(\n",
       "        'keyup',\n",
       "        on_keyboard_event_closure('key_release')\n",
       "    );\n",
       "\n",
       "    this._canvas_extra_style(canvas_div);\n",
       "    this.root.appendChild(canvas_div);\n",
       "\n",
       "    var canvas = (this.canvas = document.createElement('canvas'));\n",
       "    canvas.classList.add('mpl-canvas');\n",
       "    canvas.setAttribute('style', 'box-sizing: content-box;');\n",
       "\n",
       "    this.context = canvas.getContext('2d');\n",
       "\n",
       "    var backingStore =\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        this.context.webkitBackingStorePixelRatio ||\n",
       "        this.context.mozBackingStorePixelRatio ||\n",
       "        this.context.msBackingStorePixelRatio ||\n",
       "        this.context.oBackingStorePixelRatio ||\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        1;\n",
       "\n",
       "    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n",
       "        'canvas'\n",
       "    ));\n",
       "    rubberband_canvas.setAttribute(\n",
       "        'style',\n",
       "        'box-sizing: content-box; position: absolute; left: 0; top: 0; z-index: 1;'\n",
       "    );\n",
       "\n",
       "    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n",
       "    if (this.ResizeObserver === undefined) {\n",
       "        if (window.ResizeObserver !== undefined) {\n",
       "            this.ResizeObserver = window.ResizeObserver;\n",
       "        } else {\n",
       "            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n",
       "            this.ResizeObserver = obs.ResizeObserver;\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n",
       "        var nentries = entries.length;\n",
       "        for (var i = 0; i < nentries; i++) {\n",
       "            var entry = entries[i];\n",
       "            var width, height;\n",
       "            if (entry.contentBoxSize) {\n",
       "                if (entry.contentBoxSize instanceof Array) {\n",
       "                    // Chrome 84 implements new version of spec.\n",
       "                    width = entry.contentBoxSize[0].inlineSize;\n",
       "                    height = entry.contentBoxSize[0].blockSize;\n",
       "                } else {\n",
       "                    // Firefox implements old version of spec.\n",
       "                    width = entry.contentBoxSize.inlineSize;\n",
       "                    height = entry.contentBoxSize.blockSize;\n",
       "                }\n",
       "            } else {\n",
       "                // Chrome <84 implements even older version of spec.\n",
       "                width = entry.contentRect.width;\n",
       "                height = entry.contentRect.height;\n",
       "            }\n",
       "\n",
       "            // Keep the size of the canvas and rubber band canvas in sync with\n",
       "            // the canvas container.\n",
       "            if (entry.devicePixelContentBoxSize) {\n",
       "                // Chrome 84 implements new version of spec.\n",
       "                canvas.setAttribute(\n",
       "                    'width',\n",
       "                    entry.devicePixelContentBoxSize[0].inlineSize\n",
       "                );\n",
       "                canvas.setAttribute(\n",
       "                    'height',\n",
       "                    entry.devicePixelContentBoxSize[0].blockSize\n",
       "                );\n",
       "            } else {\n",
       "                canvas.setAttribute('width', width * fig.ratio);\n",
       "                canvas.setAttribute('height', height * fig.ratio);\n",
       "            }\n",
       "            canvas.setAttribute(\n",
       "                'style',\n",
       "                'width: ' + width + 'px; height: ' + height + 'px;'\n",
       "            );\n",
       "\n",
       "            rubberband_canvas.setAttribute('width', width);\n",
       "            rubberband_canvas.setAttribute('height', height);\n",
       "\n",
       "            // And update the size in Python. We ignore the initial 0/0 size\n",
       "            // that occurs as the element is placed into the DOM, which should\n",
       "            // otherwise not happen due to the minimum size styling.\n",
       "            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n",
       "                fig.request_resize(width, height);\n",
       "            }\n",
       "        }\n",
       "    });\n",
       "    this.resizeObserverInstance.observe(canvas_div);\n",
       "\n",
       "    function on_mouse_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.mouse_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousedown',\n",
       "        on_mouse_event_closure('button_press')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseup',\n",
       "        on_mouse_event_closure('button_release')\n",
       "    );\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousemove',\n",
       "        on_mouse_event_closure('motion_notify')\n",
       "    );\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseenter',\n",
       "        on_mouse_event_closure('figure_enter')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseleave',\n",
       "        on_mouse_event_closure('figure_leave')\n",
       "    );\n",
       "\n",
       "    canvas_div.addEventListener('wheel', function (event) {\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        on_mouse_event_closure('scroll')(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.appendChild(canvas);\n",
       "    canvas_div.appendChild(rubberband_canvas);\n",
       "\n",
       "    this.rubberband_context = rubberband_canvas.getContext('2d');\n",
       "    this.rubberband_context.strokeStyle = '#000000';\n",
       "\n",
       "    this._resize_canvas = function (width, height, forward) {\n",
       "        if (forward) {\n",
       "            canvas_div.style.width = width + 'px';\n",
       "            canvas_div.style.height = height + 'px';\n",
       "        }\n",
       "    };\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    this.rubberband_canvas.addEventListener('contextmenu', function (_e) {\n",
       "        event.preventDefault();\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus() {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'mpl-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'mpl-button-group';\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'mpl-button-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        var button = (fig.buttons[name] = document.createElement('button'));\n",
       "        button.classList = 'mpl-widget';\n",
       "        button.setAttribute('role', 'button');\n",
       "        button.setAttribute('aria-disabled', 'false');\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "\n",
       "        var icon_img = document.createElement('img');\n",
       "        icon_img.src = '_images/' + image + '.png';\n",
       "        icon_img.srcset = '_images/' + image + '_large.png 2x';\n",
       "        icon_img.alt = tooltip;\n",
       "        button.appendChild(icon_img);\n",
       "\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    var fmt_picker = document.createElement('select');\n",
       "    fmt_picker.classList = 'mpl-widget';\n",
       "    toolbar.appendChild(fmt_picker);\n",
       "    this.format_dropdown = fmt_picker;\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = document.createElement('option');\n",
       "        option.selected = fmt === mpl.default_extension;\n",
       "        option.innerHTML = fmt;\n",
       "        fmt_picker.appendChild(option);\n",
       "    }\n",
       "\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', { width: x_pixels, height: y_pixels });\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_message = function (type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function () {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function (fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1], msg['forward']);\n",
       "        fig.send_message('refresh', {});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function (fig, msg) {\n",
       "    var x0 = msg['x0'] / fig.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n",
       "    var x1 = msg['x1'] / fig.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0,\n",
       "        0,\n",
       "        fig.canvas.width / fig.ratio,\n",
       "        fig.canvas.height / fig.ratio\n",
       "    );\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function (fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function (fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch (cursor) {\n",
       "        case 0:\n",
       "            cursor = 'pointer';\n",
       "            break;\n",
       "        case 1:\n",
       "            cursor = 'default';\n",
       "            break;\n",
       "        case 2:\n",
       "            cursor = 'crosshair';\n",
       "            break;\n",
       "        case 3:\n",
       "            cursor = 'move';\n",
       "            break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_message = function (fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function (fig, _msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function (fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n",
       "    for (var key in msg) {\n",
       "        if (!(key in fig.buttons)) {\n",
       "            continue;\n",
       "        }\n",
       "        fig.buttons[key].disabled = !msg[key];\n",
       "        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n",
       "    if (msg['mode'] === 'PAN') {\n",
       "        fig.buttons['Pan'].classList.add('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    } else if (msg['mode'] === 'ZOOM') {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.add('active');\n",
       "    } else {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message('ack', {});\n",
       "};\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function (fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = 'image/png';\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src\n",
       "                );\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data\n",
       "            );\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        } else if (\n",
       "            typeof evt.data === 'string' &&\n",
       "            evt.data.slice(0, 21) === 'data:image/png;base64'\n",
       "        ) {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig['handle_' + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\n",
       "                \"No handler for the '\" + msg_type + \"' message type: \",\n",
       "                msg\n",
       "            );\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\n",
       "                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n",
       "                    e,\n",
       "                    e.stack,\n",
       "                    msg\n",
       "                );\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "};\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function (e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e) {\n",
       "        e = window.event;\n",
       "    }\n",
       "    if (e.target) {\n",
       "        targ = e.target;\n",
       "    } else if (e.srcElement) {\n",
       "        targ = e.srcElement;\n",
       "    }\n",
       "    if (targ.nodeType === 3) {\n",
       "        // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "    }\n",
       "\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    var boundingRect = targ.getBoundingClientRect();\n",
       "    var x = e.pageX - (boundingRect.left + document.body.scrollLeft);\n",
       "    var y = e.pageY - (boundingRect.top + document.body.scrollTop);\n",
       "\n",
       "    return { x: x, y: y };\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys(original) {\n",
       "    return Object.keys(original).reduce(function (obj, key) {\n",
       "        if (typeof original[key] !== 'object') {\n",
       "            obj[key] = original[key];\n",
       "        }\n",
       "        return obj;\n",
       "    }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function (event, name) {\n",
       "    var canvas_pos = mpl.findpos(event);\n",
       "\n",
       "    if (name === 'button_press') {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * this.ratio;\n",
       "    var y = canvas_pos.y * this.ratio;\n",
       "\n",
       "    this.send_message(name, {\n",
       "        x: x,\n",
       "        y: y,\n",
       "        button: event.button,\n",
       "        step: event.step,\n",
       "        guiEvent: simpleKeys(event),\n",
       "    });\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (_event, _name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.key_event = function (event, name) {\n",
       "    // Prevent repeat events\n",
       "    if (name === 'key_press') {\n",
       "        if (event.which === this._key) {\n",
       "            return;\n",
       "        } else {\n",
       "            this._key = event.which;\n",
       "        }\n",
       "    }\n",
       "    if (name === 'key_release') {\n",
       "        this._key = null;\n",
       "    }\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which !== 17) {\n",
       "        value += 'ctrl+';\n",
       "    }\n",
       "    if (event.altKey && event.which !== 18) {\n",
       "        value += 'alt+';\n",
       "    }\n",
       "    if (event.shiftKey && event.which !== 16) {\n",
       "        value += 'shift+';\n",
       "    }\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function (name) {\n",
       "    if (name === 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message('toolbar_button', { name: name });\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "\n",
       "///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n",
       "// prettier-ignore\n",
       "var _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";/* global mpl */\n",
       "\n",
       "var comm_websocket_adapter = function (comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function () {\n",
       "        comm.close();\n",
       "    };\n",
       "    ws.send = function (m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function (msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data']);\n",
       "    });\n",
       "    return ws;\n",
       "};\n",
       "\n",
       "mpl.mpl_figure_comm = function (comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = document.getElementById(id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm);\n",
       "\n",
       "    function ondownload(figure, _format) {\n",
       "        window.open(figure.canvas.toDataURL());\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element;\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error('Failed to find cell for figure', id, fig);\n",
       "        return;\n",
       "    }\n",
       "    fig.cell_info[0].output_area.element.on(\n",
       "        'cleared',\n",
       "        { fig: fig },\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function (fig, msg) {\n",
       "    var width = fig.canvas.width / fig.ratio;\n",
       "    fig.cell_info[0].output_area.element.off(\n",
       "        'cleared',\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable();\n",
       "    fig.parent_element.innerHTML =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "    fig.close_ws(fig, msg);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.close_ws = function (fig, msg) {\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function (_remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width / this.ratio;\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message('ack', {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () {\n",
       "        fig.push_to_output();\n",
       "    }, 1000);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'btn-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'btn-group';\n",
       "    var button;\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'btn-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        button = fig.buttons[name] = document.createElement('button');\n",
       "        button.classList = 'btn btn-default';\n",
       "        button.href = '#';\n",
       "        button.title = name;\n",
       "        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message pull-right';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = document.createElement('div');\n",
       "    buttongrp.classList = 'btn-group inline pull-right';\n",
       "    button = document.createElement('button');\n",
       "    button.classList = 'btn btn-mini btn-primary';\n",
       "    button.href = '#';\n",
       "    button.title = 'Stop Interaction';\n",
       "    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n",
       "    button.addEventListener('click', function (_evt) {\n",
       "        fig.handle_close(fig, {});\n",
       "    });\n",
       "    button.addEventListener(\n",
       "        'mouseover',\n",
       "        on_mouseover_closure('Stop Interaction')\n",
       "    );\n",
       "    buttongrp.appendChild(button);\n",
       "    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n",
       "    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._remove_fig_handler = function (event) {\n",
       "    var fig = event.data.fig;\n",
       "    if (event.target !== this) {\n",
       "        // Ignore bubbled events from children.\n",
       "        return;\n",
       "    }\n",
       "    fig.close_ws(fig, {});\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (el) {\n",
       "    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (el) {\n",
       "    // this is important to make the div 'focusable\n",
       "    el.setAttribute('tabindex', 0);\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    } else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (event, _name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager) {\n",
       "        manager = IPython.keyboard_manager;\n",
       "    }\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which === 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "};\n",
       "\n",
       "mpl.find_output_cell = function (html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i = 0; i < ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code') {\n",
       "            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] === html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel !== null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target(\n",
       "        'matplotlib',\n",
       "        mpl.mpl_figure_comm\n",
       "    );\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"500\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "ax.set_xlabel('x label')\n",
    "ax.set_ylabel('y label')\n",
    "ax.axis('equal')\n",
    "ax.set_title('Linarly seprable two dimensional plots')\n",
    "\n",
    "ax.scatter(X[T==0, 0], X[T==0, 1], c='red')\n",
    "ax.scatter(X[T==1, 0], X[T==1, 1], c='green')\n",
    "\n",
    "# Hyperplace (X-b)V = 0 -> x1V1 + x2V2 - bV2 = 0\n",
    "x = np.linspace(-3,3,100)\n",
    "y = -(V[1] / V[2]) * x - (V[0] / V[2])\n",
    "ax.plot(x, y)\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train binary classifiers\n",
    "1. Sigmoid binary classifier\n",
    "2. Softmax binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "def draw_training(X, W, _ax=None, _fig=None, colors=['b']):\n",
    "    w0 = W[0]\n",
    "    w1 = W[1]\n",
    "    w2 = W[2]\n",
    "    \n",
    "    #_ax.set_xlim(-3, 3)\n",
    "    #_ax.set_ylim(-3, 3)\n",
    "    #_ax.set_title(label=f\"W: {W}\")\n",
    "\n",
    "    #_ax.scatter(X[T==0, 1], X[T==0, 2], c='red')\n",
    "    #_ax.scatter(X[T==1, 1], X[T==1, 2], c='green')\n",
    "    x = np.linspace(-3,3,100)\n",
    "    if _ax.lines:\n",
    "        for line in _ax.lines:\n",
    "            line.set_xdata(x)\n",
    "            y = -w1/w2 * x - w0 / w2\n",
    "            line.set_ydata(y)\n",
    "    else:\n",
    "        for color in colors:\n",
    "            y = -w1/w2 * x - w0 / w2\n",
    "            _ax.plot(x, y, color)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    _x = np.linspace(-3,3,100)\n",
    "    _y = -w1/w2 * x - w0 / w2\n",
    "    _ax.plot(_x, _y, label='linear')  # Plot some data on the _axes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid classifier training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "/* global mpl */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function () {\n",
       "    if (typeof WebSocket !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof MozWebSocket !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert(\n",
       "            'Your browser does not have WebSocket support. ' +\n",
       "                'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "                'Firefox 4 and 5 are also supported but you ' +\n",
       "                'have to enable WebSockets in about:config.'\n",
       "        );\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = this.ws.binaryType !== undefined;\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById('mpl-warnings');\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent =\n",
       "                'This browser does not support binary websocket messages. ' +\n",
       "                'Performance may be slow.';\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = document.createElement('div');\n",
       "    this.root.setAttribute('style', 'display: inline-block');\n",
       "    this._root_extra_style(this.root);\n",
       "\n",
       "    parent_element.appendChild(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen = function () {\n",
       "        fig.send_message('supports_binary', { value: fig.supports_binary });\n",
       "        fig.send_message('send_image_mode', {});\n",
       "        if (fig.ratio !== 1) {\n",
       "            fig.send_message('set_dpi_ratio', { dpi_ratio: fig.ratio });\n",
       "        }\n",
       "        fig.send_message('refresh', {});\n",
       "    };\n",
       "\n",
       "    this.imageObj.onload = function () {\n",
       "        if (fig.image_mode === 'full') {\n",
       "            // Full images could contain transparency (where diff images\n",
       "            // almost always do), so we need to clear the canvas so that\n",
       "            // there is no ghosting.\n",
       "            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "        }\n",
       "        fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "    };\n",
       "\n",
       "    this.imageObj.onunload = function () {\n",
       "        fig.ws.close();\n",
       "    };\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_header = function () {\n",
       "    var titlebar = document.createElement('div');\n",
       "    titlebar.classList =\n",
       "        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n",
       "    var titletext = document.createElement('div');\n",
       "    titletext.classList = 'ui-dialog-title';\n",
       "    titletext.setAttribute(\n",
       "        'style',\n",
       "        'width: 100%; text-align: center; padding: 3px;'\n",
       "    );\n",
       "    titlebar.appendChild(titletext);\n",
       "    this.root.appendChild(titlebar);\n",
       "    this.header = titletext;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = (this.canvas_div = document.createElement('div'));\n",
       "    canvas_div.setAttribute(\n",
       "        'style',\n",
       "        'border: 1px solid #ddd;' +\n",
       "            'box-sizing: content-box;' +\n",
       "            'clear: both;' +\n",
       "            'min-height: 1px;' +\n",
       "            'min-width: 1px;' +\n",
       "            'outline: 0;' +\n",
       "            'overflow: hidden;' +\n",
       "            'position: relative;' +\n",
       "            'resize: both;'\n",
       "    );\n",
       "\n",
       "    function on_keyboard_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.key_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    canvas_div.addEventListener(\n",
       "        'keydown',\n",
       "        on_keyboard_event_closure('key_press')\n",
       "    );\n",
       "    canvas_div.addEventListener(\n",
       "        'keyup',\n",
       "        on_keyboard_event_closure('key_release')\n",
       "    );\n",
       "\n",
       "    this._canvas_extra_style(canvas_div);\n",
       "    this.root.appendChild(canvas_div);\n",
       "\n",
       "    var canvas = (this.canvas = document.createElement('canvas'));\n",
       "    canvas.classList.add('mpl-canvas');\n",
       "    canvas.setAttribute('style', 'box-sizing: content-box;');\n",
       "\n",
       "    this.context = canvas.getContext('2d');\n",
       "\n",
       "    var backingStore =\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        this.context.webkitBackingStorePixelRatio ||\n",
       "        this.context.mozBackingStorePixelRatio ||\n",
       "        this.context.msBackingStorePixelRatio ||\n",
       "        this.context.oBackingStorePixelRatio ||\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        1;\n",
       "\n",
       "    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n",
       "        'canvas'\n",
       "    ));\n",
       "    rubberband_canvas.setAttribute(\n",
       "        'style',\n",
       "        'box-sizing: content-box; position: absolute; left: 0; top: 0; z-index: 1;'\n",
       "    );\n",
       "\n",
       "    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n",
       "    if (this.ResizeObserver === undefined) {\n",
       "        if (window.ResizeObserver !== undefined) {\n",
       "            this.ResizeObserver = window.ResizeObserver;\n",
       "        } else {\n",
       "            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n",
       "            this.ResizeObserver = obs.ResizeObserver;\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n",
       "        var nentries = entries.length;\n",
       "        for (var i = 0; i < nentries; i++) {\n",
       "            var entry = entries[i];\n",
       "            var width, height;\n",
       "            if (entry.contentBoxSize) {\n",
       "                if (entry.contentBoxSize instanceof Array) {\n",
       "                    // Chrome 84 implements new version of spec.\n",
       "                    width = entry.contentBoxSize[0].inlineSize;\n",
       "                    height = entry.contentBoxSize[0].blockSize;\n",
       "                } else {\n",
       "                    // Firefox implements old version of spec.\n",
       "                    width = entry.contentBoxSize.inlineSize;\n",
       "                    height = entry.contentBoxSize.blockSize;\n",
       "                }\n",
       "            } else {\n",
       "                // Chrome <84 implements even older version of spec.\n",
       "                width = entry.contentRect.width;\n",
       "                height = entry.contentRect.height;\n",
       "            }\n",
       "\n",
       "            // Keep the size of the canvas and rubber band canvas in sync with\n",
       "            // the canvas container.\n",
       "            if (entry.devicePixelContentBoxSize) {\n",
       "                // Chrome 84 implements new version of spec.\n",
       "                canvas.setAttribute(\n",
       "                    'width',\n",
       "                    entry.devicePixelContentBoxSize[0].inlineSize\n",
       "                );\n",
       "                canvas.setAttribute(\n",
       "                    'height',\n",
       "                    entry.devicePixelContentBoxSize[0].blockSize\n",
       "                );\n",
       "            } else {\n",
       "                canvas.setAttribute('width', width * fig.ratio);\n",
       "                canvas.setAttribute('height', height * fig.ratio);\n",
       "            }\n",
       "            canvas.setAttribute(\n",
       "                'style',\n",
       "                'width: ' + width + 'px; height: ' + height + 'px;'\n",
       "            );\n",
       "\n",
       "            rubberband_canvas.setAttribute('width', width);\n",
       "            rubberband_canvas.setAttribute('height', height);\n",
       "\n",
       "            // And update the size in Python. We ignore the initial 0/0 size\n",
       "            // that occurs as the element is placed into the DOM, which should\n",
       "            // otherwise not happen due to the minimum size styling.\n",
       "            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n",
       "                fig.request_resize(width, height);\n",
       "            }\n",
       "        }\n",
       "    });\n",
       "    this.resizeObserverInstance.observe(canvas_div);\n",
       "\n",
       "    function on_mouse_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.mouse_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousedown',\n",
       "        on_mouse_event_closure('button_press')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseup',\n",
       "        on_mouse_event_closure('button_release')\n",
       "    );\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousemove',\n",
       "        on_mouse_event_closure('motion_notify')\n",
       "    );\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseenter',\n",
       "        on_mouse_event_closure('figure_enter')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseleave',\n",
       "        on_mouse_event_closure('figure_leave')\n",
       "    );\n",
       "\n",
       "    canvas_div.addEventListener('wheel', function (event) {\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        on_mouse_event_closure('scroll')(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.appendChild(canvas);\n",
       "    canvas_div.appendChild(rubberband_canvas);\n",
       "\n",
       "    this.rubberband_context = rubberband_canvas.getContext('2d');\n",
       "    this.rubberband_context.strokeStyle = '#000000';\n",
       "\n",
       "    this._resize_canvas = function (width, height, forward) {\n",
       "        if (forward) {\n",
       "            canvas_div.style.width = width + 'px';\n",
       "            canvas_div.style.height = height + 'px';\n",
       "        }\n",
       "    };\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    this.rubberband_canvas.addEventListener('contextmenu', function (_e) {\n",
       "        event.preventDefault();\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus() {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'mpl-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'mpl-button-group';\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'mpl-button-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        var button = (fig.buttons[name] = document.createElement('button'));\n",
       "        button.classList = 'mpl-widget';\n",
       "        button.setAttribute('role', 'button');\n",
       "        button.setAttribute('aria-disabled', 'false');\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "\n",
       "        var icon_img = document.createElement('img');\n",
       "        icon_img.src = '_images/' + image + '.png';\n",
       "        icon_img.srcset = '_images/' + image + '_large.png 2x';\n",
       "        icon_img.alt = tooltip;\n",
       "        button.appendChild(icon_img);\n",
       "\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    var fmt_picker = document.createElement('select');\n",
       "    fmt_picker.classList = 'mpl-widget';\n",
       "    toolbar.appendChild(fmt_picker);\n",
       "    this.format_dropdown = fmt_picker;\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = document.createElement('option');\n",
       "        option.selected = fmt === mpl.default_extension;\n",
       "        option.innerHTML = fmt;\n",
       "        fmt_picker.appendChild(option);\n",
       "    }\n",
       "\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', { width: x_pixels, height: y_pixels });\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_message = function (type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function () {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function (fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1], msg['forward']);\n",
       "        fig.send_message('refresh', {});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function (fig, msg) {\n",
       "    var x0 = msg['x0'] / fig.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n",
       "    var x1 = msg['x1'] / fig.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0,\n",
       "        0,\n",
       "        fig.canvas.width / fig.ratio,\n",
       "        fig.canvas.height / fig.ratio\n",
       "    );\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function (fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function (fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch (cursor) {\n",
       "        case 0:\n",
       "            cursor = 'pointer';\n",
       "            break;\n",
       "        case 1:\n",
       "            cursor = 'default';\n",
       "            break;\n",
       "        case 2:\n",
       "            cursor = 'crosshair';\n",
       "            break;\n",
       "        case 3:\n",
       "            cursor = 'move';\n",
       "            break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_message = function (fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function (fig, _msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function (fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n",
       "    for (var key in msg) {\n",
       "        if (!(key in fig.buttons)) {\n",
       "            continue;\n",
       "        }\n",
       "        fig.buttons[key].disabled = !msg[key];\n",
       "        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n",
       "    if (msg['mode'] === 'PAN') {\n",
       "        fig.buttons['Pan'].classList.add('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    } else if (msg['mode'] === 'ZOOM') {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.add('active');\n",
       "    } else {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message('ack', {});\n",
       "};\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function (fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = 'image/png';\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src\n",
       "                );\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data\n",
       "            );\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        } else if (\n",
       "            typeof evt.data === 'string' &&\n",
       "            evt.data.slice(0, 21) === 'data:image/png;base64'\n",
       "        ) {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig['handle_' + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\n",
       "                \"No handler for the '\" + msg_type + \"' message type: \",\n",
       "                msg\n",
       "            );\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\n",
       "                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n",
       "                    e,\n",
       "                    e.stack,\n",
       "                    msg\n",
       "                );\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "};\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function (e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e) {\n",
       "        e = window.event;\n",
       "    }\n",
       "    if (e.target) {\n",
       "        targ = e.target;\n",
       "    } else if (e.srcElement) {\n",
       "        targ = e.srcElement;\n",
       "    }\n",
       "    if (targ.nodeType === 3) {\n",
       "        // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "    }\n",
       "\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    var boundingRect = targ.getBoundingClientRect();\n",
       "    var x = e.pageX - (boundingRect.left + document.body.scrollLeft);\n",
       "    var y = e.pageY - (boundingRect.top + document.body.scrollTop);\n",
       "\n",
       "    return { x: x, y: y };\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys(original) {\n",
       "    return Object.keys(original).reduce(function (obj, key) {\n",
       "        if (typeof original[key] !== 'object') {\n",
       "            obj[key] = original[key];\n",
       "        }\n",
       "        return obj;\n",
       "    }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function (event, name) {\n",
       "    var canvas_pos = mpl.findpos(event);\n",
       "\n",
       "    if (name === 'button_press') {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * this.ratio;\n",
       "    var y = canvas_pos.y * this.ratio;\n",
       "\n",
       "    this.send_message(name, {\n",
       "        x: x,\n",
       "        y: y,\n",
       "        button: event.button,\n",
       "        step: event.step,\n",
       "        guiEvent: simpleKeys(event),\n",
       "    });\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (_event, _name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.key_event = function (event, name) {\n",
       "    // Prevent repeat events\n",
       "    if (name === 'key_press') {\n",
       "        if (event.which === this._key) {\n",
       "            return;\n",
       "        } else {\n",
       "            this._key = event.which;\n",
       "        }\n",
       "    }\n",
       "    if (name === 'key_release') {\n",
       "        this._key = null;\n",
       "    }\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which !== 17) {\n",
       "        value += 'ctrl+';\n",
       "    }\n",
       "    if (event.altKey && event.which !== 18) {\n",
       "        value += 'alt+';\n",
       "    }\n",
       "    if (event.shiftKey && event.which !== 16) {\n",
       "        value += 'shift+';\n",
       "    }\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function (name) {\n",
       "    if (name === 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message('toolbar_button', { name: name });\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "\n",
       "///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n",
       "// prettier-ignore\n",
       "var _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";/* global mpl */\n",
       "\n",
       "var comm_websocket_adapter = function (comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function () {\n",
       "        comm.close();\n",
       "    };\n",
       "    ws.send = function (m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function (msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data']);\n",
       "    });\n",
       "    return ws;\n",
       "};\n",
       "\n",
       "mpl.mpl_figure_comm = function (comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = document.getElementById(id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm);\n",
       "\n",
       "    function ondownload(figure, _format) {\n",
       "        window.open(figure.canvas.toDataURL());\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element;\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error('Failed to find cell for figure', id, fig);\n",
       "        return;\n",
       "    }\n",
       "    fig.cell_info[0].output_area.element.on(\n",
       "        'cleared',\n",
       "        { fig: fig },\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function (fig, msg) {\n",
       "    var width = fig.canvas.width / fig.ratio;\n",
       "    fig.cell_info[0].output_area.element.off(\n",
       "        'cleared',\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable();\n",
       "    fig.parent_element.innerHTML =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "    fig.close_ws(fig, msg);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.close_ws = function (fig, msg) {\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function (_remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width / this.ratio;\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message('ack', {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () {\n",
       "        fig.push_to_output();\n",
       "    }, 1000);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'btn-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'btn-group';\n",
       "    var button;\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'btn-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        button = fig.buttons[name] = document.createElement('button');\n",
       "        button.classList = 'btn btn-default';\n",
       "        button.href = '#';\n",
       "        button.title = name;\n",
       "        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message pull-right';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = document.createElement('div');\n",
       "    buttongrp.classList = 'btn-group inline pull-right';\n",
       "    button = document.createElement('button');\n",
       "    button.classList = 'btn btn-mini btn-primary';\n",
       "    button.href = '#';\n",
       "    button.title = 'Stop Interaction';\n",
       "    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n",
       "    button.addEventListener('click', function (_evt) {\n",
       "        fig.handle_close(fig, {});\n",
       "    });\n",
       "    button.addEventListener(\n",
       "        'mouseover',\n",
       "        on_mouseover_closure('Stop Interaction')\n",
       "    );\n",
       "    buttongrp.appendChild(button);\n",
       "    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n",
       "    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._remove_fig_handler = function (event) {\n",
       "    var fig = event.data.fig;\n",
       "    if (event.target !== this) {\n",
       "        // Ignore bubbled events from children.\n",
       "        return;\n",
       "    }\n",
       "    fig.close_ws(fig, {});\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (el) {\n",
       "    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (el) {\n",
       "    // this is important to make the div 'focusable\n",
       "    el.setAttribute('tabindex', 0);\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    } else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (event, _name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager) {\n",
       "        manager = IPython.keyboard_manager;\n",
       "    }\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which === 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "};\n",
       "\n",
       "mpl.find_output_cell = function (html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i = 0; i < ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code') {\n",
       "            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] === html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel !== null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target(\n",
       "        'matplotlib',\n",
       "        mpl.mpl_figure_comm\n",
       "    );\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA4QAAAH0CAYAAABl8+PTAAAgAElEQVR4XuxdB7gWxdU+cL0KKBelqDTFKHZN1NhFwRIUxYJYQIxEYolGQcVeAEssIIImGrtG5VoQC7/XLih2Y4kaGxoUQVSKgvRP4H/e/e5c9tu7ZWZ3ts2efR6exPvNTnln9p3zzpw502TVqlWriB9GgBFgBBgBRoARYAQYAUaAEWAEGIHCIdCEBWHh+pwbzAgwAowAI8AIMAKMACPACDACjICFAAtCHgiMACPACDACjAAjwAgwAowAI8AIFBQBFoQF7XhuNiPACDACjAAjwAgwAowAI8AIMAIsCHkMMAKMACPACDACjAAjwAgwAowAI1BQBFgQFrTjudmMACPACDACjAAjwAgwAowAI8AIsCDkMcAIMAKMACPACDACjAAjwAgwAoxAQRFgQVjQjudmMwKMACPACDACjAAjwAgwAowAI8CCkMcAI8AIMAKMACPACDACjAAjwAgwAgVFgAVhQTuem80IMAKMACPACDACjAAjwAgwAowAC0IeA4wAI8AIJITA119/TZtssgkNGzaMhg8fHqrUgQMH0r333kurVq0K9T6/lE0EunfvThgf+JfHp0uXLoR/kydP9q0+fu/RowfdfffdhLHMDyPACDACjED6CLAgTL8PuAaMACOQEgJNmjSRLnnatGmWwRvlYUEYBT2z340qCCG08G/IkCG07rrrVoDl95suVFkQ6kKS82EEGAFGIHkEWBAmjzmXyAgwAhlB4P7776+oyaeffkp/+9vf6IgjjqA+ffpU/Ia/rb322pFqjl29ZcuW0RprrGH9C/OUSiVasWIFNWvWLMzr/E5GEYgqCLHjPGLECHJbuPD7TRccLAh1Icn5MAKMACOQPAIsCJPHnEtkBBiBjCIg3NlkXDoXLlxI66yzTkZbkk610sJk0aJFkcV6OoitLpUFYTw9kNaYjKc1nCsjwAgwAvEgwIIwHlw5V0aAEcghAl6CUOx+3HDDDXTBBRfQm2++Sa1bt7Z2Y3755Re69tpr6fnnn6evvvrK+u/OnTtT37596bLLLqMWLVo0IOHmMmr/2+9+9zu64oor6L///S+tt956NGDAALr66qsrdhO9zhC++uqrdOGFF9K///1vSxz16tWLrr/+elp//fXphBNOoHvuuce3R/D7n/70J6sdyAtnvL7//nvafPPNrXz79+9f8b4fJkj42muvWW0BVtgVRT5//vOf6a9//Ss5XXVl627HaquttqLrrruOPvnkEzrmmGMa2vfCCy9Yf3/77bdp6dKlVrmnnXYanXrqqRX1f/311+nKK6+k9957j3766ScL7+22287qs27dullp582bZ6V54oknaObMmdS8eXOrb1HexRdfrBVPL0EogyPeffnllxvVB32IfvX6TZzhmzVrFl1++eX01FNPWX3etm1bOuSQQ6y2Y/zYH+yiDx061MoTu9z77LMP4bvYd999lc8Q/vrrrzR69Gjru9lwww3p9NNPp/POO69ROzCmr7rqKpoyZYr1fWHs/fGPf6Tzzz+/4tsQGL700ktWPvhf9K04b6vSzhzSF1eZEWAEGIHQCLAgDA0dv8gIMAKmIeAnCJs2bUo///wzHXXUUbTTTjsRdh7OPvts+uyzz6wgGfj7ZpttRlVVVZaxPH78eDrggAPo2WeflRKEO++8sxVQBMKlffv2lgjBuzCEL7roooY83AQhxA0McuxYQvy0a9eOJk6caAmad999V0kQ7rjjjoQdN5QD4QZR8fnnn9Mdd9xBgwYNaqgHjHIvTOrq6uiwww6zhMUpp5xiia1HH33UMuhPOukkuu222xryUam7EIS//e1v6dtvv6W//OUvtPHGG1NNTY0l0pAv8Nttt90st1/gAYH72GOPWSJm5MiRVrloD9oJEYI24X9//PFHeuONN2i//fajM88800qH///KK69YbUCZS5YsoS+++MJ6H/n6PUJgy+LpJghlcURdbrnlFqudEGfAHc8ee+xhiS2v337zm9/Q9OnTaffdd6fly5dbWGy66abWOzfffDNtsMEG1gJDq1atrPywAPL73/+eFi9ebI0zvP/iiy9aYwx/22abbaSDyuyyyy4W5lgkQP5w337rrbfogQceqFh8AAboS3xbWCDBQgz66b777rPcuh955JGGbgCGH3/8sSXc99prL9p7772tMrDjr9JO03iN28MIMAKMQBACLAiDEOLfGQFGoDAI+AnCb775hu666y5rF83+wJCGMHKeCbz00kutHRYYuTB+8fjtEGInETuDInANdjWwYzV37lzCzoZ43AQhBBB2uj766CPaYostrKR4H7uUEyZMUBKEG220EX344YcNImD+/Pm0/fbbE/4Xu2TiHCXq6YYJzjdCVECMYveuU6dOVn3wd+w6PfPMM9buIcQKHpW6C/yqq6sr2op8gBEiuEI81NbWVvTR4MGD6e9//7sl5lC3G2+8kfA37CJCiLs9aC+Cs0D4/OMf/1D+BoQglMXTKQhVcQx7hvDQQw+1BNb777/f0FdoLIQg+uaSSy5piIiLXWJg+/TTT9OBBx7YgAl2fYERdgtlo4xi0QPjQwTAgaCEuIfwQ33wYIcXf8OYxm6f/RuD8MWCzKRJkwjY4RE7pdjlxXlK+6PSTuXO5hcYAUaAEcg5AiwIc96BXH1GgBHQh4CfIMSOIHYbIP68HrjAwaUNxjzEHQxUiI8zzjjDesVPEMLYxu6I/cF7EDLIU5xXdArCH374wdrhwm4JduHsD8QojHoVl1G4qMIt1v7gb9ilxK4jRB0eCEI3TN555x1LAKPuaLv9wQ4hdm3OOeccGjVqFKnWXeB3+OGHW7th9uemm26ydvawq4pdOfvzwQcfWLu1t956K5188sn0r3/9y8IEbYJwdwvQA6GPnautt97awlU1wqwQhLJ4OgWhCo5oaxhBiB3vNm3aWO6XYvfUjht22bAjh13clStXWnhAUANP+/Pdd99Rx44dlQQhsMfut/3p3bu3JQbnzJlj/RnjDULu9ttvJ/S5/cFCyZZbbmm5MyMQFB4hCNEusauJv6u0Ux+bcE6MACPACOQHARaE+ekrrikjwAjEjICfIMRZKuwouT1wr/vnP/9piUAYzvYHOxXYscDjJwixE4Mzd/ZHGPl4DzsleJyCEGf04PJnN4xFHmKXS0UQPv7445a7p/2B+yoM8jFjxlg7a3ggkNwweeihh+jYY4+18ICrpf2BYQ73UeziYedSte4Cv3PPPdc6J2h/sJMH10i/B+fkIAAh9iA0IB4hBiGa//CHP1j1xi6jeOCCCpGJM5A4swi3XGADcRn0CEEoi6dTEKrgiLqEEYQYz7vuuqtvU+AWChdSnC3Erh4wcu7AIgPs9OEMrOwOodMF2W1so49xTtDvOfHEE+nOO++0kgBD7Dpi4cb+qLQzqF/5d0aAEWAETESABaGJvcptYgQYgVAIBAWVcTN2EbgF59MgKHCOsEOHDrTmmmta7pUQb/aIpUFBZZyX1bsZ+U5BiB0VuF/qEoQQfxBL9geiBiJu7NixDefrvK4ZePDBB6lfv35SglC17n73OOI8IUQozjwKN1XnIIC4wT/xwC0SohDnBEXgFbyP+osHQgjBVpAGAWuwGwZxjF1Dv91iIQhl8XQKQhUcwwpCsYOM9kJYuT04j7fnnns2CEKkHTduXKOkqoLQ7WJ659hGsCbsVl9zzTXWuV23B98bdnGFIMQYwT/7o9LOUMTBLzECjAAjkHMEWBDmvAO5+owAI6APgTCCcIcddrBc0rCLYhcIOCt30EEHxS4Ihdul2HVzM4RVdghlXRy9BKGfqyOiiSKCpwjwolp3P0EozpQ9+eSTBNdD1QdnECE6cD4RZyPdHuz+IigOzpLiTBuCCXk9cbqMOnFEHbATjQUEt3sIvX6D2yUCEGHsON2Nne3S7TIqIwjhFgxXaLvbtV+/ekVqVWmn6rjh9IwAI8AImIAAC0ITepHbwAgwAloQCCMIEXURAVSmTp1qRRjFg7OE+++/v7XrFPcOIcqD2x+CgjiDymDHEoa+iiD0CoKyYMECmjFjRkVQGYhC566pCIaCcP9w38PZMjwQFBBqiBppDyqjUnc/QYi6ISAJIl3irKL9ug+UD/dZuIeutdZa1hk1EYnTPnAgCBF4Bmc2EeQEjzMfcVYRUWSPPPLIQEEoi6dXUBlZHMVONYILYZHC/vj9JgL9YKxiJ9D+IDARsIJoxHPcccdZu4M6gsrICEL0AVx4seOO8e3sM0R9xbfWsmVLq35+dzmqtFMLmXAmjAAjwAjkCAEWhDnqLK4qI8AIxItAGEEIdza4a+JcGXYzIJxgNGOnCS6JSQhC7BjhigQYxrjLDQY8dsogJlAHuOLBAPd7nNckwIUQgsDv2gk3QYgyxHUJqIe4dgJnBiE6nNdOqNTdTxCiXNQV1xjAZRSBUnDucvbs2ZZQhtsrBCrqPGTIEHruueesADnizCBEDoKYAD8E8kHgFETNxO4ZRCaCr+CKEZxTxDUXyMseuMSJrSqeftdOyOKI3deePXtaLq8QvhDbaJ/YUXT7Ddd3IHgMdkiPP/54KyAPxPv//vc/6+oT4Chcme3XTgAnce0ExpjqtRMyghCYwqUXLrqIbosx2bVrV2tHHn2BMYVdRHuUUTeXUeSj0s54WYZzZwQYAUYgewiwIMxen3CNGAFGICUEwghC7Igh+AUCW8DoRMRP3ImH6ylwtikJQQi4cMYNkRtxJxyMZ+zGoWwIApyvQ+AbGUGIO+2wwwaDHS6dMMCRr9fF9F5BRCBC7BfTIx+IQUQfdV5ML1v3IEGI9mH3ERFM8b8QDthVwrUFEH8QMdglRJ0h7HC2DG3E37C7CMGBKKTY6YWbIa4NwbUGcCHFFQg4r4brFnCuDRfU68TTa3dLBUdE20QkVZxfxbi0iy6/37ALiPN6EIC4rw94oH0IogNBL87oob32i+mBE+od9mJ6LFTYH7crVfA77hbEwgv6AgIfgYkQ7RQu2ehTRELF47dDiN9V2pkSBXGxjAAjwAikggALwlRg50IZAUaAEYgfAezc4J49t3OBztLFjpb9Xrf4a+hdgkrd06ynV9lZwzOLGHGdGAFGgBFgBLKBAAvCbPQD14IRYAQYgdAIwLUTVyPY79PD38QZQuwaOu/my4og1FH30MDF+CILwhjB5awZAUaAEWAEtCLAglArnJwZI8AIMALJIwB3RpyXGzBgAG2++eaWqyTc/3Ctg9uF9241TEvA6Kh78ogHl5gWnsE14xSMACPACDACjEAlAiwIeUQwAowAI5BzBHBeDOfzcBYPwUHw3zgThyAh55xzDq2xxhqBLUxLwOioe2DjUkiQFp4pNJWLZAQYAUaAEcg5AiwIc96BXH1GgBFgBBgBRoARYAQYAUaAEWAEwiLAgjAscvweI8AIMAKMACPACDACjAAjwAgwAjlHgAVhzjuQq88IMAKMACPACDACjAAjwAgwAoxAWARYEIZFjt9jBBgBRoARYAQYAUaAEWAEGAFGIOcIsCDMeQdy9RkBRoARYAQYAUaAEWAEGAFGgBEIiwALwrDI8XuMACPACDACjAAjwAgwAowAI8AI5BwBFoQ570CuPiPACDACjAAjwAgwAowAI8AIMAJhEWBBGBY5fo8RYAQYAUaAEWAEGAFGgBFgBBiBnCPAgjDnHcjVZwQYAUaAEWAEGAFGgBFgBBgBRiAsAiwIwyLH7zECjAAjwAgwAowAI8AIMAKMACOQcwRYEOa8A7n6jAAjwAgwAowAI8AIMAKMACPACIRFgAVhWOT4PUaAEWAEGAFGgBFgBBgBRoARYARyjgALwpx3IFd/NQIDBw6kyZMn09dff51pWJo0aULDhg2j4cOH+9bznnvuoT/96U80bdo06tKli2datHv8+PG0cOHCwHYjn+7duxPyNvFB//fo0YMmTZpktTONR5SLuogHY/L000+nN954g3766ScaPHgwDRkyhDbZZBO6++67CX3IDyPACMSDwE033URjx46l6dOnU6lUsr7Bm2++mbbeems6/PDD4yk05lzBM3PmzKGPP/44sCTZOScwo4wmkJ0r46y+29z6/vvvWzz/wQcf0IIFC+iGG26g3/3ud6nPUXHiwHnnFwEWhPntO665A4GvvvrKIt0ddtgh09i8+eab1KlTJ+uf3yM7yakIQkxQNTU1tOmmm2Yao7CVy4Ig/OSTT6zqw9gUzxFHHEFTpkyhO+64gzbccENq37699b/oD/RFu3btwjaZ32MEGAEfBGCMY07485//TCeccAKtscYatPPOO1OrVq2ob9++uV0cUxGEsnNOXgeS7FwZZ/vc5laMu0WLFlmLEeutt561sNuiRQvCHIH5AXMxP4xAVhBgQZiVnuB6MAIOBGQnORVBmBbIixcvtibCuJ8sCEK3Nnbt2pXwr66uLjYIsPOBnQAYvPwwAoxAGYEHHniABgwYQG+99RbtsssuDbCss846hRGEaY2FpHhfdq5MGofq6mo66aSTrN3ouJ4lS5ZQs2bNLO7nhxGIggALwijo8buJITB79my6+OKL6emnn6Yff/zRWlnbfPPNacSIEbT//vtb9XBzGf3555/pnHPOoccee4yWL19O++yzD8F9CLsydrdNuG8ir//85z90xRVX0HPPPUdVVVVWntdddx1h9xFufq+99hq1adOGTjvtNDrvvPMq2g93pIsuush6d/78+fSb3/zGWpU+66yzqGnTpg1p3dx3sIKLer777rvUunVrayUb75988snSLqMweM4880xCXhBfxxxzjFV3uxBzurUIATVu3DjL9Qjui3A9heH0j3/8g7bYYouGej///PP097//3aojXJWww7nffvvRVVddRW3btm1IJ7BEur/97W/04osvWhMW6vLHP/6RXn/9ddp9990rsLv88sst3L/55hvq0KGD57j67LPPrH566aWXCH27wQYbWK6ht99+O6211lqWy7DTZfTf//43jRo1ysLlhx9+sN5B+ddccw1tvPHGDWXBeLnsssvo0UcfpVmzZlm4oQ/QL/369bPS/e9//7P6+JVXXqG5c+fSuuuuS9tuuy1df/31lisQHrvLqKiPs0FwA8bj5jI6depUa2y+8MILDePor3/9q+VyKh6R77/+9S/LHenBBx+06oyV5y233DKx75ILYgTiQECG71HuXXfdZe2+fP7559b3Cn4H52y11VYN3+LLL79cUUVw67333tuo2ngX35UQF+Ct2tpamjBhguVmCtfSW265hX755ReLZ8HzzZs3p+OOO46uvvpqgvEvHnAUFn/wLf/666+02WabWd/viSee2GC4v/rqqxZXYV4BP4lHlA9vgkGDBnnCK3YIUadzzz3Xmrswd6AMcDDmL/E45xxRBnj04YcfpkceeYRWrVpl4QeOt3PwQw89RHfeeSd99NFHFudiDjnssMPo0ksvpbXXXruhDLEwCbd4cCb+F9zYq1cvi7PBeZ07d65oD+r6xBNP0MyZM605wuvB3HbllVdacwd23Dp27Ei9e/emMWPGWK+4CULZ+UpmrGH3D+19++23LU6GDYDdv1tvvbXB08c+t4r6ONsDjL0WLTFPYR7EuMBchDF84YUX0tFHH91obDz77LMW50+cONGai4UojONb5DyLgwALwuL0da5beuCBB9J7771niQ8IQUxM+G+4XUD44HEKwpUrV1oTHIgWE+SOO+5oTVL333+/NVG7CUIIIKwm77bbboQJBSIGxjiMc4hAkDTEE4QThEOfPn2ssjGpYIKA6ISwweTwf//3f9bk+pe//KVihdA5OcOIhwsT3sGkA8MGK4qffvqpdeZF5gwhDBe4IJ5yyilWXpg4MYECN0wa4vEShPj7nnvuSf3797fcbs8//3xLYKEOwrD45z//aeG+zTbbWO5WOBc3evRoWrp0qWUsCINICEKIrWOPPdYS7JjEDzroIEuAQUSiD8QDgwl/R/kwTrweGDx77bWXJT4vuOACa8cNIujJJ5+0XDFbtmzpOtnifCXqh/6BwYR3YEQBd/wTYvbUU0+l++67z8JNuPpAJMPowRjAA7G1YsUKa+xstNFG1mQMrA855JAGIWgXhMASZcBlFIsQwvBD/qiHUxAi7R577GHlDSMPfYrJH2dPIFZRLh5hVMAwgriF0MaiA/4/2sgPI5BnBGT4HiIMizNYrAFnY4EG3IP/feeddyx+wPcEbsQ3Dc7G9wv3bPD1vvvuawkycC4eLDJiPhHGPL5N8HvPnj0t126UBeMc4hN/33XXXa154dprr7UWhM4+++wGyHH2u1u3btZ3jAeLURCq4C18x+LBu/gbRNGhhx5K//3vf63FOOQPLvJ7wDPgJ/A08sDc9dRTT9GNN95oiU/MPeLxEoRY8Dr44IMt/vr2228tzsHCFoSieIAddlMx74ILsSiHemPOsKfD/Iu5EZyEeQjtALcjP+AwdOhQqx/EM2/ePEtMgVsxz3o94D+IP8y9wBh5Ye6BIEff4nEThLLzVdBYw9yF+QnjAYvAWFD8/vvvrXPqWBgQiw/2uRXjC4vI4GO4JUMg44Fd4SYIkRfqgTEFPDC/QvChXfYz5qKdwBj9hnkF9cNihX0BIM/fPtc9PQRYEKaHPZesgACMfey2wTD2epyCECu0IE0Y/zD2xYOdIay8uQlC58QOwx07MFglBvniwSSHFVRM+BCFeJAf8nW6JUFEYmLCJIoJFY9zcoZogqiB8MNkgweiA6ureE9GEGLFGyvlmKDEAwMEu6pYcYTYwuMlCLGKC2NCPFgxhvEDAY1JzPlgpRN1/O6776zJUhg0SCcEIQwfrAzbH/wGQw7Gx/rrr2/9BBEIUY+V/L333tuzfyEksQjwxRdfeJ65k3EZRb0hYoE1MBKYbbfddtZKPnaT3R4YmhCPWJXGqr7X4xZUBrijP7FIIB4YNU5BCKMARiH+2c+XnHHGGZboBd44iyLaCbycOyCeFeMfGIGcIBDE91iYAgdD0Nl5C7wCIXjkkUdarqJ4hBENkfj73/++AQEvl1GRHt8cxJV4wP+PP/64tQgGrw/xYI7AYgw8ItweLEziH3gPHA2xINz7wKMQY1hUAk+Dc5EWi5j23Te3fMEz+Pbt3It08CrBjh7mDSFIvQQh5id4gohn5MiRlujBYhUWo7x4H/XFYisW6bbffnsrGeZfzEPYtYUgtj/4Dd496J8111zT+gkiEPMmhJNf0DRwMh6IX69dxCCXUb/5KmisoV8xbtD32Bn1etyCygB3pzh3m6MgKrHbjB1Iu8s/hDDKnzFjhjXGRDuxAOi2y52Tz5urmVEEWBBmtGO4WpUICDGAlTbsOO20004VLjpiQrJHGcUuFyYdGPL2XRO4JYK83QQhVn+FcEOe2DHDSh1cOOyTEXZxsBuIiRsPVvbgaglD3v6A4PGbXZQ6J2cIE6ym2nfykIcQVrKCELtVcGURjxAc2LG85JJLrD97CUKIVqzqigc4YDUdbRc7sHDVhciDAQZhAsNFPBDDwNteb7uxINLBZRNGCvKBWMUDUQMD78MPP/Qc9sAfAgkuVHDT8XrcJlv0CzCAeAcmEITiwUIB+gYP8oYRiahwYrUWk7R4YFTA2ISYxEo1jNHf/va3Fe7ASBtWECJfGCfYUYbRaX+wWw3RjkUO7LSKdjoXATyB4R8YgRwhEMT3EBf4HrCYdNRRR1W0DH/HwhF2cfCEFYTYmfrDH/7QkDd2CCHq3OYI7FaBf8WDnTMsNkGEwkvA/qBeYuEPf8f8BFEphCIWFbE4FfSAZ9BOZ/6CG7DDiJ1TPF6C8JlnnrF2QMWDNoP7sKOJeQsP3OQxf6BNmAPAg+Kxzw9CEMKl0hksBTus8NCBZwhcbDF3wGMC7cRiqNeDxT/sfAJLiEevx00Qys5XQWMN7cHCHRYwMTdgvrIHDBN1CisIv/zyS2tegfeIc6ERRyEg2rHTDdEo2ulcBAgaK/w7IyCDAAtCGZQ4TeoIYLKFuwlW6SDosLqLFVsIPrGS6dwhxI4iVtFw/sP+wPCGoe8mCDEp28/DeQVscUZ4wyomJgS4ENkfrOzh3ATqLgSQc3LGiiBWVEH+9gciDeJARhBCyHi1E5OY2Fn1EoTYEYRri3icu1eYwDGhQwjCxQoTOVaw8XfsILphiQnZLXomVjdhtKBdENAQVRB5WNn2enDGBO5FOGMhXLzc0roJQrhi4TwQ3oM7LYwV9AEMR/wTV3DA9QbjCWdmYPRhAQDGElbNMWHjwdhDHSCKIW6x0AADB67MEHN4wgpC0Ua/jw1nBo8//vgGQehmEKf+sXIFGIGICATxPYQFvgNE7oUbuf1x8n5YQejcURQLdEFzBBYBsWAIHgCngbewK4a5CzzhxudwE8ROHeY0eKPIPMgfnIHjD/YHXiUQD3ZPBi9B6Gyjkz+xmAZBBi6EIMNiKY40YKcPbq12d0bMlZhHwKNuD4QUvGuwuwgRiN02p+h2vocz++hfwXteuDgFocp8FTTWUCaOHKDvIPxxZQmiRCNYDISyOCoRVhCKNvr1Oc6swyNJtBNjDHMZP4yATgRYEOpEk/NKBAGcq8OEgnMTmCywyonHKQi9dgiF2NEpCIN2CO07cFncIQwShNi9g3DDhISgDOIRq5syWIp3hAsOduzQdygbwtnPRQqH5iG4VHcIsboLF0vUT5y/Qz2WLVtmlYcVdLc7GSH2sAuBMYbAMTCynA9WryHIYCjCCEUf4wkrCMUuKAxdewAZe7lYqcYusDDcnP2WyAfIhTACCSLgxvdBO4TYkYLbI56kBSG8B+B1AOFg9yqBeHAThNj9x64c3BJh6OPMM1xegx4dO4RBglAIN/ANXETFg4XPAw44oJEg9LsPF79hNxf8D17F4hp41S86ZtgdQpX5yo6zl20h0mB3FHljTEFwY8cYbcETVhAKbxwIbhGTwNn3EOWY/7zGctBY4d8ZARkEWBDKoMRpMokAVlOxuoadKDxeZwoeTBwAACAASURBVAgRoAU7beLxO0MYtPor8nDuEAp3Ikx22EkTD1Z+UX6aZwjtq+hhdwixQoqzIjjEjzOP4kEQAri6qAhCvIszjVgtxrkQrKL7nQ0VZcG1B4YejAT7Lq59cDpXuOFOhQP69okb6bEaj76BuHUThCJPnBXCxI9Vb69rM+DuhVViGHN4wgpCvAsjC2MQeYmzNm4fHwvCTFISVypGBOx8L84Q4nuB+5x4sLAEbw14O4jAVV5GNBZWcPwAHgH2xyu97A4hjjXcdttthKApYvcIC1pwwXcGCYNoRdAVBOqCMIQgwLlA8BwWf/weHWcIgwQhjjHAw8J5lhzCDgLPuUPoJwjhqo8gNviHNoLz/c5ii7ajPyEaRQAdN0ycO4Qq85Vbfk7bwi0NFhox/kQgtLCCEHlj5xVeKPbzsH7tdPZbjJ8dZ10gBFgQFqiz89pU7PLgvBbO82FSxUoZCBEugJhARfAAtyijcFPBOQsIFpw7xMQG9xPsbCHgiYj4JjvZCwydglBEGcWkB5dCBFoBueOKC4hR+8F95w4hJjqcIYQBgPpAeCA9zg3ANUfGZdQryigmLPvdd2EFIdxR4YYEEQdxBVdJGAswYiDQVAWhCCQDLLBCKlwy/caoiDKKsxxYlYWhgJ08rGLD5dQryihWtoGxiIwHYwRBF2CkwW1JCELs8iLAA4QvJntEWIWbLyZruDlhZRgiEsYQ6gvBhnM1Qmxi9T+qIESfY9cb+WPcoL8Q5h7jFXiLqH4sCPPKZlzvIARk+V5EGcWOOiKN4iweOB1cLKKMoiwvgQcOxzeOYE1wAQR/YCcmqiDEN4rFK4hSLHahXlg0w+IU3DsFn2OuQDosFiJwGY4+YFcRAhFnDBFkxm9RyB5lVLhzgutxrjgosrVXG50Laqg7uAjnvsHxELiYb7HwibaoCEL0BVzy4bkD7wy4u2KxLugRUUYhmrFAh7pAWOPvzsBBAlvZ+UpmrCEQGBZ1EckTYha7hHDrhUcIhD9cR/FEEYSIMoqz4ZirYMcgiigWFDA+Yb/AE8RvLAdhyL8zAjIIsCCUQYnTpIoA3PtwDg5GOdw9QfaYFLBThYhoYufG7R5CTLD2ewixM4UAIzj3Zg/IEVUQAiBMUpiYMVFh8hf3EMKFKOgeQrQN9cTKMMQIjBxMxCr3EELsImImghLgjCSCweD8m90VM6wgRPswOWFFFwEHcO4Rq+uIyiqMBWCIxwtL+yBCQB6c5YPQh/uX7IM6wDCB0QWhBCMK4eMxOXvdQwjDA/XGOxC0GAMw0BCBFkaVEIToO7hCIeod3DcxKUMwQhRiNwE70TBm0H4IdYhZ9DHOfyIioQj7HWWHEDhgjGOMYhyhTLisYizgvKM4h8qCUHbEcLq8ISDL92gXFnYQCRSiCpyHbw8BSOxBP7zEDxaY4JoNzsX37ryHMOwZQtQLQgkLUPiWwSMQDVjIgsu7EC1wIYWoxaIaOEw84HEsZKJu4p49tz4Ui5JYPMSVDtgVw0Id+AjC2B6tMuwZQpQr7hUEXphLwIkIdAJPGFVBKAK62YN5yYxPcC4WWjFPIgYAzmVi51IE33ILKiMzX8mMNSxYAk94beAMPUQ6Fg6Agf34RBRBCAyw4IhFRXA77BbMORjHiDwrAr6xy6jMaOE0YRFgQRgWOX4vtwjgriQEAoG7KQ7/85M8AsIVCbuoEDr8MAKMACPACJiNADxmsGgJjw3s+PHDCDAC2UGABWF2+oJrEgMCcKXEDhGiYmKXDiuN2DXDuS++vy0GwAOyhEskVomxY4fVZrjD+AUVSL6GXCIjwAgwAoyATgSwC4udUex0wUMDEVf5YQQYgWwhwIIwW/3BtdGMAPz/4cKIM1gIDIKzIjgLgGsgnHclaS6as3NBAG5O2JmFuxGuBMGZUH4YAUaAEWAEzEUA7pS4fxFXJ+B+RLdL781tPbeMEcgHAiwI89FPXEtGgBFgBBgBRoARYAQYAUaAEWAEtCPAglA7pJwhI8AIMAKMACPACDACjAAjwAgwAvlAgAVhPvqJa8kIMAKMACPACDACjAAjwAgwAoyAdgRYEGqHlDNkBBgBRoARYAQYAUaAEWAEGAFGIB8IsCDMRz951nLlypXW3Ti4VJejNea8M7n6jAAjEIgALobGHZQdOnSouN8z8EUDEjDfG9CJ3ARGgBGQRqDIfC8NkqaELAg1AZlWNjNmzKDOnTunVTyXywgwAoxAKgh8++231gXVRXqY74vU29xWRoAREAgUke+T7n0WhEkjrrm8+fPn07rrrmvd8dO6dWvNuSefXalUoueee47+8Ic/UHV1dfIViKFEblMMoMaQJfdTDKDGkOW8efNok002oZ9//platWoVQwnZzdI0vgfS/N1ld7yJmnEfZb+PTP2Wisz3SY86FoRJI665vAULFlhG0Zw5c6hNmzaac08+O0w8dXV11KtXL6MEIbcp+bGkWiKPPVXE0kk/d+5catu2LUEcFe0uUdP4XhixzI/pfEuypTI3yiKVbjoT+6nIfJ/0aGJBmDTimsszzUAwkdC4TZoHfUzZcT/FBKzmbItsIJjG9ywINX8cMWXH3BgTsJqzNbGfisz3modHYHYsCAMhynYC0wwEEwmN25Ttb0jUjvspH/1UZAPBNL5nQZiPb465kfspLQSKzPdJY86CMGnENZdnmoHAE4/mARJTdtxPMQGrOVsT+6nIBoJpfM+CUPMHH1N2JvIItymmwaI52yLzvWYoA7NjQRgIUbYTmGYgMElne7zxblo++sfkfiqygWAa37MgzAef8LzM/ZQWAkXm+6QxZ0GYNOKayzPNQOCJR/MAiSk77qeYgNWcrYn9VGQDwTS+Z0Go+YOPKTsTeYTbFNNg0ZxtkfleM5SB2bEgDIQo2wlMMxCYpLM93kzeeeKxl4+xV2QDwTS+Z0GYj2+OuZH7KS0Eisz3SWPOgjBpxDWXZ5qBwBOP5gESU3bcTzEBqzlbE/upyAaCaXzPglDzBx9TdibyCLcppsGiOdsi871mKAOzY0EYCFG2E5hmIDBJZ3u88Q5hPvrH5H4qsoFgGt+zIMwHn/C8zP2UFgJF5vukMWdBmDTimsszzUDgiUfzAIkpO+6nmIDVnK2J/VRkA8E0vmdBqPmDjyk7E3mE2xTTYNGcbZH5XjOUgdmxIAyEKNsJTDMQmKSzPd5M3nnisZePsVdkA8E0vmdBmI9vjrmR+yktBIrM90ljzoIwacQ1l2eagcATj+YBElN23E8xAas5WxP7qcgGgml8z4JQ8wcfU3Ym8gi3KabBojnbIvO9ZigDs2NBGAhRthOYZiAwSWd7vPEOYT76x+R+KrKBYBrfsyDMB5/wvMz9lBYCReb7pDFnQZg04prLM81A4IlH8wCJKTvup5iA1Zytif1UZAPBNL5nQaj5g48pOxN5hNsU02DRnG2R+V4zlIHZsSAMhCjbCUwzEJiksz3eTN554rGXj7FXZAPBNL5nQZiPb465kfspLQSKzPdJY86CMGnENZdnmoHAE4/mARJTdtxPMQGrOVsT+6nIBoJpfM+CUPMHH1N2JvIItymmwaI52yLzvWYoA7NjQRgIUbYTmGYgMElne7zxDmE++sfkfiqygWAa37MgzAef8LzM/ZQWAkXm+6QxZ0GYNOKayzPNQOCJR/MAiSk77qeYgNWcrYn9VGQDwTS+Z0Go+YOPKTsTeYTbFNNg0ZxtkfleM5SB2bEgDIQo2wlMMxCYpLM93kzeeeKxl4+xV2QDwTS+Z0GYj2+OuZH7KS0Eisz3SWPOgjBpxDWXZ5qBwBOP5gESU3bcTzEBqzlbE/upyAaCaXzPglDzBx9TdibyCLcppsGiOdsi871mKAOzY0EYCFG2E5hmIDBJZ3u88Q5hPvrH5H4qsoFgGt+zIMwHn/C8zP2UFgJF5vukMWdBmDTimsszzUDgiUfzAIkpO+6nmIDVnK2J/VRkA8E0vmdBqPmDjyk7E3mE2xTTYNGcbZH5XjOUgdmxIAyEKNsJTDMQmKSzPd5M3nnisZePsVdkA8E0vmdBmI9vjrmR+yktBIrM90ljzoIwacQ1l2eagcATj+YBElN23E8xAas5WxP7qcgGgml8z4JQ8wcfU3Ym8gi3KabBojnbIvO9ZigDs2NBGAhRthOYZiAwSWd7vPEOYT76x+R+KrKBYBrfsyDMB5/wvMz9lBYCReb7pDFnQZg04prLM81A4IlH8wCJKTvup5iA1Zytif1UZAPBNL5nQaj5g48pOxN5hNsU02DRnG2R+V4zlIHZsSAMhCjbCUwzEJiksz3eTN554rGXj7FXZAPBNL5nQZiPb465kfspLQSKzPdJY86CMGnEbeXdcssthH9ff/219ddtttmGLrvsMjrooIOka2WagcATj3TXp5qQ+ylV+KULN7Gf8mogMN+7D1sTx6hpbTKtPbwQIT2FpJ4wr3yfOnAhKsCCMARoul6ZOHEiVVVV0WabbWZlee+999LIkSPp/ffft8ShzMOCUAaldNPwZJou/rKlcz/JIpVuurwaCMz3LAjT/XLCl87cGB67JN80sZ/yyvdJ9ruuslgQ6kJSUz6tW7e2ROGgQYOkcmRBKAVTqolMJGluU6pDSrpwE/vJJAOh6HzPOzXSn3KqCU3kEW5TqkNKunCT+F660SklZEGYEvDOYlesWEGPPPIInXDCCdYO4dZbby1VMxaEUjClmognnlThly6c+0kaqlQTmmAgMN+vHkL83aX6OUkVzn0kBVPqiUzsJxP4PvWBIVkBFoSSQMWV7KOPPqLdd9+dli5dSuussw6NGzeOevXq5VncsmXLCP/EA0HYuXNnmjVrFrVp0yauaiaWLwjt+eefpwMOOICqq6sTKzfOgrhNcaKrL2/uJ31YxpkTDIT27dvT/PnzqaamJs6itOfNfN8YUv7utA8z7RlyH2mHNJYMTeynPPN9LJ0cY6YsCGMEVybr5cuX0/Tp0+nnn3+mRx99lO644w56+eWXPXcIhw8fTiNGjGiUNYRkixYtZIrkNIwAI8AI5BaBxYsXU//+/XMpCJnvczvsuOKMACOQAgJ55vsU4IpUJAvCSPDpf3n//fenTTfdlG699VbXzHmHUD/mcedo4qodtynuUaMnfxP7yaQV46LzPUa5iWPUtDaZ1h4ed3rmlyRyMYnvk8ArShksCKOgF8O7++23n+UCes8990jlzmcIpWBKNZGJfv3cplSHlHThJvaTSWdKis73wjCvq6uzjkqYdEzApDaZyCPcJulpJNWEJvF9qkBKFM6CUAKkuJJcdNFF1p2DEIC//PILPfjgg3TNNdfQM888Y52hk3lYEMqglG4annjSxV+2dO4nWaTSTZdXA4H53n3c8HeX7vckUzr3kQxK6acxsZ/yyvfpjwb1GrAgVMdM2xu4WuLFF1+0AsK0atWKtt9+ezr//POlxSAqwoJQW3fElpGJJM1tim24aM3YxH7Kq4HAfM+CUOvHnWBmJvIItynBARShqLzyfYQmp/YqC8LUoNdTMAtCPTjGmQtPPHGiqy9v7id9WMaZU5ENBNP4HuOEv7s4vxY9eXMf6cEx7lxM7Kci833c48WZPwvCpBHXXJ5pBoKJhMZt0jzoY8qO+ykmYDVnW2QDwTS+Z0Go+eOIKTvmxpiA1Zytif1UZL7XPDwCs2NBGAhRthOYZiCYSGi5btOKFURTphDNmkXUvj1Rt25EVVW8qp9tWmioXa7HngfGRTYQTON7FoQZIxLm+4x1iFp1mO/V8OLUlQiwIMz5iDDNQDCR0HLbpgkTiAYPJpoxY/VX0qkT0dixVOrdm0yKoseGaX6IkAVhK5ozZw61adMmP53mU9Pc8qNpbWK+z/33ZOK3VGS+T3pAsiBMGnHN5bEg1AxoDNnlkqRhHPTtS7RqlWMJqYn136Xx46muqopDxccwXnRmmcuxFwBAkQ0E0/ieF2J0fu0R8mK+jwBedl5lvs9OX+SxJiwI89hrtjqbZiCYSGi5axPchrp0qdwZtH8nTZpQabPNqG7kSBaEGeeP3I09CTxZEPIOocQwSTVJrr475nu+/zLVr8W/8CLzfdLdwoIwacQ1l8eCUDOgMWSXK+MA7Z88mahHD18kSs2bU11tLQvCGMaLzixzN/YkGl9kA8E0vucdQokBH3cS5nsWhHGPsQj5F5nvI8AW6lUWhKFgy85LphkIJhqwuWtTbS1R//4sCLPzmYeuSe7GnkRLi2wgmMb3LAglBnzcSZjvWRDGPcYi5F9kvo8AW6hXWRCGgi07L5lmIJhowOauTbxizAZCdiiuUU2KbCCYxvcsCDPwoTHfM99nYBh6VaHIfJ90t7AgTBpxzeWZZiDkTjxJ9Gfu2iTOlMyc2TioDNrLZwglej0bSXI39iRgK7KBYBrfsyCUGPBxJ2G+Z0EY9xiLkH+R+T4CbKFeZUEYCrbsvGSagWCiAZvLNomocxjq9kijTTjKaHa+/uCa5HLsBTSryAaCaXzPgjD4G04kBfN9IjDHXQjzfdwIm50/C8Kc969pBoKJhJbbNrndS9W5M9GYMXwPYU54I7djzwdfFoQcZTTrn18uvzvm+6wPq8D65XLc8QJgYL8mlYAFYVJIx1QOC8KYgNWYba5JGu5EU6YQzZpF1L49UbduRFVVlOs2efQtt0njoI8xKxaELAhjHF5ass4tlzDfa+n/tDLJ7bjjBcC0hkxFuSwIM9EN4SvBgjA8dkm9aSJJc5uSGj3RyjGxn1gQsiCM9lXE/7Zp351p7cEI4DbF/x3oKKHIfK8DP5U8WBCqoJXBtCwIM9gpjioZNfHUryCXZs2iuhYtqFfPnlTdrFn2O0Gihkb1U317TWxTkQ0E0/ieDXMJYko7yYoVVHrlFapbsIB61dRQ9d57W14ieX9M5EYT21Rkvk/6G2NBmDTimsszzUAwkdCMaZPtjEnDxfRDh1L1tdcS9emjeWQnn50x/WSDzsQ2FdlAMI3vWRAmz3NKJdZzfmnuXKqrraVe/fpRdZs2RGPH5p7zTeRGE9tUZL5X+lY1JGZBqAHENLMwzUAwkdCMaJOIQlcfcbRBEPbvT9VLlhCNH88GQppE4FG2EWPP0bYiGwim8T0LwgyShqiSjfMb+B6CcOnScoqcc76J3Ghim4rM90mzAwvCpBHXXJ5pBoKJhJb7Nol7qmbMaBi9jQyETp2Ipk3LtStR7vvJhVtMbFORDQTT+J4FoWaDQFd2Ds6v4HssAOL6oZxzvoncaGKbisz3uj5n2XxYEMoildF0phkIJhJa7ts0eTJRjx4VX0AjAwG/TppE1L17Rr+U4Grlvp9YEAZ3cs5TmMb3LAgzOiAdnO/K9znnfOb7jI499ghJrWNYEKYGvZ6CTTMQmKT1jAutudTWEvXvHywIx40j6tdPa9FJZsZjL0m0w5dV5BVj0/ieBWH47yDWNx2c7ykIc8z5zPexjiBtmReZ77WBKJkRC0JJoLKazDQDgUk64ZHmce9URS2ysEMoU8+I0PHYiwhgQq8X2UAwje9ZECb00YhiZHk0CzuEsnUNCSHzfUjgEn6tyHyfMNTEgjBpxDWXZ5qBwCSteYD4ZWeLGtqQDOdCnBHkxHmSmTOJnEFlRJCBOM+TyNYzInQ89iICmNDrRTYQTON7FoQJfTQoRoVHHZyf+BlClbqGhJD5PiRwCb9WZL5PGGoWhEkDrrs80wyExEg65tVHez8n1iaVweWIGtrwKoIF4HFGkBPp8duqVZRYlFHVeqpg4EibyX6K0B5Tje0iGwim8X1iYzRBvk+sTSrcEIZHbZxfatZs9bUTcUcZDVNXFSzq0zLfhwAthVeKzPdJw807hEkjrrk80wyEREg6gdXHTAtCl6ihFcPSK4Kc2z2E555L1ddcE8+VE2HrGfIbUx57CRuZYZql3KYwhST8TpENBNP4PhHxlDDfJ9ImlW8uCo+63UPYti3RmDG55/xQ3Jhxzg/VJpWxlELaIvN90nCzIEwacc3lmWYgxE5oCa0+ZloQupwJdB2WblFD6yfE0qxZVNeiBfXq2ZOqmzXTPKrrs4tSzxA1Uhp7KRiZIZpESm0KU0AK7xTZQDCN72MXTynwfextUv3movLoihVUeuUVqluwgHrV1FD13nvHd71Q1LoqYKPMjTngfOU2KeCVVtIi833SmLMgTBpxzeWZZiDESmhRVkoj9FusbQpTL5eooa7Z+ESQS6RNGuqpAo90m1IyMlXaItJKtylM5im9U2QDwTS+j1U8pcT3sbYpzDengUcT4xENdZWFSKlNOeF8pTbJApVyuiLzfdLQsyBMGnHN5ZlmIMRKaAmuPtq7OdY2hRlPGnBIpE0a6qkCj1SbUjQyVdrCgjAMWtl/xzS+j1U8JcwfmeV8DThIcaOOz0dDXWWrId2mHHG+dJtkQcpAOhaEyXUCC8LksI6lJNMMhFgJLcHVx8waB6iYS9TQisHpdYbQlijWfhLlaKinykcn1aYEDRaVunullWqTjoISzKPIBoJpfB+rIEyJ72NtU5jvTAOPJsYjGuoqC5F0m3LE+dJtkgUpA+mKzPdJw8+CMGnENZdnmoEQK6GlROyxtinseHJEDW3IxivKqKOcxNoUsZ4q8Ei1KUUjU6UtIq1Um8JknOI7RTYQTOP7WMVTSnwfa5vCfncReTRRHolYV1mIpNuUI86XbpMsSBlIV2S+Txp+FoRJI665PNMMhFgJLcHVR3s3x9qmKOPJ7ZB8585SEeQSbVOEeqrAI9WmFI1MlbawIAyDVvbfMY3vYxVPKfF9rG2KMkQj8KgUN0apm/PdCHWVrYZ0m3LE+dJtkgUpA+lYECbXCSwIk8M6lpJMMxBiJ7SEVh9zIQhRyZBhtGPvJ+fXErKeKh+dVJtSNDJV2sKCMAxa2X/HNL6PXTylwPextynKMA3Jo1LcGKVebu+GrKtsNaTblCPOl26TLEgZSMeCMLlOYEGYHNaxlGSagRCa0FQmjwRWH3MjCEOOytD9FLK8JF6TblNKRmYYDKTbFCbzlN4psoFgGt+HFk8Z5vvQbUrpe5Ip1kQeUWpTTjhfqU0yHZ+BNEXm+6ThZ0GYNOKayzPNQAhFaGHuB1IxKCL2Wag2RSwz7tcL36aEFxXC9qeJ/VRkA8E0vg8lnjLO96HaFPYDT+g9E3lEuU054HzlNiU0fqIUU2S+j4JbmHdZEIZBLUPvmGYgKBNaDu4HUm5ThsaXV1W4TeHdbZPsXhP7qcgGgml8ryyecsD3ym1KkhBClmUij4RqU4ILyWG6KlSbwhSU4DtF5vsEYbaKYkGYNOKayzPNQFAitJzcD9TQpp49qfrNN4lmzSJq356oWzeiqirNIyKZ7JT6KZkqRS6F2xQZwkQyKLKBYBrfK4mnnPB9RZsM4XzmxkSoLXIhJvZTkfk+8oBQzIAFoSJgWUtumoGgRGg5if7V0KahQ6n6yy9XD6FOnYjGjiXq0ydrwyqwPkr9FJhbNhJwm7LRD0G1KLKBYBrfKwnCnPB9RZsM4XzmxiBWysbvJvZTkfk+6VHFgjBpxDWXZ5qBoERoObkfqDRhAtVVVVGvfv2oesmS1SNA8s4/zUNGS3ZK/aSlxPgz4TbFj7GOEopsIJjG90qCMCd8b7XJMM5nbtTBXPHnYWI/FZnv4x8xlSWwIEwacc3lmWYgKBFaHlaMV6yg0pZbUt2oUY0FIcYCRCF2CqdNS859VMM5CKV+0jzm48qO2xQXsnrzLbKBYBrfKwnCPPA9GpQ1zme+dyUg5nu9vBxXbkXm+7gw9cqXBWHSiGsuzzQDQYmk83A/0OTJVOrVi+pqa90FoRgPkyYRde+ueXS4ZBcmQp9LNkr9FH+rtJTAbdICY+yZFNlAMI3vlQRhHvgeDcoS5zPfe/IR833sVK2lgCLzvRYAFTJhQagAVhaTmmYgKJN01u8Hqq2l0qBBwYJw3Diifv3iHWIaI/Qp91O8LdOSO7dJC4yxZ1JkA8E0vlcShEicdb5HHbPC+cz3vlzEfB87VWspoMh8rwVAhUxYECqAlcWkphkIoUg6y/cDpbFa7OYihMHbpQvRjBnuw1jRdTVUP/l9QBrcmqJ+n9rbFLVCGt43sU1FNhBM43tlQShE4eDBlVzWuTPRmDHZCNCVNOcz34diShO50cQ2FZnvQw3sCC+xIIwAXhZeNc1ACE1oGRAUruMh6fMkXi5CJ51ENGxY8JCVdF0N3U9uNdDk1hTcOP8UWtsUtTKa3jexTUU2EEzj+1CCEC9lle/r65bYuXHm+9BMaSI3mtimIvN96MEd8kUWhCGBy8prphkIJhJaQ8S5/v2pevHi1UNHd5RRPxehVavkhqyk66q2ftLo1iTXQO9U2toUtSIa3zexTUU2EEzj+9CCUOM3EkdWiXA+832krjORG01sU5H5PtIAD/EyC8IQoOl65eqrr6YJEybQZ599Rs2bN6c99tiDrr32Wtpiiy2kizDNQDCR0Bra5LyTSqeb0/LlRB07Es2ZIz12XBMmuUOYsYumjR57vXpRdXV1tLGRkbfzaiAw37sPIKO/u7g4n/k+MhsZPe6Y7yOPjyJmwIIwxV4/8MAD6dhjj6Wdd96Zfv31V7r44ovpo48+ok8++YTWXnttqZoVXhDqcB3SkYdPbzVMPD17UvWbbxLNmkXUvj1Rt256rprASvGppxLNni01ZlwTpXGGMGNh5NlACD98knwzr4KQ+V6TIIzK11HflxjssXI+871EDwQnYb4PxigLKfLK91nATrUOLAhVEYsx/ezZs2n99denl19+mfbee2+pkgotCHWcPdORR0BPlZYupbpnn6VeixdTtU4hiHK93Ib86gTxZ3chDeG6qmUyzdhF01raJPXVJpfIxDaZYiAw35e/A6UxGpWvo74v+enGxvnM95I9EJxMadwFZ5eJFCa2yRS+z8QACagEC8IM9dKXX35JXbt2tXYJt912W6maFVYQ6jh7piOPoF6aMIFK559feTE9LqIfU7F1nwAAIABJREFUOzZ6RLwgl0u3uo0YQXT77XIR+nxW0rVMPLxDGDR6Iv+upZ8i10JvBqYYCMz3ioIwKl9HfV92GMfF+XHzPdrnwflaeIT5XnYEhU6npZ9Clx7Pi6bwfTzo6M2VBaFePEPntmrVKjrssMPop59+oilTpnjms2zZMsI/8UAQdu7cmWbNmkVt2rQJXX5WXgShPf/883TAAQd4n3nCpLXddkQzZ7pXGzteOE/34YfeLpk68ggCbeJEouOPp1KzZvT8XXfRASeeSNVLlhCJHbn77iPq3TsoF+/fX32V6OCD5d63Y4I33niD6PvviTbckGj33RvjhLqff34lxsD02mutOgf2E/ANKkP0wXffVe5YihbJ9KNc66VSBbZJKpdsJTKxTTAQ2rdvT/Pnz6eamppsAS5ZG+b71UBJjdGofB31fcl+pTg5P06+R/t8OL904IH+8zLzvewIiTWd1LcUaw30Z24C3+tHJZ4cWRDGg6tyrqeffjo99dRT9Oqrr1In7CB5PMOHD6cR2OVxPOPGjaMWLVool8svMAKMACOQJwQWL15M/fv3z7UgZL7P04jjujICjEBaCJjA92lhp1ouC0JVxGJIf8YZZ9Djjz9Or7zyCm2yySa+JfAOIRGNH080aFBwT9x5J1Hfvu7pdOThtypqW80tNW9euUNor9FTTxHttVdwW9xSyK4Yt21bvrRZZjdSciW99N579PyLLzbeya1fIa84o4i6++2Kuq1MY1Hkmmvk6hwOvUZvmbi6amKb8r5izHxf+elJjdGofB31fVQ5aBcsbs6Pg+9FuwI8bkq/+Q09f+WVzPea5pq4spH6luIqPKZ88873McESS7YsCGOBVS5TuA3BOHjsscdo8uTJ1vlB1aeQZwh1nEWImkdQcAJbwBQIwrraWurVr1/ZZdT+SN775zouxJkSuM563TPYrl35vOCaa8oNLUlcSi+9RHULFlAve3jroDMufpFME4j8FwSAiecvTGxTXs+UMN+7f4FSY1SSl8jr2pyo7wfxPZoWN+fHwfeotwQ2DXMY833QNJLq71LfUqo1VC88r3yv3tL032BBmGIfnHbaaQRXzyeeeKLi7sFWrVpZ9xLKPIUUhEETo8wVClHykAlO0Lo1UY8eVhf6CkLJe/88x4KoCxJEjBxqlSEZ+bM0bhzVtWhRKQglDAurjKhtlvkwQqQxcTI1sU15NRCY7yMIwih8jWKjvC/D9336VAir2DhfN99Lcr6rIGS+DzHLxPsK8328+JqeOwvCFHu4iXCjc9Th7rvvpoEDB0rVrJCCEMjomBjD5CG7C/bll0SbbmoFZUFQmUY7hDKiVWoE1GMxeLB/5FDZHTjJSd51h1BSTFKUXVFZTEKkM3EyNbFNeRWEzPcRBKEOzo+T76dNKzeuS5f4Od9tt7Jz5/KxAAhTIYARnE7mzlsJzncVhMz3IWaZeF9hvo8XX9NzZ0GY8x4urCAUBkKQEArqX5nJ1Z6HxORpJccu2Lx51hlGazIdN261y2iIe/+CmlERLnz99cvJf/yRCPce4sL6s8+uFIxeV19IrqSXvviifLei3YVIBZvu3QOblHQCEydTE9uUV0GoYzybxvfARGmMqvK1E3TV91U5rV50xs759gW+KHwvxGO9kHU9etCkCZU224zqRo5kvtfxEceYh9K3FGM9dGZdZL7XiaNMXiwIZVDKcBrTDARlQpPd+fLrQ5U8VFdF3e6kcq7m6hxfbgaPW/5+olRiJb3UuzfV1dW5nyH0OtOoc1dUJ2b1eSmPvRjqoDtLE9tUZAPBNL5XFoRCwMjufrl9UHHyPcpLkvN18H19nRsCsLkcPSiNH091VVXM97oJWnN+zPeaAS1YdiwIc97hphkImSc01RVjrIAvXVreTVu8mKqxY9etm/f9iFHGo9dZF688/QRawEq6Zz9JiMkGt6YobY3h3djGnooBqrldsbVJcz1VsmNB2IrmzJljxL2zoQShymCJmjYE31ttSoLzdfK9EIUeHjeuC4ASQtKKCC7cWKP2heb3Y+XGlDg/1jZpxl82uyLzvSxGutKxINSFZEr5sCBMGHhJl0rCmZKqKqtyiZB00NlGP5i8grz4TGq+bVJ1y0q4C72Ki6WfZKITxtj+WNoUY31lsi6ygWAa3yfGjzIDyy1NCL5PpE1x8D0q7sH5zPcKAyhFzme+V+gnTtoIARaEOR8UphkIuSA0xV2wRNoku5LtNt5DBHkJbJPsCqlsugS+08A2qdZBNjqhar4K6bW3SaHsuJKyIOQdwrjGlmu+inyfiCDMK9/7iM5E+zSuhdqUOZ/5PulRZFZ5LAhz3p8sCFPqQIVdsERIWvZsoxtcIa6B0NKmFFdS3WDQ0iaRcdAKfkLnKbW2KaVPzVksC0IWhIkPRQW+T0QQ5pHvAUyGOF87N2aA87W3KfEPrXGBReb7pOFnQZg04prLY0GoGVCV7CR3txIh6TArxhFESeQ2pbySGkkQyvS7bH+EEOMqQzRyP6kUllDaIhsIpvF9IuJJ17iU+e7ry4r9u5PlF3vb0+R7IQb79q28Lxd/jyPqtkSfS/eRbL/L9kmMnC/dJgl8spKkyHyfdB+wIEwacc3lmWYgmEhoibQp6KyLc9xFnIQjtSkDK6mhBaHsCvcDDxANGBD8tYdw1w3OdHWKSP2kUlCCaYtsIJjG97kShApjPPbvLk98D9wyyPlSfSTL92hjBjhfqk0K4zgLSYvM90njz4IwacQ1l2eagWAioSXWJq+zLm5jLuLVF5HalIGV1FCCUHZXE+lOOYVozpzgrz3G1WJTje0iGwim8b2pYzQSPwazRjlFXvgedc0g5wf2kSzfi77IAOcHtkl2bGUoXZH5PuluYEGYNOKayzPNQDCR0KTaJOuWEjR+vM66jB5N1LYt0axZ5cvqw1x9YatjacMNqW7Bgsp7qYLqJn6XPf/itnumCyeXuvr2k+wKN3A++ujGblHO8iK4b8nCbKqxXWQDwTS+N3WMBnK+Lh7LA9+jkzPI+Vr4HtHEn3iCrDsc7fc3upF0ApwfOO5UJo+MpC0y3yfdBSwIk0Zcc3mmGQgmElpgm1TcUmTGjy5jw16Wo46l5s2prraWeq1YQdWq90yFXS3WjZMDS99+kq1zu3ZEs2f791JEd12ZISDSBI49lcwykrbIBoJpfF9IQaibx7LO91F2CHVjZeMwLXz/wgtEAwcSzZiRCc5nvs/IJJXTarAgzGnHiWqbZiCYSGi+bVJxSwkaq3EYBijTpY4NgrB/f6q+7z61y4eDzr+4raTqxMkDR99+kl3hDuoj/A7R+M9/qmEmk69LGhO/JxaEHGU05OeQ2Gue351OHssL3wP1DHK+Fr6/5BKiK68MHlcJcT7zfXBXcApvBFgQ5nx0sCDMfgd6krSsG6LtknvP1sa1kupRxwpB2KYNkUwdnTuOcLPBY3e1cds904mTz3DRsmIsMxzvv5/ouONkUkZOwwZCZAgzlYFpfA9wTRyjrm3SyWN543v7wmJGOF8L38sKwoQ438RvqcgLgElPPiwIk0Zcc3mmGQgmEppnm2TdEIMCj+hcdXaOT486NgjCfv2oeskSoqA6uo172bu9ouIkuZIudaZk5kz3syIQsjijGeQuChzCYBWSN0z8nopsIJjG94UShFF5THBAXvleiMLBgytdLN0CnEXBKim+79SJ6O67ifbfP5idE+J85vvgruAUvENo7BgwzUAwkdA826QjTLXOVWe3r8TDVbKRIAx7fYLM5C3rrulWB4WV9MCx5xXVT+xqPvQQ0dlnE/mJRhgRqrupEdgrsE0R8k7rVRaE7DKa1tiTLdf1u4vCY6LgvPM92hEn5yfJ9+PHEx12GFGXLpnhfOZ72S+U07khwDuEOR8XLAiz34GuJK3raoIoK6ky0MW5QyhTPtKEbaPiSrrUZBq0qxkkGmFEqAbhkcXJJZ1UmyLkn/Srv8xfQP84vw9deOuLNH/+fKqpqUm6CqmWZxrfF2qH8PLLiYYNCx4/frtJYbkwuNRyiizwfVjOT4PvUdcMcb5pfA94i7wAKPvZ6krHglAXkinlY5qBYCKhNWqT18TlHEMyYap1rDr7jV2PYAAVO4TrrFN2AVpzTfWvQGa1OExAghAr6dJjL6jOQaJRHaXQb0i3KXQJyb14+/WXUtsXJ1CnqSXa5cupLAhxdteAx6QxKrrDlfOPPNK/t/LC9y1aEMEbont3oqoqtREYxJ0iN1XOT5PvhSiUcYVVQ0s5tYnfEgtC5WEQ+gUWhKGhy8aLLAiz0Q9+tagg6aZNyy4musJUy64YIzz2fvuFA8tlBbSRyyhcIceOVdv9UnDvUV6FlcXFthqvZTIVBg/cRnGeENHlOnYMd+9juN6qeEu1TStWrqAp06fQrF9mUfuW7anbRt2oqqmi0aeh3vYs5s7+kR665Eja9Y051GI50c+rVtAeX7AgbMOCUPNI05ddaM4P8iCQ5bW4+R5QqXK+Ct8LkSUbeEwWF918j3pmiPNV+d6qfgY5v4L/586ltm3bFnIBUB8jyeXEglAOp8ymYkGY2a5pqFgFSb/2GlGPHsGVlg1THbSSKkpSnbydNfS6h1AElVG9W0/RvceqjsrOW4id0zCTaQVMqgZP8CiInEKlTRM+nUCDnxlMMxasvlOrU00nGnvgWOqzVZ/IdQmTwdjhp9LmU16hLjNXWa9Pb9+E3tt5R7po5AOFNBBM43v0qcoYDTOG0ngnFOePGEF02WX+1c0K36OWKpwfhu9VOD8NvveqX9S5NsKAVf2Wssj5zubzDmGEAaH4KgtCRcCyljwVA0HW7SMEWKqEFqKIxF+paBNWgPv3D66DSphqrzMM9lJUJm+v2i1fXt7tmjOHGu0QCgNBJmhKCPeehirJjr2kV4zDGjzBIyFSCtnvCYZB34f70ioqCy/xNKEm1v8df/T4REXhx++/T2+OPZV2eWcBVa8gWlpN9NauremQy2qp+dprF3bFOBW+xwCQ/e5CjFbZMRoi69ReCcX5soG5kuJ79Dl49OijqbRkCdXV1lIvsQDYQBBNyjuFfoGyovC97NhLmu+FGMQOpv3aJDEPWqSZ7HlxFKnyLWWN870+VhaEydEYC8LksI6lpMQNhJh3QVQILRZAY8g01Gqxaphq9MuZZ5ajnXk9MmdU/Npvm3RdBaF4N6juISZv5W55+GGiY47xfw3nXxYvbjj7GHrsRTV4lBsn/4JMm+Ay1GVsl4qdQacoxE7htMHTEnEfvf7CY+h3Uz6kDnPKtZi6cVOa0eMgOu2CUdZ/F9lASJzvheHrPB+lcRdEZozKj/hspIyd85Pge0BZz9W+fI90aQbCQflYrGzenGjlSu8BgOMauCKp/qx7pHGXUc6XbVPWON/vqy0y3yfNZiwIk0Zcc3mJGggJ7ILIEppmGGPNzvU8SRxXE7z4Yrx3ItnccnwNhKCV7hDuPUodhMm6fXvlOwFDj70kBK4SAKsTy7Rp8teTqce9wW7Mk06YRN27dA9Zk+DXXnr6SZp57zDa8T9Lqekqol+aE721ZwcaeNVj1LLV6miiRTYQEuV7IQZj3gWRGaPBoydbKRLh/Lj5HpDWc3WgIPTj/Lj5HvWUxcJ2tjLSuMso58u2KSucL/PVFpnvZfDRmYYFoU40U8grMQMhoRUxWUJLAerQRTZqU1xhqlUnXlU3sLzsEMpO1uhRmyETeuyp4h56JKm/KNOm2o9qqf+EYDfmcX3GUb/t+qlXQuKN0eccSrtMmUptFpQTf7TFGvTrIYOo/0lDGr1dZAMhMb4H6sz5EiPXPUkinK/KO6p8j6blZYfw0kuJrrwyuL8uuYToiiusdDLc6JmhKvbBNdOSQrZNWeB82QYXme9lMdKVjgWhLiRTyicxA0HWyA5yFwzASZbQUoI7VLGubVIJkCJbqso9V/PmEam6gdkCGpSaNWt8pkTWJVUmMAIiKP7wg3pYc2AlO1kjrY6ocwl9G7LDwJ5O5ntKc7X4wbtupKZP3kHbfVayqj23huidvTals0b/n2dzi2wgJMb3NjEQOO6Y8xtBlAjnq/BOGL63LQqU5s2junHjop0h9PKKQTlw3wdvH3VU4HBzTZC0IFTBHtdzJPTI8L31aWfEK0QGliLzvQw+OtOwINSJZgp5JWYgyBrZQe6CLAhXIxBmxdYLPwhM2Xuurr++fL4uzGH4+t1Ny4XIbiCoBq2Rqe+jj6pdYyGwkZ2sEcl11qwG0Sk7mTbqgiCBKyuUY+APmTaJ8yQzF8xsFFQGVUJgGd1nCHHB/N0X96FdX59JNYuJcPLnvd82o/YDLqX9e/tHNC2ygZAY36ssrDDnywlCIbCmTCnzDtzau3ULt+iFvB55xAr44vt07kwUhe+R+YQJVDr++MaCUIXzZfge+YUNxJK0y2hGOV+G761hWH9uPEnODzu9FZnvw2IW9j0WhGGRy8h7iRkIskY2rxbLGwe6xlCQa5coBxPugw8SnX66FSnU9ZERL+edR6VbbqG6++9fvWKMFd6zzya67jo5wYuJP2g1GMaMX/Q6L/xk8UDgGVsdZCdT12LjcgOOOEZk2yQizqE4e6TROKKM3nLtedRh0lO0+dflABCz2hC93217Oueah6RaW2QDITG+R08w50uNR7dEst9d6AJkOQ4XyIOXvYKNyfA93CsvvJDqdtutcofQyfl+C5yygjAoYqkf52+wASJOeUPq8DqJ3EcZ5HyVNiXJ+aHHecGDiEXBLcy7LAjDoJahdxIzEBJaEVMhtAx1g29VYm+TrOGGXcGXXlIOtlLROLFD6OUyKlZ4sXp92mmVwlNEJjzssFBBX5T62ysAksjk3HMrxWvUMyXI1ysC70knEXXtunpHAGl17RIEgKIy9tzupOpc05nGHDhGy5UTuGD+kYv60C5vzaXmy4lKVUTv/L4l7TrkVtp2hx2ku5cFYSuaM2cOxX4xPXO+9Jh0JlT57kIVIsv5AwcS3XNPcBF+C7leO4TIVezqYaz48X2XLkQzVt9v6luhsIvKQaLT4XGipY8yxvmqbYqb84MHXnCKIvN9MDp6U7Ag1Itn4rklJgiFwYuIc3js7oYqriMaDdjEwQ5ZoCpJKxcj686rkrGbG5htVdoz6hxWYffbjwi7b24Pxsrw4UTDhsnVxqseMoLKbbKGm+jNNxMdcUQjUVZauZLq6uqoV69eVF1dLVc/Zyr7KvnUqUS3315pCAEfPPaVbI0h/KMapnAlmjJ9Cs36ZRa1b9meum3UTctVE2OG/Zm2fOV12nhW+Z7Db9o3oc/23oOGjLhDGeciGwiJ8j1zvvLYFC/kjvO93H7rOb80d677PYRoMK57wHUOOvgeeTjronK0wu9aLCxE2uaN0m67Ud2zz0bje9Q3Q5wfZtzFxfmhPx7Hi0Xme10YyubDglAWqYymS8VAcAYjgWvfmDHhzns5cA1DaBntmoZqxd4m2dViFaDcVmllo4wGldO6NRGCHMg8znqo3oPpZkw88YRrQJ3S2LFUV1UV3UCwG9LOc5peRhP+Hvb8jA+OsY+9gD7EBfNv33AK7fzvX2iNlURL1iR6a7e2dPRVj1KbduvLjIBGaYpsICTO92IsM+crjdXYvzvdnO+1KycbZdQPHRW+Rz72uqjyvVOgiXOaLpxf2mwzqhs1Sg/fZ4TzYx93Sl+BnsRF5ns9CMrnwoJQHqtMpkzFQFBZsVNELTOEprGNsbcpyLVLpQ/8zpTI3kOoUp5fWkfQF8slM+qdaD55NATKcdshVBkPsud77G2XPMujCm3sY8+nQtdfcAztMOVDal9/rOeLTZrS9/seQqece61qMyrSF9lASIXvvYxsnCHT8KQ5Riuqr/KNB7Q79jbp5Hy/s9qy9xBqGAdWFva66OB7H6FWatGiHChnxQqq7uMSyEplPGSE82Mfd7r6WSGfIvO9AkxakrIg1AJjepmkZiDE1ORMEFqYVUkfPBJpk9cB9zD9NGIE0cUXN45+p2uHEHWSWTW2B30JmnBlBFVAHg0GQs+eVN2s2WrkVMdDlNX7sOdnPPo5kbHnKPuFiRPo+/uvoB1wwTwRLWhB9NYeHelPV02ouGA+zNDEO0U2EEzje/RnGmO00dhT/cbTFoR2oYP/L+OJ4FXnIUOI4E7pFvG0/iqjwIvpgz5m8P1PPwXXU5zz08H3YiHD4/xiQ5vOPZeqP/20cr5THQ8Z4fxMfEtBY0Hx9yLzvSJUkZOzIIwMYboZmGYgpE5oulYlbcMisTa5TWJYxcfkqvq4nWuzRQaNbCBAdOIsoZch4wz6Ijvh+gmqgDwa2lRTQ9U9epQRCzMeopzpjBjC39nNiY29+oJHn3Uw7fra/6h1/QXzH25VTat6/5mOPfFM1RHomb7IBoJpfJ8JQRjmG8+CIBT85HTnDfulOTnfhos2vvcSrzhbfdttq4+d6OB7lOWTT0Wb6uqIxH2BYcZDRjg/ab4PO9RU3isy36vgpCMtC0IdKKaYh2kGQqqEpmtV0jEeEm2TcHPBmQmc6wz7OAMFObCJZCAIV1C3s3wi6IsIXiTqLzvh+gmqgDwa2rR4MVX361cW0n7R8bx2JWWNGbe+0bhDiGABr0x7hRZ8vIBqtq2hvTfZW0uAGLdqP/DPUbTm0/fStp//av2MC+bf7taVzr7+ybAjkAWhCwKm8X3qgjDsN54VQYh6oA1XXSUfqMur7nbOx46hjfti43sIwTPPbOyRooPv0U6ffCradOedRDnn/CT5Xjup+2TIgjA5tFkQJod1LCWZZiAkKp6cPSJ7ua2i0Z54m4KMHNmRaBc8iM4mds3g5oWL6WtrK++lks3X6QoqIr+tXx9k5McfG1/aLCuybriB6Iwz3C97Vt0hlC3zhRfK5YnLpvfYg2jTTct3f8m6csm4vMrii43NTyfQ4GcG09yFc6l2+1rq92E/arNOGxp74FgtV0iIquCC+XsuOpx2fX0WtVxCtLIJLphvTh1PGE77HnSoQo3lkxbZQDCN71MVhODJm24iOuus4MGXZc7XxfdAQfDQ3XcT7b9/Ay6Z5ftLLilHtXZzd0XtVXcIZTkf8wzuPRRBa1AWBHRKnJ8U3wd/KPpTFJnv9aPpnyMLwqQR11yeaQZCYuLJeWAcF7Wfcopc9EtFt77E2iTGluykJjsWYQxB7PTvH91AcLn/z8o06MyGShAFr2scAvJodIZQdpXaeR4S5WO1edSoMl5BolDjtS0WlJ9OoL4P97UumG/etHmDIFy6cqlVnfFHj9ciCm++Zih1mvQ0df2mfMH8d22J/tPtd3T21bWyIytUuiIbCKbxfaKCMOh6AL/RmGXO1833wAFC68or88H3qGUIznc9QyjL+faxIsrG39yu5XIbVxo5Pym+D0XWGl4qMt9rgE8pCxaESnBlL7FpBkIi4slNfKh0bZTV4qZN47+UXHZSO/xwoscfD245jCGshEbZIfRyBRViUCZ6aNBl86IlfpOtV/CdJk3Ku56IOieijIY1tET5Q4eWXZbsFzK73UOo8doWuA11GduFZiwoXwJtF4RLVi6hJtSEOtV0ommDp4V2H50xfTo9dXk/2uWtedSsRLTcumC+hva+4C7afKttgsdTxBRFNhBM4/vEBKHJnC/L93/9K9G661YIPc9PMaogTJLv0YgQnO8aZTQM59vLRl2cZzpj5Pwk+D4iXUd+vch8Hxk8xQxYECoClrXkphkIsQtCWVHhtaqH1cBp09xdEj0GR0ObEN7aOVnEcSm57KQGtxdZdym45NhcYqRdiA48kKhnT6LTTiNac83GCAW5OzldKc87j2jkyODP0M8F08047NyZSmPGVN5DqLIr6ayRKP/LL4lef321OylwxGO7INnT3Sm4lY1STP56MvW4tz4gjosgFC9MOmESde/SXbmE0ZeeSNtOeZM6f1++YH5axyY0tdveNHj4P5XzCvtCkQ0E0/g+EUFoOufL8j0WMvHYFvY8v0G4wQ8c2OACmXm+F6LQa3524fxS165UN3Jk5T2EYTnfPt+48XtMnB8334flaJ3vFZnvdeIokxcLQhmUMpzGNAMhVkEYJD6C+hmkH+Ly8IY29e9P1YsXV5ai0XWkIeOgSc0uVoLOutXUEGFled99y2HDjz7acoGUNhBEpTp2JLrxxtVR5MTfVYwZIUrtO25Bfea1m+tyx1Rp5Uqqq6urNBCiXuehuJsc1Jyg32s/qqX+E1a79jp3CMX74/qMo37b9QvKruH3D955g96/8Uz6/bsLrQvmF+OC+T3a0TFXjA99wbx04Y6ERTYQTOP72AVhFM6PwM2Jcr4s32MhE0/QWTdw/iOPEC1YUOZ7nBlv1kztzHhafI/KSnJ+abfdqO7ZZxtfTB+F8w3h+7DcHMd7Reb7OPD0y5MFYdKIay7PNAMhVkEoKz7c+gjnxG6/vbGgkejP0tKl5YmnXz+qXrLE/Q242EDouO2iSZTRKImPa6SVVghblckPri9/+hPRDTdQac011QwEUUFxz5T4b1l3J3FXlszqth0MhbM/nmPPbUcRWMytv3Hdr3/uv5/ouOPC9GCod+JYMb7+vL6006v/pQ3mlav02W+a0pz9+tBJ51wRqo5RXyqygWAa38cuCKNwfgRXbinOx27W118reZt4fjuyfI8MZDkfHHfiiZbbe2nu3HzwPdonyfm+tkZYF2MD+D4qP+t+v8h8rxvLoPxYEAYhlPHfTTMQYhWEsuLDrc/hQoNoZiGe0qRJVLdgQXBEzrZtiW69NZTotEKPO90Q3a51cDNyQkx+yjuEAjcYGT/8sNoIkjXYIJjh4jpggFoPKKzY+o49J774b1sUPs9KRelTtZZaqZf/upw63tCR5iyeY/232w5huxbt6IaeN1DHmo7UbaNunmcJnxp/H/380PW040fLrLzm44L5vTrTiVeM13LBfIjmWa8U2UAwje9jF4RhOd8vWrHEwJXmfNzHetllEjk6kkTheyEKZe8vHDZmTa/fAAAgAElEQVSMStddF04QJs33aJsk5wfaGnaMMWfJHK9ImO9xhnDjMRvTzF9muvI9zox3bNmR7jn8Hvpx0Y/UvmV7X85XH4jxv1Fkvo8f3coSWBAmjbjm8kwzEAJJOgp+suLDXoaG6wBKtbVU16JFsCBEuWHcUv0idOI+KbQb//Dg8l38wzUJ9geTH9IcdVTZNTTgCS0Ika9dXKNcBKyZPTuoyLIglJmUBY5wW7rnHiK3ayxcSlMae0FuWs4xFMLVOBiQyhQi9LgIKINfvVxGxZsIMON2FcUNZ/WiXV+dRuv9Uk75n62rqbrPaXTkgFNVq6U9fZENBNP4HoND6btTHU2qnK+B7602qXC+02siqI1BfI+FQVx/AE7FQhp40O1ahuXLy78hwrbf07o1lZYsCScIk+L7EJyvNO4yyPdoMjj/5Ikn09wlZW8VZ1RpRJpu07xNw+9I48X5QcMurd+LzPdJY86CMGnENZdnmoGgRNKqWKqQuphg8L8RjXnp1WJRpkrgGq+ACX6RLr0C2SgYT5EEISLYXWFzN4TIGzMmuDfhjnPBBfJ3PTndOgMC+DS4eeFienG/lFM422sp63qlycj0A8geetyeLkgQYgXZGuL1V1Hce/PVtPYz42ibL8oXzM9phQvmt6BzRklEow3uQS0pimwgmMb3sQtCFc6PcGbQObCVOB9eG7KByoL4HnMVHpngZUnx/UUXEV111WqI4uJ7lKDA+XnmeyEGxfVCAlw731etUUULly9sxLlOztdCyjFmUmS+jxFW16xZECaNuObyTDMQYhWEFotOkL8ryOle6eam4ycY6vu6YeJxCyrjNR5k3F7CBkzwMnxkJ+qoF9PjigkELRCPrGECTObNK/df0N1+brgGhCYvnX8+1Y0atXonVyYCLMbTqac22uFc0YRoysZEs9Yhar+QqNs3RFUvTSrvzmp+nKHH7dkLA+G4j46jRSsWuU8C1IQ6VHWgC77rSru9/j2tvbR8wfy7v2tOXQZdTXvv31NzjaNlV2QDwTS+j10QqnC+Jr632iRzbtz+Gejge/CbuBPVyY9u3KfC9x06UN3NN8t5uTg/75Yty54affqUf4mD73Gd08ryfagVjxfnT5hAOvkeZTbi/LteoKoe4Y6ZBDGkF+fbBeHyVctpxaoVnpwf9fqhoDrq+r3IfK8LQ9l8WBDKIhVDuldeeYVGjhxJ7777Ls2aNYsee+wxOhx3wyk8phkIsQtCYSA4V1BhDFx/fdnFBpewO3eIgi5O9+kz34hzXu/JHIyXnVjdynDuWqF9Rx4pPfIi7RCiFLubVJCwddb18suJhg2TrmsjA8G5A1u/SNAokp7sjsEDD1ScbZywFdHgA4lmtFpdcqf5RGO3HEJ9Tr4hXL193nIGkrEnDdohRNqhiw6nfT74nDadXr5KYmY7og+77URn/+1+7XXVkWGeDYSonG8a32M8pMb54IGTTiLq2lUr31e0yS+QmP1jiJvvUZadR3G+XIXvr7iC6rbbjnqpLGra22c/CpEk3zvbjQVczXxvmRRunL9Gaxrb53bqs1W9ENZBfvV5eHG+DN/bqxH2+iGNTQnMKs98H9i4jCVgQZhihzz99NP02muv0Y477khHHnkkC8KkjAP0ucpun4ybjlj9dBlPFfcQuuwmuQ5BmRXjsAETKmaESeXzJQhFLnudQ6dOVBo9murWXLNyxRgH6n/5hWhZOQiJ5+PmQqkSJU9hZduzDgJfm3HiKnJl3D1twhyGQV/czGEZIqtLb4I/4NaSox/VbiA4r5qwt9nPQFh/cSu6fPZWtOvbP9NavxItX4Po7Z1b0T7n3ZnIBfNhqTfPBkJUzmdBGHbUKHB+RL6vEIQffUTVl14aXOmk+B41EXcMqvD91KnlSNlDh1I17lYVD/hRxlvDbQESnh547O+7LcLpmOdQDjC2zXU6+B7ZenK+1bgmDe74wYNAPoUX56sKQtXrh+RrqC9lnvleHwrJ5MSCMBmcA0tp0qQJC8IkBWFgj9QnUF3NdMm3YgUck5/fQX4ZASLKiLJDKPLAyjR2Q1Wuc3j0USr17l2+s6+mhqq//371CrvKyrPTCPK4MN46XygEt+JOpmc3ixV5G4a+u55+Blv9GFnx3QzqMphoRk2lGGywnaiJdaB/2uBpnpE9ZYelPV2YHcIRC4+g3d7+lDr/UM7pf52a0Kd77k5DR9wZpgqJvmOKgRCG81kQxjzUNPA9atjA+T17UvVmm5XPPbs9SfM96oAz3FdeKQ+kne/RnjffXO1Fg4A0CEQm+9h5VIbvkW8UjxB7vRxzXVS+R5+uoFXUZUh2OF9VEPIOoezALUY6FoQZ6ecwxgGqbpqBkIj7kFufe+0YyoouH8HQqE1eK9CiXghFfvHF5UigfjuZKgETvMY56g0X2f6rLzP3/CTWWYfo3nstcebbT7I7eG5uUjLtlV3Z9vu2RX/ZVp99DYQgl64JE2jyOUdSj4HBhKJ7EhbnSWYumEmIKmd/hIEw4KMBtHDFQtpm8aY0dOa6tNN7i8oXzK9F9Obu7ejqTv+jby76RqtQDUYiXAoWhK1ozpw51AYBNAx4UuH8GPm+QhD26kXVEyf6n1tPku9VBKEs32M+g+stzncHPU4eDfLUCRLoQeXZf3fMdVH5HmfZL++2iobtG1yJpDhf8H3/D/vTslXL+AxhcNdwChsCLAgzMhxkBeGyZcsI/8QDQdi5c2frDKIJBgKMg+eff54OOOAAqq6uTqZ3MGGff37lKi528a69tuz+OGhQcD3uvHP1pO9I3dCm/faj6nfeIXrqKaL77iu7V3o9KB8uNYgaZ19dFvXq3bv8Jup+/PHl/y/jumMvD2doPvyQ6I03iA4+OLiNTz5JtM8+VrpGbcIu4YYbEu2+u3x+wGGvvSrLhQGA+tjzE4F7Xn1Vrp5+LcGKPDBEu5GvLU8YCM/fdRcdcOKJVL1kSWUubnV1lHPhbcfQzfOeCcTxzkPvpL5b17tLBaaWSzDx84k04LHGdzTCQLhr27vonWbvUM1z/6FdX/+S1q+/VeTTzaroua03oftavkT3HXEf9d6ifkzJFZlaKgjC9u3b0/z586mmBtux+XxkON90vq/gkqQ4P2a+r2hTy5ZUjTvswB+PP+4e+AQvROV74bopgsp4fRLg/JtvJjr00OCPRpbvwaMvvyyXp5NH/fgeNZTlfASuWbjQfQ60c75trovK9xPvv4wGfDu24miAF6hJcb7g+xM/PpFO3vlkuvGtG8umgW2hUEQZzQvnm8L3wR9c+ilYEKbfB1YNZIwDpBs+fDiNwIqi4xk3bhy1aNEiI63hajACjEBWEJj+2QfU9p0J9LuPl1tV+nltXDC/EW2y72lZqaJSPRYvXkz9+/cvhCBkvlcaGpyYEWAEDEPAFL7PQ7ewIMxIL8kKQtNXjBPdIcTq5Hbb+Z/v6NChvOoIt8qgHTjn7l392CpNnEjPV1W57zyFGX/2Fc+6uvIOYVDd3MrBajICBGCFV+w0+uWDcrGz2bs3ebZJBAU44ohyNDfnI36vz6fhZ6/y7enXW09uhxCr0D/91HjXF6vj11xj1b/iqS/bWjG+887V/eRVVxcsX53+Kh08TmKXtf7dji070rX7X6tlVw4uo9vdsh3N/KXxOaWrZnej3V6bTuvW3zjxwbZr0vwD+lKr3TajDdfZkHbvtHsu3ETtkJuyYizD+abzPfo1Mc5PiO/BpaWTT67kkjA8L95R4Xuv6xdEXsIrRGbeUOH7Cy8k+vnn8u6jM9CMG4/K8D14WnaHUIXzU+D7Dut0oOsOuE4L36MrvThf7BAO+ngQtV67NX34lw+tnn9jxhv0/cLvc8n5pvB9FApI6l0WhEkhHVCOjHHglgWfIYzQgbLnA7EjO3x4uaAgwYQ09ovsV6yg0pZbVt5vF6HKFa+qRotzKxdtu+yy8i+o97HHls8tuj0iAMKXX1Jpm23Ctcl515c1u63wj3Iqyv38c6KNN25051+jquKOQ7jbBp1Psb/odi+VW109+s4v0qcrlI4L4aMMCbegMqcsOoj+8Ml02urLcl/+uB7Rv/fM1gXzYdvMZwj5DGGosZMQ3yNic2nuXKqrrQ13b59X43TwPfIW56fj4ntx9l20w+1+R7+o1rZ5hqZMIcJVXHAH9XpwlhZuuUFn7g3hezRD9toJ3WcXQ313EV8yhe8jwpDI6ywIE4HZvZCFCxfSl/UhnHfYYQcaPXo09ejRg1q3bk0bbbSRVM1YEErB5J5INpw1DsKvtRaR8+5CP9E0bVp5gpo8mUq9euk3DlC2arQ4t/raJ1NZg+n006l0113qbbIHT7DXRbZcXGuBqHZBDwwQgX9QWtvvDZdJL15M1c57KAPy8Yv06fUqznLoiDpaIUaXEv39p71p1zd+pLWXlS9L/veOLejmzkvorOMupX7b9VNAJJtJ82wgROV80/geIyyxoDIJ8T0iNke+p9Xt09PB98j3/vuJjjtO/oL4sHw/ZAjRYYeVr3oQ58AtNTNZLqq1LN/b5zAFysor36OJstdO5OFaiaAuyzPfB7Uta7+zIEyxRyZPnmwJQOdzwgkn0D333CNVM9MMhMSMA5WJyX5n3U03ESGKZtBji2JZGjRIXTwF5a9LECIfl4ibQcUrGzx+4dVlDbWgStl/l7nXy5FflLHnF+kzqNpRV3GFGD1/0RHU7f3P6DffliONztiA6I1dt6bf7vlH6vdhP6o7vo66d+keVJ3M/55nAyEq55vG94kKQlkhEpHvEbFZmR9lvjpdgvCGG4gg1hR4V7k9zPeePRqV7y3T5evJ1OPexraj89oJHWXJDM040+SZ7+PEJY68WRDGgWqCeZpmIEQxypVhD7q2wW1Sk51Ebffcad8hFPW6+26i/fdXbnajF0RdX3xROj9lA0EU6ibUZA01lZYGXRPhklfUsTfh0wnU9+Fy9FDn9Q9+Vbev4kJYTpk+hWb9Movat2xP3TbqFnjG78upn9GkqwfSLm/PpzV/JVq2BtFbu6xLl7X7lH5ZZznVbl9L5047lz4989PAvFQgTittkQ0E0/g+UUGYEN9r3yHUzfdih5D5vnyXLq4GCRHRXAffY/yH4XyvBUj7tRNt1mmj/c7bNDi/yHyfNN4sCJNGXHN5phkIUY1yZXjFnYCWFW+7w00chLefB7SW5iTdXWyrzNrPEKJuqNchhxAhsqzXmT9ZMERdkzAQvO4exJkSXK8RJjiOWzsT3iEUVYCRMPiZwTRjwQxZ9Ems4rq9C5fSsQeOpT5b9XHNb/TFA2j7Ke9Sxx/LP3/VuQm9vMMWNGrtx63/btG0BY3bfhyt2HQF9dnWPQ/pimYkYZENBNP4PlFBiMIS4HvrDOG8eVQ3bpyeM4SC7+F+iTPUXpfcy36fzPcWUjpsjSh8bw1Hl/kiiPPtc41zAdIuCO/re5/nvCE7VLKQrsh8nzT+LAiTRlxzeaYZCDpIWhliGAnO84FeAUVCrDKXJkyguqoq6tW/P1UvXqxcvUYvnHsu0XXXyYtTvxLt5+1kdz8xmTZvHs4N1kuoeRlqYdAKe4awVIq0YiyqKlZ8cVH8Wc+eRXMWz3HdMbSfIXzi8yes3UXnzqK4M2r80eMrJvc3Xn6Bvrj1fNrp/cVUtYpoUTOiN/fYgK7uMJW+W/FdA2pdW3WlkZuMDL0KHgb+uN8psoFgGt/rMsyVxlzMfA/RWTr+eH2CUPA9Gnn55UTDhik1tyIx830DHLpsjTB8X9W0yhKDKpzv1ulOQSkEIS8Ahv9EivwmC8Kc975pBoIuklbuVsWIlFYUSzwSu4oNbRo6lKrrgwgp10+8YHdjHTqUaMyY0FlZ4cHtO6Cyu59hBKHfmZKGJU8XYd6uXXBUUScCwoBS6VNNK8bOqni5FQmhN7z7cNp0vU0t4Th78WzXvnQGnxk99HD6/aufU7ufy8k/6VpFCw86jk447cJG7ke7td+Nnn3mWRaE4b+STL1pGt+nIghRqAo3qO4qgkvEImC/flS9ZEl5DCH4ydy5auPJyZtYuLyxfNm48sN8XwFZHLZGEN8/3Pdhart2WxKLhbKc79fXdpfTDVtsSAs+XsB8r/xx8AtAgAVhzseBaQZCHCQdSxe7rTLjjqeTTiLq2pXIFqWyoU09e1L1m2+W7zTE77NnE519NtEMm3uhrABCxM6oK8UQk336rDaOHn+caOxYKbgqdgj/9jeiDTYgmjq1XCeve6ic7rduJTkNtT32INp0UzV3UqyCjx5dDv5jxxb9g/ahzS6P29gLc77DTRQ63UjbNG9jJZu7RN5A/Mf6w6n18xPot5+UL5j/aR2it/fqQkPGPO3ZZ7n5nqRGXTkR7xDytRMKw0VPUgW+R4EN311NDVV///3q+eCJJ+SiVTtrDc+KefOIjjwyXHvs3Cc4Fq6np51GtGBBYJ7M94EQNSRwcwPtXNOZjt32WKr9uDbUcQLZ0pnvZZHidG4IsCDM+bhgQZhiB9rFC8TQ7be7CpBS797erohOAYRJesCA4EbhUnkYCGEehPPGBcJHHVU+UyNznYajnAYD4dxzqfrTT1eHFVdxx5Ktuy53Uq9zofX1cE6mUc53OJtmF5ZT502lYZMV3L6WEt2w6ADa7bVvqVX9BfPvb7cWtTrmHDq47/G+KLKBIDvI8pHONL6vEE8hg3sk1nOSfI8FJ9/vzp7PJ58QXXllcBMQCOaCCyrnl+C3Vqfo2HH1ziLzvYVLUnyP4GCzF82mY8YfoxRsDHVUvTaC+V7lo+C0TgRYEOZ8TJhmIOSS0IRgcQZEqRcgpfHjy2cInTuEzvuZMBYV3DZDD10hjOByOmpUqEAuDYJwxQqqdu64qbhjyTYipHB1zR6uWw89RNS9e8X9WPaxN/HLiXTkw41X473O9AU1w37OZMizQ6xzhTLPaQsPof0/mUZbflW+YP6H9Yje7bY1nXPdozKvawmcIFVQgol4h5B3CBMcbo2LCuB7uOE3LAIGcb4s3+OqCJnrjryAcXptKALIfF95hlsGvrCcb89b9dqIXNpPAWAWme9lxpnONCwIdaKZQl4sCBMG3cul0e6aaK9SkyZU6tSJ6m66iXr99a9UHeTCGBS0BnnX1Ei5+fgiAwOhadPQEUpLnTuX25Tkqj6wkb0HUmZYOFxIxWTa88Ce1GlsJ093TtUL5cNEomu2ZE26/qddaNc35lCL5US/NsUF82vTtn8ZTTvvubdM66w0bCBIQ5WLhKbxfebHaAi+p44dqXT33VS3aFEw5wfxPXgau3t//CMRXPNTehoEIfO9VA+E4fwKs4GaEKKNThs8TemqIOZ7qe7hRB4IsCDM+dAwzUDIFKE5jYE5cxqfS4P7Jf7u83hG5PRyYdTlIhnX2G7XjkrTplHdCy8kKwjRniADSqXNDvzF2Puo1Ud06cuXBuYks3rrFUnOL/OLFx1Oe7z7OW0ys3wNyrcbNqEP9tyRzrvq/sA6ORNk6ntSrr37C0VeMTaN7zMlCDXxvdUmryjMbpzvF7QGXidhgtFo+tZENqXNNqO6UaOY77t0D0Q2DOc7xSD+2xlZOrBgXgCUgYjT+CDAgjDnw8M0AyEzBqxGF0XfKxq8om9qLF/rEK+PVOd7LlJrgS6ZBRlQKuXb8C+tXGmd9Txt6mn03aLVVzd4Zed3vgPuQpO/nkxHjz+a5i2RO+vZaXE7Gvb9JrTzvxfQmiuIllUTvblra2p2/Ik0cJ9BKq1qSJuZ7ylU7VkQOhEwje8zIwg1860y57uVnwEhaI0/eLmIYw9J7hCKwZ8Tvp8yfUpg9FAZGkQAmjEHjgl1hyDzvQzCnMYLARaEOR8bphkImSA0rzMiIceK1J19bvfz2aPB4fwIopKm+cBAue02K0qnZxS9qqroNZQ5g+gVvGbkSKIzzlDHatIkKu25pyUI+33Yj5asrA8V79OaEd1H0GX7XNYoRRh3oSsWHk67v/kZdajv4i83bkpf7Lk3Dbnk70ouQ87KZOJ7ij4iKnLgHUI+Q6h1SGnme0vkytzT6uR8O++tvz7RwIHhg8joAqie86XPRIYtN4jzvfj++usbR+qWqUPKfO+sYrsW7eiGnjdQx5qO1G2jbqE5n/lepvM5DQtCQ8cAC0LNHStcEr3OBIYoTso4GDeOqF8/99xlAw+EqJvUK4hoish0F1/cEITF9Z6tgGsdpMryCu/udl2E04hwu8ZDqlCEcxtHpb59lQQhsn706EcrVnJV3YX2WLQtnfJtNe30wRJquorol+ZEb+3Rngb+7XFq2apGtvae6dhAiAxhpjIwje8t8VQqeUdhjhv9GPheWhDmiPM979JNkvPdRCOu8Tj5ZPX7HVPie+dwDhukzOuzSPVbiulbLfICYEyQembLO4RJI665PNMMhNQJLQbxJSUIsVqMqKNTpqy+p1BEIa2tJerfX/PIkchuyBCiww4r18u+8zdhApWOP57qxo2jXvaLlwOudQi8DFoiel/FvYniPkfUD4ZB376hIqZaSIRYMcZrnVp2oq+HfG2t6MJNtMvYLtL3TI2cfzDt+tpX1HZ+uS8+3mINmt7tIDpn6HUSnSOXJPXvSa6aSqmKbCCYxvepC8IY+F5aEL7wQplX7TwmeDZJzhcRSHG3reMOXfFhui4A4se8cn4KfO9GclHcQ93yY75Xmko4sQMBFoQ5HxKmGQipE1oME7HUeRK/i9SxQ9ejR3wj1RkYBxe7i0vrnaXWr6iX5s6lutraSkEoDASsGk+b1khENrrv0HlZcpcu3i5S4qyfm4sQ8lmyRH2V2FFfcYZw6LSh9NX8r6TvjBKuozgz2OPe4H46ctE+dNTUebT9pyUL3XktccH8JnTm9RNDuwp5DY7Uv6cYRi0Lwv9n70rAoyjS9puEhBCSIDcxoEFgBRQUlSWAURBYEEUQAhqEBcUThQCC+guKKF6AQsRlV10PBIlCQBGNi6KgkUs8UUEuQU24QUlCCISE//l6poeenu6uqu6eyRzdz+OzS6a6jq+q3/requ9wTEZtW1Z+wHsmISQsI0yvVUszb6106OUnoqopNyO8pxcqK1HRurUrqIzyAFCuTMQPvroxX8Nn3N94r5Q53Qg2SGhgi3moQwhtQwGnIrcEHEIY4kvBIYQ2T6AfNmImIdTLByifvlLOvAkTjP1J6GSZyJqZh5IeU2hzrZNqdX1u+TBvPSlvVuPGQEqKKwrrkCG+t3fK02V/k14tuehEGa1sUYnMpZnchJCqJtPRk6dPYugyg5tcSjBf2gOd1xYhuQyoAvB9+5qof/ND6DPwZjMzx3zHIYRMEYVUgXDDe4k8VafJqB/wnkkI9VacEo/IMoPw88gR/6zPKVOAtm1d+KyVD1fZ6po1qOjbV/sAUFku2DE/0Hivmjm7zUMdQuifTyOSa3UIYYjPfrgpCNWqHLhPQ0E3VUVF5s0PVWtKlzzRySzLKV4+0bzpJlcSeb2HFIj333f9SqHKRR6tgDZ677tP1JmEUPm+EVmVx/f008CwYSK9Fi9LpPOoIuKn6mRcnZj+7g/uxqEyvkA+ZPrzev/X0XNBT81+3Xe8H3r8uAsX7iYaCOyvB3xzZVtM5EwwLz5Y1xvV/j2Z7bjBe84NoXNDaNuysjONjaJTuvjIsmaQ8ZAsRgYPZg/TbMJ5QcyvGDWKTQiDEfOVfQog3mtNnN3moVptOHjP/mScEvoScAhhiK8OhxD6YQJtzgOoqRzQaSpFwySfQR5z0IYNjSNn0manFWGTdXNIUeQOHPA27zQSKe8Noei0kDwokqqfnlPRwLzX7sauU/vR4lQiRrcZjrire3iNW72Znjp9CqmzU3G4zDjPpNzlqVdPxbTPp3mNgBLMP3/07+i04TBquRPMb7oiCR3unY0Onbr6abRnq3UUBL+LOKANhBveB8Whhc14L41JHWV00CDgvvtcVhw9tQ+NvBYSC+8Jt//1L9+IyizCSY3QXqE26WdgPtcNoeiXEAjMf2oAdtUsQ4sGrTB68EzE1azl6aU/8F4tAruih/KK1sF7Xkk55bQk4BDCEF8X4aYgBA2gGYW5pttDAfKiSQjlCHN2+rCofQHp3yNGuG4hjZ6lS6VUElyP7EN49KhvUBmuCnQKkdnqQw/ZejMrt/RAT+D5LkBl9Nm2Y6JiMKHzBMzodTaAi9bao6ihgxYP4hpZvVr1vHIOTjk+EF2+3oq0va4b299TovB9l8vx4JMLuOqzo1DQfE92DMZdh3ND6NwQ2ricXFUZ4T2Rs08+AZ56irtZH8wPFN7Pmwds2uQ6HNR7Jk0CZggErmL5EHJLRVWwmjHfbrzXEsPbg97GTRffZFZCwu85eC8sMucFhQQcQhjiy8EhhH6cQL3cSPT3Jk1cvnEcjyYhlE12/OTDInVLNieKjwfKy/V7KnpLqBdllEMWukVIHmTOSZFC6RE1e9WpmMjgTLqIi9IuMKnLJA8p1NtMH1vzmM/Nn9FQ08rOxZT9qej4dQliK4ETccDGTvXwf/U2I7ZBPF66/iVTSYfNiNdREMxILXjfCTe8J0kHzRo1yoX36ad8N3vupeOD+YHE+8REoLTUPrynOVq2DPkxMeg7dChiy8rs+UCqGfPtwnsjYVAAGQfvrS2XSD4AtCY58bcdQigus6B6I9wUhKBRDlizTDeEFImT4/FSDoiYKaNwsnxYiNTRTZ+/k9JTCPQePThG4yqiG4acuwZ3QXWEOjqpN5NXSqNdMhNNmOJ9M6guRjeFZQ+XIa5GnKZiKiWZ/ygbhSWFuiOjYAF1a9WVbgefLBmA9A2/IMV9VrAjLRqftWuBnMQVnvepfN6QvICQwpD5ngTWTSQrCOGG90FFCI3WoKAlhwfziUDRgZtsohmqeC8H/pk4EbE7dwp8rRpFgwTzo85E+eS/FMV7HkE4eM8jJf0ykYz31iQn/rZDCMVlFlRvhJuCwKXAGp3kBmp2BG72vJQDSo+Ql+dtoqnnw6KOMmpjoBsfMVHUuf9FQLMAACAASURBVCee4JaeZ56SkxG7f7/LD1HAjNarIaXJKs3t+ee7TEctPs8/3R/3n1zOrGV279kYlz7OhxCKJJmfXH8MLiwowGU/lIMsUynB/IYuKchOXg3Ee3eBFISmyU2xO3u3J80E5TAs+L0A+0r2ISUpBRnnZdiSgoLre2JKKLgKRLKCEG54z0UIQwzvpTHJPoRECBcsCB+8790bsRs2uCJShzjm33v5vV6EUATvR1wyAvN/mM8FjA7ec4lJt1Ak4701yYm/7RBCcZkF1RvhpiAwFVgtXw9lbqNAzQ7rpJf64Q7o4lEOJk1C7DPPaPvr6fmwyPkA/RD4wEtUZglh376IjY0FlixxpZYQfdTmqpxEuzIKKDgf2JcIpJQCGb8BMYrgqsvaACNuronSMyeZPRp9xWgMvmgw9h3bh4Q9Cejdp7c0Jt4k87P+uhad1u5G/WJXUz+2roH3WjVEbuJqw7ZXj1iNbmndIJ1K/y/bK6E9EcacPjmWbxGZ3xNTOsFXIJIVhHDDeyYhDEG89yKElZWI1fLPDnW8p0EGEPNZeE/dEcH8ARcOwJA2Q0zhvYyI0YhGlZQ8iO9x8J5PTupSkYz35iRm/i2HEJqXXVC8GW4KgqECK5MitX+ZKr9QwCaGdbO3eLFk7lmxbx/yExLQl05XyZ9P72GdhD/wAEDhyM3mGzQSjKjJqDJ3WHQ0pFQdhfpmlcbMaDXQrZurCIdpFm382X2Awjpna216DMj5HzBwq0sxyKS0hzp+g+q+JNdMRvHJYtSKroXc9rmgRMXDOwzH1DVTDbudVdodA7YfQrttp6VyR5KBDV0vwKRz8rmW4KKBi1CzRk1kLvbNeWhXziqHEHJNRcgUCje8NySEIYr3dHtW0aQJ8ouL0Vc+MNNaYSy8t9F83qd5K3hPB4DUdyt5EpVpL8aNA3JydL9BFt7Ti6KYT++I4r1VkHDw3pwEHUJoTm5m3nIIoRmpBdE74aYg6Cqw8o2cHulQ+yUEao70TrDvuANo1UpK/FuRno78lSuNlQNWf/WUI9Z79DsRtiqDk0zRoDLqQBBr1/KlztDrqxyBj36/8kqA6tN5PBs//a4kfGdc/3xnCTChN1CYrB9IRq9uWUEYunkoyqoMAieUAznF3ZG+bh+STrgSzH97aTxebXYGBYk/ol5CPa5UFauGr8LI5SO9bgaVfdMyNeKZbmUZhxCKSiy4y4cb3usSwlDCe0rjQAd15OtNppR2Yj6lqxB9/I33RAgFg+z4DEHGfDrkNIiIysL7vMVA/1+AtHHimM+N9wz5894UOngvupBd5R1CaE5uZt5yCKEZqQXRO+GmIOgqsJymhBBJuGvXPCpPenfsAF5+2csHrqJlS+TPmmWeELKUI9Y4KKn9O+/olxJJO+GuxWueyCdy6FBWL/R/l+ds4kTDFBlkNmS48Z8BGh4HDiWa64qsIGRtzsKJqhOalWSX9sM1P+5Cqz0ugr2vAbAhvTUmJ73nKT+j5wzMWDdDlxTKRM8okb2ycdnUyMyoHEJoRmrB+0644b0uIQwVvE9JcQX8mjDBy0KiWjHf33hPhPCRR4Dp081/KIT5JDcDNwMevG9WDLz+HtBzhHhXePCet1bZykSrvIP3vFLULucQQmvyE3nbIYQi0grCsuGmIOgqsBymhNL0KG+bAjVfMiFcvlwz8mhFQoIrZ5+WPwnLbIjGwKsc6Y2X8j3VqgVkZ3ubdVrwvfSaJys3hHKSZJID9VFxk6n2G6F/m9n4eZeBkYJQpywRTx9tj/QNRxFfAVTEAF91TMaTjQuxJ2GvVxP3dbwP3Zt3l0xB6TmDs86NSlPQk6dPYugyNpEmU6Osdlm8w/Aq5xBCU2IL2pfCDe91CWEw4z112izm8+A91W/lBs7feG+FEMqWPBSp9Nxz6frH862Zxfsp3yVheocS4W/WTkJIQclyNrjMXh28F54KwxccQmivPI1qi1hCSBsr75OcTPZnwfmEm4Jg+YZw9mxgzBhXQBd/POoNXeN0WN2sV1CZrVvP9k3L3JTMjii58ODBZ6sRSHGhOWRKrPyf/wD9+wMFBS6zpkaNXEUPHpRMnJCRISQzr3kiEyWBvIyePpJyIEdcpeA5iiilWn4jdcuAPxPsmdSkuCSUnPJWIvQUhEdLb0SXr3/BeftcxG7PuVFYd0UbTK+9TLMzctRSrWAxzZKbYU6fOVKwmDV71qD7/O7MATk3hN4iMqMgOHjPXGbVVkAT83kPwSgYFqXLEcQv7sFqETg6+FMfrqkq1MR8LbyvV89V1+TJ3vsCuRxQXlYzjxbeE8Z36QKsW+cxaxWRmc8cmSGsSl9/Gnf3s9hnBe8f6vwAnlk/Q1hSdhJCwmhKO6QODubgvfC0+LxgBu+ttxqZNUQsIYyOjkaUDFA6c3/mzBmpTKU/gnjYtN4ihhDyRPWUZSrffCkJkAnS4zNFWhs6xzx65SHMz3cFUGH5BE6aBMyY4Spnxo9E2S910B1W5D6jU2z3bz6BckQjztHNoBxBlfpKJP7FF6VeG/mN6CWYVw43PiYe5ZXlHDPjXUStIPytrBke3NsYHb8pRY0qV4L5DZ0b4P66X6G81inN+pV5DamAUToJ+o0imRYVF3mdKssVOz6E2lNoRkFw8F74cwjYC5qEUATvqaeE+eTHR2TI7ccnQng0B6uFk+RvrbjV0hOSD+YTucvMBNQB0eQKqF5yNaDHqBzPrGgFWbOI+RVffOEKkkNphq66ytWLc84BSkt5euQqo8R8xQ2wVbzvmdYTq/as4u+Hu6RdhLBpUlPsGbdHShPk4L3wNDBfMIP3zEqdApoSiFhC+Pnnn3Mviauvvpq7bKALRgwhJMHypl6gDZE2XvXmbUVpYBE4g4n3Ug5efdXlN8ETlfPttwHyqzMbvVPZJ9lUh5Qmal8vUiu1R5u1sk2ZYFN97pNxz5goUfGzz7paYhHXW291neanpvqe6LtvCJl+IzZ+YImxiSitOKvQKBWER4/1Qqf129HEbdG0vXk0VrVrgVfrrMKJ09r+hdS1SV0mYUYv/tNqOfcVvatnakS3iWYfx2TUJTkH782uIP+/p7tGefFer4vVhPfUHS/Mf+UV4KGH+HCck3AyZ0UZZI1uNLVIpkwcOTC/4sgR5Ofmom9WFmKpj1lZhsFgpP5NnerCeS0rFPcNcCDxvl6tetItnvzwEMLkuGQUnzK2Jls6ZCl3eiAH75kr16eAQwjFZWb2jYglhGYFFmzvRRQhlEkhw1xHaI54/OgsBnXxOS2mDirMZXT7W6cOcOyY0HCYhekEncxcRR6ZYCve8Uq8fOIEQCZARifnrEimp05JPoRrzqtC95EinbOvLCkIM2pNRt2vluDSzSelBPPFCcD6rqkYn/ipJ8H8TRfdhLwteag8U+lpnG4GJ3SeIEQG5ZdZpqVWRugQQivSC753ww3vJfKkTGFD/mnKx6RVhu7MBQDvpTHJiemJQD31lJc5fEBXFaWXoCAzHLeaXv1SYb7XeMrL9W865UpYeO9OW7Em6UjA8J5w+52fzwZX4yGENJxp3abhhY0v4MiJs/6O9Pf6terj5X4vc5NBB+/NrXyHEJqTm5m3HELollpBQQFeeukl/Prrr1iyZAlSU1OxYMECNG/eHFdSKPwgfcJNQeBSYGkzmTvXnk2WJ4chrz+Lzhrx8Seh/IRWonIGwVr0UhCIEPI8rAiwEycid+VzGOqKxRLwhxLMp3+5G/XcroWb28TivRYN8HaSd4J5CvIyqM0gzPt6HnYd3YUW9VqAktvH1Ygz3WcjUyPTlbKUbSsVV+O7digIDt5X4wSqmmZivmzCTn5rViJbUrsBwHsvQjhpEmLppmzYsOoROFltUBRpi49f8H7JEuQ+PiQgeE8Hdgk1ElBScdZvnJcQEt4PuWiI5PNN/9HTLa2b9B+ZiZp5HLznl5odeM/fWmSXdAghCC+XYvjw4bjlllskErhlyxZccMEFmDdvHj744APkk99XkD4RSQhpLnij0PHMGyuHocW2fKKMWiSYmkPyx22igexMKQgU/Y7MRQ18fNYMuxLdW+nnIeSZTtEyI0p64/rtRbhouyvB/OE6wMauLTCpzoeaVVkJ8iLaN6vlmcq21Qaq4X2rCoKD99UwaUZYYnRDqHzPIg57qvIz3kuEUBlZWhVAxRbp8+J9UhJQIh6BU91HU3hPEb/JPUEOZKbhx7/moZvRvZZBSiRbhKVdCS8hdPDej5PAUbVVvOdowiniloBDCAF06NAB48ePxz//+U8kJSXhhx9+kAjh999/jz59+mD//v1Bu2BClhDqBC7hVmD9Qaq0brBsuI2saNUK+TNnuhzyaS1RhE9eMx4y8Tx82DgYAeUYJF8NVRhvr0VLShBFy7XJBNWUgqA2V9Uw35ICrTzdCEUVR3FGmXjeH19gOfDCsW5IX7cfieVAVRTwzaW18HKz01hb+0efFu0I8uKPYRidNnN/T/7omJ/qtKogOHjvp4kxqtYgUBX3GrUb8/2E9xIhVGL+3r3AuHEuHLf6kCkmL95TxGpR9wCd/pnC+2nTAPKf1PJHH+jyi3bhfWMUVRzxP96rxsYihMGK97LcCn4vwL6SfUhJSkHGeRnSbSX3t2R1HQbwfat4H8CuhnxTDiEEkJCQIN0KpqWleRFCMh9t27YtyslmPkifkCSEBhHPKvr1k25k+/bti1i1P4lyDkSj0PHMnzqHoVX/FVIC+vdHxaFDyI+Lcznky+aVPMEDiDBRGg05ea8yEIxW5FBWUJexY4EXXuCRBLOMKQVBXauO+Vbez3kYnKdIu8HsjXiBSccHIOP7bWj5uyuVxN6GwIZObXBxxgjoJaYnBSFvSJ6wz4h47/jf0PI/bJrcFDl9cqR+OgqCrywdvOdfX7aUZES45F6jdmO+3XhPwjLCfB5h0r4gp5uwivfUFwrWZcMjhPeE63QrSuPQC14mpxsCEAi81xIBDyEMNryncRhhfr+WnPqTDWsiUFU4hDBQkgYcQgigRYsWkv9gz549vQjhm2++iWeeeUYii8H6hBwh1IvW6SYHFXl5yI+JYRNCCRmXuaKn0aMXzltk4pQnxhaiinqF1162DBXDh7sS0ysJIU+/lKHI1YF0lCG8eYLeyCfLPXvytMws46MgJCa6QpBrBKAxrExlviVtdh9lo7CkkNkHMwUowfyzh9uh01d/omYFcIoSzP89GY832o0jiaXIbZ+rSQgptHjOtS6SFSyPHLFOGZ2U+kbElR5SZhwFwXe2HLwP4Apm4D3lIeU+BLQb883gvV4EayUeW8X8227zjfQcbHivt4T05KMsr8D8ZduX++TuC9TqNCKEwYj3MhnMXJzpk6bIg/mD8hCzi1N/CpSgLbbjEEKLAhR43SGEoHRvMzB//ny89tpr6NWrl3RD9dtvv0lmpI8++ijuu+8+AZEGtmhIEUIWcYmKQkXLli7zStYNoSxmo1xRvORE7VPC6qdyimmjplQOZJ6j9o1z1+MVsps3AIuk2bttJuk01SinIq8pFUWcGzmSL/w5Yxn7EML4eODBB4H//hcoKjr7Nm9U09WrsazxUQxaPMhvH9C00huR/tVWNDvgamJ30yh82aE1nk58V/q3noJAUeYmZ0w2HUDAHwOScxgWFmsTZ9ncafvo7Vj5v5X835M/OmtznVYVBAfvbZ4QvepYOOrG3Yrt25G/UmCNWrXcsIr3dOumh8d2YT6ZherlVBTFe8JjvQNTzoM8zRvCyy4Dfv/d2xSW9sPbb3elnGA8yxZPQ+aWxzRzsLLeteP3UMJ7Gi8P5res0xIzmwvoT3YI0s91WMV7P3cvrKp3CKF7OidPnozZs2d7zENr1qyJiRMn4oknngjqCQ8pQsixkXk2Hl5CKCFlpa/jOuVe4klPoWW2yNFPaVGQOSclVI/RiTTmrkfI3EZrtZHp6J49+u3wBlsgE6maNfUTH5MstHJSafRJd0zU1zvuAFq1AiiIACkjHBH2Kt9aiMaF2T6hva1+fJR7KuVIXdxfdA6u+Pa4lGC+LA7Y2KUh7j9no1eCebWC0Cy5Geb0maN5K+ivKHG846Vod93nd2cW/2zYZyj+qdghhCpJOXjPXDrWC3DiaMVnn7mSnlvBfPLPGz+efdjlT7wniQUC883gPfVNixTKEUjvvtvQ39BwDyPiesstLpJMvuwcUbSl/INP1EPh6bN5Aa0vOLEaQgnvpaXFgfnymIS+JTGxBby0QwgDJ3KHECpkXVZWJpmHVlVVSb6DiXR6FuRPSBFCjo3MFCHUmyM1UdRSGpSmOHI9HP2Uiqp9UNT9cNdjmRBSveSg/+ij2iOlcOw8pqCyiZTWCbvaJInaovxZOg9zTG5fGoms9+wJUgAKzgf2JQIppUDGb0CMy31P+m3O/Lsx8df/2P61zSgdgI5rf0Fjt97xS4sYfHxRGv5T2zeCaLPEZpjbci7K0sqQUueso766Uyy/PdsHoVFh7o+5GLpsKLOpRQMWIWFPgpiyzay1egvYpSA4eO/neeTE0YpFi5CfYMMarW68J3EGAvM5iTZ48Z76/dZbhgd3TLyX96jJk12Hs92762I+4f3cTsD4Pv5bf2QhoTalV7cmk6fi84rRrG4zT2CWYMR7aWlxYL5DCP23piKhZocQqmb5jz/+QFRUFJrSTUcIPCFFCDk2MlsJodb8GUS78xTn6KdUlpVXz67TYrljdPKqNiPiuQnVCrOuJQdqRw4RfuCAYZ5HLgWB6mvaFMvO/QvZXUtRWOfshDQ9BuT8z/Xv7OtiUJh4Nsm7HZ/dtcc7Y9juMnT48aRU3bHawIauzTC+/ic+1Yv4X/D47QXCz5DntJgG6twQGq8mB+/t+Np06uDEUVM3hLzdDiTeU5/sxnw6CJStLej2jaxRyIXg5ptdljFaDy/eU12cOR5F8J4sZ5bl3I3sjkd8MD/rRyC3Hbz+zjuVrHKymfzz/3ge4z8eDz1zeqqHyhJ5WtR+keFhWbDgvbS0nBtCJFOkdOfxmwQcQgjg9OnTmDZtGl544QWUUmAMQLodHDNmDKZOnWoc7dJvU8NXcUgRQg6fEmEfQj4xeZdiKQmsaHasPFZya1b9SdRjkzdw+e88kUp5EjFTfVq3hur2FP3hVRCWtQEyhwDSZaAijUTUGfffVH/nnU7a0Mkc9MiJIz6vPH+0Nzqt/Q11XZ8yfmgbhxoD7kGDq1pj+S/L8daPb+FQ2SHPe7JpKCsAC48PB0X43J292+/+hnJfioqLNE/CHR9C/ZXk4D3vV2axHCeOCvsQinYrUHhP/bIb85VjpUPqrCxg1izjIGqE+YoonrriEvDF5MV7aSvRwXzXJuB+TKQUMsJ8ZSAtOpBTmvTvOLoDr3zzilegMgnze80xDMASTHgvLS1K0ZGTBiPMd3wIRcHBKa+UgEMIAdx9991499138fjjj6Nz586SfNavX4/HHnsM/fv3x3/+Y78pm13LMKQIIW1Ad94JHPFV4uUAKkJRRs0IkREC3VMllTNK40C+F+5cSobdoIhzw4YhPzdXPMqomfGp39HI9edTrYmIqjwKguQnMg4opEM9LQWAFASTigGNYWKXifj31/9G6SkX8xtV2gfX/vIH2u5wnZwfOgf4+soLMWHWe15D1vP/Y4W/5zmhpYYClchYPr2mNpXmUU6UUeMPx8F7O4CFs44HHgBmztQu7CYuQlFGOZv1wnG1L7kWJtqF99RwdWI+HeKR2epgRtoeQcznwXuJtPgZ82+48Aas2LYCVajyWgmJcYmYP2C+bhRoLcyvqqwyTHEVbHgvLa2ty0BRRnUx34kyKooQTnmFBBxCCKBOnTp4++23ce2113otjo8++gg333wzjtmUzNsfKy9kCCFrA6LbrrFjXQl9yZ+kd2/EUuRKOx+OEOgekmejglAxfTry27WrHkJIkUW7dfMNuiMHwmHd2pL8NW4KtRQEtZ8g/bvnCDsn0FUX3cJlXZyFmevcimY5MPevq5G+/gBql7uUkm86JKDshizcc/NE7g6wCCGPDwc1tmjgImS1y+Ju10rBJT8vwej80ThcdjbptTIYDmtMVtqurnet+hA6eB+gmWNh/k03ufK0NmkiHlSGZwjVhPfUtWrFfHJlIPNS2fyfgnvJ5qYSa6sE0tLYAXgUMubBe/ILJz/x7iN5JkesDGHaZSmXYfm25bovTuoyCTN6zeCumIWNwYj3NDgjzGdZuXALJ4gKWsX7IBpK0HfFIYQAGjdujDVr1qBNmzZeE7Z161ZcddVVOHTorHlZsM1oSBBCng0oOhqoqoJn45k4EbHPPst3C8czKaw+KM1AqT6jDZPXZNTdr4rycldYddE8hDzjYpWh4C5kPlSoSE+gPCHnDUhDEVV/+w1YsEC64VUrCGQmlN3H2zekXhlwNIHVQfHfV96yEre9fxuKSoow8fgAXP3dNrT4w2WPVNjIlWD+0cR3JeK4J3sPt/kmS0HgPTFeNXwVelzQQ3xggm9oBbdpmNAQ8/rOQ+ZFrlNk1pgEmwyK4lYVBAfvAzCNLLzVIhuVlYjlsbrg6T6rfT/ivfTdHT+O/FWrQhvzp0xxHQb+61+u8SisXLTwnvzCM38G5nThmSD+MrN7z8Zdl92FpGeSUHlG39c8JioGZQ+XIa5GHFflLGzkxXvq35i/j+HeZ7g6p1OIhfmsMVlpu7retYr31dXvUGzXIYSAZCr6yy+/4PXXXwelm6Dn5MmTGDVqFFq1aiX5EfrzmTdvHmbOnIl9+/bhoosuwpw5c5BBJ3ocT0gQQs7gAtJGWquWa+MZOhSxlLOPxxeCQ06ysz+zKJ2u0tOdHdKfGVTG3ZgHpKuDEGoNWPYrpBQTr74KHOUI/U3KwWOPuWorKEDFBx8gPyNDmqcV55dp+glKPiMmTEJZczQlYwpeXjkXjx9qg05f/YWap4FTNYCNHc/BI4224mDCMU8VIuabrM2U5cMhNxqIpMa8wQ5YY2LJOhh/t6ogOHgfgFk1i/l04GQHKeRt3w94L+1jq1e7bj1DGfMXLnSlk6isRMUXX3jGsyLthLFfuM2YTxYXB44fwPiV45kLl8jZuPRxzHLSHFVUGJqM8uI91UWHjzl9cnRNVrk6xCjEg/nODaEdko7cOiKWEA5UbTqrVq2SyOAll1wirYYffvgBp06dQo8ePbCMTE/89LzzzjsYPnw4iBR27doVL730Ev773/9K6S/OO+88ZqshQQg5w497EULaSMvLpSiV2L1bPwefLCFW4ADePlAqCXqGskP6M9NOVCchJNJHt656keiYK0ujgOJmUd5Me0+6H62u32XsJ0hV2agkPH3iJly+/gc0dSeY/7VZFAo6tMaztV0J5pWPiPkmS0GgevV8OJRtqgMcmBG10TsiwQ5YfjJ29y0Q9ZkhhA7eB2JmFG3w4q36EJBcB0Ic76V9LDfX5frAIoQXXOAar17ieJFpsxvzGzQAXnpJIugevD99Cq023IzC2lWamE7BwqLPuMz27cJ8OtRbumUpXtz0IlMa93W8D3P7zmWWk+aIQQh58d61vbk2uLwheX4hhbyYv330dqz830onzRDXCnAKqSUQsYTw1ltv5V4NdHPor6dTp0647LLL8O9//9vTBJmuDhgwAE8//TSz2ZAghLyntUrlQLmRstI78ASK4e2DH06MhW8IyXeSyLDZhxQDOxQMH7Rws7q8PMiBIJLb1MY1ub2Ee6rME8WTM4oa6FDWGtmFCbj82zIph+HxmpRgvhHuO+cLQMfd1M4bQnmQRArHfjRWMlnVe+Qon/6IOMprykRj75ra1fAUXHjiguAFM4TQwfsATxwv3mphfojjvUQ2RG4IJ00C5s4NXsxX4v3Fybhm4TVci4kX13kwdO5XcwN+Q6jE++z/ZRumsZBJob+iTPNivpNmiGtpOoV0JBCxhDAYVgTdQCYkJGDJkiW48cYbPV3Kzs7G999/j88//5zZTdOEkHWjxmxZoAAr/LiiKs1oZkYJ4HkDB7B8SqgPlOOPfOUaNXLdEFIie71HLpua6u2wr1HeQwjJL3LnTmPB2UHm6CYvPd1lbmv34/a9kUPFUxL3oe+xb1MpRcTRE2dNU+XAJ9Q9ns322WP9kL5uBxr+5RrQlpYx+KhNM7ya6E5mqDFOu30IlU18+uun6LmgJ1O6IoSUWZm7gEiwg8zWmQ4h5BWsn8tVK97T2EIF8+3E+6Ii/cMxymk2bx4b7wnzCOffeAM4eBBQB2nRwnxev3Gqu1497cjbRuuxTh1AGeyO9qMrrgA++sjeVWwS78lsM29LnheJIsy/+eKbkftTLhe5ooHIN24nTp1A4jOJqDrjHV1UOVi7fQiVddMNHS8prVbMH7AICXsSnBtCe7+CiKnNIYTVONV79+5Famoq1q5diy5dznpiP/XUU5g/fz62bdvm0zvybaT/5IcIYbNmzST/w/pkbsPzrFgBPPggQJul/NCGR0Fc+vXjqUG8DLU5fLjrPYPbKyKEn7z2GnrddpvLh5AeuiklktakCUBpQZQRMtu18x6HsmfyRr55s+udRx8FcnLE+856gyE7IoSffPIJelHQhGHDWLWZ+53mnqL2nXOOS3HZu9dcPYy3yBRofTNg/+MPIqHmpfgx6Uc8tfYpZlsPdHkAdWvVxe4/d6N53ea4vcPtHud/2mzXF67H/tL9aJLYRIqY+fCnD0s3cH2Kr8DgX4/h0p9PSW38lQhs7Ho+ZjX7yYtganVg4Y0L0e9C/vXsmadevZi5R0nZGfX+KOa4X73hVWS2dQV4sev58vcvcd2i65jVfTj0Q3RK6eRaexxjYlYYJAXohjAlJUWK/hxKiYqrDe9p3kIJ8+3CexqzVbyVD+iItCl9rXkxX7mP2f39EOZ37Ahs2iROKAX6Qpi/7q1nUBJ1Pn6r/xseWv0Q8+2HrnwIXZp2QcHvBVLZjPMycOV5V0qBV4zwXq6Y/LCf6fmMhN+UZuLBVQ8aWmTQe9mdsvF4g5exeAAAIABJREFU98eZfZMLiOA9vRMKmP/BTR+gZGuJg/fcq8ApqJSAQwjd0sjLy8PixYvx+++/S76Dyufbb7/1y6qRFYR169Z58h9SQ08++SQWLFggBbpRP5Qbcdq0aT5/X7RokXTb6DyOBMJFArs/m4dOX/6Oc467RvTdxXE40nEgzmt9abgM0RmHCQmUlZVh6NChlgihg/cmBO+84kjAkYAjgQBLwA68D3CXQ7Y5hxACeOGFFzB58mSMGDECr7zyCsjfZNeuXdi0aRPuvfdeiaD54zFjQmTphpBMhkRu1PwxaOrD+vXA/v2uGz9KUj/CnazuzBkpyqjPDaHXEYbbj42i0dFN6Sj2LY0USZPauo59q2I4ZHdqDKZY6DSZHveJsmdMdENYt671fjA7YF+BFRcCD/YEiii5vOKpFV0Lr138Gm776TacqHLf5Ao0KzvhL7hxgc8t3psvTkfdVUvRZpcrxPjBusA3V7bB6OmL0O7f7QxPiuvXqo9f7v2FO/S4sstaJ8bq0+zOTTt7TrmpL3tL9nolhZfro/GlJqVi8z2b/RKOnE7Nh7/runHXSkovy1X0FFxgCqutqNUbwojBe5qhYMP8XbtcFh/SwnWlijHEfDkisijeZ2YCX35pDWvpBo6ijrOsLc4915UInkz03VY3njH9/DNip0+vtm9FtGFNvD8D1IrxH97r9ZGwl4X3yTWTsePeHYiPE89ZLIL30qfk7k8wY36fC/o4FiGii94p75GAQwgBtG7dWkotkZWVhaSkJCnC6AUXXIBHH30UR48exYsvsqNbmV1TFFTm8ssvl6KMyk/btm3Rv39/+4PK8Dr6UzLzHv7Po+YZsCIojMeHcNgwxJaWaotVziH13/8CvXuzRU/jId8Pnsih7NqES3il0pg/Hxg/3qU4+CPwi3Dv9F/IawMMHuL+XRUllAhhbvtcZG3OMkUIqVZ14JWSY8V488F+SF9/EAkngdPRwNeX1cbF9zyPjl2vgr/99tRR57RyPinDi+tFHPV3xDl5xrT6p0xKLynbHJH0bFwyAanKTFAZZcciBu9p0LyYzwrkYufMqoKAafqNK9sTxXvyAx8zBli8ODgwv1Yt1+Ggg/dSegbeQFvBhve0JIMd8x28txOoIq8uhxACkqklJaE///zz0ahRI+mEhdJP7NixA+np6SAFxF+PnHbiP//5j2Q2+vLLL0u3lD///LPUH9YjFFSGNxQ43XC98oo9+aBYA5B/dwc8qNi3jy9cN71HYbGNAr/IdZO/x513An7OJ6k3VC9CSKfOzz3n8vejJ0iVhCVtgaxMoDJae1R2EEK5ZnLC/+HNxWhZsAYXuBPM/9EY+CmjIyZMf1MqRhvxHSvuYPoNUlmRVBPK0Sk30xU7VyBzcabP7Z+a7PGQMi0J0mkz+dfsK9mHlKQUyceG/GtEH1Y9joLgK9GIwXsaOi/mjxvnCqgVqEcR4Kbi4EHkp6WxUzTw4j2NgQJr3XFHcGA+EULSIewIGOan+Qk03ndL62Y4kmDFe3kvUgdCUx/EqQfHwmmRaTWqy8F7EUk6ZdUScAghIN0Gkk8JpX/o2LEjbr/9dtx11134+OOPcfPNN0u3hP586HZwxowZUmCYiy++GLNnz8ZVV13F1aQQIeQ9LaaWafOyKyk810hchYRTNAjUDYoqV1IScBLmcwJON5Zr17oC3Ph5bYmIRy67rA0wiG4GDXIH2kUIm5Y1xKMHL8DfNx1D3GngZA3gq0510ev/5qN5y1YeMjho8SDuoawavgo9LhC/4fbk2urTG63mtdKNhCebg74x4A0cPH4QjWo3kvpG/5+H3LFuHnkJJJVjkUp/KAh2Kjfck6ooaPWGMGLwnmQmgvlLlwb2ENA9p9w5+8wsFjqAq4bbOR/MHzkSeP/9iMd7mkIK9pWanKp7GEb4GGx4TzebO8fsxLrCdVK/RTDfDN6TnLRwloX5Dt6bAQnnHVkCDiEEJAJIkTrJbJRu6iZMmCAlif/6669BCY1fJR+0IH2ECKFA+geJEPImhbdRNn4lhFb7SZFKq6qECaWPcpCU5CKm1fno+ENSRLm0cdBPNO/usx2E8InSG5G+YStSD7kq3XleFL649EK8nfI1cvrkSAl+aVNsPKsxjpzgv6Un370Xrn1BOEGwvPaSBXJtyVMom5L2v7C/IUmTTY6Ufn9Uh5GZqZZCQb6S9CjlojRnlftlt4JgVrnhXeo8ZNMqIYwYvJe0ykogLY3PRL1ZM76k8LyTyVlOKGcfZ52eYjLOid7OyRGqqSIT5v1MM1jRcVgtHwR4Lw+hQUIDKZK0GjuDHe8bJjTEoTL3ZkWX0MlNMfsfs9GgdgNDcstjaaKeXrOYH2p4L0EUw1rGKt5b/XQi6X2HEIJ0/Crpvxo1akhzT9FGv/zyS7Rs2RJ333034uLignZNCBFCGoVe3j69EQbSt0R5Q0g5+ygAQTCYVMqBDSZOBGbNcklKoF9BpxxQ/8nHhpIhq541aUD3kezlboUQdixtjfsKa+Hy708gmhLMxwPruzTB2DprpATzSnL008GfMHXNVHaHFCXM+PDRpvTF7i9Q/FMxvk38Fo9/yR++nJqWkzATUdMjadRGWk6a4c2j2sdGj0BqCURr3HYqCGbIrMjE8ZJNqwpCROG9jPmDOG/YA4z31L0KOWff0KGILSsTWTL8ZemmkNf1Q8Z7OY8rBagRwHtpTLVqIT83l20Gyz8CayWrEe9ZHQ9FvNcbk/JQzgzeS5/r1mWa7go8mB9KeC+PVW1+qz7YtIr3rPXn/H5WAg4hDPHVIEwIZQWB/Ct4zBWNkgT7QXYeQKOInLQRC5Iv7i4pE8uTHyIFeiks1H6dTs7nzHGZU6kCIvC0F3TKAXVaxx8n92JgKEfaPLOEcMax65C+dhcaHHNJ7ue/1cBHF56L1xI/9hKlZJaZnIrjp47jz/I/ecTsU4ZOdGf3ni3VY+SjJxORI6VHLAfKUXdCqezUq1UP3ed3Z45FTmzMUij0FAQlqbRLQWD1RR0giDlIVQERshnJCoIpvCdZE74RhrGeAOM9dcezRokQUu5ZQfLFGpL0O1m7yInld+wAXn5ZP3+tEu/l/TI7W39/0OhA0GF+NeE919y4D9QiHe9JViycZWF+VWUV8vPzLSemZ/XDKt5Ln5UO8VUfbEYy3vN+P3aVi1hCuJmSlXM+7du35ywZ+GKmFYRPPwV69mR3OMAnxl4KLCUWVm/EdppbUhCFxo0BLQWBNlBKaty/P5CR4UpsLz9yQITly7mUrGpTDihQRJ06gEbeSr2J99cN4aDjV2PwzqNov6VCavrPJGDDlWm4/5z/sdegDSW0zCnVm5JZksvqnrx5Pt3jaQx7dxiruCcozpo9a7gIpFaFMqm0ixDy9kVulzlIRQFR5cOMghDxeM/rSxhgvPcihHQI6E+8J99twnHC7YULvQOSGeG9pKlXUoJg7iA11Yb5lP5iyRLuz89feM/dAT8VDEW8J1Hw4qwe5ndN7WoLIeTthxm85yG+SsL5159/oUGDBpbyzvppmYVdtRFLCKOjoxEVFYUzjNNIKlNJm0GQPqYJIcu3pLp9CPv2RWxs7NmNONABWJRmQ3QzqPdw3BgGXDmg4Dmvv+4is+Q/pHPzSf6CBecD+5KAlBIg4zfXIMmHkPIOntEJKhMdFY1xV4zDVRVXsdNOlAOzS3sifW0h6pQBVQC+b1cT77RMxor4goB9VVrmlGoi4i9CKA+SbivHrxzPHLO8yeb+mIuhy4Yyy2sVkCOt2kUIeftiJsKrqPJhhhA6eM/wJawmvPcihIT55OtGxMsfeE/Rs/WsYqzgvYY5asAxnyxeKHWVkaUL8VoZ8xOBlFKgy+9Ai2xjvI+JisGjGY/ikuJL2Hivg1Z14+uatvQwA4ChiPc0Tl6c1cP8zNaZthBC3n6YwXsR4kt7Ybukdg4hNPMRmHgnYgnhb7+5tV8OofGkf+Coxi9FTBNC6o3sT0j/X0mMeTdHkREpwowjJcX31s1dl48CK+rzKNInVlkeJYnGRafv9B8955wDkK+h4jGtHJCiQQ+v74vcJp1+33KLYYRBiiSa3QcorHO2o02PAc+vBLa2SMbUy4t1pbM4czFizsQgZleMoYIwuvR69Pr5V1z4K9FAYH894Ksuf8P/Ja3AiEtG4I0f3mDNgK2/q81c1Hmu/E0IKbreQ58+hKLiIt1k9kpzT16ipCWkULohFFU+zBBCB+8DjPe0KM1gPlmF8PrsiQaKYaEJD95THadOucgX+bi3aAG0beuTD9cU5mvhPe8YCfMpvVJ3fZN0Pcy/aU9tPHfJcUO8H/C3ARLRGLp5KMqq+P08ZWIWDHivxlMevFcHkmEtIeXvonhP71rF/FC5IRTB/H+c+w+HEIosPAtlI5YQWpBZUL1qiRDSSLRuuNQ+FFZHrNUG+XTQKbDq9s2LENJpscENl0+36JT00NkoYFa77Xlfz4xKa1y0KZeXe4U651IO5Kh2sp+LTJqpEwUFLjMnHh8gKi/3VycHGSkGmUOAM1TWILWEWn5yriX6+/C84VjUfpEmIYw/EYfn/vw7Oq0/jIRTrgTzmy5PxLPnHsDheselm3mRqKG2zaO7IiJLR08c9clrqKcgTMmYgrYN20qhxke+NxJFJdqEjtVPuV2KOkePVqTRvCF5nuio8g2mHoHUak9Neu26IWT1xYpPCa8SJJNcM4SQNTeh8ntI4L3evsLC/N69EduqFb+vXqDxXm9cZjFf6ddoBe9lzN+3DxiqbVFgBvOVufWW/bSMeQCo9Q3Vi68XFHhPeQ/H/2885mw860fLwntKIdSlaRe0mNtC9xDPCDdE8Z7qYuEsC/Pt9iHkPbwUxU8RzHduCEWla768QwjNyy4o3rSsIEgoVOkiHbShGNzemRqw3g2fzi2klwJLufoMTjw9/ZkyBejRwxUkgPz+7H60Ai0YjUtlhsxNCPXyPsrmvXpBb5TjVYaO1/Ab4k0roRbhtG7TMDljsvRnipSpF4Bl8vEB6PrNNqQVSXQTvzeJwvqOrTEt8V27Z8V0fePSxyFnQ44PIdNTEJR+ErIjvBah0+uQmixRHXeuuNOHFFOE0pf7veyVLsNKxDnqj12EUNKF3UEA1GM3E9VVKSuWEqSWn0MI6+Dw4cOoL98oiX4J/sR7mTRp3fKxMD85GbHXXMMeTXXgPWtcZjBfL++jCN4rbzVpD9fYL81i/pLMJci8KFMiKa1faI1ZzWeZNhllT6r/SpBZY80aNX1yG/LgvRHu+QPvqc68n/MwOG8wl0DU2BsKeC+pnO6I2zyE0/Eh5FoKthRyCKEtYqy+SmwhhP7qPmtj0zDR8QK0Bx7guxWTCRtv4ATR8apvCHnGRf4qtWpJp91MQsi6kRUZl1LJqKxEZfPzURBThH1unxHyhu3JkVZCKSKlQk6J0ClSpnozpQTzU/c3R8evixFXCZTHAhs71cND9TfjWEKpqMSZ5dXpHZgvKAqo82DJP6nHpHfrpZcjim495fQTcp1aZImVeJkIK+UzpMioy7ct1ySPSXFJiIuJ8yKVyhN9uX07FQRZOVKHCddqV2Q+RMmmQwgtEkLRyREpz4ONqvy2njVaVoZYnRsury4EGu8lDdbtg6l3KEd7GS/mE5GnKKd6vukieE/tygeJGnhPfuHkJ86TSsgI8/su6Gt7BGaRZZUYm4jSCnP7yKrhqzBy+UifdD9ahJCwbHf2bsREK4LIuQ/D1Lin1X878F6vHa3cs2rsDRW8F8H8SMZ7kW/EjrIOIbRDitVYR1ATQt6NTUG4vNJOiObOYgXKEZ0nPZ8S3nG5o9pVfPAB8jMyvHNSkbkT+flpRTFV91PH9NNnOBRVlCKnuh8iHmOX3YGi00c9f6tbBvyZICoIV3m6KdtXsk8KdKLcTKcc641OG37Bue5cwzvOj8aa9i0xO/F9cw0x3prVaxYubXIpei7giJKrqIs2ayKDyuTCyqa0CKHShFNZViuZLpE3FlliRdRUtsEiveTL2bB2Q92kyFSX3QqCpBszEgmbnXQtoq1FNiNZQQhqvKeJ58VGLcz/8UfEPvIIe/nI7wYK70XGZYT5RATHjgUmT/aOWm0W74mAvvKKh1hK38+7d6Kw4oinxgalwNAfgRc6s8WqVULG/FHvjapWQjij5ww8sOoB4UEQfrze/3XNvUKLEC4dstTLQsMI8w8dP4QJH0/wIppqvLIb7we2GQg6lKV9mExa1emUQgnvSbY8mB/JeC+84C2+4BBCiwKs7teDWkHgJTIKk0wPoFFi+p07jcWrRdiMAuWI5LcyCqwjOC7PmMgkav9+cbNcTiWr8rNVKGgeI20WO47u0E7oTpacAn6Dygkg0xvahOQbwudqT0XiV2/jsh/KpQTzJbWADV1SkJ28Wkow76+HTnzJJ8Qoybte2+M6jfPyI9EjhLVq1sIr/V7xUg54iBCrDK/vBEt2vD57/lAQWH2z8jtLflR3JCsIQY33NDmC2Oh1aHH//YilQC1GD90u7tlzllAFAu9NjMsS5nPiPVatQmX3bhJBWP7Lcl1ck5zFLWJ+dd8QrrxlJUatGOVzy8fCmkldJqFDkw6a0ZrVhJAsMygStPzwYBGrjIP3rBliHzBGMt6zpWdvCYcQAhg5ciRuu+02XHXVVfZKNwC1BbWCwLuxaZ0WZ2W5khSzHi0/DL1AObffzp1HCkZmnILjsqyUM07CKylS+3XJyOlaA0fLz94GskQn+jttlhR1jVInPLCvI9K//BX13cFIf7qwBlZc2AQLaq8SrVa4vBzqWsS/Tm6EfCGnrpmq2aZSQVhxywr0uKCHp5zWSaZeritl5WqFgXwmeHIR8gqFlQfK8trj7UgAy1lVEBy89+NkCWKjFyHkwXzKqfroo94D8DfeU2uC47L03XHgfcHlDbF82lAs/OktHC5zm2b4YVplzP+/T/4Pc1vOrTYfQtkPkAJyqYNxGQ2bDs4e6/aYJuarCaHaV1xt7cGD99QXJeZvObQF0wum2zYzDt4n2yZLpyJfCTiEEMCgQYPw4YcfolmzZrj11lsxYsQIpFLksBB4gpoQskx6jHwIeZQDlYmk13RpBU6gAqyopWSGs3gx0K2bvlmP4LgsKQfyoHROwil63J39gCMmzUB5l3g0olGFKtxc0h0Ddh1G+62uBPNHk+lWsDkm1v2ItyrL5SjqJ5E12ceOx7eDGiXlIDU5Vco9urdkr25QmUm7J2Hr2K0ePxI94skKpqJFIq2EMdcSHCsPlC1rz/KM2VuBVULo4L298+GDu4SxFOBLyyLDKuZrBfiStHCNwGgy3uv1hX5PTATee88Y7+X6BcZl+bszwHt1uiB/zSblH6w848rBzJOiwV/9oHplzNcy06S8uFVnXKmN1I8R5stjolQa9RPre3wHzeI9ta2F+XbKxcF7hxDauZ58vpczrMzs/mw9iOomJWPhwoV444038NNPP6Fnz54YNWoU+vfv70qQHqRPUBNCCSGXufJK0cOR69CzkfIQQr10EEZzJdgf3aoE6rGsHMidoDbvvNOTl9BMKHHleCgwScmpEr6VXQ7klFyDTuv2ItmdYP67S+LxRloNfJrwNV8dNpeST2wpAAuZTVFeQZ7TWL1bwoToBCmVRmWLSgy8eKDUW5YPiFHgGa3TbHXQGasicU6MzSkIDt5bXXkG7wtgI9XiV8zX6ws1rAzIwiMOgXHZgvmqm0+reE9D1AuoxRp+dRNCuX+E+bP/MRsNajeQXCMOHD8gWaywHi3MVxLCBZkLJPcAs3gvqTruKMwiN5isfqt/d/DeHN6LyjlSyzs3hBoz/9133+G1117Df//7XyQmJmLYsGEYPXo0WlGOpCB7gp4QSki5DMjO9s4vpWOSWVFejvyVK9F39GjE7t2rLW3eBMJ6cyXQH8Pp5qzHFuVAlqM7nLvZUOLK8Tx85cPo1aIXk0iNPd4P1/y4C3/b7TqF3Vcf2Nj5QrTNuBV3bbsLB04cqJavQn1Dx5vslnxFFvywwCftQ2rtVPyr1b/Qt29fzyEQrw+IcqNmKRV2CCtcfQh5ZGP1hlDdhoP3PFIXLMOJjXSzV/HFF8gvLkbf++5DrFEUT1V0Uu4e8faFp0LOumzDfIogOngw7MB7Gh4lS6egWjwkSikOJSGMi43DsZPHeKRlexk7MV8ek/IA0Aze0yD9jfkO3jfAsWPHkJzsEELbPypFhQ4hVEl33759ePPNNyVCWFRUJJmT0t9Wr16NGTNmYPx49mmUPydMXXdIEEIJMTlyHS5bhooHH0T+rFneETmVgzYK9sISvLIPjRq5Sh88KB7kRdkOx7hsUQ5UYc/XpImHEleLh4KzkOmlHpGiBPPPH+2IThuOoJY7wfxXVyRhekoRDiT+6Yk6lxifKPmx+PNkVG9qtVJisJaB3u/yDaGSEPKSTKUpD69SYfa0nmWqqhyfLWvPrED99J6dhNDBez9NEg/mu8lVxZEjyM/N9S/mk9nooUMARXcmd5CMDONIn0ZiqQbMtwPvaUjKSNEiM6++IbTb9F2kL3ZhvhYhNIP31HdezBcZp1zWwfsjaNDAIYRm1o7oOw4hdJusvP/++3j99dfx8ccfo3379rj99ttxyy23ICkpSZLp22+/jXvuuQd//vmnqIz9Wj5kCCFLCm5znIr4eGPlgJWzT68drZNdOnHOydHPB8XqM+fvtijlqqAGuRcDQ92WuJzd8ClGaQsGXzRYczN75PhAdNm0FefvcyWY/y0lCuuvaI3H3QnmlQrCnVfciZyvcsx2w5b3SNEhn0KKPGqU7Jb8TWS/GHXDMiHs3ac34mu6wqTybvTKG0JepYJO68mnUY4QqJXDkEi2Ov2ESN4/W9aeLTNkXyVWCSHJxMF7++bDVE2y+eWZM9bztEYA5tuB94QjByYekMzrKVK0yKMmhJ1TO2N90XqRKmwvaxXz5TEpfcbN4D0NjBfzyRey9FSpFBHWwXu+JWEV7/lacUqRBBxCSHb1DRqgqqoKWVlZuOOOO3DppZf6rA4igpdddhl2794dVCsnLAih4vZLN4k75XB65x2287/W7CiUD6+frdw2CqwCW5RyVTh3O06M5SS8NBSZSJ1floIp+1LR8ZsSxFYCJ+KAjen1MaHeJpTTNaH7USoItWvWxuET/ot0xyNqdeRRekd5Y8njuyePKfniZHRv4VKYZFMgI5JJfi3KZMZmlAqjfEyyj6Re7ikj+diy9ngmIIBlrCoIDt4HcLK0mlJZO/hgPuFygwaunKpmb/PCDPPtwHuZECrxnteqI1h8CJXLySrmK8eUPzxfSmVkBu+pT6KY7+A9PwZZxXv+lpySDiEEsGDBAgwePBjx8X5MnuantRYWhFBx+6VLCEl+ZoLIqJQPn2mw6o/IMa+2KOWqG0LyKWlyP3A4kaMDBkXkmy3aoLa9+iY6rd+GFHde4+1p0fisfQu8UHuFTw3BpiCwQoYT+R3UZpB+ri5FJL2ytDJkXZLlGbMcLECLZNLf1MnrzSoVrJxWZmbalrVnpmE/vmNVQXDw3o+Tw1O1Cst0Md8M3kunOJXG0aRDEPPt8iFU4j0FveI9OAs2vJfUgRGrJRJHjx7BMsJ85ZheHfAqstq5MF8U76UlV1XJtE5RHxw6eM8DFpGdd5ZPQvaVcgihfbKslprCghAqbr8MCaFeyHEjyQvmjxKexAD7k1TuLUTBecC+RGBbfWCaaz+0lHi44Z6a2L/gCXTYXI5oAMUJwMYu5yI76TPUr1tfCr6ivmELJgVBvumMiY7xTJ/WZssyldK6IZQrNDrRpeh06seMUiG89jhecAghh5BCqEi44T2JXhfzzeA9VRhGmF/Z/HwUxBRJeL+jHjBVtvS0kGheSXzUKXsIS2+++GbMWjfLiyyawft6terh6An/5MW1A/O1bgjN4r1ZImk39Dh4b7dEI6s+hxCG+HyHhYLgzxtClaml7nSbUT44/RLtAullrz+A7B9norDO2VEkngRK48wTwuf+ug6d1u5CPXeC+R9bx+Jgj/6o360tUpJSdHP91Y6ujbfavyWUqJiSHDeu3RiNajfCiPdGoKikyPLXR0RVfUOnVSkRRDLrGZI3RFdJ0fIhVNYleqIrSiL1hCHarrIeu9ae5YmyWIFSBkmVSejXoV9ERp0LN7w3JIRmbwjDBPMl/Hj3ThRWuE02ANQvA05GA6UmjZnUaQv0sEWNXWYIIQUto0M6Mrcft3KcFHjMjscuzNfyIbSC9zIp1CLZc/rMkdJa8DwO3rukJMthV9Eu3H7l7RGJ9zzrxc4yDiG0U5rVUFdYKAiKRO+aQWWsmPj467RYwEfFDqVcN1nuGfKVA5Ki41Fyppx7BWaVdseA7YfQbttp6Z0jycBXGa0w4bn3NeuQwVkOgCKiINAGThE1iRBSEBU5obxWnj7uAQBIjkvGbZfdBvKxozqVN4TKeniSBVMfaUyUh1AZZVSkP3pElG4mzfj/6SkYcv5FHgXDjrVnVQZW31fPX/ypeJQ/VR6RCkK44T3lptX0ITSbYoIWWxhgvhHe0xAzf41HXouTQpGdKTJo4fhCxNWgE0T2Q5j/ZMGTmLpmqlBiejkZ/Bv938DB4welg0VKKH9T3k1So7x+i1o9tBPztfLOsqXCLmGF0GntVZGG9+p9L5Lxnr3a7C3hEEJ75Rnw2sJCQZAQwJXAXlIOFi06G4LcauAXBdkk5cPnMUM2BX1UrCjlPDdbtAHXT6jPdwJLCeaLuyN93T4knQCqooBvL4lH6ohpuObaGwzXrzLXkgghVFcqb3D0d/Vpqto0lQIhSIT1xNlT8sS4RFC00OKT7mtNAMpNU7kh7zi6Q1JoWI8UubPXHMTsirGVELLaNfpdVymEy16M52bUytqz0ne73tWSQSQrCOGG97ROvA4By92HWpR/byDfjYrPWgtxzD91+hSazm4q5QvUeujrpwO2QyZu3ETIhRW8V0dGpnazLs5C7k81vUewAAAgAElEQVS5KCwu1IUH2WRVXc4I89VBt2TyySKereq0wszmMx28twusbapHjfmRjPc2iZS7GocQcosqOAuGjYLgJoU+eQjNpplQTpd8m0d/U5JCs2RT8ATarFLOc7MlsirHl96Abpt3otVvrgTzextQBNHWaD1quI85C8sHzwohVOZVos2cTqFzNuZ4mXKS/0l2p2xMzpgs9VW+ZSOC99iax3xOmeU6J3aZyFQ6lDKjdij9BgUnqKqsQn5+flAoCKxEx5GQqFhPBpGsIIQb3iM7G155CCm66Jw55smg/HGHMObf9cFdfId7IuDvLmuU006N+fTvngt6Sm9awXt6X26XsLZB7QZSqp23fnzLi/TSDea8vvOQeVGmx1yQLCuMMF8rLU9MVIxuaiHqi4z5XVO7YuX/Vjp4b2Id+esVLcyPZLz3l5z16nUIYaAlbnN7flMQOIKl2DwUqbqK8nLkr1yJvmVliE1JEU8grNdvLX8/s2RT0EfFDCHUux0yI/M6ZYl45kh7dNp4FPEVwKkYYNMVyZjWZDeKEg5Lt2vKtAl6ZiuZbTI9UTrtUBCo3Vm9ZuGmpS5TIuWjpbjQyXnq7FTblSXZr8bMPJmZD553RMOY69UZTGPiGbeyjJ4MIllBCDe8p4igFV98gfziYvRNTkbsVVeJJYw32qciGPONvjWtwyQtzFcGhbGK9zIpJJeBUR1GYdrn06od84kQBssBoIP3ruWgJYdIxnvRPdNqeYcQWpVgNb/vFwWBM1iKP4ZuSYFl9dsukuvnG0LW7ZCe3OmElRz3laYyU0tvROdNv+C8/S5z2d2pUVh3+YV4svZ7XtWow5GrzW38FWWUTD+rzrhuLLVIoUxWl29bjrs/uFvXjMrKWpTzWVlae1Y6oPEub6Jjue/hSAj1ZBDJCkK44T2tW9PfHQvvqfIwxvzkmskoOVli2iePhflKTLGDEPJApJKs+hvzM1tnBg0hdPDetTq05BDJeM/zzdhZxiGEdkqzGuqyXUEQCJbij+FaUg4yM71NQqmDZs1CjQZHSkbjxpQgR79U/frAgQPSabfomHhPC9WN39/5fjy//nnpz63KmuKhvY1xxTelqFEFlFGC+c4NcH/dr7wSzMt13NfxPgxoPQAj3xuJwhJtHw/51o7IYqAUBOof5ZJaunWpP5abVKdzQ+g30Vqq2Lkh9BVfuOG9aUIY6H0qSDFf2uIQZYoU8mC+vAIDifeBwnznhtASPPvlZeeG0C9i5a7UIYTcogrOgrYqCILBUvwhEVHyJPUh0P32s3LAe1qoln+9+HoY02kMan3wPTqt24bG7hRQv1wQjVUXXYB5iR/YNmWBVhBs67iqImU+K1Nrz08dM5PoWKsrwTQmUVHpySCST4zDDe9NEcJA4728x/jxENAs5ifFJYFuCpUpfMhSRC8gjeg3WF2E0Gw/ed6TMT8YfcYpRYdWMJxI8BmXPrOqSqTlpEmpSmQ5RDLe86xnO8s4hNBOaVZDXbYqCIKmkP4YrikFNtD9FmxPdEyPf/44V2RMtfz/cfzv+Ofuk7jsx5PST8cowfyVTfFsw5+xv2q/qVNkvTkOF0I4qcskzOg1Qxqm6Dz5Y/0r6zST3F4dGCI9JT1oAieYkZeWDCJZQQg3vDf13Qnir5l15/OOYJuiWGIW86mfFNFzbKexaFWvlZTioUvTLmgxt4VhNE9RmYQL3tO4ZcwXnSNRmYmWN4P3MomSg641SWiC4p+KgyJQjuj45fJqOUQy3puVodn3HEJoVnJB8p6tCoJgsBR/iMAUSAe637ztLVwI3HKLENEghf78OecLJ21/7q8+SP9yD+qWuGblh7ax2H/NDRh333QQwA5aPMjW6QoXBSFYbwiVmyNvomOtwBAt67TErOazQl5BUMogkhWEcMN7U4SQF38XLQKysuzBPd42A4j5yoGpk7U/8MkDmLlupj1jtyHKqG0dsaGiYLwhNIP39I4a8+V9ubJFJQZebDJtiw0ytlqFclyRjPdW5Sj6vkMIRSUWZOVtVRAET0H9IQpThDDQ/eZtj0Kov/QSKvr143ZeF/UfHFHSG9dvL8JF210J5g+dA2zs0hIP1PkAZDpEyeCbJDbBkCVDcLTcbUNqw8TxEMJoRKMK2gFjbOiCVxU01pGXjsSsdbOkv7NyUClfDkYfQmX/eBId60Wl9VfyZbvnj1WfUgZJlUno16Gfk5ie/JStPLw4tno10K2blZYM3xXG/OroN2+bAcB8LWFKyeCTUvHGgDewv3Q/xq0cZ2s0Zh6899sC0amYktQXnyo25UNJmB9MPoSieC+TwczFmV57nTxPQzcPxYLMBT7ppAI9R1bakzF/V9Eu3H7l7RGJ91bkZ+ZdhxCakVoQvWMrIfRHQl9BWQkrB1R/oPvNak855qgoVOTlIT+GL+E5ty9JOfDCsW5IX7cfieVAZRTwTYcE/Cu1DJsSfxGUunjxYFIQiAwWji9EXI04nxNTnpEFY5RRnn7LZYyi0srzNGn3JGwduxUx0TEiVQdl2SNHjqBBgwYRqSCEG97TAhPGfBb+UiCxpk2B3bvFUlgYrXZWm4HA/Gr8GoMJ70kMMuZ/sOMDqC0oeMREmB9MUUZ5+qwmjeRrV1jsHQBOSQjrJ9b3Sicl2kawlI9kvA/0HDiEMNASt7k9WxUE6dhpGUDROumxI4m74HiFlQO5/kD3W6899XiJELZsifyZM7nM9nhuCCcdH4CM77eh5e+uVBJFDYEN6W3wSOK7gtIWL07+KkdPHEV8dDxy2+cia3MWTlSdEK/Ihjf0kizLJ4uf/vopphdMZ7YU7DeErAEYrRmlIpc/PB/d0vx308Pqp12/R7KCEG54b4oQVtc+VY2Yb9e3Y7aeYCGEWpivtB44cPwAxq8czxxmMN8QMjuvk6+P3lPPk7y38dQZrGUiGe8DPScOIQy0xG1uz3YFQd5ss7OBQsXpk9kk7oLjNU0Iq6PfpCDcfTdw6JDhKCtq1UJ+bi4XITSKLkkJ5p893A6dvvoTNSnBfA3gq4518HijX1GYYNwHwWnQLE6bMSUrPnLiiN/STlDewbJTZVzmreQLMqfPHF2zGFakThpkTFQMcgflYvBFg8VvKhhCVSoqjWo3kkofPH5QCvyQcV6Gbbd1RrfKSgXh1QGvIqudTT5Vdiwok3VEsoIQbnhvmhBWB97LbQYQ801+Ira/5i9CSBj+/D+el0xclZFS9QZgJ+YP+NsAblcOHoGqTfspuM+6wnXYV7IvYJivnidWnlqecVV3mUjG+0DL3iGEgZa4ze35RUGgPtqV0FdwvJYIYXX0+623gGHDbCOEks6xdRnIN4Ae2RduWumNSP9qK5odcDX1a9MorLusDZ6qvUxQwtaLk+9GxekK5g0hkS1KOs/rz0f+jk2TmuLuD++WSKfec32r63F/l/u5SJWeb52ybjkgQ7+Wvr6ePP57yrrk8pRUeeHmhbp+PER8c/rk2OLj4dwQWl/ToVJDuOG9JUJYHXhPbQYI86kpOcfgI1c9gjkb5qDklDtqWIAXbO2Y2nir3Vu2WoQQ3o/5+xgQVt654s7AY/6gPMTs8nblEMV7aQlWVeLJgieRszFHsp6RH9r/Ks9Uev4dCMx3bggD/GGEWXMOIQzxCfWbglBNcrFMCAPdb45gAyI3hHL35ShbdfbXxMSic3D5t8ddCeZrAhs6N8I/n12BT/au8iGOgRo+z4kxhfemIC+8hHBcp3HSpsoqr46oxxpz3s95uHnpzV6bs5oQ0ma9ffR2rxQNWlE7jTZ1rfJ6fdMzd2WNRet3o5tQx4fQjESD951ww3vLhLA6psrPmK/0C1PeiOmlJQiECHjwXrQfdHtVs0ZNaQ+rDsynCMwzm5915RDFexovvcMis7JcAoH5jg+h6Cp0ynvpQmfOKB3FHOGEmgTCTUEIOULICjYg6EOoXH/PTbwRV6z9BY3+dP11a8sY/NljMEaNn+opRmRndP5o2xMRs74DloIwqM0g0H87ju7A3I1zcfjEYVaVUqAA3oTKynQRrIp5/DKpjs+GfebJ4bRi5wpNRUVvU+e5iVT3kzfZMGt8smKivlWmv4dLlFGlDCLZhCjc8D4kCaEfMZ91QyWRlo+yUVjiHUyEByOslGHhPdXdp2UfdG7amRvvVw1fhZHLR3LlSxTFSh7Ml8fUt29fiOK9EnNZZNZL4UYU6FBxd/Zuyy4DWgcE4RRlVJZbJOO9lW/WzLvODaEZqQXRO+GmIIQcIZR2BuNAPCJRRqm6Fe/MR+mS53HpT6eklfZXbeCrK5shO+djr5UnciNl15KVSRuPgiC3SeHQi08W65o70WbfIKEBNxmU6+V1mOeN3LpowCIk7ElA7z690WpeK11FRa2cGEX55JE77zhYdWmth1Z1WnmdgrPqCIXfI1lBCDe8D0lC6AfM5/3u6Bsfkz8Ge0v38r5iqZyMzaXlpUwXARG8J1L0ev/X0XNBT6H+8WIlD+bLe5go3lOHgxXzwyUPoXMAKPRZ2FbYIYS2ibJ6Kgo3BSEkCaGsIOgE4hHJQzgnuw86rf0N55S61tP3F8UhfuAY3HjL7T5kkMfUxu5VeX/n+/HOz+/gaOlRLGq/iMunRPaDob4o/7/8b/rf7PRsyUdG5OF1mOc5LaZ2n7j6CbQ71g7JFyfjmoXXMLsiKye89etVyDsOZofcikrB7wWeQAbpKeleZrA8dQR7GYcQ1sHhw4dR32oewiCZ6EjHfN5pMGOFwFu3UbnFmYvx8CcPY1bzWbbhfd6QPJw8fRJDlw0V6iIvVvJgskyeRPGeOsxTv9HAeMfBIxzlrXKThCYeK5fY2Fie14O+TCTjfaAnxyGEgZa4ze1FNCGspsA3ulOo0x8ehef1uU8geeU7aLvT5YROCea/zmiNCTN9U0lYPZ20sgTJVPO5fzyHkctGchNCmfhRhNL4GvFe0eToxnFe33loULsBus/vLtQ1MjmivHqsKG480UapYdm8siytDEPfYysq8qbOcxptNDDeU28h4bgL86w9M/VW5zuRrCCEG97TOuJeo8GG99R5C5gv8g1VF+ZTmqEDEw9g+ZblUgAWSnheVlXG7LockVqN91Tf2E5jMTljMujgqjoxXyaEBTUL8NzG55hjUpK4YMV87m+JOdrgKRDJeB/oWXAIYaAlbnN74aYgcAMamWmqb+QoGXFODjBwoM1SZldn5PthNKaSY8V486F+6LTuIGqfPJtg/m93PYvOV2ub01g9nWSPxrgEERi6ISQFQTQP4dSrpuJfX//LK/ommQ9R6PEJH0/g8ieh3iXXTEZSbBKKSos8naUIpTnXakfupNP1QYsHGQ5MJoS129ZGr0W9mGKyekMo6hfD7JBGAe7vyUzl1fROJCsI4Yb33IQwyPBe4oJVlRKp0TqQsvu7q07MV+bsm7h7InYe28n95dOh3do/1vpE4JQx/978e7ldBYhMEmYq/dFZQb6MMF8mhGN2jsEfpX8wx6Q8uDM7H/7GfLvXHVMoASgQyXgfAPF6NeEQwkBL3Ob2wk1B4AI02WdPHQ8pKsol3by8gJJCVnQyvTHNfeJeXPDFalzwhyvBfGEjYHPGFbj/yQWGq8Tq6aRe5bThGqV7kN+jk9LM1plSDicyt9lfth9bDm3hSgJvNLCbLroJi39ezIw4x/qElg5ZqpnO4fHPH8fUNWcD8qjrkRUEIoS3fnArioqLNPui50OoV16rv3ZGnDOSB9f3xBJokP0eyQpCuOE9FyEMMrynPpvFfLOfkj8w3wzek7/dhn0bsHTLUry46UXmcMalj0POBv3I0RPSJ+D5Dc8z6zEqYBR12gjzlX7wifGJ0iGlVoAYLRLHa3Wi7HcgMN/Be0tLKeJfdghhiC+BcFMQmIAmR3gr1ImyRqSQbgp37wZiYvw+u3p+HUrwV+e3271zBz558p/otOkvxJ0GTtYANv79HPzj4TfRvGUrZp/Nnk5qVUwmm5QPKjU5VTrx5nHyV54YU4Q28lWwq0/14+vjZNVJlJ5yO1FqdJrMkMpPl+vKSTZzInNS5cNSqmQFgUxGa8a5wqHTo1QSWFFG1eX1OslKsMxcBJwFmN8TZz3BVMwhhBHkQxhkeC+TQS3/bSPMt/r92IWvgcb7BrUaGEaYplx917W6Du9vf19XRElxScz8i3pRp40wX0kI7+p4l3SLyYv3ynWgh/nRiEYVqjzjCgTmO3hv9UuL7PcdQliN8//kk0/iww8/xPfff4+4uDj89ddfwr2JOELIkQNKEuLq1UC3bsLyFHmB5dchnywq89vNfexWtC/4BqkHXS3tahaFPVf3xH1TXuBu2szppLpyLWLDqld5UlpVWSXdEMqEkPUu9+DcBem28JNfP/FK9EvKzD2X34PHCx5nVkemSj0u6OFVjqVUKYMMdG/RXfMWwGhT17o1oD7f0u4WXP+366W+HDx+EClJKcg4L8Ny2HGmEET8s3gqC5IyoUoIHbzXXkCGSmwQ4T313gzm2xHcg9Uu69MMdrynYGXzf5jv5UpAPudjO47F3E1zuSxXtHyxjTBfSQjzh+dLe032/7K93BZYJE4L82U/yYe6PoR1heuYPu6suRP53SGEItJyyvrohU4ewupbFFOnTsU555yDwsJCvPrqqw4h5FFgc3OBoeyAH1i0CMjK8uvksgiG3Djlt9ue/y3i1y/C5d+VIeYMcDwe2NClMUY8/T6S6iQL91Mk4hxtUGrnfr2NTi/5sVqh0Np47EqcLBPPnWN2+myoj615jMs0dUrGFDxxzRNecmWRVtmHkMyi4mvGS++y8oKpJ060vPDEC77gKAiCAvNjcQfvTRDCIMJ76r0I5hf/VOw5MLNjWfH4QVM7oYb31Gfaj7TwXiTwjFbkTiPMlwnhpN2TsHXsVumAzgx+m3nHjvWgVYeD9/6SbGTU69wQBsE8v/HGGxg3bpxDCHkIYRCdGLNMEOWlNavkBnT8cjsaui+At7SKQfE/bsKtYx6xtPpYeQiVJK7/hf11AyCoO6FVr5pAam08tDE+WfCk5DNytPyopbHRy1onvo989oglQkj90/IjJFmRgkCpNORbT8sDCIIKHAUhCCZB1QUH770FEko3hLyYL+c0tRtLCJvvXHGn5o1ZoPGeZjHv5zzck3+P182e2S9OC+955a23X8h7khbmyweAlS0qMfDiwAeiMysno/ccvPeHVCOnTocQBsFciygIJ0+eBP0nP2Qy2qxZM+zbty8s8lIRoH3yySfo1auX5Jvm85BPSbt2wN69gDqoDBUmH8LUVGDzZr/7EH75+5e4btF1uivo+pJ0DNx5BJdscSWY/zOREsyn4d6Z+v4SosuRNrz1hevx4fYPsXjLYq+NmZK9D2k7BNf97Tp0btpZyERRrnd/6X40SWzi8756nlZsW4EHVz3olVJCdCzq8q/e8Coy27r8+OTn8z2f44a3b2BWTSZI1zS/xtNvVv8oQukz1zyDmN0x+muP2erZAiz5CVRlqSjze7JUe/W8TCajKSkpOHbsGJKTxW/Xq6fXZ1t18N57BgzXaBDhPfWahfnyyD646QOUbC2xBUvU65WwZdb6Wfj31//Gnyf+9PwcSLynRlmYKvqdvdLvFQy5aIjXa7zyToxLBN0QXnnelZ59jtW/FsktMD1tum1zFAyY7+C96Kpzyisl4BDCIFgPIgrCY489hmnTpvn0etGiRUhISAiC0ThdIAns/vRfSF/7B+ocd8nju3Y1cfTvA9Hsb5c4AnIk4EjAggTKysowdOjQiCCEDt5bWCjOq44EHAmEvARCHe9DaQIcQmjzbOlt4MpmNm3ahCuuuMLzJxFCGPE3hLLUVqwAHnwQKDqbh06KLvrMM0C/fjbPqn51dAo5/N3hUgGKRnlb6TXo8dMetN7lSjB/oC7w9ZVt0OrKEejRs4cUvdKfj9wfdfhs2ZxowY2ulBbq27zUpFQ82/NZ9LvQJTvWaad8Eklj6vBKB1tvBpXyWXjjQk+fPFO/bQWGvTvMFjGSXGjsm+/ZDAqUY3g7zdEij/xlGXNUZ7mIc2JsWYSGFTh4b12+XGs0SPCeRqvGfFkChCWEuw9nPIwWdVog4fcEv2M+L95o3ZYpMZ8X78lyJzomGu3+3S5gmE9958V7eQ7oxlAvUrWM+d/e8S0+XfWp5RtC3jmw/qWwa+D6ltjVBFWJULcICSphMjrjEEKbZ+vw4cOg/4yetLQ0xMe7glbQI0II1fVGXJRRpQDInKigANi3D0hJATIy/G4mqjWv5Ndxf979mFjUDOnrDyHhJHA6Gvj6stp4LvUv/Jq4F7ntc0FJfZ/t/axPjjy7nNJZkehoI0yqmYTik8U+w1D6n9CP6mhr6gTAsq8C5SG8ZuE1Nn9FZ6vTCidO42w8qzFX5Dnejmml0uB9Vy7HI3+S4+7s3ULmu6L9UJZ3fEqsSI/9roP3bBmxSnCv0SDBexqPXnRJ+o1yucoBS/yJ+Tx4Q2akWe2y8MJG3yjWMuZP7DIRuT/lekXX1MN78olcW7QW3ed3Z02r6d/VmO8PvKfOUbA3q4F/eOYgkJjP/S2Znp3AvxiqUaUDLynrLTqE0LoMLdfgEMKzIgxFQMuZdjdaFXyB5oWuBPN/NInCur+3xrTa70r/lpWDoZuH4kTVCeQNyfOQQlaCY5HFxRsBT69OUhAo1DeF39a7YZT7Ls8T5ewb+h5H1Fd3o/IJrsi41MEGrI5Tq23yP8lsnemVSkOkj1SWt19awRNE2+ItH4rfE2tsoa4gOHjvPcOhukaVB3k7ju4ARUCWcTMQmM+LN6zvSet3o6jSeb/kYegyPsw3g/fUHyVG+mucdgT+4e1boDA/VL8lozUa6nhv5vurrnccQlhdkgfw+++/4+jRo3j//fcxc+ZMFNBtF4CWLVsiMTGRq2dhe0PYuzdiN2yo9ts/eRK0bvF2bfsFXzx7GzpuKkZcJXAyFtjYqR5mNzuAbdjlmT9lvqPyqnLIJ4bLty2Xkp+zyJfRQlD2a8uhLVwROLkWlkYhydQmORVv9H8DB0sOImFPAmq3rY1ei3pxV0mnvz2a98AbP7zB/Y46nLhI5DneRuy4IeTtl1Z4dN5+ipZzFARRifmvvIP32rL1rNEQwHxKTaB+tG6JAoH5S7csxYubXvTbglWm/1n721rpNo0sQqqiqtBzQU+udgnvb7/sds3IzkYVKDGSF1e5OqQoZMcNIW/fAoX5Dt6LrgKnvFICDiGsxvUwcuRIzJ8/36cHq1evRjfOpOphSwgnTkTszp1nZVOvHpCdDUyeHHCzUK1bvOmlNyJ9w1ace8jVxZ3nReGP7teibVY/H3MapXJAN4T0fDzsY9z2/m1eZjpeHyaiPMRRSwmhsqy0E/5c2h6TqF8n4s+KPzVvFeX2KTn77N6zJTJJCdlJgUp4KgGVZ1x+lqzHnzeEstJDZpzkQ5ifn2867cTjnz/OpfgE6rSY5OooCKzVFbjfHbxnEEI15jdoAMybBwweHLhJcrckYrmhdUukhfmrhq/CyOUjQw7zyez0ePlxye0ha3MW6teujxOnTxhiPlmaLM5cjG5p3SSJnj/nfCGfQ3/eEMqYv330dqz830rTeE/jCjbMd/A+4FARVg06hDDEpzPsCOGyZciPiUHfrCzEnnCRJ6+nfn3g5ZeBgYHJG6ROAN+xtDXuK6yFy78/gegzQCklmO+agpFPvSclmNc6MdRSDpLiklByqoS5+vTIg0hiemYjJgooTaLKqsqkGtTmQWqzI2UzD3zyAGaum2nYspKsKUkxK8G86HDGpY8D5WlMT0k3rSBIfZqThsKSQsPmKb3FnnF7HB9C0UlSlI9kE6Jww3vp0IKF+ZMmATNmWFgxYq/qYasenvFifmabTORtzWN2JhgxX33jKVu18GI+yXTQ4kHMsWthvt14T50gzL+h5Q2WfAiDEfMdQshcYk4BAwk4hDDEl0dYKQiVlaho3Rr5s2bpE0KJeUQBeXl+J4VqU6CZx65Dp7W70OCYa9H8dGENrGrbAvOeXOpR8D/99VMfcxotQsi77LRMTViO7Lx1WymnVhDoRDi+RrzXKbA6mb26PSKFz617DlWo0uwKKQdKf0tlIVlpo7+pTW55xxUTFeN1S9myTkvMaj7L1Ikxry/JtG7T8OjVj/J20XI5R0GwLMKgqiCs8J4ky4v5S5YAmd45Sf0xMSxs1SIskYD56j1M9jcXwXzC7DtX3KkbDMzoANEOvKf1osR8eUxmE9MHI+Y7eO8PVIicOh1CGOJzHZIKgl60uDVrUNG3L/Jzc40JIc1Zs2bA7t1+NR+VAX/Q8asxeOdRtN9SIa2Wo0nAxq7NcX/dj6R/K0907VYOtE6LeTciM0s7uWYySk6WMEmWnkkU3eTtK9mHlKQUyTxUz9xV7tup06dw1wd3SVHuTlae9Opy/Vr18XK/l32issqFtMy61CRPSwZ6IckTohOwqP0imFEQgs2XRB63oyCY+QqC952wwnsSMy/mN2zo8ieP8fXhs3O2eLE1nDCfR356h5pkBiuC+US4ScZTPpuCjUUbvfYZwu4JnSdgRi/t22CzeE/j08J8pZXLgswFuvuMnnyCEfMdvOdZzU4ZPQk4hDDE10bIKQjLlrl8AQsVpnWUPzAnBzh5EhWjRvERQomJrQY4fS3NTPNrX76G4ncWofPaIiSXQbrH+q59PN44PwafJn7jqZLlAG/mhlB9Em0meMyUjClo3aA1xq0ch8NlxqlQ5MGM6zQOORtzpH8a3bxpjcms47yeOZHRibHcX3Wwny5Nu+CZtc9o+vLJ5k1ENCk0vPqRxzRp9yRsHbuVSWaV75tRJM2sSdF3HAVBVGLBXT6s8J7M/nNz+THfz3hPM29Gyec1GWWtLCXmU9mC3ws8h2tFxUVcufhGXzEai7cs5sb76/92PT7c/qEpvKeXzGC+kbuDkVUItaeF9+sK14ECtM3ZMMdHxEaYrySE9RPrC6cDCkbMd/Ce9eIhXYIAACAASURBVJU5vxtJwCGEIb4+QkpBIDJIZj9nXOkZPA+ZgNLz2GOoeOYZfkK4aBGQleWXGfzPzIfR5LPluHC3y5xxfz1gY+cL8X/Jy33aYznAixJCNREyGzxG7hev/wYNjN6htBPqPIR65ImCDMiBcswESzFjosUz4VoyY0W8U85T/vB8T0AEnvZY46A6tHIq8tRtpYyjIFiRXvC9G1Z4T2b/9erxW4X4Ee/lmTaj5PMGlfHa8txJ7OW/sfLAUmCuQ2XuCGYGy1LGbx5/PSO8V7ent4eJYj4LJ/X8xnm+RFHMV4/JzFiM8uFaGQvPeLXKOHhvVnLOeyQBhxCG+DoIGQWBzETT0rxvBtWkMDUVFfHxbB9C+T0/nBgfOXQQ70wZhPT1h1HrlCvB/KbLE/HsuQewPeEPr9XC6wDPIoTqzVfpe2cmeIxWv5b8vARZS7N0I3sa3Ug2qt0II94b4eUfqDWmJZlLkHmRmJ+PGQWM95PVShWy+OfFujm0lGN6dcCrUkJnkYcVKGdSl0m65lAi7YiUdRQEEWkFf9mwwnuyDNm5ExUtWiD/xRfZbgJ+wHv1jLMCmBhhfmHxWasXI8wnP+JXvn3FK9qojPnUH71UREYWG+p+WcF7MvcnS4sWc1t4+qg3HlHM58V7MkXtcUEP4Q9SBPPVYxK97eQ5aF06ZKmwKarwoBUvOHhvRXrOuw4hDPE1EDIKwpo1QPfuTGlXPPEE8tu1YweVIWXCZh/COVPvxIUFXyJtr+sG8/eUKPzQ5XI8FPeW9G/lhiziAK+3mSrzPJHZi9r3jnWaqiVMo37l/ZyHwXm+Idx5TDNJwRiSN8TTpNaYzNyA8ZpokSnr7D6zmeuHVcBIIVGO6f6u90sKCY8fJLXJM1dm5MMaD+t3R0FgSSi0fg83vCez/4pDh5AfF2eM+QHwGZdXgl4AEx7MVyemV1pQGJmEEs7Qk5aTppuWQm+l6vXLCt5TW0rM19vDRDGNF+8pSNkr/V6xhUzpYb56TJQaaczfx3C5CvDgPbkmHJh4gKs+u1DIwXu7JBmZ9TiEMMTnPWQUhNxcYOhQprQrFi1CfkIC+t57L2KLinzLy+alNkYZ/em777Bxzl3o+HUJYiuBckown14P1z+Si6bnnaeZ748VQVNpvqK1mSo3cUp5oPQXkUkI72mqUkgi/ZLfY71D5dT5luwyIRIZox2nrUY3AFpjaprcFDl9cpiKCe84RM2SmB8Mo4CjIFiVYHC9H254j0WLUJGZ6cr/qZdqKEBRpZUzrWd+OKfPHMMgV3IUTa2onFQ/RU22iveUF1DpE26E32bGIctBiflGN54imMaLk9QHlj8h75eph/kO3vNKsHrLRXKaoUBL3iGEgZa4ze2FjILAe0P42WfILy5G3969EUu5pyjYzNGjZ6VGJ8Vz5tiWcuK5h25Chy83I8Udc2VHWjSKul+Hex70jnSmZYrCE0Fz3tfz8NnOzzAqaZSU1Ff2t1OaCKn99WQScvL0SV3zRuUyouAxbRu25Y7syRqLluN+yvMpkm+h/OgpCKJmNzwnrbJyQHKhBPIsubM+Mb0bABZxH9hGP/cl78m3qHxYY2H97hBCloRC6/dww3vphrBrVxchrKhA7OjRwCGFr5zNeC8y2yycVNclR9D899f/xhe7v8ArrV/xYL6deL/wxoVITU7ljubMGofW7zS2RrMaeTDfiBCKYBrLJFcpUzt98LQw38F7ka+h+so6hDBwsncIYeBk7ZeWQkZBkH0I6dZPHVRG0vijgKZNUbF9O/JXrjybC04vRYVFaa5asQz7Fj6By34oRzSAklrAhivPxa3T35USzFt9aAMa+9FYye9O3nju3XEvBrcbLJ0Q0y0gRUbT8xeh9h/r9phmtEx130ROaJXvaikC1Cc1QVWfSFMdIjeELIWExxdD7rfZsaplpnVyzjLtVZNR5bgOHD+A8SvHM5eNXf1nNuQu4BBCXkmFRrlww3sy+6+oqnIRwr59ERsdDRQUuFJMpKQAGRl+TzVhx8yr8UTGks/jPscNbW4ICryncaqx+NDxQ5jw8QQvM1U6eLvjsju89h6RG0IevNfa9/TmwS7M1Jsj5UGtpIogCnqHj/LYKL3U9ILpzKVjV9+ZDTl4zysip5yBBBxCGOLLI2QUBJKzHGWU/r+SFCrMQCv69TurHMTG+mV2Zk+4Hh2/3IX6xa7qf2xdA6evG4Whd4wTak9v41MTHPVmSqaPRAqN/EVoU0pNSpX6Q6RSK6CAlRNULUKkl45BSyhaCoKWz4RWO1pmmESmtMKGq9sWOY1mTaZ6c2cF/1Fu7qI5sazMFWscRr87hNCK9ILv3XDDewwciFBZo0Z4ryY4Siy5L/0+PN3j6f9v70ugrCqutTe0TI3MosxCAI2KGhAEFRCeIASDohIi/BDjiyRifoMa/HkOQXjGgSBTNA7RxAFeIwTRCOKMA6hhcAafE4PQ2EQRGWRU9F91mtO5ffveW1XnVJ06VfXdtVgrsat21f72rq/2rjpVZZTvg+n3fxdwb48OE6Ls+SYfN2Zzvijfs3KjF46u9NVJvtGmmvPvWHFHsHgnw/cy+PGSSp2sYstYksEAO4QyaMUri4QwHn7Ga1sVIIRJYfY7hBmfBekktJL7ZtBhi/5KJ374bWC3L+sTrezZnq6atkjajvkmvulnT6fLnrys0jt32RMPm0TnDp1L/Wb147bLbqSb+NLEoFyuS22YnKZ1mwp/PhRObDIrtKIJIevrhDMnVBTPd0NqrksQRM+W6FhxDT/35AUIYWAie/OryKU9XEeIWEDneIrYpdjVfA4QXON75gw2+Gghvr/q2auqXASTzSU3nnkjTXp5Etf3eXxf6AxiIeGynJUtKx83ZnK+DN8z+WyXTWQOVM35snwvO2eC77luLlXAZ76XAkpBYSSECkA0KcK6AIGBVeAzUB3Bwa4dO+nB686n7q99RvX2lj8w/+bJtan5yN9Tv8H5z4Xls2uhiS/XTl6uyfS6ntfRLctu4boOS0JqHVarysouO5NyUaeLaM7qOVU+9yl0CYromT1ex3IluZk3qvHayfXMBdsxZQ8wq94N5ekSJqO8hJAFJuxT36NnHF3pCY5s+UXViio97yFyaQ+vj1H/rmM8Re2Lqno+Bwiu8b0NCaEs3zOdsrmkfq36tHP/oU9SCgyEQnzPLrRhv3xnzvOdceZxsci45H0RwmtD9ImmzL7o+qpChu/7tO1DB749QC2nt6x0kU8hzMD3Ih4lXsZnvhdHSU1JJIRqcDQmxcoAoQBaqgPYuyf/P2rx4pN0zIbyB+bLjiB6q+dJ9Lvb5kayGW/iyyU012Q68sSRNPu92dw+hKuj2Z8rbd29NXgGIjt54q1Oiu7E8TqW6zPYzIBEtJ3szzDZziX7iT7xweunyN9Dm277ehuVnFRS6fIfVj8zMLl56c1C5zrZFeZH1T1K+KIfkX5GKaN6PEXpg+o6PgcIrvF92hPCKHyfKyEUHQP5+F7kzDnbPcyVFIpycb4+Mv6rXb02zTlpTiVuzLz1WbSN7N2+KE98iGKZr5wM37Mz9Zctuoy+2JNx0VEewexyN5mniuLqkas++F4Hqv7IREJoua1dCxBUEVrpxo206Kbh1P2f26j2N0TfFBGt7FqPul95L3Xq3Dmy1UUnvswG8u0QPvzuw5F2xHhBSqGVVdHbMPMB1KBWA9qxf0fFCvg166+h2wbcViUQEW0n+3xInGvSIxv10PmaUfNHVUkIMxNsJv/CeRcKNaPy3ItQg3kKqRpPcfqgui4Swga0detWatKkiWpojchLs49G4ft8CSF7W++rvV9F+gLCFOf/4uRf0HPrnqNtu7dVJIRHHH4EZT+/EZXvGVYmOJ+1KcL3Mkcr0sD5aR5LUcnFZ76PilnUekgIoyKXknpICKsaYvqE/6Tjlv6Tji4rf2B+Q4tq9GGvM+jKSffFshqblNl5PpHbxXgJ4fOjng8Sqyg7YqJBSq6zF6J1CwHVtLgp/bzTz6nXgV40YOAAql2rdpXiou3k6iPvlrpYRixQecHqBVS0tijn8yC8S4Cyxao+9xJVZwQIUZFLZz3X+J6hnFYfjcr3+RJCkfOB+T77jMOnonXzeXyreq3o111+TSfuOJHqd6pPvdv1rvL0j2gb+XjRBOer5HuGXRo4P61jKQ6bIiGMg55cXSSEcnilrrRrAUIcQnt75ev05p9+S93e+JoO+45ob02if552BP3sD49Sk6ZHxrKd6A1tuRopdN4uyuponNVYmXeg8gHGds2YTuzzyuCq+By3wfLa0XU+JI6RQ99jQc+WPVsqfe4pGvCw9tkZEhXvJcbRJawbZzypaF+HDJ8DBNf4Pq0JYRy+z5UQhpyQ61kfkTNnJjkffF+YxdI0l4Hvdcw4/shEQmi5rV0LEKIS2tTxw6jLsveo2ZflBv2wXXXa8h/n0WXX8C9u4bmA6hvaMs9esLZlVkdZ2fDabF6/861YFjq3wc7uiTw/UVy9OEgI8+0Qsr7NXzOffjr/p1W6yTvnyNNL198L+Z5oQMb6lm3fOP2V8Y1c7UQdT3H6rLsuEkJ8MqrTx+LyfXZCuO+7fZR5vk9mTKt69y4f54viCL7PjxSbz/Kd3xTFNywn4xvge1l0UZ6HABJCHkIp/7vvCeHTCx6hrY9Mps7vlj8wv7OYaPkZreiSPzyq5IF53tkNEfcIdwjHrR9HkwdMznnwX0SO6Kq1yIploZ1J9nkkL+kMdWK7aX3b963S/UJ9zbUiHnciFMGPV6ZQ8iS6Q5j99AavzUJ/F33Tq5AMJIRxLJC+uq7xPUM4TT6qgu8zE8J8Z6xFPEuU75msOJw/7expVLqrNHibL98PfJ8bGXZ84p6f3BN5Ts+UCr7PjbHPC4AiPKGyDBJClWgakOVagCATHEy/6hw6ddk6aryrHPh3j69B1QZfRsMuuVyZJUQTgUINhpNpod00XodFV61ldt8KJWG8HbFQpz1t99Dwk4dX6j6vr/OGzqOfnvDvnUMVEyEPP5G/F/I93iewTH6r+q1ow9gNVc7XiLSdXUb2Ta98bciMpyj9NFHH5wDBNb5PW0Kogu+ZThN6TqAuX3cp+AVFobHD49DMuio4H3xfo5I5RPieJYOlV5VSzcNqxqZB8H1+CH3m+9iOJSkACaEkYGkr7lqAIBLA/s89t1OtxQ/RCR+VPzC/tQF7YP4YunrqP5SbhzdRijQYJk/5ztvxZMisWoucR+G1x/7OC4zyrRjz+pq9kq1qIhTRiVeG53tJXZEui2EhvXg68TBJ4999DhBc4/u0JYQq+J7pVDKkhIo3FOc9Y11oXPHGf3ZdFZwPvq+cEDKMwffpYH+f+T5pCyAhTBpxxe25FiAUCmDLH5gfQt1fKyt/YL4a0Rs/qkNtf3kr9e43QDGy5eJ4E6VIo3ETQtE+sLfvrjj1CiU7VLwV0nxnSkT7Gj7yzh6jL91ZmhNGkc+gRPAXLSOSPEW5BEi0/bCcDIbs4WQkhLII21veNb5PW0IoOvZ4HrRk5BLauXpnpIRQtA8q370D31dNCMOkcOzTYyvNUSoS8Ez/EbW3yC2mInMYz3fT9nckhMlZBAlhclhracl4gHDwINHSpURlZUTNmxP16kVUVBRZ13yEdtetv6PWLz5FHTaWPyXx2RFEb/fuTL+7pSRyWyIVeRMlTwZLajo06EBT2k2JFBww+aKr1qrfQSq0QprvllGZvjav15z6PlT1/GE2piITIc8OIn8XnUx1n3eUwXD4iZU/183WU1QnEXzyldGNR3a7PgcIxvmeGSMhzo/jk1HrquB79un4R5d/RM88/Uwkzlc5/mVwAN/nRks3v6m0dxJ8H1DAdwdp6calVLarrNJt3DL+JlrWZ74XxUhVOSSEqpA0JMdogLBgAdHYsUSlGTs8rVoRzZxJdMEFkRDJJjT2wPyT/30RdV/+FdX6hugAe2C+W33qPf5vdMxxJ0RqQ7aSzHmOTNkVZzsunB+8bxf1k1GVK4gyujPSv3npzTRz+UzatndbRdVghbT/jJw6yfSVTSYjFozgdkl1opuvQZHJNImJUAZD0zuEJs5/+hwgGOV7NnAS4HwuIWguEJvvh82nwR0G0+LFiyNxvsrxLwMV+N5MQqjS3iJzmIxP5CqbNOf7zPdxbSVbHwmhLGIpK28sQGCBwdChRN+X79hV/KpVK/+f8+dHSgozCe3Pk35Fxy9bTq23lLexvmU1+rh3bxp74z2JW4GR4OiFoyslRrxOhJ+WxAkOWBu8VWsdn1bmIv3GdRrT2O5j6fpe19N3B7/LGfCI9vWTKz6hu1bdVfBmuxDftOwQJjURimIo8tahzgDB1PlPnwMEY3wfJoOaOT/Xm6Y8ntXx9zh8zx6XjzPuVI5/UWzA9+KfjLId4JkDZyq5WVT1/B7H70R8xQTn+8z3IjZRWQYJoUo0DcgyEiCwT4batq28M5idFLKdwvXrpT8fZYT2wF/+TDVen01d39wdPDC/pybR8tOb0s9umh/7gfk4Jnph3QvUb1Y/rojssx0qSFrVAXeRHS4R0i+U5PL6Ou70cTRn9Zy8ZwdDgHUkuoWMV8hOIpiwQFDVj4eh6LtXKnwvl068iy902s7nAMEI3wdRq17Oj7qbpmq85ZITle+ZrLjjTtX4B9/n9xCejZLkfFX25ukUZ7yY4nyf+T6OvaLURUIYBbUU1TESILz0ElFf/tkvevFFoj6FL73IhnLq/7uQTln2Ph116AvFD9oX0Zf9LqRLr5pkHPWoK7eqSDruhSYiO1yipM87I5Ovrxd1uohuf+12+p6ydpazrCtzlboqx8hnJ1FMRHbsZPoa194qAtN8/VX5mZMMJqyszwGCEb5noGvkfFX8KOtHvPJR+V7VuIs7/sH3hS0s8sxQkheexbW3Kr9LG+f7zPc8jlL9dySEqhFNWJ6RAGHOHKIR/LNfVFJCNLzwpRchXE/On0U75k6lzu/tD/7TjrpEy3u2prEzn00Y0cLNRVnJUxnwiKz45tJAdLVTNNAXuUUvu6+ntzqd2t/RnrszyPqv+iY3ESfKZydRTHR82hrV3qG+Kn0vE0OVFyGI2CazjM8BghG+Z+Br4HzdPirrV4V4k/0tcxGLt2ClatxFHf/ge771C9nIFOdHtXcSY8kU5/vM93wvVlsCCaFaPBOXZiRAULxaPOPKH9OpyzZQo6/L4Xvn+JpUdN4Y+unFlyWOp0iDsit5qoIDkb7lKiOzwzVvzTyxi14ivLMlOsmqfD5DBrN8djI1Ecr0PV9ZXb4naksdSbLPAYIRvmfOpZjzM/1Vl4+qGD9MhizfszomdQLfi1m+kI1s5XydfmeK833mezFPVlcKCaE6LI1IMhIghOdJNm+ueqkMQ4FdLCNwhvChu26lw5/6Hzr+44MBdl80ZA/MH0sdel0S6Xa2JA0gs5KXi6Rl6mfqFaWeDJGztkSeghDZIcy2R9on2TTuEMb1aV0BQpzP6eLq5HOAYITvmcEUcX4u2+vy0bh+Fod3TXI++F7M8mncIRTref5SOseSKc73me/j+oNsfSSEsoilrLyxACG8ZZThkXnTqMAto+yB+Yf+61zq8fq/qO4+ooPsgfnOxXTMrydT19PPjHxdd8pMU9GdbJIWOduRS5eo9WQSsWEnDCP2WPzmnZtznvMLLwvJdYaQl6zKBCq85xR02Jp3hpCHieozhCp01BkgRPl8WoVOPgcIxvieGS4G5xeyu04fVeFvUWSY5Pyk+D5YJyjwHp2tfB/qJTIPpo3zdY8lE5zvM99H4Z44dZAQxkEvBXWNBwjZ7xC2bk00Y0beJyfuuPlKavfSs9R+U/mlIpuPJHq35yl09S2zg/+vm9B0mazQxJip08JPFtLQeUOrJFvhmZSJfSZSx8Ydqzz2KnomJJd+shOzCOln3zIqkqyaWmEUtXkh3xPBROUto6J95pXTPZ6ifE7H6zPv7z4HCEb5PkwKJTmfZ0/dPsprP8rfeYtfJjk/Cb4PXOF/F9DYp8dWOhOe+SSDzXwf6sfmavaTOT8axZ9U1UliLCXN+T7zvSq/EJWDhFAUqZSWMx4gsE+Jli4lKisjat6cqFevnE9NrP/kY3r2lp9T9xXbqda3RAcOI1rerSH1v/5hatehYwW6SRCaalPyJsZQpwEDB1DHuzoKXarC+hhOrucde16waxf1xrMoEzOP9GUCnswnEtKcWPF8j4eJar9SIY+nk4o2eMGxijYyZfgcIBjne2YIQc4XtXsSPiraF5FyPL5nMkxyvm6+Z29Fii5Q2sz3+ZJeExeeifhlpt8NGjSIdL7pmSTn+8z3onZXVQ4JoSokDclJRYDA0X3aDT+nTktXUut/lRdc17oarevdl674/Z+r1LQxOCi048eSoXA3rX6n+vQfs/9D2FMydw1vfOlGbr1CF3hEmZhFdj15SW6u9+jSmliJ+F6SEyHX4AIFRHQSEJOqIj4HCDbwvayz2OSjoolQqJMpztfF9yzRqF5UXWqB0ma+D9Y/CnwWK+vrusvbNJZEsfCZ70UxUlUOCaEqJA3JSXOA8PrLz9OHfxlPXd/cQ0XfE+2uxR6YP5J+fttCqtegfk7EbCI00dvcwvN2e9ruoRGPCzzXkYEMS6ga1WlE2/YeepixgJ+VXFBCw0/M/8yHyolZNuDJTlbTOMna5HuidOOiTj4HCGnme1GfzC5ni4+K8j07V/bdwe+Cs/AmOV8H37OE8NXNrwpdPJbJ+eD7qKNDrp4tY0lGK5/5XgYnFWWREKpA0aCMtAYIU8edT11f/YCO/KocnPc7FNHOAT+jS674fUG0bCI00bMa4Y2csqvFsm4lcsW/qok5tJNowMNLVmV11VHeJt8T1d9FnXwOENLK96L+mKucLT4qyveMh89oeUaQEJrmfNV8zxLC+R/MF3uaiLNAGcdnVNS1xe9kdHVRJ5/5Xsb2KsoiIVSBokEZaQsQHvuf+2nfY3fQj1YfCFDZfjjR8jOOpitnPi2Ekk2EJnyb26E3+8LPK/PdVlkIoLo16tLub3bnLJLrs0whsGMUirtDGKNpbVVt8j1REFzUyecAIW18L+qHhcrZ4qPCfH9BCQ394dAgIXSF8zNtFGWHUIWfqJZhi9/J6O2iTj7zvYztVZRFQqgCRYMy0hQgzBg7gLov20gND+Utb3WqSfV+ejUN/tnFwgjZRGiiK8aZb/aFt4wyQDJvLhMGKKtgeM4w8+KWqLLCeiKrytmXJtj4JEM2ToV8TwSTuLjrqG/TeBLV3+cAIU18L2ovXjlbfFSU7zN3CNmOWto5X4TbMm0UniG0nfPB97yRmY6/+8z3SVsACWHSiCtuLw0Bwl+nT6JGz/+djltb/sD8542I3uh5HF09ZYG0trYEB2wSZQHCsPnD8p7vy/dmX66zHdJAHaqg+sYzkRv0WNOZdsoX8OhIVqPiJFIvn++JYiLSRtJlbBlPMrj4HCCkge9lbCVS1hYfPfDtAWo5vSVt3bM1p1qZX2qEZwjD2x7Tyvmi3JZtoyiX1oj4QpJlwPdJoh29LZ/5Pjpq0WoiIYyGW2pqmQwQ2APzD48fTD1e/5yK9xN9W51oVZe61GnMNOp2Ru9IGNkQHIhM7pnJUPabfQyYzFXZj7d9TOwWUVZHdNewSZ0mNHfoXGIPuBdVL6rAWmS1N59hRG/Qy04Iw2vIs9+kUp2sRnIoiUq5fE8GE4mmEitqw3iSBcPnAMEk38vaSbS8DT7K4/zsxa9cOungfFN8z2yr8tIaUV9RWQ58rxJNfbJ85nt9qOaWjIQwacQVt2cqQJj535dTh1deoh+Ulj8wv+kootW9utHVf3g4loZpDw7yJQjZSmcmQyI68QKOXKBmXyIjutqbS5bMDXosAeUFPM3rNadebXpVSlZjOUZG5ThBUKE+ZOski4kq/VTKEfE9le0lIcvnAMEU3+u0a9p9VITzsxe/RHSKy/mm+Z75hC4uzvQ3XW2A73WOanWyfeZ7dSiKSUJCKIZTakslHSB89L9r6OU//pJOXbmDan5LtL8G0YpTG9GgGx+hVm3axMZJZCKN3UhEAbwEgYnNtXMnqlM48T36/qN058o7ub3MvLkz7k6WzPkYtispqhNXCckCcYIgXlPZOsliwpNv4u+m7KRTV58DhKT5XqcdQ9lp9lERzm9a3JRKryqlmofVrIBLVKeonA++n0kXHHdBLPcE38eCL7HKPvN9YiAfaggJYdKIK24vyQBh2nUj6aSlb1DLL8qV+KRNNfq0zwD6v9dNV6aV6ESqrEEJQVETBFmdZNvhBS0it5DK3KDH3jqU1UkC5rxF4wZBvD5k6ySLCU++ib+bsJNuPX0OEJLke912tCEhlOXiqDrJtMO+vGg7sy2V7izNaSLwvZjngu/FcDJdyme+Txp7JIRJI36ovQ0bNtBNN91ES5YsoS1btlCLFi1o5MiRdP3111PNmv9eaeR1L4kA4ZXnn6ENf7uWury1t/yB+dpE/zz9KLr41ifyPjDP63e+v6c5gI2aIMjqFCZ4ore4yQQTbHcv109WhqxOUf0hrKci6eX1ASvGPITS8XcbAwSb+D5pKyfNJTL6pZHzl25cKv0wfLbO4PvKF6Oxc/CymMj4UVJl0zyWomJgI99H1dV0PSSEhizw9NNP09y5c2n48OHUoUMHWr16NY0ePZpGjRpFt99+u3CvdCeEU8cNoW7LPqSm28u7tOaYw2j3wBF08eXXCvdRpmCaCS3qhBFFJ5lb3KIGLZl2kU1Co+gk4wfZfbtjxR101TNXcUVkn6vkVsgokO9MiWhiLtNWUmWTtFNSOtkYINjC90nZMLOdNPtoGjnfdb5nvvHCuheo36x+XHcE31eGKM1jiWvMPAVs5Puoupquh4TQtAUy2p8yZQrdfffdtG7dOuFe6UoIH519D32z4C46+f1vgr58VY9oxRlt6coZTwn3LUrBNBOabNIU6h9VJ9Fb3KIGLdn2kUlCJU77+gAAIABJREFUo+ok6xOyFy9knquUbSuXTjKYyLaXRPmk7JSELmEbrgQIaeL7JO2X3VaafTSNnO8y3zPfYJw7euHovM85ZfoP+B4JoUnucq1tJIQpsugNN9xAbCV51apVeXu1f/9+Yv/CH0sIW7duTWVlZdSkSZPY2rCnJGb993DqvmwTNdhTLu7NE2tRg59eTQPOHx5bPk8ACw6ee+456t+/P7HPONL2W/jhQhr12KigW5lPRITXjs86fxYNPnZwpW7H0YkFJK+Xvk5bvt5CzQ5vRqe1Oq3KzZ2szIl3n0if7fos57MVrG8t67Wkd8e8y731k+k3/vnxtHnX5godWtVrRbf1u62SXnF0ErVpiLXoUxxM7jWnXxM8xZELJ167+XQSxYQn38Tfk7BT0nqxhLB58+a0Y8cOql+/ftLNK2svDXyvTJkYgtLuo2njfFf5nrmQLOeD76smhGmOn6LQhCt8H0X3pOsgIUwa8TztrV27lrp06UJTp06lSy+9NG+vJk6cSJMmTary95KSEiouLo6lzcdvvkDtV75AP1z3XSDnX42JVp1xHHXseXEsuagMBIAAEFCFwJ49e2jEiBFWJ4Rp4HtV9oAcIAAEgIAuBFzge13YqJaLhFAxovkStsxmVq5cSV27dq34T5999hmdeeaZwb/777+/YI907BB++cXntGDSRdT99a1UfKD8gfmVpxxOJ4+ZRid166EYocLi0r5aHPY+184d+1uu3bykdFK5k5VLFttlnNxvcrBTqFunZRuX0Tkl50T2vUI7tvmE6tYpsjIxKrqoU5pWjG3k+xjupKWqLT6azfmntjiVVny2IufXG0no5BLfM8d6ecPLdO4j50byMfB9OWxJ+F0kA8WolCa+j6GGFVWRECo209atW4n9K/Rr27Yt1a5dOyjCksG+fftS9+7d6cEHH6Tq1atL9SjuGcKZEy+jjktfoXabyx+Y39isGq3p1YOuvulvUv1QVTjN50kK6VjofbzBHQbT4sWLadCgQdo/g1XxiK/I8w66dWIXyMz454xYbiVy/XpmA7b6XiGQXNQpTWcIbeP7WANKU2UbfZT3HmpSOrnC9zLnBvO5Ifi+6s2pmoZsomLTxPeJKm6gMSSEBkAPm9y8eXOQDJ5yyik0e/ZsKioqku5N1ISQPTD/ym3/Sd1W7aSaB4n21SBa3r0x/WTCHCUPzEsrcqhCUhNp1P7lqsdNoC6cT0VrixJJCOPqJfq8w0eXf0TPPP2MtE4iAQzD88J5F8ZVpaK+6E10NvoeDyQXdbI1QDDJ9zw/Mfl323yUy/fD5pPuBTNV9tLN96yfPM7Ph2dUHcH3ySw+R7WPbD1b+V5WzzSUR0JoyArhZ6Jt2rShhx9+uFIy2KxZM+FeRUkIp107nE5e+ja1OLSR+cnR1WlTn4F0+bVThdvVVdC24EBkQu3QoANNaTdFOnnShXEhuaI32C0ZuYR2rt4ppRNvVT0MHgo9uhz2na0GN67TmL7c+yUXJtGb6DJ9r3pRdWLvfZXtKqPm9ZoTewy6qLr8gg23c5oL2DaeROCwMUAwyfcimJosY5OPivB9q/qtKOqCWdJ20Mn3TBce5/PwjIIH+B4JYRS/QR0iJISGvIB9HnrJJZfkbP3778s/3xT5ySSES556gjY/NJG6vLOXqn9PtKsO0fIzWtAvbn5M+QPzIn3PVcam4ID1X2RCrVO9Ds05aY5U8hQVv7j1hN+4GlJCxRuKhXUSWVW/4LgLhPAMdZzUZxLd+NKNXJVlV4wPtj9IY58bS6U7SytksyBv5sCZxPpo08+28SSCrY0JoQm+F8EyDWVs8lERvmeYRlkwM2ELXXwfJoND5w2tcvN1eN5v/rD5waJe34f6KlUdfI+EUKlDeSQMCaHlxhZNCKf97lw6denH1GRnucKrjz2MDvz4Yvo/l41LFQI2BQcMOJEJ1aaEUEfAw1sFzjz7MW/NPBqxYATXJ6/sfiXdfvbtxHYTVT0cH/reiHdH0J7vDr25cqgnmUGMTUmhbeOJa3gisjEhFNFLpIwo34vISksZm3xUhO8ZriWSC2ambKGD75kuopx/61m30sjHRnLVb1KnCd1zzj101bNXge8LoGXTWOIa/VABn/leFCNV5ZAQqkLSkBxegPDI3/5E1Z+4n078oPyB+W3sgfmeP6Crpj9pqMeFm7WN0EQmVJsSQtGHmGU+iRLBiHkFW9llP5EV43AVWOXD8fv27wvORQ5/dzjt/W5vFUeVvbQgDQPMtvEkgpnPAQKP70XwS1sZm3xUlMts2SHUwffMv0Rxmj5gOrELxHi/50c9T2f94KzgE1S268h+ud4BZruOogt24Hse6un4u898n7QFkBAmjbji9vIFCOyB+Qeuv4C6v7aZ6u8hYi8LvnVybTriZ+Np4AUXKe6FOnE2BQdMa5EJ1aYzhEwnkUlX5tIE4VX1C0po2AnDpHf9cp1TaV2/Nc0YOEM4OAgS0rUvBuci8yWEoZeLfpKkblREl2TbeBLR1OcAAQmhiIfoKyPC9zadIdTB90ymKOfPPn82/dcL/yW16we+z+/f4Ht9Y98HyUgILbdyrgDh3injqdmSRXTM+vIH5rc0IXqz54n0u8nzUq+tjYTGTaAsumU0dBDepCtjJ9HV4ji7fryb7EQcf847c4JzkbyEUPTSApE2dZeRsZPuvqiSj4SwQfC0UZMmTVRBalSObT7K5XuLbhnVwfdMpgznb9u7TXrXD3yfe8jaNpZEiMdnvhfBR2UZJIQq0TQgKzMhpO8O0jy2K/jPL6nOoQfmV3StR92vupc6de5soHfyTdpKaIUSKJndNHnE9NUoNOnK2El0VX392PUVN3nyElJZrUUCCOwQyqJqprzPAQJ2CM34XHarPH6S4cd0aFT4eQhZfWQ5n4enLEbge/1vHsvaJGp5n/k+KmZR6yEhjIpcSuqFAcIt40fRj1asorafld9Q+mnzavRB79Ppykn3p6SnYt2QnXjEpCZTKt8kZLNO+ZCT1UlkVT377IfIpC5iWd7V56GM8ExJrktlWBmcIRRBW38ZnwMEJIT6/Uu0BVULZqLtmSwny/esr7KcD76Pb+Eodorfql4JPvO9XmSrSkdCmDTiitsLA4RXj+1IjaiI9tYkWt6jCQ27eQE1aXqk4tb0i3OJ0ComuB1lwaeIAwYOoNq1ausHMYEWothJ9SqwiJqiz10wWZm3jLJLZeJeWiDSP91lothJd5/iyvc5QEBCGNd79NXPTGiaFTeTfqdVX8/iS47KI0lzPvj+G1q8GM9OxPd4PyUgIbTc7mGAsKJDR/qsfQ3actZP6NfXTLZWq6gTT9oUzpwIw1tGx60fR5MHTJa66CRteoX9iWonVavAIriIXn0efqoa6pTrHcIol9SI9FF3mah20t2vOPKREOIMYRz/0VE3O/EJOZ9xyQWd7Hq7NBc+cXgkKc4H3/97UXPQIHwyqmOcuy4TCaHlFg4Twsmjz6QxU55IzQPzUWGNM/FEbVN1vexVyjA4YJ8isp0nmaux4/ZN12Rsg51kLjbo07ZPxQ4hm0yrF1WnpRuXUtmuMmperzn1atOr4nxjXJskWd8GO8nigYQQCaGsz+gsn2tXKpPzZw2dlegioA7Ot4FHwPdICHWOcx9kIyG03MqufUJkw8RTyGVyrVKGwQG7vXLfd/uIXUueeYGKLhcUPTsXpX0b7CR69Xl4a6gNOsnaykWdkBAiIZQdB7rK59uVykwImxzeJBG+Zzrq4nwbeAR8j4RQ1zj3RS4SQsstjYQwXQbMtUqZmRCGD57rfstO5ixFFARtCBDirBjXqFEjCiypq2ODnWRBQ0KIhFDWZ3SVz8cx2Zyvm+/DZJA92p559pn9d3YZFvvF+TLFBh4B3yMh1DXOfZGLhNBySyMhTJcBc61S5koIdb5lJ3uWIgqCNgQIslef26CTrK1c1AkJIRJC2XGgq3y+XalsztfJ90w33ZxvA4+A75EQ6hrnvshFQmi5pZEQpsuAadghlF0pjYKgDQFC5qo5+9+8W0Nt0UnGXi7qhIQQCaHMGNBZNi07hLo53xYekXnqwhadZPzXRZ185nsZ26soi4RQBYoGZSAhNAh+jqZzrVImfYZQ9ixFFATjTDwilx6IlBHtt+jV53F0Eu1L0uVc1MnnAME1vmfjwWYfzbcrlfQZQt2cH8dGIlwuUkaUO8H3eHZC1FdQrjICSAgt9wjXAoQ4E09aTJm9Spn0LaO6V4vjBHEilx6IlJG1tUjA4YLvZePiok5ICLFDKDv+dZbPtSuV9C2jujk/Ko+IcLlIGVn7ge/x7ISsz6A8ERJCy70ACWE6DZjrHcJr1l9Dtw24TfsV5LJnKaIgGCVAELnohvVF18UIPD2j6MSTafrvLuqEhBAJoelxld2+6XcIdXN+FB4B3yfvpVHslHwv5Vr0me/lkIpfGglhfAyNSkBCaBT+go1XrFLuKKPiDcU0YOAAql2rdiIdljlLEaVDshOPyKUHLeu1DLpSuqs0Z5fYbXk6n+yQ1SkKbknXcVEnnwME1/iejQdXfDRzV6pZcTPauXonJflAuE7Ol7UR+D5ppi9vT9ZOZnop16rPfC+HVPzSSAjjY2hUgmsBgouEZkon0bMUURxYVifRT5pE+qLrCndZnUT6arqMizr5HCC4xveuBrGmxp0uzpfVB3xvhvll7WSml3Kt+sz3ckjFL42EMD6GRiW4FiC4SGgmdRI5SxHFgWV1Er30QKQvuq5wl9VJpK+my7iok88Bgmt8j4RQPUPo4HxZHgHfq7eriERZO4nINF3GZ75PGnskhEkjrrg91wIEFwkNOhEluWIcNSCCnRSTkyZxPgcIrvE9EkJNg0SxWFluTJLvmapROF9WJ8WQahHnok4+870WJykgFAlh0ogrbs+1AMFFQoNO/344efPOzZXeAwyHAzsfGJ4h3LwrfxneGcI4N9bBTorJSZM4nwME1/geCaGmQaJYrCw3ilxyo4LvmZpROV9WJ8WQahHnok4+870WJ0FCmDSsybXnWoDgIqFBp/LxIHLpASvHbhllP95D8tmrwlt3b6Vh84dVSThZssl+84fNL3jDK+yUHG/FacnnAME1vkdCGGckJFc3Cjfq5vtebXrRPz78R+RbqaPolBzi0VpyUSef+T6aF0SvhR3C6NiloqZrAYKLhAad/j1URC49iFqmqFoRHfz+YM5xKXJDKeyUCkrjdsLnAME1vkdCyHX3VBSIyo1Rubx1/dY0Y+CMigW8XHLYDuO+b/fRl3u/jMT5UXVKhUHydMJFnXzm+6R9DQlh0ogrbs+1AMFFQoNOlZ1e5LxHoTL53rcSGVqFbiiFnUQQNF/G5wDBNb5HQmh+PIn0IA43muR7pls+zo+jkwhmJsq4qJPPfJ+0DyEhTBpxxe25FiC4SGjQSZ3T89634rU0+/zZ1LJ+SyrbVUbN6zUn9tlRUfWioBrsxEMvHX/3OUBwje8x7tIxpni9MMWNcfme6ZWP803pxMM6zt9d1Mlnvo/jC1HqIiGMglqK6rgWILhIaNBJ3YCJe3td0+Km9MWeLyo6xC6pmTlwZvBpEuykzk46JfkcILjG90gIdY4UdbJNcWNcvmcI5OP8wR0G0+LFi2nQoEFUo0YNdWAZlGTKTjpV9pnvdeKaSzYSwqQRV9yeawGCi4QGndQ5vcr3rVivMi+cQYCgzk46JfkcILjG90gIdY4UdbJNzWGq+b4S5184n4rWFiEhVOcmWiT5zPdaAC0gFAlh0ogrbs+1AMHUxKPYLJXEQSd16KpYMc7uTXjhzEeXf0TPPP0MAgR15tIiyecAwTW+R0KoZYgoF2pqDtPB92FS2KFBB5rSbgr4Xrm3qBXoM9+rRZIvDQkhH6NUl3AtQDA18eg0MnRShy7vfSvWUvZto9mfDOXrzZKRS2jn6p0IENSZS4sknwME1/geCaGWIaJcqKk5jMf3bDGvcZ3GVOewOlS6q7RCbxHOr1O9Ds05aQ74Xrm3qBXoM9+rRZIvDQkhH6NUl3AtQDA18eg0MnRSiy7vfau5Q+dS07pNKy6O2bxzM418bCS3EyVDSqh4QzECBC5SZgv4HCC4xvdICM2OJdHWTc5hPL5n78ued+x5tHTjUinOR0Ioan2z5Xzm+6SRR0KYNOKK23MtQDA58Sg2TYU46KQeWZH3rcJWRT87wg6hejvpkOhzgOAa3yMh1DFC1Ms0PYfJ8D3TXoTzkRCq9xMdEn3mex14FpKJhDBpxBW351qAYHriUWyeQBx00oEqkcj7Vqxlkc+O2G2jOEOox06qpfocILjG9+BH1aNDj7w0zGGifC/K+ThDqMdXVEv1me9VY8mTh4SQh1DK/+5agJCGiUe1yaGTakTl5Yl8doRbRuVxNVHD5wDBNb5HQmhiBMm3aeMcxuV83DIq7wgGavjM90nDjYQwacQVt+dagGDjxMMzKXTiIZTM33mfHcFOydghbis+Bwiu8T0SwrijIZn6tnJjIc7HAmAyvhO3FZ/5Pi52svWREMoilrLyrgUItk48hdwCOqVn0BT67Ah2So+dCvXE5wDBNb5HQmjHmLOZG/Nxvs065fMaF3Xyme+TZgckhEkjrrg91wIEFwkNOil2ek3iYCdNwCoW63OA4BrfIyFUPDg0iQM3agJWsVgX7eQz3yt2D644JIRciNJdwLUAwUVCg07pHkNh72AnO+zkc4DgGt8jIbRjzIEbYSdTCPjM90ljjoQwacQVt+dagICJR7GDaBIHO2kCVrFYF+3kc4DgGt8jIVQ84DWJc5FHoJMmZ1Es1me+VwwlVxwSQi5E6S7gWoAAkk63v2E3zQ77uGwnnwME1/geCaEdfIJ5GXYyhYDPfJ805kgIk0ZccXuuBQiYeBQ7iCZxsJMmYBWLddFOPgcIrvE9EkLFA16TOBd5BDppchbFYn3me8VQcsUhIeRClO4CrgUIIOl0+5vLO0/wPTt8z+cAwTW+R0Jox5gDN8JOphDwme+TxhwJYdKIK27PtQABE49iB9EkDnbSBKxisS7ayecAwTW+R0KoeMBrEucij0AnTc6iWKzPfK8YSq44JIRciNJdwLUAASSdbn/DDqEd9nHZTj4HCK7xPRJCO/gE8zLsZAoBn/k+acyRECaNeEZ75557Lr399tv0+eefU6NGjahfv340efJkatGihXCvXAsQMPEIm95oQdjJKPzCjbtoJ1sDBPB9brd10Udd08k1fbAQITyFGC9oK98bBy5CB5AQRgBNVZXp06fTaaedRs2bN6fNmzfTuHHjAtGvvfaacBNICIWhMlYQk6kx6KUahp2k4DJW2NYAAXyPhNDYoInZMLgxJoAJVXfRTrbyfUImV9oMEkKlcMYT9sQTT9CQIUNo//79VKNGDSFhSAiFYDJayEWShk5GXUq4cRft5EqAAL4vd2MXfdQ1nVzTB34nPIUYL+gK3xsHUqADSAgFQEqiyLZt22jMmDHBTuGyZcvyNsmSRfYv/LGEsHXr1lRWVkZNmjRJoqta22ATz3PPPUf9+/cXToq1dkiBcOikAMQERMBOCYCsoAkWILCvKnbs2EH169dXIDF5EeD7f2OOcZe8/8m2CBvJImamvIt2coHvzXiDfKtICOUxU1pj/PjxdOedd9KePXuoR48etGjRooKJ3cSJE2nSpElV+lBSUkLFxcVK+wZhQAAIAIG0IcC4csSIEVYmhOD7tHkT+gMEgECaEbCZ79OMa66+ISFUbLF8CVtmMytXrqSuXbsG/2nr1q3EVos//fTTINFr0KBBkBRWq1YtZ8+wQ6jYYAmIc3HVDjol4DgKmnDRTmlaMQbfx3dSF33UNZ1c04d5LXSKP3aTkJAmvk9CX5NtICFUjD5L8Ni/Qr+2bdtS7dq1qxQpLS0NPv9kl8qwy2ZEfjhDKIKS2TI4f2EWf9HWYSdRpMyWS9OZEvB9fF/AuIuPoW4JsJFuhNXId9FOaeJ7NVZKrxQkhCmyzaZNm6hNmzb04osvUp8+fYR6hoRQCCajhVwkaehk1KWEG3fRTq4ECOD7cjd20Udd08k1feB3wlOI8YKu8L1xIAU6gIRQACQdRVasWEHsX8+ePYM3CNetW0cTJkwILodZs2YN1apVS6hZJIRCMBkthMnUKPzCjcNOwlAZLWhjgAC+z+8yGHdGh5NQ47CREEzGC7loJxv53rgjROwAEsKIwMWt9t5779HYsWPpnXfeod27dwe35g0cOJBuuOEGatmypbB4JITCUBkr6CJJQydj7iTVsIt2sjFAAN8jIZQauCkr7CKPQKeUOVme7tjI93YgW7WXSAhttdyhfiMhTL8BMfGk30ash7CTHXbyOUBwje8x7uwYc+BG2MkUAj7zfdKYIyFMGnHF7bkWIGDiUewgmsTBTpqAVSzWRTv5HCC4xvdICBUPeE3iXOQR6KTJWRSL9ZnvFUPJFYeEkAtRugu4FiCApNPtb2HvYCfYyRQCPgcIrvE9EkJTo0iuXfC9HF6mSrtoJ5/5Pmk/QkKYNOKK23MtQHCR0KCTYqfXJA520gSsYrE+Bwiu8T0SQsWDQ5M4cKMmYBWLddFOPvO9YvfgikNCyIUo3QVcCxBcJDTolO4xhF1PO+wT9tLnAME1vkdCaMfYwxwGO5lCwGe+TxpzJIRJI664PdcCBEw8ih1EkzjYSROwisW6aCefAwTX+B4JoeIBr0mcizwCnTQ5i2KxPvO9Yii54pAQciFKdwHXAgSQdLr9DbtpdtjHZTv5HCC4xvdICO3gE8zLsJMpBHzm+6QxR0KYNOKK23MtQMDEo9hBNImDnTQBq1isi3byOUBwje+RECoe8JrEucgj0EmTsygW6zPfK4aSKw4JIReidBdwLUAASafb31zeeYLv2eF7PgcIrvE9EkI7xhy4EXYyhYDPfJ805kgIk0ZccXuuBQiYeBQ7iCZxsJMmYBWLddFOPgcIrvE9EkLFA16TOBd5BDppchbFYn3me8VQcsUhIeRClO4CrgUIIOl0+xt2CO2wj8t28jlAcI3vkRDawSeYl2EnUwj4zPdJY46EMGnEFbfnWoCAiUexg2gSBztpAlaxWBft5HOA4BrfIyFUPOA1iXORR6CTJmdRLNZnvlcMJVccEkIuROku4FqAAJJOt7+5vPME37PD93wOEFzjeySEdow5cCPsZAoBn/k+acyRECaNuOL2XAsQMPEodhBN4mAnTcAqFuuinXwOEFzjeySEige8JnEu8gh00uQsisX6zPeKoeSKQ0LIhSjdBVwLEEDS6fY37BDaYR+X7eRzgOAa3yMhtINPMC/DTqYQ8Jnvk8YcCWHSiCtuz7UAAROPYgfRJA520gSsYrEu2snnAME1vkdCqHjAaxLnIo9AJ03Oolisz3yvGEquOCSEXIjSXcC1AAEknW5/c3nnCb5nh+/5HCC4xvdICO0Yc+BG2MkUAj7zfdKYIyFMGnHF7bkWIGDiUewgmsTBTpqAVSzWRTv5HCC4xvdICBUPeE3iXOQR6KTJWRSL9ZnvFUPJFYeEkAtRugu4FiCApNPtb9ghtMM+LtvJ5wDBNb5HQmgHn2Behp1MIeAz3yeNORLCpBFX3J5rAQImHsUOokkc7KQJWMViXbSTzwGCa3yPhFDxgNckzkUegU6anEWxWJ/5XjGUXHFICLkQpbuAawECSDrd/ubyzhN8zw7f8zlAcI3vkRDaMebAjbCTKQR85vukMUdCmDTiittzLUDAxKPYQTSJg500AatYrIt28jlAcI3vkRAqHvCaxLnII9BJk7MoFusz3yuGkisOCSEXonQXcC1AAEmn29+wQ2iHfVy2k88Bgmt8j4TQDj7BvAw7mULAZ75PGnMkhEkjrrg91wIETDyKHUSTONhJE7CKxbpoJ58DBNf4Hgmh4gGvSZyLPAKdNDmLYrE+871iKLnikBByIUp3AdcCBJB0uv3N5Z0n+J4dvudzgOAa3yMhtGPMgRthJ1MI+Mz3SWOOhDBpxBW351qAgIlHsYNoEgc7aQJWsVgX7eRzgOAa3yMhVDzgNYlzkUegkyZnUSzWZ75XDCVXHBJCLkTpLuBagACSTre/YYfQDvu4bCefAwTX+B4JoR18gnkZdjKFgM98nzTmSAiTRlxxezt27KCGDRvS+vXrqXHjxoqlJy+OTTzPPvssnX322VSjRo3kO6ChReikAVQNImEnDaBqELlt2zZq164dbd++nRo0aKChhfSKdI3vw4QQnJ9en4ON0m2bzN65OIf5zPdJex4SwqQRV9zeunXrqH379oqlQhwQAAJAIN0IrF27ln7wgx+ku5OKewe+VwwoxAEBIGAFAj7yfdKGQUKYNOKK22Or5I0aNaKNGzc6sVrOPolq3bo1bdq0ierXr68YLTPioJMZ3GVbhZ1kETNTnu2StWnThr766qvg6wiffq7xPbMdxl36PRg2Sr+NXB1LPvN90l6HhDBpxBW3F54pYYPGhQTKNX1CkmaftrliI+ikeBBrFIfxpBFcA6JhTwOgR2jSNTu5pg/msAhObaiKi75nCEpus0gIuRClu4Brg8U1fTDxpHv8ZPYOvmeHrVy0kyjyLuoOnUStb64cbGQOe5mWYScZtFA2GwEkhJb7hGsE4Jo+SAjtGWDwPTts5aKdRJF3UXfoJGp9c+VgI3PYy7QMO8mghbJICB3zgf3799Ott95K1157LdWqVct67VzThxkEOtnhlrAT7JR2BOCjabdQef9cs5Nr+rhoI+hkBzekuZfYIUyzddA3IAAEgAAQAAJAAAgAASAABICARgSQEGoEF6KBABAAAkAACAABIAAEgAAQAAJpRgAJYZqtg74BASAABIAAEAACQAAIAAEgAAQ0IoCEUCO4EA0EgAAQAAJAAAgAASAABIAAEEgzAkgI02wd9A0IAAEgAASAABAAAkAACAABIKARASSEGsFNWvS5555Lb7/9Nn3++efUqFEj6tevH02ePJlatGiRdFeUtLdhwwYnUhc1AAANEUlEQVS66aabaMmSJbRly5ZAj5EjR9L1119PNWvWVNJG0kJuvvlmevLJJwM7MR22b9+edBdit3fXXXfRlClTqKysjE444QSaMWMG9erVK7ZcUwJeeeWVQJ833ngj0Omxxx6jIUOGmOpO7HbZrcMLFiygDz74gOrUqUOnn356wAPHHntsbNmmBNx9993E/jFOYD/mdxMmTKAf//jHprpkvF3wvXETCHUAnC8EU2KFXON7Bhw4PzH3cbohJIQOmXf69Ol02mmnUfPmzWnz5s00bty4QLvXXnvNSi2ffvppmjt3Lg0fPpw6dOhAq1evptGjR9OoUaPo9ttvt1KnG2+8kRo2bEilpaX017/+1bqEkNmD4c+SwjPOOIPuvfdeuv/+++n999+nNm3aWGmTp556il599VXq0qULXXjhhdYnhAMHDqSLLrqIunXrRt9++22wgPLee+8FNqpbt66VNlq4cCEVFRUFPMB+Dz30UJDEv/XWW0Fy6OMPfG+H1cH56bKTa3zP0AXnp8vHbO0NEkJbLSfQ7yeeeCLY6WBvCNWoUUOgRvqLsCCQ7RSsW7cu/Z0t0MMHH3yQrrzySusSwu7duweJE7NB+DvuuOMCP2OrlLb/qlWrZn1CmG2DL774go488kh6+eWXqXfv3rabqKL/jRs3DpLCX/7yl87oFEcR8H0c9PTXBefrx1i2BRf5nmEAzpf1BJRnCCAhdNQPtm3bRmPGjAl2CpctW+aMljfccAOxncNVq1ZZrZONwcGBAweouLiY/v73v9P5559fgf/YsWODT2BZwmH7z8UA4ZNPPqGOHTsGu4SdOnWy3UR08ODBwAcvvvjiYIfw+OOPt16nuAqA7+MiqL8+OF8/xrItuMj3DANwvqwnoDwSQgd9YPz48XTnnXfSnj17qEePHrRo0SJq0qSJE5quXbs22J2aOnUqXXrppVbrZGNw8Nlnn1HLli2DzyvZubTwd8sttwSf8H344YdW2yQgRMd2CL///ns677zz6KuvvqKlS5dabR+W0LJP4vft20eHH344lZSU0KBBg6zWKW7nwfdxEUyuPjg/OaxFW3KN75ne4HxR66NcNgLYIUy5T0ycOJEmTZpUsJcrV66krl27BmW2bt1KbLX4008/Deo1aNAgSAoZ8aXlJ6sT6zdLRs4888zgHzuzlqZfFH1sDg7YmVQWmIc/dmnCrFmzgktMbP+5FiD85je/CS4xYl8JtGrVymrzsB3qjRs3Bp9ZP/roowEPsF1pl3YIZbkEfG/GpWXtxHoJzjdjq0Ktusb3TFdwfvr8zJYeISFMuaXYhM/+Ffq1bduWateuXaUIu7ikdevWwaUymQG8aZVldWLJYN++fYmdX2OTavXq1U2rUKl9WX1sDQ7wyWiq3I7bmSuuuIIef/xxYrfqtWvXjlvetgLsFuX27dsHFxu58ovCJaHu4PvkvCCKnWxMCF3nfNcSQnB+chzgYktICF206iGdNm3aFNz8+OKLL1KfPn2s1JSdgWTJ4CmnnEKzZ88Obhp04WdjcMBwZ0k5swW7ZTT8sR0a9lkiLpVJh2eyT4ZYYMCez3jppZeC84Mu/s4666xgwYuNJfyIwPfp9gJwfvrs40pCCM5Pn2/Z2CMkhDZaLUefV6xYQexfz549gzcI2S2c7J0u9q7amjVrqFatWtZpGn4mypLahx9+uFIy2KxZM+v0YR1mn7yxT3rZjYDshsTwXBe7Tp+di0r7L3x24p577gl2nf/yl7/QfffdF/jY0Ucfnfbu5+zf119/HRzCZ7/OnTvTtGnTgkUIdouljU9pXH755cH5un/84x+V3h5kn4+zdwlt/F133XXBm4MsAdy1axc98sgjdNtttwUXTPXv399GlWL1GXwfC75EK4PzE4Wb25hrfM8UBudzzY4CAgggIRQAyYYi7MIFdtvjO++8Q7t37w7eImRv07BbOdlFIDb+2IrqJZdckrPrbEXMxt8vfvGL4AKW7J9Nu7hsd/CPf/xjsNjAbq1k76HZ/JwB20VjCWD2j91iaePuU77zwg888AAx/7Pxx56WeOGFFwKfY4ntSSedROxCFR+TQWY/8L09XgzOT5etXON7hi44P10+ZmtvkBDaajn0GwgAASAABIAAEAACQAAIAAEgEBMBJIQxAUR1IAAEgAAQAAJAAAgAASAABICArQggIbTVcug3EAACQAAIAAEgAASAABAAAkAgJgJICGMCiOpAAAgAASAABIAAEAACQAAIAAFbEUBCaKvl0G8gAASAABAAAkAACAABIAAEgEBMBJAQxgQQ1YEAEAACQAAIAAEgAASAABAAArYigITQVsuh30AACAABIAAEgAAQAAJAAAgAgZgIICGMCSCqAwEgAASAABAAAkAACAABIAAEbEUACaGtlkO/gYAmBDZs2EDt2rWjt956i370ox8JtcIeX96+fTs9/vjjQuVzFQofDP7qq6+oYcOGkeWgIhAAAkAACIghAL4XwwmlgIDrCCAhdN3C0A8ISCKAAEESMBQHAkAACFiKAPjeUsOh20BAMQJICBUDCnFAwHYEECDYbkH0HwgAASAghgD4XgwnlAICriOAhNB1C0M/rxH44osv6MQTT6Tf/va3dN111wVYLF++nHr16kWLFi2is88+uwo+2QHCwYMH6Ve/+hUtWbKEtmzZQm3atKHLL7+cxo4dW1E3/GS0c+fO9Oc//5n27dtHw4cPpzvuuINq1qwZlPv+++9pypQpdM8991BZWRkdc8wx9Pvf/56GDh0a/B2fjHrtqlAeCACBmAiA72MCiOpAwGMEkBB6bHyo7gcCixcvpiFDhtBrr71GP/zhD4klbeeccw7NmDEjJwDZCeE333xDf/jDH+gnP/kJHXHEEYEcliA+8MADNGzYsEAGSwgfffTRQC5L8piMSy65hEaPHk0333xzUOb666+nBQsWBO127NiRXnnlFbrsssvomWeeoTPPPBMJoR/uCC2BABDQiAD4XiO4EA0EHEYACaHDxoVqQCBE4De/+Q09//zz1K1bN3rnnXdo5cqVVLt2baGEMFchJu9f//oXzZ8/vyIhXLhwIW3atImKi4uD/8Z2Aq+55hrasWMH7d27N0gm2S7jaaedViHy0ksvpT179lBJSQkSQrgrEAACQEABAuB7BSBCBBDwDAEkhJ4ZHOr6iQBLyDp16hQkbKtWraKTTjopLxC5zpSw5O7++++nTz/9NEjuDhw4ENxAumLFioqEcOPGjUHCF/5Y4snKMHmff/45nXrqqVS3bt1K7TI5bMeSfcaKT0b99E1oDQSAgFoEwPdq8YQ0IOADAkgIfbAydPQegTVr1lDXrl2Jff752GOP0eDBg4UTwnnz5tHFF19MU6dODXb36tWrF5wFZEnc22+/zU0IWRLJzgz26NEjSPpatmxZqe1atWpR69atkRB676UAAAgAARUIgO9VoAgZQMAvBJAQ+mVvaOshAmwXju3Osd06doZw2rRp9N5779FRRx2VE43sHcIrrriC3n//fXrhhRcqyvfr14+2bt1aKSFkn4yWlpZSnTp1gnL33nsvjRs3LvhkdPfu3dS0aVO67777aNSoUTnbxQ6hh84JlYEAEFCKAPheKZwQBgS8QQAJoTemhqK+IsDO8bGzfuwTzsMPP5z69u0b7PKxW0Zz/bITwpkzZ9KECROI7RSyB+tnzZpFf/rTn4L/nblDyC6VYTuPN9xwQ/BpKbtUhv279dZbg2bYf2efnrKdxp49e9LOnTuDC2pYn9gOJBJCXz0UegMBIKAKAfC9KiQhBwj4hQASQr/sDW09Q4AlWf3796cXX3wxSMLYj531Y2cIWaI2ZsyYKohkJ4T79+8PbgNln5pWq1YteE6iQYMG9NRTT1VKCLdv304nn3xy8OwEq3PRRRfRnXfeSeyTUPZjz06wZyjuuusuWrduHTVs2JC6dOkSPIfRu3dvJISe+SbUBQJAQC0C4Hu1eEIaEPAJASSEPlkbugIBIAAEgAAQAAJAAAgAASAABDIQQEIIdwACQAAIAAEgAASAABAAAkAACHiKABJCTw0PtYEAEAACQAAIAAEgAASAABAAAkgI4QNAAAgAASAABIAAEAACQAAIAAFPEUBC6KnhoTYQAAJAAAgAASAABIAAEAACQAAJIXwACAABIAAEgAAQAAJAAAgAASDgKQJICD01PNQGAkAACAABIAAEgAAQAAJAAAggIYQPAAEgAASAABAAAkAACAABIAAEPEUACaGnhofaQAAIAAEgAASAABAAAkAACAABJITwASAABIAAEAACQAAIAAEgAASAgKcIICH01PBQGwgAASAABIAAEAACQAAIAAEggIQQPgAEgAAQAAJAAAgAASAABIAAEPAUASSEnhoeagMBIAAEgAAQAAJAAAgAASAABJAQwgeAABAAAkAACAABIAAEgAAQAAKeIoCE0FPDQ20gAASAABAAAkAACAABIAAEgMD/B5Hm6Loq5jSHAAAAAElFTkSuQmCC\" width=\"900\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train a sigmoid classifier to find optimal W (0.00507323563459043, 0.5901809003819253, -0.8072550818078359) for the boundary.\n",
      "iteration 0 Loss 0.6481638890447898\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/conda/envs/python_programs/lib/python3.8/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-aec3df366268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m train_binary_classifier(\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mD\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/home/repository/git/oonisim/python_programs/nlp/src/network/test_020_binary_classifier.py\u001b[0m in \u001b[0;36mtrain_binary_classifier\u001b[0;34m(N, D, M, X, T, W, log_loss_function, optimizer, num_epochs, test_numerical_gradient, log_level, callback)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;31m# if W.shape[1] == 1 else callback(W=np.average(matmul.W, axis=0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-41800f144b2c>\u001b[0m in \u001b[0;36mdraw_training\u001b[0;34m(X, W, _ax, _fig, colors)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0m_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0m_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python_programs/lib/python3.8/site-packages/matplotlib/backends/backend_webagg_core.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Swap the frames.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python_programs/lib/python3.8/site-packages/matplotlib/backends/backend_webagg_core.py\u001b[0m in \u001b[0;36mrefresh_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrefresh_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweb_sockets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_diff_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdiff\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweb_sockets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python_programs/lib/python3.8/site-packages/matplotlib/backends/backend_webagg_core.py\u001b[0m in \u001b[0;36mget_diff_image\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0;31m# Swap the renderer frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             self._renderer, self._last_renderer = (\n",
      "\u001b[0;32m~/conda/envs/python_programs/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2165\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m             \u001b[0;31m# do what we can to clean up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python_programs/lib/python3.8/site-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \u001b[0m_write_multiple_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m         \u001b[0mImageFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_idat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python_programs/lib/python3.8/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m                     \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m                     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(9,5))\n",
    "for i in range(2):\n",
    "    ax[i].scatter(X[T==0, 0], X[T==0, 1], c='red')\n",
    "    ax[i].scatter(X[T==1, 0], X[T==1, 1], c='green')\n",
    "    ax[i].set_xlabel('x label')\n",
    "    ax[i].set_ylabel('y label')\n",
    "    ax[i].axis('equal')\n",
    "    ax[i].set_xlim(-3, 3)\n",
    "    ax[i].set_ylim(-3, 3)\n",
    "    ax[i].grid()\n",
    "\n",
    "fig.suptitle('Trainig progress plotted here', fontsize=13)\n",
    "ax[0].set_title(\"sigmoid binary classifier\")\n",
    "ax[1].set_title(\"softmax binary classifier\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Train a sigmoid classifier to find optimal W {tuple(V)} for the boundary.\")\n",
    "MAX_TEST_TIMES = 50\n",
    "\n",
    "M = 1\n",
    "W = weights.xavier(M, D+1)    # Xavier initialization for Sigmoid\n",
    "optimizer = SGD(lr=0.3)\n",
    "draw = partial(draw_training, X=X, _fig=fig, _ax=ax[0])\n",
    "ax[0].set_xlim(-3, 3)\n",
    "ax[0].set_ylim(-3, 3)\n",
    "\n",
    "train_binary_classifier(\n",
    "    N=N,\n",
    "    D=D,\n",
    "    M=M,\n",
    "    X=X,\n",
    "    T=T,\n",
    "    W=W,\n",
    "    log_loss_function=sigmoid_cross_entropy_log_loss,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=MAX_TEST_TIMES,\n",
    "    test_numerical_gradient=False,\n",
    "    callback=draw\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun \\\n",
    "    -T train_sigmoid_binary_classifier.log \\\n",
    "    -f train_binary_classifier \\\n",
    "    train_binary_classifier(\\\n",
    "        N=N,D=D,M=M,X=X,T=T,W=W,\\\n",
    "        log_loss_function=sigmoid_cross_entropy_log_loss, \\\n",
    "        optimizer=optimizer, \\\n",
    "        num_epochs=MAX_TEST_TIMES, \\\n",
    "        test_numerical_gradient=False \\\n",
    "    )\n",
    "\n",
    "print(open('train_sigmoid_classifier.log', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax classifier training\n",
    "Two class classification with softmax activation. \n",
    "Plots in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train a softmax classifier to find optimal W {tuple(V)} for the boundary.\")\n",
    "MAX_TEST_TIMES = 100\n",
    "\n",
    "M = 2                      \n",
    "W = weights.he(M, D+1)\n",
    "optimizer = SGD(lr=0.3)\n",
    "draw = partial(draw_training, X=X, _fig=fig, _ax=ax[1])\n",
    "ax[1].set_xlim(-3, 3)\n",
    "ax[1].set_ylim(-3, 3)\n",
    "\n",
    "train_binary_classifier(\n",
    "    N=N,\n",
    "    D=D,\n",
    "    M=M,\n",
    "    X=X,\n",
    "    T=T,\n",
    "    W=W,\n",
    "    log_loss_function=softmax_cross_entropy_log_loss,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=MAX_TEST_TIMES,\n",
    "    test_numerical_gradient=False, \n",
    "    callback=draw\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun \\\n",
    "    -T train_softmax_binary_classifier.log \\\n",
    "    -f train_binary_classifier \\\n",
    "    train_binary_classifier(\\\n",
    "        N=N,D=D,M=M,X=X,T=T,W=W,\\\n",
    "        log_loss_function=softmax_cross_entropy_log_loss, \\\n",
    "        optimizer=optimizer, \\\n",
    "        num_epochs=MAX_TEST_TIMES, \\\n",
    "        test_numerical_gradient=False \\\n",
    "    )\n",
    "\n",
    "print(open('train_softmax_classifier.log', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Classification\n",
    "\n",
    "Use Matmul and CrossEntropyLogLoss layers to classify M categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from common import (\n",
    "    prediction_grid,\n",
    ")\n",
    "from data import (\n",
    "    linear_separable_sectors,\n",
    ")\n",
    "from network import (\n",
    "    train_matmul_relu_classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearly separable multiple categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data X and Label T\n",
    "Training data to linearly classify into M categories and labels T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train a categorical classifier\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train a categorical classifier\")\n",
    "N = 1000\n",
    "D = 2      # Dimension\n",
    "M = 3\n",
    "\n",
    "rotation = np.radians(35)\n",
    "# x0 = X[::,0] is the bias 1\n",
    "X, T, B = linear_separable_sectors(n=N, d=D, m=M, r=2, rotation=rotation)\n",
    "X_backup = copy.deepcopy(X)\n",
    "T_backup = copy.deepcopy(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot X, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "/* global mpl */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function () {\n",
       "    if (typeof WebSocket !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof MozWebSocket !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert(\n",
       "            'Your browser does not have WebSocket support. ' +\n",
       "                'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "                'Firefox 4 and 5 are also supported but you ' +\n",
       "                'have to enable WebSockets in about:config.'\n",
       "        );\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = this.ws.binaryType !== undefined;\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById('mpl-warnings');\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent =\n",
       "                'This browser does not support binary websocket messages. ' +\n",
       "                'Performance may be slow.';\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = document.createElement('div');\n",
       "    this.root.setAttribute('style', 'display: inline-block');\n",
       "    this._root_extra_style(this.root);\n",
       "\n",
       "    parent_element.appendChild(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen = function () {\n",
       "        fig.send_message('supports_binary', { value: fig.supports_binary });\n",
       "        fig.send_message('send_image_mode', {});\n",
       "        if (fig.ratio !== 1) {\n",
       "            fig.send_message('set_dpi_ratio', { dpi_ratio: fig.ratio });\n",
       "        }\n",
       "        fig.send_message('refresh', {});\n",
       "    };\n",
       "\n",
       "    this.imageObj.onload = function () {\n",
       "        if (fig.image_mode === 'full') {\n",
       "            // Full images could contain transparency (where diff images\n",
       "            // almost always do), so we need to clear the canvas so that\n",
       "            // there is no ghosting.\n",
       "            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "        }\n",
       "        fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "    };\n",
       "\n",
       "    this.imageObj.onunload = function () {\n",
       "        fig.ws.close();\n",
       "    };\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_header = function () {\n",
       "    var titlebar = document.createElement('div');\n",
       "    titlebar.classList =\n",
       "        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n",
       "    var titletext = document.createElement('div');\n",
       "    titletext.classList = 'ui-dialog-title';\n",
       "    titletext.setAttribute(\n",
       "        'style',\n",
       "        'width: 100%; text-align: center; padding: 3px;'\n",
       "    );\n",
       "    titlebar.appendChild(titletext);\n",
       "    this.root.appendChild(titlebar);\n",
       "    this.header = titletext;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = (this.canvas_div = document.createElement('div'));\n",
       "    canvas_div.setAttribute(\n",
       "        'style',\n",
       "        'border: 1px solid #ddd;' +\n",
       "            'box-sizing: content-box;' +\n",
       "            'clear: both;' +\n",
       "            'min-height: 1px;' +\n",
       "            'min-width: 1px;' +\n",
       "            'outline: 0;' +\n",
       "            'overflow: hidden;' +\n",
       "            'position: relative;' +\n",
       "            'resize: both;'\n",
       "    );\n",
       "\n",
       "    function on_keyboard_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.key_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    canvas_div.addEventListener(\n",
       "        'keydown',\n",
       "        on_keyboard_event_closure('key_press')\n",
       "    );\n",
       "    canvas_div.addEventListener(\n",
       "        'keyup',\n",
       "        on_keyboard_event_closure('key_release')\n",
       "    );\n",
       "\n",
       "    this._canvas_extra_style(canvas_div);\n",
       "    this.root.appendChild(canvas_div);\n",
       "\n",
       "    var canvas = (this.canvas = document.createElement('canvas'));\n",
       "    canvas.classList.add('mpl-canvas');\n",
       "    canvas.setAttribute('style', 'box-sizing: content-box;');\n",
       "\n",
       "    this.context = canvas.getContext('2d');\n",
       "\n",
       "    var backingStore =\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        this.context.webkitBackingStorePixelRatio ||\n",
       "        this.context.mozBackingStorePixelRatio ||\n",
       "        this.context.msBackingStorePixelRatio ||\n",
       "        this.context.oBackingStorePixelRatio ||\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        1;\n",
       "\n",
       "    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n",
       "        'canvas'\n",
       "    ));\n",
       "    rubberband_canvas.setAttribute(\n",
       "        'style',\n",
       "        'box-sizing: content-box; position: absolute; left: 0; top: 0; z-index: 1;'\n",
       "    );\n",
       "\n",
       "    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n",
       "    if (this.ResizeObserver === undefined) {\n",
       "        if (window.ResizeObserver !== undefined) {\n",
       "            this.ResizeObserver = window.ResizeObserver;\n",
       "        } else {\n",
       "            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n",
       "            this.ResizeObserver = obs.ResizeObserver;\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n",
       "        var nentries = entries.length;\n",
       "        for (var i = 0; i < nentries; i++) {\n",
       "            var entry = entries[i];\n",
       "            var width, height;\n",
       "            if (entry.contentBoxSize) {\n",
       "                if (entry.contentBoxSize instanceof Array) {\n",
       "                    // Chrome 84 implements new version of spec.\n",
       "                    width = entry.contentBoxSize[0].inlineSize;\n",
       "                    height = entry.contentBoxSize[0].blockSize;\n",
       "                } else {\n",
       "                    // Firefox implements old version of spec.\n",
       "                    width = entry.contentBoxSize.inlineSize;\n",
       "                    height = entry.contentBoxSize.blockSize;\n",
       "                }\n",
       "            } else {\n",
       "                // Chrome <84 implements even older version of spec.\n",
       "                width = entry.contentRect.width;\n",
       "                height = entry.contentRect.height;\n",
       "            }\n",
       "\n",
       "            // Keep the size of the canvas and rubber band canvas in sync with\n",
       "            // the canvas container.\n",
       "            if (entry.devicePixelContentBoxSize) {\n",
       "                // Chrome 84 implements new version of spec.\n",
       "                canvas.setAttribute(\n",
       "                    'width',\n",
       "                    entry.devicePixelContentBoxSize[0].inlineSize\n",
       "                );\n",
       "                canvas.setAttribute(\n",
       "                    'height',\n",
       "                    entry.devicePixelContentBoxSize[0].blockSize\n",
       "                );\n",
       "            } else {\n",
       "                canvas.setAttribute('width', width * fig.ratio);\n",
       "                canvas.setAttribute('height', height * fig.ratio);\n",
       "            }\n",
       "            canvas.setAttribute(\n",
       "                'style',\n",
       "                'width: ' + width + 'px; height: ' + height + 'px;'\n",
       "            );\n",
       "\n",
       "            rubberband_canvas.setAttribute('width', width);\n",
       "            rubberband_canvas.setAttribute('height', height);\n",
       "\n",
       "            // And update the size in Python. We ignore the initial 0/0 size\n",
       "            // that occurs as the element is placed into the DOM, which should\n",
       "            // otherwise not happen due to the minimum size styling.\n",
       "            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n",
       "                fig.request_resize(width, height);\n",
       "            }\n",
       "        }\n",
       "    });\n",
       "    this.resizeObserverInstance.observe(canvas_div);\n",
       "\n",
       "    function on_mouse_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.mouse_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousedown',\n",
       "        on_mouse_event_closure('button_press')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseup',\n",
       "        on_mouse_event_closure('button_release')\n",
       "    );\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousemove',\n",
       "        on_mouse_event_closure('motion_notify')\n",
       "    );\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseenter',\n",
       "        on_mouse_event_closure('figure_enter')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseleave',\n",
       "        on_mouse_event_closure('figure_leave')\n",
       "    );\n",
       "\n",
       "    canvas_div.addEventListener('wheel', function (event) {\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        on_mouse_event_closure('scroll')(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.appendChild(canvas);\n",
       "    canvas_div.appendChild(rubberband_canvas);\n",
       "\n",
       "    this.rubberband_context = rubberband_canvas.getContext('2d');\n",
       "    this.rubberband_context.strokeStyle = '#000000';\n",
       "\n",
       "    this._resize_canvas = function (width, height, forward) {\n",
       "        if (forward) {\n",
       "            canvas_div.style.width = width + 'px';\n",
       "            canvas_div.style.height = height + 'px';\n",
       "        }\n",
       "    };\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    this.rubberband_canvas.addEventListener('contextmenu', function (_e) {\n",
       "        event.preventDefault();\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus() {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'mpl-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'mpl-button-group';\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'mpl-button-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        var button = (fig.buttons[name] = document.createElement('button'));\n",
       "        button.classList = 'mpl-widget';\n",
       "        button.setAttribute('role', 'button');\n",
       "        button.setAttribute('aria-disabled', 'false');\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "\n",
       "        var icon_img = document.createElement('img');\n",
       "        icon_img.src = '_images/' + image + '.png';\n",
       "        icon_img.srcset = '_images/' + image + '_large.png 2x';\n",
       "        icon_img.alt = tooltip;\n",
       "        button.appendChild(icon_img);\n",
       "\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    var fmt_picker = document.createElement('select');\n",
       "    fmt_picker.classList = 'mpl-widget';\n",
       "    toolbar.appendChild(fmt_picker);\n",
       "    this.format_dropdown = fmt_picker;\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = document.createElement('option');\n",
       "        option.selected = fmt === mpl.default_extension;\n",
       "        option.innerHTML = fmt;\n",
       "        fmt_picker.appendChild(option);\n",
       "    }\n",
       "\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', { width: x_pixels, height: y_pixels });\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_message = function (type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function () {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function (fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1], msg['forward']);\n",
       "        fig.send_message('refresh', {});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function (fig, msg) {\n",
       "    var x0 = msg['x0'] / fig.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n",
       "    var x1 = msg['x1'] / fig.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0,\n",
       "        0,\n",
       "        fig.canvas.width / fig.ratio,\n",
       "        fig.canvas.height / fig.ratio\n",
       "    );\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function (fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function (fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch (cursor) {\n",
       "        case 0:\n",
       "            cursor = 'pointer';\n",
       "            break;\n",
       "        case 1:\n",
       "            cursor = 'default';\n",
       "            break;\n",
       "        case 2:\n",
       "            cursor = 'crosshair';\n",
       "            break;\n",
       "        case 3:\n",
       "            cursor = 'move';\n",
       "            break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_message = function (fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function (fig, _msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function (fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n",
       "    for (var key in msg) {\n",
       "        if (!(key in fig.buttons)) {\n",
       "            continue;\n",
       "        }\n",
       "        fig.buttons[key].disabled = !msg[key];\n",
       "        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n",
       "    if (msg['mode'] === 'PAN') {\n",
       "        fig.buttons['Pan'].classList.add('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    } else if (msg['mode'] === 'ZOOM') {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.add('active');\n",
       "    } else {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message('ack', {});\n",
       "};\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function (fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = 'image/png';\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src\n",
       "                );\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data\n",
       "            );\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        } else if (\n",
       "            typeof evt.data === 'string' &&\n",
       "            evt.data.slice(0, 21) === 'data:image/png;base64'\n",
       "        ) {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig['handle_' + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\n",
       "                \"No handler for the '\" + msg_type + \"' message type: \",\n",
       "                msg\n",
       "            );\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\n",
       "                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n",
       "                    e,\n",
       "                    e.stack,\n",
       "                    msg\n",
       "                );\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "};\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function (e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e) {\n",
       "        e = window.event;\n",
       "    }\n",
       "    if (e.target) {\n",
       "        targ = e.target;\n",
       "    } else if (e.srcElement) {\n",
       "        targ = e.srcElement;\n",
       "    }\n",
       "    if (targ.nodeType === 3) {\n",
       "        // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "    }\n",
       "\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    var boundingRect = targ.getBoundingClientRect();\n",
       "    var x = e.pageX - (boundingRect.left + document.body.scrollLeft);\n",
       "    var y = e.pageY - (boundingRect.top + document.body.scrollTop);\n",
       "\n",
       "    return { x: x, y: y };\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys(original) {\n",
       "    return Object.keys(original).reduce(function (obj, key) {\n",
       "        if (typeof original[key] !== 'object') {\n",
       "            obj[key] = original[key];\n",
       "        }\n",
       "        return obj;\n",
       "    }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function (event, name) {\n",
       "    var canvas_pos = mpl.findpos(event);\n",
       "\n",
       "    if (name === 'button_press') {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * this.ratio;\n",
       "    var y = canvas_pos.y * this.ratio;\n",
       "\n",
       "    this.send_message(name, {\n",
       "        x: x,\n",
       "        y: y,\n",
       "        button: event.button,\n",
       "        step: event.step,\n",
       "        guiEvent: simpleKeys(event),\n",
       "    });\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (_event, _name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.key_event = function (event, name) {\n",
       "    // Prevent repeat events\n",
       "    if (name === 'key_press') {\n",
       "        if (event.which === this._key) {\n",
       "            return;\n",
       "        } else {\n",
       "            this._key = event.which;\n",
       "        }\n",
       "    }\n",
       "    if (name === 'key_release') {\n",
       "        this._key = null;\n",
       "    }\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which !== 17) {\n",
       "        value += 'ctrl+';\n",
       "    }\n",
       "    if (event.altKey && event.which !== 18) {\n",
       "        value += 'alt+';\n",
       "    }\n",
       "    if (event.shiftKey && event.which !== 16) {\n",
       "        value += 'shift+';\n",
       "    }\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function (name) {\n",
       "    if (name === 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message('toolbar_button', { name: name });\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "\n",
       "///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n",
       "// prettier-ignore\n",
       "var _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";/* global mpl */\n",
       "\n",
       "var comm_websocket_adapter = function (comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function () {\n",
       "        comm.close();\n",
       "    };\n",
       "    ws.send = function (m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function (msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data']);\n",
       "    });\n",
       "    return ws;\n",
       "};\n",
       "\n",
       "mpl.mpl_figure_comm = function (comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = document.getElementById(id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm);\n",
       "\n",
       "    function ondownload(figure, _format) {\n",
       "        window.open(figure.canvas.toDataURL());\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element;\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error('Failed to find cell for figure', id, fig);\n",
       "        return;\n",
       "    }\n",
       "    fig.cell_info[0].output_area.element.on(\n",
       "        'cleared',\n",
       "        { fig: fig },\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function (fig, msg) {\n",
       "    var width = fig.canvas.width / fig.ratio;\n",
       "    fig.cell_info[0].output_area.element.off(\n",
       "        'cleared',\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable();\n",
       "    fig.parent_element.innerHTML =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "    fig.close_ws(fig, msg);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.close_ws = function (fig, msg) {\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function (_remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width / this.ratio;\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message('ack', {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () {\n",
       "        fig.push_to_output();\n",
       "    }, 1000);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'btn-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'btn-group';\n",
       "    var button;\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'btn-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        button = fig.buttons[name] = document.createElement('button');\n",
       "        button.classList = 'btn btn-default';\n",
       "        button.href = '#';\n",
       "        button.title = name;\n",
       "        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message pull-right';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = document.createElement('div');\n",
       "    buttongrp.classList = 'btn-group inline pull-right';\n",
       "    button = document.createElement('button');\n",
       "    button.classList = 'btn btn-mini btn-primary';\n",
       "    button.href = '#';\n",
       "    button.title = 'Stop Interaction';\n",
       "    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n",
       "    button.addEventListener('click', function (_evt) {\n",
       "        fig.handle_close(fig, {});\n",
       "    });\n",
       "    button.addEventListener(\n",
       "        'mouseover',\n",
       "        on_mouseover_closure('Stop Interaction')\n",
       "    );\n",
       "    buttongrp.appendChild(button);\n",
       "    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n",
       "    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._remove_fig_handler = function (event) {\n",
       "    var fig = event.data.fig;\n",
       "    if (event.target !== this) {\n",
       "        // Ignore bubbled events from children.\n",
       "        return;\n",
       "    }\n",
       "    fig.close_ws(fig, {});\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (el) {\n",
       "    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (el) {\n",
       "    // this is important to make the div 'focusable\n",
       "    el.setAttribute('tabindex', 0);\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    } else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (event, _name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager) {\n",
       "        manager = IPython.keyboard_manager;\n",
       "    }\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which === 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "};\n",
       "\n",
       "mpl.find_output_cell = function (html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i = 0; i < ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code') {\n",
       "            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] === html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel !== null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target(\n",
       "        'matplotlib',\n",
       "        mpl.mpl_figure_comm\n",
       "    );\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"500\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Radius of a circle within which to place plots.\n",
    "radius = 2   \n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Plot area\n",
    "# --------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "for i in range(2):\n",
    "    ax.set_xlabel('x label')\n",
    "    ax.set_ylabel('y label')\n",
    "    ax.axis('equal')\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-3, 3)\n",
    "    ax.grid()\n",
    "\n",
    "ax.set_title(f\"Categorical data of {M} classes\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Cirle within which to place random plots.\n",
    "# --------------------------------------------------------------------------------\n",
    "r = np.linspace(0, 2 * np.pi, 100)\n",
    "ax.plot(radius * np.cos(r), radius * np.sin(r), \"b--\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Classify plots (x, y) if inside the coverage sector\n",
    "# labels to classify outside/0/red or inside/1/green.\n",
    "# --------------------------------------------------------------------------------\n",
    "Y = COLOR_LABELS[\n",
    "    T\n",
    "]\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Plot color-classified points.\n",
    "# --------------------------------------------------------------------------------\n",
    "ax.scatter(X[::,0], X[::,1], marker='o', color=Y)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Plot sector bases\n",
    "# --------------------------------------------------------------------------------\n",
    "for i in range(B.shape[0]):\n",
    "    ax.plot((0, radius * B[i, 0]), (0, radius * B[i, 1]), COLOR_LABELS[i])\n",
    "\n",
    "# ax.legend()\n",
    "fig.suptitle('Categorical classifiation data', fontsize=16)\n",
    "\n",
    "plt.draw()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on linearly separable multiple categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected dL/dW \n",
      "[[-0.10218159  0.05796373 -0.11349377]\n",
      " [ 0.          0.          0.        ]\n",
      " [-0.00247219 -0.16111212  0.08100672]]\n",
      "Diff\n",
      "[[ 0.0100598  -0.0043972   0.01018124]\n",
      " [ 0.          0.          0.        ]\n",
      " [-0.00133438  0.0076613  -0.01015743]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 Loss 1.0590592004899888\n",
      "iteration 10 Loss 0.8063486918384951\n",
      "iteration 20 Loss 0.7419612932775844\n",
      "iteration 30 Loss 0.7062301653015103\n",
      "iteration 40 Loss 0.6824379025180858\n",
      "iteration 50 Loss 0.6650577221055576\n",
      "iteration 60 Loss 0.651449452085238\n",
      "iteration 70 Loss 0.6403690596592583\n",
      "iteration 80 Loss 0.6311100738332798\n",
      "iteration 90 Loss 0.6231982408706376\n",
      "iteration 100 Loss 0.6162803025043344\n",
      "iteration 110 Loss 0.6102628629711598\n",
      "iteration 120 Loss 0.6049354352194768\n",
      "iteration 130 Loss 0.6002145439428405\n",
      "iteration 140 Loss 0.5959565034555376\n",
      "iteration 150 Loss 0.5920236081040708\n",
      "iteration 160 Loss 0.5884078954193742\n",
      "iteration 170 Loss 0.5850786534666522\n",
      "iteration 180 Loss 0.5819988122352552\n",
      "iteration 190 Loss 0.5790971858768537\n",
      "iteration 200 Loss 0.5763909871321818\n",
      "iteration 210 Loss 0.5738686088755812\n",
      "iteration 220 Loss 0.5715239631943041\n",
      "iteration 230 Loss 0.5693185508207114\n",
      "iteration 240 Loss 0.5672190169344993\n",
      "iteration 250 Loss 0.5652273158259474\n",
      "iteration 260 Loss 0.563327852882352\n",
      "iteration 270 Loss 0.5615244267179287\n",
      "iteration 280 Loss 0.5597913025973975\n",
      "iteration 290 Loss 0.5581316322366981\n",
      "iteration 300 Loss 0.5565468868331577\n",
      "iteration 310 Loss 0.5550378570805544\n",
      "iteration 320 Loss 0.5535977933625911\n",
      "iteration 330 Loss 0.5522123785979436\n",
      "iteration 340 Loss 0.5508780628293604\n",
      "iteration 350 Loss 0.5495917005586193\n",
      "iteration 360 Loss 0.5483549874169354\n",
      "iteration 370 Loss 0.5471587091701704\n",
      "iteration 380 Loss 0.5460007374053336\n",
      "iteration 390 Loss 0.5448833899583009\n",
      "iteration 400 Loss 0.5437994568393454\n",
      "iteration 410 Loss 0.5427613481507938\n",
      "iteration 420 Loss 0.541754243795512\n",
      "iteration 430 Loss 0.5407750811349444\n",
      "iteration 440 Loss 0.5398316591475918\n",
      "iteration 450 Loss 0.5389137375714015\n",
      "iteration 460 Loss 0.5380169954673618\n",
      "iteration 470 Loss 0.537139346628697\n",
      "iteration 480 Loss 0.5362904361084384\n",
      "iteration 490 Loss 0.5354673981142021\n"
     ]
    }
   ],
   "source": [
    "MAX_TEST_TIMES = 500\n",
    "W = weights.he(M, D+1)\n",
    "W_backup = copy.deepcopy(W)\n",
    "optimizer = SGD(lr=0.5)\n",
    "\n",
    "# W = train_classifier(\n",
    "W = train_matmul_relu_classifier(\n",
    "    N=N,\n",
    "    D=D,\n",
    "    M=M,\n",
    "    X=X,\n",
    "    T=T,\n",
    "    W=W,\n",
    "    log_loss_function=softmax_cross_entropy_log_loss,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=MAX_TEST_TIMES,\n",
    "    test_numerical_gradient=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predictions\n",
    "\n",
    "Run preditions against the grid coordinates (x1, x2).\n",
    "```\n",
    "x1: X[:, 1].min() - 1 <= x1 <=  X[:, 1].max() + 1\n",
    "x2: X[:, 2].min() - 1 <= x2 <=  X[:, 2].max() + 1\n",
    "grid = np.meshgrid(x1, x2)\n",
    "\n",
    "# np.argmax(scores) selets the highest score for each data point in X.\n",
    "# e.g score[i] = [0.2, 8.2, 0.3], then np.argmax(scores[i]) selects index 1 as the prediction. \n",
    "# Then cluster of predition/label == 1 will form a contour.\n",
    "sores = grid @ W.T\n",
    "predictions = p.argmax(score, axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "/* global mpl */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function () {\n",
       "    if (typeof WebSocket !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof MozWebSocket !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert(\n",
       "            'Your browser does not have WebSocket support. ' +\n",
       "                'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "                'Firefox 4 and 5 are also supported but you ' +\n",
       "                'have to enable WebSockets in about:config.'\n",
       "        );\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = this.ws.binaryType !== undefined;\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById('mpl-warnings');\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent =\n",
       "                'This browser does not support binary websocket messages. ' +\n",
       "                'Performance may be slow.';\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = document.createElement('div');\n",
       "    this.root.setAttribute('style', 'display: inline-block');\n",
       "    this._root_extra_style(this.root);\n",
       "\n",
       "    parent_element.appendChild(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen = function () {\n",
       "        fig.send_message('supports_binary', { value: fig.supports_binary });\n",
       "        fig.send_message('send_image_mode', {});\n",
       "        if (fig.ratio !== 1) {\n",
       "            fig.send_message('set_dpi_ratio', { dpi_ratio: fig.ratio });\n",
       "        }\n",
       "        fig.send_message('refresh', {});\n",
       "    };\n",
       "\n",
       "    this.imageObj.onload = function () {\n",
       "        if (fig.image_mode === 'full') {\n",
       "            // Full images could contain transparency (where diff images\n",
       "            // almost always do), so we need to clear the canvas so that\n",
       "            // there is no ghosting.\n",
       "            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "        }\n",
       "        fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "    };\n",
       "\n",
       "    this.imageObj.onunload = function () {\n",
       "        fig.ws.close();\n",
       "    };\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_header = function () {\n",
       "    var titlebar = document.createElement('div');\n",
       "    titlebar.classList =\n",
       "        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n",
       "    var titletext = document.createElement('div');\n",
       "    titletext.classList = 'ui-dialog-title';\n",
       "    titletext.setAttribute(\n",
       "        'style',\n",
       "        'width: 100%; text-align: center; padding: 3px;'\n",
       "    );\n",
       "    titlebar.appendChild(titletext);\n",
       "    this.root.appendChild(titlebar);\n",
       "    this.header = titletext;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = (this.canvas_div = document.createElement('div'));\n",
       "    canvas_div.setAttribute(\n",
       "        'style',\n",
       "        'border: 1px solid #ddd;' +\n",
       "            'box-sizing: content-box;' +\n",
       "            'clear: both;' +\n",
       "            'min-height: 1px;' +\n",
       "            'min-width: 1px;' +\n",
       "            'outline: 0;' +\n",
       "            'overflow: hidden;' +\n",
       "            'position: relative;' +\n",
       "            'resize: both;'\n",
       "    );\n",
       "\n",
       "    function on_keyboard_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.key_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    canvas_div.addEventListener(\n",
       "        'keydown',\n",
       "        on_keyboard_event_closure('key_press')\n",
       "    );\n",
       "    canvas_div.addEventListener(\n",
       "        'keyup',\n",
       "        on_keyboard_event_closure('key_release')\n",
       "    );\n",
       "\n",
       "    this._canvas_extra_style(canvas_div);\n",
       "    this.root.appendChild(canvas_div);\n",
       "\n",
       "    var canvas = (this.canvas = document.createElement('canvas'));\n",
       "    canvas.classList.add('mpl-canvas');\n",
       "    canvas.setAttribute('style', 'box-sizing: content-box;');\n",
       "\n",
       "    this.context = canvas.getContext('2d');\n",
       "\n",
       "    var backingStore =\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        this.context.webkitBackingStorePixelRatio ||\n",
       "        this.context.mozBackingStorePixelRatio ||\n",
       "        this.context.msBackingStorePixelRatio ||\n",
       "        this.context.oBackingStorePixelRatio ||\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        1;\n",
       "\n",
       "    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n",
       "        'canvas'\n",
       "    ));\n",
       "    rubberband_canvas.setAttribute(\n",
       "        'style',\n",
       "        'box-sizing: content-box; position: absolute; left: 0; top: 0; z-index: 1;'\n",
       "    );\n",
       "\n",
       "    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n",
       "    if (this.ResizeObserver === undefined) {\n",
       "        if (window.ResizeObserver !== undefined) {\n",
       "            this.ResizeObserver = window.ResizeObserver;\n",
       "        } else {\n",
       "            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n",
       "            this.ResizeObserver = obs.ResizeObserver;\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n",
       "        var nentries = entries.length;\n",
       "        for (var i = 0; i < nentries; i++) {\n",
       "            var entry = entries[i];\n",
       "            var width, height;\n",
       "            if (entry.contentBoxSize) {\n",
       "                if (entry.contentBoxSize instanceof Array) {\n",
       "                    // Chrome 84 implements new version of spec.\n",
       "                    width = entry.contentBoxSize[0].inlineSize;\n",
       "                    height = entry.contentBoxSize[0].blockSize;\n",
       "                } else {\n",
       "                    // Firefox implements old version of spec.\n",
       "                    width = entry.contentBoxSize.inlineSize;\n",
       "                    height = entry.contentBoxSize.blockSize;\n",
       "                }\n",
       "            } else {\n",
       "                // Chrome <84 implements even older version of spec.\n",
       "                width = entry.contentRect.width;\n",
       "                height = entry.contentRect.height;\n",
       "            }\n",
       "\n",
       "            // Keep the size of the canvas and rubber band canvas in sync with\n",
       "            // the canvas container.\n",
       "            if (entry.devicePixelContentBoxSize) {\n",
       "                // Chrome 84 implements new version of spec.\n",
       "                canvas.setAttribute(\n",
       "                    'width',\n",
       "                    entry.devicePixelContentBoxSize[0].inlineSize\n",
       "                );\n",
       "                canvas.setAttribute(\n",
       "                    'height',\n",
       "                    entry.devicePixelContentBoxSize[0].blockSize\n",
       "                );\n",
       "            } else {\n",
       "                canvas.setAttribute('width', width * fig.ratio);\n",
       "                canvas.setAttribute('height', height * fig.ratio);\n",
       "            }\n",
       "            canvas.setAttribute(\n",
       "                'style',\n",
       "                'width: ' + width + 'px; height: ' + height + 'px;'\n",
       "            );\n",
       "\n",
       "            rubberband_canvas.setAttribute('width', width);\n",
       "            rubberband_canvas.setAttribute('height', height);\n",
       "\n",
       "            // And update the size in Python. We ignore the initial 0/0 size\n",
       "            // that occurs as the element is placed into the DOM, which should\n",
       "            // otherwise not happen due to the minimum size styling.\n",
       "            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n",
       "                fig.request_resize(width, height);\n",
       "            }\n",
       "        }\n",
       "    });\n",
       "    this.resizeObserverInstance.observe(canvas_div);\n",
       "\n",
       "    function on_mouse_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.mouse_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousedown',\n",
       "        on_mouse_event_closure('button_press')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseup',\n",
       "        on_mouse_event_closure('button_release')\n",
       "    );\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousemove',\n",
       "        on_mouse_event_closure('motion_notify')\n",
       "    );\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseenter',\n",
       "        on_mouse_event_closure('figure_enter')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseleave',\n",
       "        on_mouse_event_closure('figure_leave')\n",
       "    );\n",
       "\n",
       "    canvas_div.addEventListener('wheel', function (event) {\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        on_mouse_event_closure('scroll')(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.appendChild(canvas);\n",
       "    canvas_div.appendChild(rubberband_canvas);\n",
       "\n",
       "    this.rubberband_context = rubberband_canvas.getContext('2d');\n",
       "    this.rubberband_context.strokeStyle = '#000000';\n",
       "\n",
       "    this._resize_canvas = function (width, height, forward) {\n",
       "        if (forward) {\n",
       "            canvas_div.style.width = width + 'px';\n",
       "            canvas_div.style.height = height + 'px';\n",
       "        }\n",
       "    };\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    this.rubberband_canvas.addEventListener('contextmenu', function (_e) {\n",
       "        event.preventDefault();\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus() {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'mpl-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'mpl-button-group';\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'mpl-button-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        var button = (fig.buttons[name] = document.createElement('button'));\n",
       "        button.classList = 'mpl-widget';\n",
       "        button.setAttribute('role', 'button');\n",
       "        button.setAttribute('aria-disabled', 'false');\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "\n",
       "        var icon_img = document.createElement('img');\n",
       "        icon_img.src = '_images/' + image + '.png';\n",
       "        icon_img.srcset = '_images/' + image + '_large.png 2x';\n",
       "        icon_img.alt = tooltip;\n",
       "        button.appendChild(icon_img);\n",
       "\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    var fmt_picker = document.createElement('select');\n",
       "    fmt_picker.classList = 'mpl-widget';\n",
       "    toolbar.appendChild(fmt_picker);\n",
       "    this.format_dropdown = fmt_picker;\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = document.createElement('option');\n",
       "        option.selected = fmt === mpl.default_extension;\n",
       "        option.innerHTML = fmt;\n",
       "        fmt_picker.appendChild(option);\n",
       "    }\n",
       "\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', { width: x_pixels, height: y_pixels });\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_message = function (type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function () {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function (fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1], msg['forward']);\n",
       "        fig.send_message('refresh', {});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function (fig, msg) {\n",
       "    var x0 = msg['x0'] / fig.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n",
       "    var x1 = msg['x1'] / fig.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0,\n",
       "        0,\n",
       "        fig.canvas.width / fig.ratio,\n",
       "        fig.canvas.height / fig.ratio\n",
       "    );\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function (fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function (fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch (cursor) {\n",
       "        case 0:\n",
       "            cursor = 'pointer';\n",
       "            break;\n",
       "        case 1:\n",
       "            cursor = 'default';\n",
       "            break;\n",
       "        case 2:\n",
       "            cursor = 'crosshair';\n",
       "            break;\n",
       "        case 3:\n",
       "            cursor = 'move';\n",
       "            break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_message = function (fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function (fig, _msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function (fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n",
       "    for (var key in msg) {\n",
       "        if (!(key in fig.buttons)) {\n",
       "            continue;\n",
       "        }\n",
       "        fig.buttons[key].disabled = !msg[key];\n",
       "        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n",
       "    if (msg['mode'] === 'PAN') {\n",
       "        fig.buttons['Pan'].classList.add('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    } else if (msg['mode'] === 'ZOOM') {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.add('active');\n",
       "    } else {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message('ack', {});\n",
       "};\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function (fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = 'image/png';\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src\n",
       "                );\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data\n",
       "            );\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        } else if (\n",
       "            typeof evt.data === 'string' &&\n",
       "            evt.data.slice(0, 21) === 'data:image/png;base64'\n",
       "        ) {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig['handle_' + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\n",
       "                \"No handler for the '\" + msg_type + \"' message type: \",\n",
       "                msg\n",
       "            );\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\n",
       "                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n",
       "                    e,\n",
       "                    e.stack,\n",
       "                    msg\n",
       "                );\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "};\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function (e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e) {\n",
       "        e = window.event;\n",
       "    }\n",
       "    if (e.target) {\n",
       "        targ = e.target;\n",
       "    } else if (e.srcElement) {\n",
       "        targ = e.srcElement;\n",
       "    }\n",
       "    if (targ.nodeType === 3) {\n",
       "        // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "    }\n",
       "\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    var boundingRect = targ.getBoundingClientRect();\n",
       "    var x = e.pageX - (boundingRect.left + document.body.scrollLeft);\n",
       "    var y = e.pageY - (boundingRect.top + document.body.scrollTop);\n",
       "\n",
       "    return { x: x, y: y };\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys(original) {\n",
       "    return Object.keys(original).reduce(function (obj, key) {\n",
       "        if (typeof original[key] !== 'object') {\n",
       "            obj[key] = original[key];\n",
       "        }\n",
       "        return obj;\n",
       "    }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function (event, name) {\n",
       "    var canvas_pos = mpl.findpos(event);\n",
       "\n",
       "    if (name === 'button_press') {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * this.ratio;\n",
       "    var y = canvas_pos.y * this.ratio;\n",
       "\n",
       "    this.send_message(name, {\n",
       "        x: x,\n",
       "        y: y,\n",
       "        button: event.button,\n",
       "        step: event.step,\n",
       "        guiEvent: simpleKeys(event),\n",
       "    });\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (_event, _name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.key_event = function (event, name) {\n",
       "    // Prevent repeat events\n",
       "    if (name === 'key_press') {\n",
       "        if (event.which === this._key) {\n",
       "            return;\n",
       "        } else {\n",
       "            this._key = event.which;\n",
       "        }\n",
       "    }\n",
       "    if (name === 'key_release') {\n",
       "        this._key = null;\n",
       "    }\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which !== 17) {\n",
       "        value += 'ctrl+';\n",
       "    }\n",
       "    if (event.altKey && event.which !== 18) {\n",
       "        value += 'alt+';\n",
       "    }\n",
       "    if (event.shiftKey && event.which !== 16) {\n",
       "        value += 'shift+';\n",
       "    }\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function (name) {\n",
       "    if (name === 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message('toolbar_button', { name: name });\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "\n",
       "///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n",
       "// prettier-ignore\n",
       "var _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";/* global mpl */\n",
       "\n",
       "var comm_websocket_adapter = function (comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function () {\n",
       "        comm.close();\n",
       "    };\n",
       "    ws.send = function (m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function (msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data']);\n",
       "    });\n",
       "    return ws;\n",
       "};\n",
       "\n",
       "mpl.mpl_figure_comm = function (comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = document.getElementById(id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm);\n",
       "\n",
       "    function ondownload(figure, _format) {\n",
       "        window.open(figure.canvas.toDataURL());\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element;\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error('Failed to find cell for figure', id, fig);\n",
       "        return;\n",
       "    }\n",
       "    fig.cell_info[0].output_area.element.on(\n",
       "        'cleared',\n",
       "        { fig: fig },\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function (fig, msg) {\n",
       "    var width = fig.canvas.width / fig.ratio;\n",
       "    fig.cell_info[0].output_area.element.off(\n",
       "        'cleared',\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable();\n",
       "    fig.parent_element.innerHTML =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "    fig.close_ws(fig, msg);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.close_ws = function (fig, msg) {\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function (_remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width / this.ratio;\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message('ack', {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () {\n",
       "        fig.push_to_output();\n",
       "    }, 1000);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'btn-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'btn-group';\n",
       "    var button;\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'btn-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        button = fig.buttons[name] = document.createElement('button');\n",
       "        button.classList = 'btn btn-default';\n",
       "        button.href = '#';\n",
       "        button.title = name;\n",
       "        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message pull-right';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = document.createElement('div');\n",
       "    buttongrp.classList = 'btn-group inline pull-right';\n",
       "    button = document.createElement('button');\n",
       "    button.classList = 'btn btn-mini btn-primary';\n",
       "    button.href = '#';\n",
       "    button.title = 'Stop Interaction';\n",
       "    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n",
       "    button.addEventListener('click', function (_evt) {\n",
       "        fig.handle_close(fig, {});\n",
       "    });\n",
       "    button.addEventListener(\n",
       "        'mouseover',\n",
       "        on_mouseover_closure('Stop Interaction')\n",
       "    );\n",
       "    buttongrp.appendChild(button);\n",
       "    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n",
       "    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._remove_fig_handler = function (event) {\n",
       "    var fig = event.data.fig;\n",
       "    if (event.target !== this) {\n",
       "        // Ignore bubbled events from children.\n",
       "        return;\n",
       "    }\n",
       "    fig.close_ws(fig, {});\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (el) {\n",
       "    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (el) {\n",
       "    // this is important to make the div 'focusable\n",
       "    el.setAttribute('tabindex', 0);\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    } else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (event, _name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager) {\n",
       "        manager = IPython.keyboard_manager;\n",
       "    }\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which === 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "};\n",
       "\n",
       "mpl.find_output_cell = function (html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i = 0; i < ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code') {\n",
       "            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] === html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel !== null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target(\n",
       "        'matplotlib',\n",
       "        mpl.mpl_figure_comm\n",
       "    );\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"500\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oonisim/home/repository/git/oonisim/python_programs/nlp/src/drawing/functions.py:67: UserWarning: linewidths is ignored by contourf\n",
      "  axes.contourf(grid[0], grid[1], predictions, cmap=plt.cm.gist_rainbow, alpha=0.4, linewidths=3.0)\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,4)) \n",
    "ax.set_xlabel('x label')\n",
    "ax.set_ylabel('y label')\n",
    "ax.axis('equal')\n",
    "ax.grid()\n",
    "ax.set_title(\"Predictions\")\n",
    "#ax.set_xlim(-3, 3)\n",
    "#ax.set_ylim(-3, 3)\n",
    "\n",
    "x_grid, y_grid, predictions = prediction_grid(X, W)\n",
    "plot_categorical_predictions(ax, [x_grid, y_grid], X, Y, predictions)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization\n",
    "Observe the effect of the batch normalization by inserting the layer in-between activation and matmul layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from network import (\n",
    "    train_matmul_bn_relu_classifier\n",
    ")\n",
    "from common import (\n",
    "    prediction_grid_2d\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 Loss 1.2306061399101034\n",
      "iteration 10 Loss 0.6213103633379669\n",
      "iteration 20 Loss 0.6148412794693418\n",
      "iteration 30 Loss 0.608779874968272\n",
      "iteration 40 Loss 0.6029425183285261\n",
      "iteration 50 Loss 0.5973210198724935\n",
      "iteration 60 Loss 0.5919186958501125\n",
      "iteration 70 Loss 0.5867264331945117\n",
      "iteration 80 Loss 0.581709157461279\n",
      "iteration 90 Loss 0.5768629121312668\n",
      "iteration 100 Loss 0.5721594894879087\n",
      "iteration 110 Loss 0.5675999086585709\n",
      "iteration 120 Loss 0.5631756140536551\n",
      "iteration 130 Loss 0.5588744068442181\n",
      "iteration 140 Loss 0.5546850160294176\n",
      "iteration 150 Loss 0.5506113823577287\n",
      "iteration 160 Loss 0.5466415769053088\n",
      "iteration 170 Loss 0.5427809195750514\n",
      "iteration 180 Loss 0.539021833983767\n",
      "iteration 190 Loss 0.5353598613409056\n"
     ]
    }
   ],
   "source": [
    "MAX_TEST_TIMES = 200\n",
    "np.copyto(X, X_backup)\n",
    "np.copyto(W, W_backup)\n",
    "optimizer = SGD(lr=0.5)\n",
    "\n",
    "# W = train_classifier(\n",
    "W, objective, prediction = train_matmul_bn_relu_classifier(\n",
    "    N=N,\n",
    "    D=D,\n",
    "    M=M,\n",
    "    X=X,\n",
    "    T=T,\n",
    "    W=W,\n",
    "    log_loss_function=softmax_cross_entropy_log_loss,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=MAX_TEST_TIMES,\n",
    "    test_numerical_gradient=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "/* global mpl */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function () {\n",
       "    if (typeof WebSocket !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof MozWebSocket !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert(\n",
       "            'Your browser does not have WebSocket support. ' +\n",
       "                'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "                'Firefox 4 and 5 are also supported but you ' +\n",
       "                'have to enable WebSockets in about:config.'\n",
       "        );\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = this.ws.binaryType !== undefined;\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById('mpl-warnings');\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent =\n",
       "                'This browser does not support binary websocket messages. ' +\n",
       "                'Performance may be slow.';\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = document.createElement('div');\n",
       "    this.root.setAttribute('style', 'display: inline-block');\n",
       "    this._root_extra_style(this.root);\n",
       "\n",
       "    parent_element.appendChild(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen = function () {\n",
       "        fig.send_message('supports_binary', { value: fig.supports_binary });\n",
       "        fig.send_message('send_image_mode', {});\n",
       "        if (fig.ratio !== 1) {\n",
       "            fig.send_message('set_dpi_ratio', { dpi_ratio: fig.ratio });\n",
       "        }\n",
       "        fig.send_message('refresh', {});\n",
       "    };\n",
       "\n",
       "    this.imageObj.onload = function () {\n",
       "        if (fig.image_mode === 'full') {\n",
       "            // Full images could contain transparency (where diff images\n",
       "            // almost always do), so we need to clear the canvas so that\n",
       "            // there is no ghosting.\n",
       "            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "        }\n",
       "        fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "    };\n",
       "\n",
       "    this.imageObj.onunload = function () {\n",
       "        fig.ws.close();\n",
       "    };\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_header = function () {\n",
       "    var titlebar = document.createElement('div');\n",
       "    titlebar.classList =\n",
       "        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n",
       "    var titletext = document.createElement('div');\n",
       "    titletext.classList = 'ui-dialog-title';\n",
       "    titletext.setAttribute(\n",
       "        'style',\n",
       "        'width: 100%; text-align: center; padding: 3px;'\n",
       "    );\n",
       "    titlebar.appendChild(titletext);\n",
       "    this.root.appendChild(titlebar);\n",
       "    this.header = titletext;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = (this.canvas_div = document.createElement('div'));\n",
       "    canvas_div.setAttribute(\n",
       "        'style',\n",
       "        'border: 1px solid #ddd;' +\n",
       "            'box-sizing: content-box;' +\n",
       "            'clear: both;' +\n",
       "            'min-height: 1px;' +\n",
       "            'min-width: 1px;' +\n",
       "            'outline: 0;' +\n",
       "            'overflow: hidden;' +\n",
       "            'position: relative;' +\n",
       "            'resize: both;'\n",
       "    );\n",
       "\n",
       "    function on_keyboard_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.key_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    canvas_div.addEventListener(\n",
       "        'keydown',\n",
       "        on_keyboard_event_closure('key_press')\n",
       "    );\n",
       "    canvas_div.addEventListener(\n",
       "        'keyup',\n",
       "        on_keyboard_event_closure('key_release')\n",
       "    );\n",
       "\n",
       "    this._canvas_extra_style(canvas_div);\n",
       "    this.root.appendChild(canvas_div);\n",
       "\n",
       "    var canvas = (this.canvas = document.createElement('canvas'));\n",
       "    canvas.classList.add('mpl-canvas');\n",
       "    canvas.setAttribute('style', 'box-sizing: content-box;');\n",
       "\n",
       "    this.context = canvas.getContext('2d');\n",
       "\n",
       "    var backingStore =\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        this.context.webkitBackingStorePixelRatio ||\n",
       "        this.context.mozBackingStorePixelRatio ||\n",
       "        this.context.msBackingStorePixelRatio ||\n",
       "        this.context.oBackingStorePixelRatio ||\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        1;\n",
       "\n",
       "    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n",
       "        'canvas'\n",
       "    ));\n",
       "    rubberband_canvas.setAttribute(\n",
       "        'style',\n",
       "        'box-sizing: content-box; position: absolute; left: 0; top: 0; z-index: 1;'\n",
       "    );\n",
       "\n",
       "    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n",
       "    if (this.ResizeObserver === undefined) {\n",
       "        if (window.ResizeObserver !== undefined) {\n",
       "            this.ResizeObserver = window.ResizeObserver;\n",
       "        } else {\n",
       "            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n",
       "            this.ResizeObserver = obs.ResizeObserver;\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n",
       "        var nentries = entries.length;\n",
       "        for (var i = 0; i < nentries; i++) {\n",
       "            var entry = entries[i];\n",
       "            var width, height;\n",
       "            if (entry.contentBoxSize) {\n",
       "                if (entry.contentBoxSize instanceof Array) {\n",
       "                    // Chrome 84 implements new version of spec.\n",
       "                    width = entry.contentBoxSize[0].inlineSize;\n",
       "                    height = entry.contentBoxSize[0].blockSize;\n",
       "                } else {\n",
       "                    // Firefox implements old version of spec.\n",
       "                    width = entry.contentBoxSize.inlineSize;\n",
       "                    height = entry.contentBoxSize.blockSize;\n",
       "                }\n",
       "            } else {\n",
       "                // Chrome <84 implements even older version of spec.\n",
       "                width = entry.contentRect.width;\n",
       "                height = entry.contentRect.height;\n",
       "            }\n",
       "\n",
       "            // Keep the size of the canvas and rubber band canvas in sync with\n",
       "            // the canvas container.\n",
       "            if (entry.devicePixelContentBoxSize) {\n",
       "                // Chrome 84 implements new version of spec.\n",
       "                canvas.setAttribute(\n",
       "                    'width',\n",
       "                    entry.devicePixelContentBoxSize[0].inlineSize\n",
       "                );\n",
       "                canvas.setAttribute(\n",
       "                    'height',\n",
       "                    entry.devicePixelContentBoxSize[0].blockSize\n",
       "                );\n",
       "            } else {\n",
       "                canvas.setAttribute('width', width * fig.ratio);\n",
       "                canvas.setAttribute('height', height * fig.ratio);\n",
       "            }\n",
       "            canvas.setAttribute(\n",
       "                'style',\n",
       "                'width: ' + width + 'px; height: ' + height + 'px;'\n",
       "            );\n",
       "\n",
       "            rubberband_canvas.setAttribute('width', width);\n",
       "            rubberband_canvas.setAttribute('height', height);\n",
       "\n",
       "            // And update the size in Python. We ignore the initial 0/0 size\n",
       "            // that occurs as the element is placed into the DOM, which should\n",
       "            // otherwise not happen due to the minimum size styling.\n",
       "            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n",
       "                fig.request_resize(width, height);\n",
       "            }\n",
       "        }\n",
       "    });\n",
       "    this.resizeObserverInstance.observe(canvas_div);\n",
       "\n",
       "    function on_mouse_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.mouse_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousedown',\n",
       "        on_mouse_event_closure('button_press')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseup',\n",
       "        on_mouse_event_closure('button_release')\n",
       "    );\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousemove',\n",
       "        on_mouse_event_closure('motion_notify')\n",
       "    );\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseenter',\n",
       "        on_mouse_event_closure('figure_enter')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseleave',\n",
       "        on_mouse_event_closure('figure_leave')\n",
       "    );\n",
       "\n",
       "    canvas_div.addEventListener('wheel', function (event) {\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        on_mouse_event_closure('scroll')(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.appendChild(canvas);\n",
       "    canvas_div.appendChild(rubberband_canvas);\n",
       "\n",
       "    this.rubberband_context = rubberband_canvas.getContext('2d');\n",
       "    this.rubberband_context.strokeStyle = '#000000';\n",
       "\n",
       "    this._resize_canvas = function (width, height, forward) {\n",
       "        if (forward) {\n",
       "            canvas_div.style.width = width + 'px';\n",
       "            canvas_div.style.height = height + 'px';\n",
       "        }\n",
       "    };\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    this.rubberband_canvas.addEventListener('contextmenu', function (_e) {\n",
       "        event.preventDefault();\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus() {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'mpl-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'mpl-button-group';\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'mpl-button-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        var button = (fig.buttons[name] = document.createElement('button'));\n",
       "        button.classList = 'mpl-widget';\n",
       "        button.setAttribute('role', 'button');\n",
       "        button.setAttribute('aria-disabled', 'false');\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "\n",
       "        var icon_img = document.createElement('img');\n",
       "        icon_img.src = '_images/' + image + '.png';\n",
       "        icon_img.srcset = '_images/' + image + '_large.png 2x';\n",
       "        icon_img.alt = tooltip;\n",
       "        button.appendChild(icon_img);\n",
       "\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    var fmt_picker = document.createElement('select');\n",
       "    fmt_picker.classList = 'mpl-widget';\n",
       "    toolbar.appendChild(fmt_picker);\n",
       "    this.format_dropdown = fmt_picker;\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = document.createElement('option');\n",
       "        option.selected = fmt === mpl.default_extension;\n",
       "        option.innerHTML = fmt;\n",
       "        fmt_picker.appendChild(option);\n",
       "    }\n",
       "\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', { width: x_pixels, height: y_pixels });\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_message = function (type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function () {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function (fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1], msg['forward']);\n",
       "        fig.send_message('refresh', {});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function (fig, msg) {\n",
       "    var x0 = msg['x0'] / fig.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n",
       "    var x1 = msg['x1'] / fig.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0,\n",
       "        0,\n",
       "        fig.canvas.width / fig.ratio,\n",
       "        fig.canvas.height / fig.ratio\n",
       "    );\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function (fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function (fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch (cursor) {\n",
       "        case 0:\n",
       "            cursor = 'pointer';\n",
       "            break;\n",
       "        case 1:\n",
       "            cursor = 'default';\n",
       "            break;\n",
       "        case 2:\n",
       "            cursor = 'crosshair';\n",
       "            break;\n",
       "        case 3:\n",
       "            cursor = 'move';\n",
       "            break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_message = function (fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function (fig, _msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function (fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n",
       "    for (var key in msg) {\n",
       "        if (!(key in fig.buttons)) {\n",
       "            continue;\n",
       "        }\n",
       "        fig.buttons[key].disabled = !msg[key];\n",
       "        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n",
       "    if (msg['mode'] === 'PAN') {\n",
       "        fig.buttons['Pan'].classList.add('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    } else if (msg['mode'] === 'ZOOM') {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.add('active');\n",
       "    } else {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message('ack', {});\n",
       "};\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function (fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = 'image/png';\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src\n",
       "                );\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data\n",
       "            );\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        } else if (\n",
       "            typeof evt.data === 'string' &&\n",
       "            evt.data.slice(0, 21) === 'data:image/png;base64'\n",
       "        ) {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig['handle_' + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\n",
       "                \"No handler for the '\" + msg_type + \"' message type: \",\n",
       "                msg\n",
       "            );\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\n",
       "                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n",
       "                    e,\n",
       "                    e.stack,\n",
       "                    msg\n",
       "                );\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "};\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function (e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e) {\n",
       "        e = window.event;\n",
       "    }\n",
       "    if (e.target) {\n",
       "        targ = e.target;\n",
       "    } else if (e.srcElement) {\n",
       "        targ = e.srcElement;\n",
       "    }\n",
       "    if (targ.nodeType === 3) {\n",
       "        // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "    }\n",
       "\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    var boundingRect = targ.getBoundingClientRect();\n",
       "    var x = e.pageX - (boundingRect.left + document.body.scrollLeft);\n",
       "    var y = e.pageY - (boundingRect.top + document.body.scrollTop);\n",
       "\n",
       "    return { x: x, y: y };\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys(original) {\n",
       "    return Object.keys(original).reduce(function (obj, key) {\n",
       "        if (typeof original[key] !== 'object') {\n",
       "            obj[key] = original[key];\n",
       "        }\n",
       "        return obj;\n",
       "    }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function (event, name) {\n",
       "    var canvas_pos = mpl.findpos(event);\n",
       "\n",
       "    if (name === 'button_press') {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * this.ratio;\n",
       "    var y = canvas_pos.y * this.ratio;\n",
       "\n",
       "    this.send_message(name, {\n",
       "        x: x,\n",
       "        y: y,\n",
       "        button: event.button,\n",
       "        step: event.step,\n",
       "        guiEvent: simpleKeys(event),\n",
       "    });\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (_event, _name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.key_event = function (event, name) {\n",
       "    // Prevent repeat events\n",
       "    if (name === 'key_press') {\n",
       "        if (event.which === this._key) {\n",
       "            return;\n",
       "        } else {\n",
       "            this._key = event.which;\n",
       "        }\n",
       "    }\n",
       "    if (name === 'key_release') {\n",
       "        this._key = null;\n",
       "    }\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which !== 17) {\n",
       "        value += 'ctrl+';\n",
       "    }\n",
       "    if (event.altKey && event.which !== 18) {\n",
       "        value += 'alt+';\n",
       "    }\n",
       "    if (event.shiftKey && event.which !== 16) {\n",
       "        value += 'shift+';\n",
       "    }\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function (name) {\n",
       "    if (name === 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message('toolbar_button', { name: name });\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "\n",
       "///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n",
       "// prettier-ignore\n",
       "var _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";/* global mpl */\n",
       "\n",
       "var comm_websocket_adapter = function (comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function () {\n",
       "        comm.close();\n",
       "    };\n",
       "    ws.send = function (m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function (msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data']);\n",
       "    });\n",
       "    return ws;\n",
       "};\n",
       "\n",
       "mpl.mpl_figure_comm = function (comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = document.getElementById(id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm);\n",
       "\n",
       "    function ondownload(figure, _format) {\n",
       "        window.open(figure.canvas.toDataURL());\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element;\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error('Failed to find cell for figure', id, fig);\n",
       "        return;\n",
       "    }\n",
       "    fig.cell_info[0].output_area.element.on(\n",
       "        'cleared',\n",
       "        { fig: fig },\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function (fig, msg) {\n",
       "    var width = fig.canvas.width / fig.ratio;\n",
       "    fig.cell_info[0].output_area.element.off(\n",
       "        'cleared',\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable();\n",
       "    fig.parent_element.innerHTML =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "    fig.close_ws(fig, msg);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.close_ws = function (fig, msg) {\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function (_remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width / this.ratio;\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message('ack', {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () {\n",
       "        fig.push_to_output();\n",
       "    }, 1000);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'btn-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'btn-group';\n",
       "    var button;\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'btn-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        button = fig.buttons[name] = document.createElement('button');\n",
       "        button.classList = 'btn btn-default';\n",
       "        button.href = '#';\n",
       "        button.title = name;\n",
       "        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message pull-right';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = document.createElement('div');\n",
       "    buttongrp.classList = 'btn-group inline pull-right';\n",
       "    button = document.createElement('button');\n",
       "    button.classList = 'btn btn-mini btn-primary';\n",
       "    button.href = '#';\n",
       "    button.title = 'Stop Interaction';\n",
       "    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n",
       "    button.addEventListener('click', function (_evt) {\n",
       "        fig.handle_close(fig, {});\n",
       "    });\n",
       "    button.addEventListener(\n",
       "        'mouseover',\n",
       "        on_mouseover_closure('Stop Interaction')\n",
       "    );\n",
       "    buttongrp.appendChild(button);\n",
       "    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n",
       "    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._remove_fig_handler = function (event) {\n",
       "    var fig = event.data.fig;\n",
       "    if (event.target !== this) {\n",
       "        // Ignore bubbled events from children.\n",
       "        return;\n",
       "    }\n",
       "    fig.close_ws(fig, {});\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (el) {\n",
       "    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (el) {\n",
       "    // this is important to make the div 'focusable\n",
       "    el.setAttribute('tabindex', 0);\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    } else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (event, _name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager) {\n",
       "        manager = IPython.keyboard_manager;\n",
       "    }\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which === 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "};\n",
       "\n",
       "mpl.find_output_cell = function (html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i = 0; i < ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code') {\n",
       "            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] === html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel !== null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target(\n",
       "        'matplotlib',\n",
       "        mpl.mpl_figure_comm\n",
       "    );\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAfQAAAGQCAYAAABYs5LGAAAQzUlEQVR4Xu3VAQ0AAAjDMPBvGh0sxcF7ku84AgQIECBA4L3Avk8gAAECBAgQIDAG3RMQIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQMCg+wECBAgQIBAQMOiBEkUgQIAAAQIG3Q8QIECAAIGAgEEPlCgCAQIECBAw6H6AAAECBAgEBAx6oEQRCBAgQICAQfcDBAgQIEAgIGDQAyWKQIAAAQIEDLofIECAAAECAQGDHihRBAIECBAgYND9AAECBAgQCAgY9ECJIhAgQIAAAYPuBwgQIECAQEDAoAdKFIEAAQIECBh0P0CAAAECBAICBj1QoggECBAgQOAAgosBkU93nWsAAAAASUVORK5CYII=\" width=\"500\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,4)) \n",
    "x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "Y = COLOR_LABELS[T]\n",
    "\n",
    "x_grid, y_grid, predictions = prediction_grid_2d(x_min, x_max, y_min, y_max, prediction)\n",
    "plot_categorical_predictions(ax, [x_grid, y_grid], X, Y, predictions)\n",
    "\n",
    "ax.set_xlabel('x label')\n",
    "ax.set_ylabel('y label')\n",
    "ax.axis('equal')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linearly separable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data import (\n",
    "    set_in_a_radius,\n",
    "    sets_of_circle_A_not_B\n",
    ")\n",
    "from common import (\n",
    "    prediction_grid_2d\n",
    ")\n",
    "from network import (\n",
    "    train_two_layer_classifier\n",
    ")\n",
    "from drawing import (\n",
    "    plot,\n",
    "    scatter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data X and Label T\n",
    "Training data set that cannot be linearly classified. ```X = ((A not B), (B not C), (C not A), (A and B and C and D))``` for circles A, B, C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f861c4b7b50>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "__M = 3        # Number of circles\n",
    "__N = 250\n",
    "radius = 1\n",
    "circles, centres, intersection = sets_of_circle_A_not_B(radius=radius, ratio=1.2, m=__M, n=__N)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,4)) \n",
    "ax.set_xlabel('x label')\n",
    "ax.set_ylabel('y label')\n",
    "ax.axis('equal')\n",
    "ax.grid()\n",
    "r = np.linspace(0, 2*np.pi, 100)\n",
    "\n",
    "# (A not B), (B not C), (C not A)\n",
    "for i in range(__M):\n",
    "    circle = circles[i]\n",
    "    if circle.size > 0:\n",
    "        x = centres[i][0]\n",
    "        y = centres[i][1]\n",
    "        ax.scatter(circle[::, 0], circle[::, 1], color=COLOR_LABELS[i])\n",
    "        ax.plot(\n",
    "            x + radius * np.cos(r), \n",
    "            y + radius * np.sin(r), \n",
    "            linestyle='dashed', \n",
    "            color=COLOR_LABELS[i]\n",
    "        )\n",
    "\n",
    "# (A and B and C and D)\n",
    "M = __M + 1\n",
    "ax.scatter(intersection[::, 0], intersection[::, 1], color='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:(608, 2) T:(608,) \n"
     ]
    }
   ],
   "source": [
    "# Stack all circles and intersect\n",
    "X = np.vstack(\n",
    "    [circles[i] for i in range(M-1)] + \n",
    "    [intersection]\n",
    ")\n",
    "\n",
    "T = np.hstack(\n",
    "    [np.full(circles[i].shape[0], i) for i in range(M-1)] + \n",
    "    [np.full(intersection.shape[0], M-1)]\n",
    ")\n",
    "N = T.shape[0]\n",
    "assert T.shape[0] == X.shape[0]\n",
    "\n",
    "# Shuffle the data\n",
    "indices = np.random.permutation(range(T.shape[0]))\n",
    "X = X[indices]\n",
    "T = T[indices]\n",
    "Y = COLOR_LABELS[T]\n",
    "\n",
    "\n",
    "print(f\"X:{X.shape} T:{T.shape} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on non-linear separable data\n",
    "\n",
    "During the training, the loss often does not decrease. \n",
    "\n",
    "> Iteration [19976]: Loss[0.06914290965513335] has not improved from the previous [0.06914225566912098] for 1 times.\n",
    "\n",
    "<ins>If reduce the **learning rate** at those points, the situation gets worse </ins>(continuous non-improvements instead of sporadic) and the training fails (the result model cannot classify). If keep using the same learning rate, the non-improvement continues more frequently but the training itself makes a progress. \n",
    "\n",
    "Need to understand why it happens and why reducing the rate will make the training fail. Possibl approach is visualizing the loss function with contour lines and the track of the gradient descent to see the terrain it went through. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 Loss 1.330658143270141\n",
      "iteration 100 Loss 0.21639333699440044\n",
      "iteration 200 Loss 0.18983966961209311\n",
      "iteration 300 Loss 0.17676391005730774\n",
      "iteration 400 Loss 0.1685263060282\n",
      "iteration 500 Loss 0.16220325396540455\n",
      "iteration 600 Loss 0.15678332256415003\n",
      "iteration 700 Loss 0.15199696662437848\n",
      "iteration 800 Loss 0.1474573707318883\n",
      "iteration 900 Loss 0.14311248841804738\n",
      "iteration 1000 Loss 0.1389285471162465\n",
      "iteration 1100 Loss 0.13496326992546115\n",
      "iteration 1200 Loss 0.13131037221028374\n",
      "iteration 1300 Loss 0.12793534276701043\n",
      "iteration 1400 Loss 0.12471217058844439\n",
      "iteration 1500 Loss 0.12167303524424901\n",
      "iteration 1600 Loss 0.11883733035091286\n",
      "iteration 1700 Loss 0.11613674404894044\n",
      "iteration 1800 Loss 0.11358499372092772\n",
      "iteration 1900 Loss 0.11117410124703322\n",
      "iteration 2000 Loss 0.10858930177265308\n",
      "iteration 2100 Loss 0.10594999958295612\n",
      "iteration 2200 Loss 0.10371722532956003\n",
      "iteration 2300 Loss 0.10168159125775642\n",
      "iteration 2400 Loss 0.09990404012696413\n",
      "iteration 2500 Loss 0.09832735099054647\n",
      "iteration 2600 Loss 0.09690691170434979\n",
      "iteration 2700 Loss 0.09565167128831042\n",
      "iteration 2800 Loss 0.09449534380371669\n",
      "iteration 2900 Loss 0.09344055102482836\n",
      "iteration 3000 Loss 0.09247466517209296\n",
      "iteration 3100 Loss 0.09156836443828202\n",
      "iteration 3200 Loss 0.09070757244733213\n",
      "iteration 3300 Loss 0.0898930196833929\n",
      "iteration 3400 Loss 0.08911911891170306\n",
      "iteration 3500 Loss 0.0883885326436398\n",
      "iteration 3600 Loss 0.08770952936175148\n",
      "iteration 3700 Loss 0.08708573358618352\n",
      "iteration 3800 Loss 0.08650901865046293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [3843]: Loss[0.08628157615203533] has not improved from the previous [0.08628152203552258] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3900 Loss 0.08597620886661936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [3935]: Loss[0.08580406350935883] has not improved from the previous [0.08580344813485924] for 1 times.\n",
      "Iteration [3968]: Loss[0.08563886244682357] has not improved from the previous [0.08563795560396957] for 1 times.\n",
      "Iteration [3996]: Loss[0.08549963264486891] has not improved from the previous [0.0854995611875228] for 1 times.\n",
      "Iteration [4005]: Loss[0.08545611517835733] has not improved from the previous [0.08545539362448083] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4000 Loss 0.08547517687216306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [4057]: Loss[0.08520947837075293] has not improved from the previous [0.08520778691797694] for 1 times.\n",
      "Iteration [4083]: Loss[0.0850883833788816] has not improved from the previous [0.08508630736383414] for 1 times.\n",
      "Iteration [4090]: Loss[0.08505501462637306] has not improved from the previous [0.08505467766059506] for 1 times.\n",
      "Iteration [4103]: Loss[0.08499551746869471] has not improved from the previous [0.0849945039141123] for 1 times.\n",
      "Iteration [4109]: Loss[0.08496889488565051] has not improved from the previous [0.08496679972445353] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4100 Loss 0.08500382262648468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [4115]: Loss[0.08494062945060454] has not improved from the previous [0.08493993978265212] for 1 times.\n",
      "Iteration [4138]: Loss[0.08483652555335279] has not improved from the previous [0.08483645327168755] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4200 Loss 0.08455776946040888\n",
      "iteration 4300 Loss 0.08413518802123758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [4409]: Loss[0.08370252572527026] has not improved from the previous [0.08369794872428502] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4400 Loss 0.0837289653743296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [4423]: Loss[0.08364806661422337] has not improved from the previous [0.08364361161579725] for 1 times.\n",
      "Iteration [4435]: Loss[0.0836020004504467] has not improved from the previous [0.08359730642707217] for 1 times.\n",
      "Iteration [4443]: Loss[0.08356621593647055] has not improved from the previous [0.08356500383665813] for 1 times.\n",
      "Iteration [4452]: Loss[0.08353603794394482] has not improved from the previous [0.0835334956007726] for 1 times.\n",
      "Iteration [4457]: Loss[0.0835178137224185] has not improved from the previous [0.08351629036978829] for 1 times.\n",
      "Iteration [4462]: Loss[0.08349784791382832] has not improved from the previous [0.08349591812875094] for 1 times.\n",
      "Iteration [4467]: Loss[0.08348074248310704] has not improved from the previous [0.08347857930380952] for 1 times.\n",
      "Iteration [4470]: Loss[0.0834673651318174] has not improved from the previous [0.08346484051402062] for 1 times.\n",
      "Iteration [4473]: Loss[0.08345652611089205] has not improved from the previous [0.08345617294394604] for 1 times.\n",
      "Iteration [4478]: Loss[0.0834378739650582] has not improved from the previous [0.08343549766608094] for 1 times.\n",
      "Iteration [4481]: Loss[0.08342730611404629] has not improved from the previous [0.08342635282063676] for 1 times.\n",
      "Iteration [4489]: Loss[0.08339765564193034] has not improved from the previous [0.0833956208831534] for 1 times.\n",
      "Iteration [4492]: Loss[0.08338532551241042] has not improved from the previous [0.08338242806929898] for 1 times.\n",
      "Iteration [4495]: Loss[0.08337530122470323] has not improved from the previous [0.08337302384210535] for 1 times.\n",
      "Iteration [4498]: Loss[0.08336287703457401] has not improved from the previous [0.08336024894189586] for 1 times.\n",
      "Iteration [4501]: Loss[0.08335239227050653] has not improved from the previous [0.08335111385498006] for 1 times.\n",
      "Iteration [4505]: Loss[0.08333326564454656] has not improved from the previous [0.08333223271004056] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4500 Loss 0.08335111385498006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [4509]: Loss[0.08332454047705844] has not improved from the previous [0.08332040510923326] for 1 times.\n",
      "Iteration [4516]: Loss[0.08329638408509764] has not improved from the previous [0.0832938027780573] for 1 times.\n",
      "Iteration [4519]: Loss[0.08328695239557808] has not improved from the previous [0.08328372426571055] for 1 times.\n",
      "Iteration [4522]: Loss[0.08327316270857014] has not improved from the previous [0.08327252087190991] for 1 times.\n",
      "Iteration [4525]: Loss[0.08326519006043209] has not improved from the previous [0.08326109338697711] for 1 times.\n",
      "Iteration [4529]: Loss[0.08325052456975865] has not improved from the previous [0.08324962868611227] for 1 times.\n",
      "Iteration [4532]: Loss[0.08323622651698279] has not improved from the previous [0.08323613833875798] for 1 times.\n",
      "Iteration [4535]: Loss[0.08322906527383374] has not improved from the previous [0.08322419019508213] for 1 times.\n",
      "Iteration [4541]: Loss[0.08320753524333506] has not improved from the previous [0.08320140427770921] for 1 times.\n",
      "Iteration [4543]: Loss[0.08319402479820452] has not improved from the previous [0.083193569146409] for 1 times.\n",
      "Iteration [4545]: Loss[0.0831919534250799] has not improved from the previous [0.08319063932608] for 1 times.\n",
      "Iteration [4549]: Loss[0.08317730488150382] has not improved from the previous [0.08317618536208689] for 1 times.\n",
      "Iteration [4555]: Loss[0.08315791137866899] has not improved from the previous [0.08315051303852292] for 1 times.\n",
      "Iteration [4558]: Loss[0.08313948842492042] has not improved from the previous [0.08313937756798673] for 1 times.\n",
      "Iteration [4560]: Loss[0.08313769336184001] has not improved from the previous [0.08313604181953113] for 1 times.\n",
      "Iteration [4564]: Loss[0.08312284637783765] has not improved from the previous [0.08312224270256081] for 1 times.\n",
      "Iteration [4568]: Loss[0.08310936678894261] has not improved from the previous [0.08310818066574537] for 1 times.\n",
      "Iteration [4571]: Loss[0.08309937056259364] has not improved from the previous [0.08309324617966014] for 1 times.\n",
      "Iteration [4576]: Loss[0.08307975869472915] has not improved from the previous [0.08307918041440232] for 1 times.\n",
      "Iteration [4580]: Loss[0.08306661367506873] has not improved from the previous [0.08306468868316179] for 1 times.\n",
      "Iteration [4582]: Loss[0.08305294596964236] has not improved from the previous [0.08305253958599548] for 1 times.\n",
      "Iteration [4584]: Loss[0.08305155549946401] has not improved from the previous [0.0830501652385469] for 1 times.\n",
      "Iteration [4589]: Loss[0.08303364908346064] has not improved from the previous [0.08303204886590126] for 1 times.\n",
      "Iteration [4593]: Loss[0.08302035802272652] has not improved from the previous [0.08301817153533751] for 1 times.\n",
      "Iteration [4598]: Loss[0.08300183648374053] has not improved from the previous [0.08300013783992465] for 1 times.\n",
      "Iteration [4600]: Loss[0.0829889176076068] has not improved from the previous [0.08298834297517384] for 1 times.\n",
      "Iteration [4602]: Loss[0.08298796232959957] has not improved from the previous [0.08298538694904745] for 1 times.\n",
      "Iteration [4607]: Loss[0.08297103085555445] has not improved from the previous [0.08296841733306697] for 1 times.\n",
      "Iteration [4609]: Loss[0.08295792485308913] has not improved from the previous [0.08295591656152879] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4600 Loss 0.0829889176076068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [4611]: Loss[0.08295688168897013] has not improved from the previous [0.08295269255563202] for 1 times.\n",
      "Iteration [4614]: Loss[0.0829398596800564] has not improved from the previous [0.08293857510793232] for 1 times.\n",
      "Iteration [4616]: Loss[0.08293905721620609] has not improved from the previous [0.08293518692047376] for 1 times.\n",
      "Iteration [4618]: Loss[0.0829263288247364] has not improved from the previous [0.082923834450453] for 1 times.\n",
      "Iteration [4620]: Loss[0.08292633589068732] has not improved from the previous [0.08292059288200246] for 1 times.\n",
      "Iteration [4623]: Loss[0.08290916748767442] has not improved from the previous [0.08290631468724538] for 1 times.\n",
      "Iteration [4625]: Loss[0.08290821390558749] has not improved from the previous [0.08290244493678826] for 1 times.\n",
      "Iteration [4628]: Loss[0.08289157600386264] has not improved from the previous [0.08288866872500755] for 1 times.\n",
      "Iteration [4630]: Loss[0.0828906646950645] has not improved from the previous [0.08288472160587837] for 1 times.\n",
      "Iteration [4633]: Loss[0.08287359840002204] has not improved from the previous [0.08287112286293191] for 1 times.\n",
      "Iteration [4635]: Loss[0.08287399989822902] has not improved from the previous [0.0828680502895099] for 1 times.\n",
      "Iteration [4638]: Loss[0.08285763679776585] has not improved from the previous [0.08285366009642554] for 1 times.\n",
      "Iteration [4640]: Loss[0.0828488125651935] has not improved from the previous [0.08284849998969694] for 1 times.\n",
      "Iteration [4642]: Loss[0.08284779157517157] has not improved from the previous [0.08284360463962169] for 1 times.\n",
      "Iteration [4644]: Loss[0.08283575490164845] has not improved from the previous [0.0828325178817584] for 1 times.\n",
      "Iteration [4646]: Loss[0.08283513870320995] has not improved from the previous [0.08282850273794834] for 1 times.\n",
      "Iteration [4650]: Loss[0.08281704662599641] has not improved from the previous [0.08281191134950153] for 1 times.\n",
      "Iteration [4653]: Loss[0.082803902089876] has not improved from the previous [0.08280167558278193] for 1 times.\n",
      "Iteration [4655]: Loss[0.08280354592685588] has not improved from the previous [0.08279833499367631] for 1 times.\n",
      "Iteration [4658]: Loss[0.0827867573053894] has not improved from the previous [0.08278383031750453] for 1 times.\n",
      "Iteration [4660]: Loss[0.08278749323896722] has not improved from the previous [0.08278038248733753] for 1 times.\n",
      "Iteration [4662]: Loss[0.08277634048249552] has not improved from the previous [0.08276987433513967] for 1 times.\n",
      "Iteration [4665]: Loss[0.08276308660651029] has not improved from the previous [0.08276001581469061] for 1 times.\n",
      "Iteration [4667]: Loss[0.08276277767148704] has not improved from the previous [0.0827556638514482] for 1 times.\n",
      "Iteration [4669]: Loss[0.08275186364909841] has not improved from the previous [0.08274557605173737] for 1 times.\n",
      "Iteration [4672]: Loss[0.08273894910305296] has not improved from the previous [0.08273571043479609] for 1 times.\n",
      "Iteration [4674]: Loss[0.08273965770158805] has not improved from the previous [0.08273157896376297] for 1 times.\n",
      "Iteration [4676]: Loss[0.08272670857680432] has not improved from the previous [0.08272227544157333] for 1 times.\n",
      "Iteration [4679]: Loss[0.0827155566744633] has not improved from the previous [0.08271152097299857] for 1 times.\n",
      "Iteration [4681]: Loss[0.08271470298635893] has not improved from the previous [0.08270644670250266] for 1 times.\n",
      "Iteration [4683]: Loss[0.08270409206532416] has not improved from the previous [0.08269799149750193] for 1 times.\n",
      "Iteration [4686]: Loss[0.08269120474721266] has not improved from the previous [0.08268768460850798] for 1 times.\n",
      "Iteration [4688]: Loss[0.08269205120973006] has not improved from the previous [0.08268308801998903] for 1 times.\n",
      "Iteration [4690]: Loss[0.08267861905643432] has not improved from the previous [0.0826749109329502] for 1 times.\n",
      "Iteration [4695]: Loss[0.0826621037025532] has not improved from the previous [0.0826571726781053] for 1 times.\n",
      "Iteration [4698]: Loss[0.08264986150802475] has not improved from the previous [0.08264696819000128] for 1 times.\n",
      "Iteration [4700]: Loss[0.08265089699166697] has not improved from the previous [0.08264272010424002] for 1 times.\n",
      "Iteration [4702]: Loss[0.08263796878835852] has not improved from the previous [0.08263360685831671] for 1 times.\n",
      "Iteration [4706]: Loss[0.08262333515801061] has not improved from the previous [0.08261955268066924] for 1 times.\n",
      "Iteration [4708]: Loss[0.08261575320137803] has not improved from the previous [0.08261452009311893] for 1 times.\n",
      "Iteration [4710]: Loss[0.08261628552013452] has not improved from the previous [0.08260892172235618] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4700 Loss 0.08265089699166697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [4712]: Loss[0.08260429028003372] has not improved from the previous [0.08259951594164583] for 1 times.\n",
      "Iteration [4715]: Loss[0.08259434135380717] has not improved from the previous [0.08258882011417007] for 1 times.\n",
      "Iteration [4717]: Loss[0.08258621763980871] has not improved from the previous [0.0825830808857537] for 1 times.\n",
      "Iteration [4719]: Loss[0.08258667539061623] has not improved from the previous [0.08257744872512121] for 1 times.\n",
      "Iteration [4721]: Loss[0.08257317926910761] has not improved from the previous [0.08256989549553519] for 1 times.\n",
      "Iteration [4722]: Loss[0.0825741617984679] has not improved from the previous [0.08257317926910761] for 3 times.\n",
      "Iteration [4724]: Loss[0.08256456274396609] has not improved from the previous [0.0825585394540722] for 1 times.\n",
      "Iteration [4728]: Loss[0.08254950653928668] has not improved from the previous [0.08254535141412388] for 1 times.\n",
      "Iteration [4730]: Loss[0.0825430669210705] has not improved from the previous [0.08254017626989017] for 1 times.\n",
      "Iteration [4732]: Loss[0.08254346275050568] has not improved from the previous [0.08253317617078677] for 1 times.\n",
      "Iteration [4734]: Loss[0.0825288857777427] has not improved from the previous [0.08252658088916409] for 1 times.\n",
      "Iteration [4735]: Loss[0.08253094385171188] has not improved from the previous [0.0825288857777427] for 3 times.\n",
      "Iteration [4737]: Loss[0.08252019152559498] has not improved from the previous [0.08251555394212147] for 1 times.\n",
      "Iteration [4738]: Loss[0.08252022345233807] has not improved from the previous [0.08252019152559498] for 3 times.\n",
      "Iteration [4740]: Loss[0.08251120299924537] has not improved from the previous [0.08250511702433053] for 1 times.\n",
      "Iteration [4744]: Loss[0.08249639099123092] has not improved from the previous [0.08249255650176343] for 1 times.\n",
      "Iteration [4745]: Loss[0.0824971090738804] has not improved from the previous [0.08249639099123092] for 3 times.\n",
      "Iteration [4748]: Loss[0.08248284720344472] has not improved from the previous [0.08247858626598911] for 1 times.\n",
      "Iteration [4750]: Loss[0.08247569938181588] has not improved from the previous [0.0824731181918756] for 1 times.\n",
      "Iteration [4752]: Loss[0.08247638591811116] has not improved from the previous [0.08246672818364165] for 1 times.\n",
      "Iteration [4754]: Loss[0.08246208564550911] has not improved from the previous [0.08245969818361013] for 1 times.\n",
      "Iteration [4755]: Loss[0.08246432669976661] has not improved from the previous [0.08246208564550911] for 3 times.\n",
      "Iteration [4757]: Loss[0.08245339361952365] has not improved from the previous [0.08244896959475381] for 1 times.\n",
      "Iteration [4758]: Loss[0.08245481912081051] has not improved from the previous [0.08245339361952365] for 3 times.\n",
      "Iteration [4760]: Loss[0.08244360425942107] has not improved from the previous [0.08243959298160905] for 1 times.\n",
      "Iteration [4764]: Loss[0.08243037359099192] has not improved from the previous [0.08242687620582147] for 1 times.\n",
      "Iteration [4765]: Loss[0.08243046854024404] has not improved from the previous [0.08243037359099192] for 3 times.\n",
      "Iteration [4769]: Loss[0.0824134962248281] has not improved from the previous [0.08241002372566995] for 1 times.\n",
      "Iteration [4770]: Loss[0.08241403036721728] has not improved from the previous [0.0824134962248281] for 3 times.\n",
      "Iteration [4773]: Loss[0.08240259536804646] has not improved from the previous [0.08239586962749686] for 1 times.\n",
      "Iteration [4777]: Loss[0.08238831108124427] has not improved from the previous [0.08238374749257804] for 1 times.\n",
      "Iteration [4782]: Loss[0.0823713028280424] has not improved from the previous [0.08236724057834877] for 1 times.\n",
      "Iteration [4787]: Loss[0.08235460330998592] has not improved from the previous [0.08235064175320965] for 1 times.\n",
      "Iteration [4788]: Loss[0.08235524384417972] has not improved from the previous [0.08235460330998592] for 3 times.\n",
      "Iteration [4791]: Loss[0.08234360645004758] has not improved from the previous [0.0823366401475155] for 1 times.\n",
      "Iteration [4795]: Loss[0.08232830829566551] has not improved from the previous [0.08232498172056552] for 1 times.\n",
      "Iteration [4796]: Loss[0.08232896213039631] has not improved from the previous [0.08232830829566551] for 3 times.\n",
      "Iteration [4798]: Loss[0.0823197708873792] has not improved from the previous [0.08231386385371421] for 1 times.\n",
      "Iteration [4799]: Loss[0.08231998702047806] has not improved from the previous [0.0823197708873792] for 3 times.\n",
      "Iteration [4801]: Loss[0.08230971335826097] has not improved from the previous [0.08230483134071381] for 1 times.\n",
      "Iteration [4804]: Loss[0.08229936105964035] has not improved from the previous [0.08229482094331875] for 1 times.\n",
      "Iteration [4805]: Loss[0.08230021663867537] has not improved from the previous [0.08229936105964035] for 3 times.\n",
      "Iteration [4807]: Loss[0.08228988111897832] has not improved from the previous [0.08228510566214642] for 1 times.\n",
      "Iteration [4811]: Loss[0.08227526384266481] has not improved from the previous [0.08227369662146665] for 1 times.\n",
      "Iteration [4812]: Loss[0.0822777332012251] has not improved from the previous [0.08227526384266481] for 3 times.\n",
      "Iteration [4814]: Loss[0.08226703035234771] has not improved from the previous [0.08226240577274486] for 1 times.\n",
      "Iteration [4815]: Loss[0.08226850847010993] has not improved from the previous [0.08226703035234771] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4800 Loss 0.08230483134071381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [4817]: Loss[0.08225664635347027] has not improved from the previous [0.08225322776491376] for 1 times.\n",
      "Iteration [4818]: Loss[0.08225832629822204] has not improved from the previous [0.08225664635347027] for 3 times.\n",
      "Iteration [4820]: Loss[0.08224629657864849] has not improved from the previous [0.08224348415060284] for 1 times.\n",
      "Iteration [4821]: Loss[0.08224880469865481] has not improved from the previous [0.08224629657864849] for 3 times.\n",
      "Iteration [4823]: Loss[0.08223662125168152] has not improved from the previous [0.08223402750710343] for 1 times.\n",
      "Iteration [4824]: Loss[0.08223903911478896] has not improved from the previous [0.08223662125168152] for 3 times.\n",
      "Iteration [4826]: Loss[0.08222600259729512] has not improved from the previous [0.08222459017885649] for 1 times.\n",
      "Iteration [4829]: Loss[0.08221814020401624] has not improved from the previous [0.08221666154774653] for 1 times.\n",
      "Iteration [4831]: Loss[0.08221203422512102] has not improved from the previous [0.08220860835756419] for 1 times.\n",
      "Iteration [4833]: Loss[0.08220550867362482] has not improved from the previous [0.08220139945537593] for 1 times.\n",
      "Iteration [4835]: Loss[0.08219951103366352] has not improved from the previous [0.08219481792044987] for 1 times.\n",
      "Iteration [4838]: Loss[0.08219068790805091] has not improved from the previous [0.08218473609182712] for 1 times.\n",
      "Iteration [4841]: Loss[0.0821795306638118] has not improved from the previous [0.08217578163312717] for 1 times.\n",
      "Iteration [4842]: Loss[0.08218108508309914] has not improved from the previous [0.0821795306638118] for 3 times.\n",
      "Iteration [4844]: Loss[0.0821694678240734] has not improved from the previous [0.082166609353975] for 1 times.\n",
      "Iteration [4845]: Loss[0.0821728060935495] has not improved from the previous [0.0821694678240734] for 3 times.\n",
      "Iteration [4847]: Loss[0.08215856716061438] has not improved from the previous [0.08215780491359977] for 1 times.\n",
      "Iteration [4848]: Loss[0.08216250842814209] has not improved from the previous [0.08215856716061438] for 3 times.\n",
      "Iteration [4850]: Loss[0.08214861903607211] has not improved from the previous [0.08214799230037688] for 1 times.\n",
      "Iteration [4853]: Loss[0.08214132927373284] has not improved from the previous [0.08213829757304887] for 1 times.\n",
      "Iteration [4855]: Loss[0.08213551691954706] has not improved from the previous [0.08213115467777148] for 1 times.\n",
      "Iteration [4857]: Loss[0.08212960247541054] has not improved from the previous [0.08212407555326853] for 1 times.\n",
      "Iteration [4860]: Loss[0.0821197487602816] has not improved from the previous [0.08211475036026193] for 1 times.\n",
      "Iteration [4861]: Loss[0.08212145615859852] has not improved from the previous [0.0821197487602816] for 3 times.\n",
      "Iteration [4863]: Loss[0.08210849574651029] has not improved from the previous [0.08210656690746183] for 1 times.\n",
      "Iteration [4864]: Loss[0.08211186309504141] has not improved from the previous [0.08210849574651029] for 3 times.\n",
      "Iteration [4866]: Loss[0.0820979565792392] has not improved from the previous [0.08209734198485287] for 1 times.\n",
      "Iteration [4867]: Loss[0.08210273366694541] has not improved from the previous [0.0820979565792392] for 3 times.\n",
      "Iteration [4873]: Loss[0.08207709461106878] has not improved from the previous [0.08207500990835732] for 1 times.\n",
      "Iteration [4874]: Loss[0.08207990563021658] has not improved from the previous [0.08207709461106878] for 3 times.\n",
      "Iteration [4876]: Loss[0.08206797979819208] has not improved from the previous [0.0820649798065584] for 1 times.\n",
      "Iteration [4877]: Loss[0.08207040532666744] has not improved from the previous [0.08206797979819208] for 3 times.\n",
      "Iteration [4879]: Loss[0.08205764080428574] has not improved from the previous [0.08205607516229219] for 1 times.\n",
      "Iteration [4880]: Loss[0.08206208653424606] has not improved from the previous [0.08205764080428574] for 3 times.\n",
      "Iteration [4883]: Loss[0.08204626935343087] has not improved from the previous [0.08204598024444731] for 1 times.\n",
      "Iteration [4886]: Loss[0.08203621820863827] has not improved from the previous [0.08203440452416093] for 1 times.\n",
      "Iteration [4887]: Loss[0.08203923294715876] has not improved from the previous [0.08203621820863827] for 3 times.\n",
      "Iteration [4889]: Loss[0.08202580089327248] has not improved from the previous [0.08202450645367157] for 1 times.\n",
      "Iteration [4890]: Loss[0.08203007609085931] has not improved from the previous [0.08202580089327248] for 3 times.\n",
      "Iteration [4893]: Loss[0.0820211452427972] has not improved from the previous [0.08201522817331038] for 1 times.\n",
      "Iteration [4896]: Loss[0.08200536240103304] has not improved from the previous [0.08200493879317294] for 1 times.\n",
      "Iteration [4900]: Loss[0.08199926565524535] has not improved from the previous [0.0819937602029434] for 1 times.\n",
      "Iteration [4903]: Loss[0.08199000874546235] has not improved from the previous [0.08198423676088233] for 1 times.\n",
      "Iteration [4906]: Loss[0.08197452606405967] has not improved from the previous [0.08197311768699675] for 1 times.\n",
      "Iteration [4908]: Loss[0.08196861522615402] has not improved from the previous [0.08196387375127122] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4900 Loss 0.08199926565524535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [4909]: Loss[0.08197009051999082] has not improved from the previous [0.08196861522615402] for 3 times.\n",
      "Iteration [4911]: Loss[0.08195623990950021] has not improved from the previous [0.0819562326821838] for 1 times.\n",
      "Iteration [4912]: Loss[0.08196194323051001] has not improved from the previous [0.08195623990950021] for 3 times.\n",
      "Iteration [4915]: Loss[0.08194705859142243] has not improved from the previous [0.08194596637473396] for 1 times.\n",
      "Iteration [4917]: Loss[0.08194088842405171] has not improved from the previous [0.0819360126144561] for 1 times.\n",
      "Iteration [4918]: Loss[0.08194230668127127] has not improved from the previous [0.08194088842405171] for 3 times.\n",
      "Iteration [4924]: Loss[0.0819170844251559] has not improved from the previous [0.08191636849555539] for 1 times.\n",
      "Iteration [4925]: Loss[0.08192120754858598] has not improved from the previous [0.0819170844251559] for 3 times.\n",
      "Iteration [4928]: Loss[0.08191261548275833] has not improved from the previous [0.08190635893051271] for 1 times.\n",
      "Iteration [4931]: Loss[0.08189700447073667] has not improved from the previous [0.0818957856271152] for 1 times.\n",
      "Iteration [4933]: Loss[0.08189011517365928] has not improved from the previous [0.0818864718523216] for 1 times.\n",
      "Iteration [4934]: Loss[0.0818932718624015] has not improved from the previous [0.08189011517365928] for 3 times.\n",
      "Iteration [4937]: Loss[0.08187837273201866] has not improved from the previous [0.08187781226218552] for 1 times.\n",
      "Iteration [4939]: Loss[0.08187324001050017] has not improved from the previous [0.08186804991338907] for 1 times.\n",
      "Iteration [4940]: Loss[0.0818741559821623] has not improved from the previous [0.08187324001050017] for 3 times.\n",
      "Iteration [4943]: Loss[0.08186695703273261] has not improved from the previous [0.08186008411728932] for 1 times.\n",
      "Iteration [4946]: Loss[0.08185085771100899] has not improved from the previous [0.08184937755225656] for 1 times.\n",
      "Iteration [4948]: Loss[0.08184469314032501] has not improved from the previous [0.0818403120373519] for 1 times.\n",
      "Iteration [4949]: Loss[0.08184778034775306] has not improved from the previous [0.08184469314032501] for 3 times.\n",
      "Iteration [4952]: Loss[0.0818329798668392] has not improved from the previous [0.08183209025898446] for 1 times.\n",
      "Iteration [4954]: Loss[0.08182577256389323] has not improved from the previous [0.08182235533589226] for 1 times.\n",
      "Iteration [4955]: Loss[0.08182932998315644] has not improved from the previous [0.08182577256389323] for 3 times.\n",
      "Iteration [4958]: Loss[0.08181463280690698] has not improved from the previous [0.08181367255581676] for 1 times.\n",
      "Iteration [4962]: Loss[0.08180861539017399] has not improved from the previous [0.08180289944394975] for 1 times.\n",
      "Iteration [4965]: Loss[0.08180093268951073] has not improved from the previous [0.08179230002370586] for 1 times.\n",
      "Iteration [4970]: Loss[0.08178511556090579] has not improved from the previous [0.0817776809790747] for 1 times.\n",
      "Iteration [4973]: Loss[0.08177013938503952] has not improved from the previous [0.0817664440650476] for 1 times.\n",
      "Iteration [4975]: Loss[0.08176114831452429] has not improved from the previous [0.08175934476088316] for 1 times.\n",
      "Iteration [4976]: Loss[0.08176658099209563] has not improved from the previous [0.08176114831452429] for 3 times.\n",
      "Iteration [4979]: Loss[0.08175143939617825] has not improved from the previous [0.08174918956594866] for 1 times.\n",
      "Iteration [4981]: Loss[0.08174496925376655] has not improved from the previous [0.08174078413069458] for 1 times.\n",
      "Iteration [4982]: Loss[0.0817484390126631] has not improved from the previous [0.08174496925376655] for 3 times.\n",
      "Iteration [4985]: Loss[0.0817338354012432] has not improved from the previous [0.08173073700511943] for 1 times.\n",
      "Iteration [4987]: Loss[0.08172546934464923] has not improved from the previous [0.08172338740971233] for 1 times.\n",
      "Iteration [4988]: Loss[0.08173042819604355] has not improved from the previous [0.08172546934464923] for 3 times.\n",
      "Iteration [4991]: Loss[0.08171588031326538] has not improved from the previous [0.08171310936669601] for 1 times.\n",
      "Iteration [4993]: Loss[0.08170780731277341] has not improved from the previous [0.08170518182751357] for 1 times.\n",
      "Iteration [4994]: Loss[0.08171244475041889] has not improved from the previous [0.08170780731277341] for 3 times.\n",
      "Iteration [4997]: Loss[0.08169763832516397] has not improved from the previous [0.08169463209322965] for 1 times.\n",
      "Iteration [4999]: Loss[0.08169042199229895] has not improved from the previous [0.08168725013633053] for 1 times.\n",
      "Iteration [5000]: Loss[0.08169513462690771] has not improved from the previous [0.08169042199229895] for 3 times.\n",
      "Iteration [5003]: Loss[0.08168059603144698] has not improved from the previous [0.08167626815380193] for 1 times.\n",
      "Iteration [5005]: Loss[0.08167140642838538] has not improved from the previous [0.0816699462947263] for 1 times.\n",
      "Iteration [5006]: Loss[0.08167695915110021] has not improved from the previous [0.08167140642838538] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5000 Loss 0.08169513462690771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [5011]: Loss[0.08166295675443848] has not improved from the previous [0.0816545871467208] for 1 times.\n",
      "Iteration [5014]: Loss[0.08164781204145508] has not improved from the previous [0.08164334970288163] for 1 times.\n",
      "Iteration [5016]: Loss[0.08163958510852602] has not improved from the previous [0.08163709577241425] for 1 times.\n",
      "Iteration [5017]: Loss[0.08164472232855265] has not improved from the previous [0.08163958510852602] for 3 times.\n",
      "Iteration [5022]: Loss[0.08163053473361892] has not improved from the previous [0.08162160546004807] for 1 times.\n",
      "Iteration [5025]: Loss[0.08161535398734754] has not improved from the previous [0.08161016208274502] for 1 times.\n",
      "Iteration [5027]: Loss[0.0816055813126171] has not improved from the previous [0.0816051081935873] for 1 times.\n",
      "Iteration [5028]: Loss[0.08161242390427471] has not improved from the previous [0.0816055813126171] for 3 times.\n",
      "Iteration [5033]: Loss[0.08159837786660884] has not improved from the previous [0.08158897446122289] for 1 times.\n",
      "Iteration [5038]: Loss[0.0815832523174118] has not improved from the previous [0.08157571055570241] for 1 times.\n",
      "Iteration [5043]: Loss[0.08156907081084776] has not improved from the previous [0.08155938859812979] for 1 times.\n",
      "Iteration [5048]: Loss[0.08155425582561965] has not improved from the previous [0.08154510056603088] for 1 times.\n",
      "Iteration [5051]: Loss[0.08153765880439563] has not improved from the previous [0.08153354258142381] for 1 times.\n",
      "Iteration [5052]: Loss[0.08154131535690028] has not improved from the previous [0.08153765880439563] for 3 times.\n",
      "Iteration [5057]: Loss[0.0815283587969939] has not improved from the previous [0.08151861523240142] for 1 times.\n",
      "Iteration [5060]: Loss[0.08151166598438295] has not improved from the previous [0.0815078690249087] for 1 times.\n",
      "Iteration [5061]: Loss[0.0815150546847801] has not improved from the previous [0.08151166598438295] for 3 times.\n",
      "Iteration [5066]: Loss[0.08150180196721661] has not improved from the previous [0.08149286244925846] for 1 times.\n",
      "Iteration [5069]: Loss[0.08148569262003562] has not improved from the previous [0.08148147963682424] for 1 times.\n",
      "Iteration [5070]: Loss[0.08148910358571419] has not improved from the previous [0.08148569262003562] for 3 times.\n",
      "Iteration [5075]: Loss[0.0814767402051309] has not improved from the previous [0.08146729739144346] for 1 times.\n",
      "Iteration [5078]: Loss[0.08145874922735101] has not improved from the previous [0.08145603848182174] for 1 times.\n",
      "Iteration [5079]: Loss[0.08146325184063254] has not improved from the previous [0.08145874922735101] for 3 times.\n",
      "Iteration [5084]: Loss[0.08144068294660474] has not improved from the previous [0.08143989198490571] for 1 times.\n",
      "Iteration [5086]: Loss[0.08143519100951237] has not improved from the previous [0.08143285824630964] for 1 times.\n",
      "Iteration [5087]: Loss[0.08144008408997792] has not improved from the previous [0.08143519100951237] for 3 times.\n",
      "Iteration [5092]: Loss[0.08142782405906344] has not improved from the previous [0.08141673234707783] for 1 times.\n",
      "Iteration [5095]: Loss[0.08140846354234688] has not improved from the previous [0.08140754523271793] for 1 times.\n",
      "Iteration [5096]: Loss[0.08141479648384332] has not improved from the previous [0.08140846354234688] for 3 times.\n",
      "Iteration [5101]: Loss[0.08138970215971779] has not improved from the previous [0.08138931964007246] for 1 times.\n",
      "Iteration [5103]: Loss[0.08139543601222636] has not improved from the previous [0.08138617473222845] for 1 times.\n",
      "Iteration [5106]: Loss[0.08137905679390718] has not improved from the previous [0.08137541798237655] for 1 times.\n",
      "Iteration [5107]: Loss[0.08138270076439351] has not improved from the previous [0.08137905679390718] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5100 Loss 0.08138931964007246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [5112]: Loss[0.08136156964372504] has not improved from the previous [0.08135968416039945] for 1 times.\n",
      "Iteration [5114]: Loss[0.08135395759322575] has not improved from the previous [0.08135352226501145] for 1 times.\n",
      "Iteration [5115]: Loss[0.08136062250072515] has not improved from the previous [0.08135395759322575] for 3 times.\n",
      "Iteration [5120]: Loss[0.08133570463270266] has not improved from the previous [0.08133545938801368] for 1 times.\n",
      "Iteration [5122]: Loss[0.08134149140452165] has not improved from the previous [0.08133181622565804] for 1 times.\n",
      "Iteration [5125]: Loss[0.0813242935068506] has not improved from the previous [0.08132163689042242] for 1 times.\n",
      "Iteration [5126]: Loss[0.08132880018446864] has not improved from the previous [0.0813242935068506] for 3 times.\n",
      "Iteration [5129]: Loss[0.08131235797119582] has not improved from the previous [0.08131055223927942] for 1 times.\n",
      "Iteration [5130]: Loss[0.08131854644160502] has not improved from the previous [0.08131235797119582] for 3 times.\n",
      "Iteration [5134]: Loss[0.08130656648810654] has not improved from the previous [0.08129957916835756] for 1 times.\n",
      "Iteration [5139]: Loss[0.08128501677329232] has not improved from the previous [0.08128199351542008] for 1 times.\n",
      "Iteration [5140]: Loss[0.08128918891403691] has not improved from the previous [0.08128501677329232] for 3 times.\n",
      "Iteration [5145]: Loss[0.08126981687202856] has not improved from the previous [0.0812653018400153] for 1 times.\n",
      "Iteration [5148]: Loss[0.08126859790878815] has not improved from the previous [0.0812592124403712] for 1 times.\n",
      "Iteration [5152]: Loss[0.08125645145334938] has not improved from the previous [0.08124797766632949] for 1 times.\n",
      "Iteration [5156]: Loss[0.0812452851886644] has not improved from the previous [0.08123694496991346] for 1 times.\n",
      "Iteration [5162]: Loss[0.08122234214772178] has not improved from the previous [0.08121813221344613] for 1 times.\n",
      "Iteration [5165]: Loss[0.08121984408845986] has not improved from the previous [0.08121205600931652] for 1 times.\n",
      "Iteration [5169]: Loss[0.08120913294600907] has not improved from the previous [0.0812006147486196] for 1 times.\n",
      "Iteration [5175]: Loss[0.08119289574185098] has not improved from the previous [0.08118281094030688] for 1 times.\n",
      "Iteration [5179]: Loss[0.08118102109719799] has not improved from the previous [0.08117336804296783] for 1 times.\n",
      "Iteration [5185]: Loss[0.08115838612859966] has not improved from the previous [0.08115441709304501] for 1 times.\n",
      "Iteration [5188]: Loss[0.08115697212964229] has not improved from the previous [0.08114826187710251] for 1 times.\n",
      "Iteration [5192]: Loss[0.08113892086276547] has not improved from the previous [0.08113688113327727] for 1 times.\n",
      "Iteration [5197]: Loss[0.08113211392183177] has not improved from the previous [0.08112184961880593] for 1 times.\n",
      "Iteration [5201]: Loss[0.08112055290748114] has not improved from the previous [0.08111227516135064] for 1 times.\n",
      "Iteration [5207]: Loss[0.08109859289565895] has not improved from the previous [0.08109349433122137] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5200 Loss 0.08111227516135064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [5210]: Loss[0.0810899986680841] has not improved from the previous [0.08108533044518172] for 1 times.\n",
      "Iteration [5213]: Loss[0.08108837686812058] has not improved from the previous [0.0810774708121301] for 1 times.\n",
      "Iteration [5219]: Loss[0.08106307977331775] has not improved from the previous [0.08106098279501595] for 1 times.\n",
      "Iteration [5224]: Loss[0.08105814229361911] has not improved from the previous [0.08104746227721014] for 1 times.\n",
      "Iteration [5228]: Loss[0.0810473502930658] has not improved from the previous [0.08103812595502002] for 1 times.\n",
      "Iteration [5234]: Loss[0.08102166623600857] has not improved from the previous [0.08101930272793133] for 1 times.\n",
      "Iteration [5235]: Loss[0.0810270867932743] has not improved from the previous [0.08102166623600857] for 3 times.\n",
      "Iteration [5239]: Loss[0.0810100711107499] has not improved from the previous [0.08100849420426641] for 1 times.\n",
      "Iteration [5244]: Loss[0.08100384613147918] has not improved from the previous [0.08099292819519158] for 1 times.\n",
      "Iteration [5252]: Loss[0.08098174998768257] has not improved from the previous [0.08097272300861247] for 1 times.\n",
      "Iteration [5258]: Loss[0.08095953634835429] has not improved from the previous [0.08095441768490241] for 1 times.\n",
      "Iteration [5261]: Loss[0.08094684903116946] has not improved from the previous [0.0809464713399416] for 1 times.\n",
      "Iteration [5263]: Loss[0.0809438096569544] has not improved from the previous [0.0809415105119639] for 1 times.\n",
      "Iteration [5268]: Loss[0.08093250856612445] has not improved from the previous [0.08092786988584182] for 1 times.\n",
      "Iteration [5271]: Loss[0.08093582643036883] has not improved from the previous [0.0809207078607261] for 1 times.\n",
      "Iteration [5277]: Loss[0.0809066634290058] has not improved from the previous [0.08090388256207866] for 1 times.\n",
      "Iteration [5278]: Loss[0.08091131139223523] has not improved from the previous [0.0809066634290058] for 3 times.\n",
      "Iteration [5286]: Loss[0.08089059744275999] has not improved from the previous [0.08088169537821104] for 1 times.\n",
      "Iteration [5296]: Loss[0.08086390041738138] has not improved from the previous [0.08085589595801966] for 1 times.\n",
      "Iteration [5304]: Loss[0.08083475553214842] has not improved from the previous [0.08083218254179514] for 1 times.\n",
      "Iteration [5311]: Loss[0.08082525804545176] has not improved from the previous [0.08081502059344207] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5300 Loss 0.08084314499510314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [5315]: Loss[0.08080398918112316] has not improved from the previous [0.0808038963739514] for 1 times.\n",
      "Iteration [5317]: Loss[0.0807984156567411] has not improved from the previous [0.08079790341216342] for 1 times.\n",
      "Iteration [5319]: Loss[0.08079567497748574] has not improved from the previous [0.08079367975633232] for 1 times.\n",
      "Iteration [5324]: Loss[0.08078484393351484] has not improved from the previous [0.08077969352130417] for 1 times.\n",
      "Iteration [5327]: Loss[0.08077484335148322] has not improved from the previous [0.08077279947642534] for 1 times.\n",
      "Iteration [5329]: Loss[0.08077051108625086] has not improved from the previous [0.08076698294742017] for 1 times.\n",
      "Iteration [5330]: Loss[0.08077428949626204] has not improved from the previous [0.08077051108625086] for 3 times.\n",
      "Iteration [5336]: Loss[0.08075141577473827] has not improved from the previous [0.08074855262736956] for 1 times.\n",
      "Iteration [5339]: Loss[0.08074338474228301] has not improved from the previous [0.08074319629946869] for 1 times.\n",
      "Iteration [5341]: Loss[0.0807360664734974] has not improved from the previous [0.08073582301210082] for 1 times.\n",
      "Iteration [5345]: Loss[0.08074038289798423] has not improved from the previous [0.0807280890162251] for 1 times.\n",
      "Iteration [5354]: Loss[0.08071333068101186] has not improved from the previous [0.08070387728720915] for 1 times.\n",
      "Iteration [5361]: Loss[0.0806891045236341] has not improved from the previous [0.08068575724141891] for 1 times.\n",
      "Iteration [5364]: Loss[0.08069179350318881] has not improved from the previous [0.08067785522573975] for 1 times.\n",
      "Iteration [5373]: Loss[0.08065582742399624] has not improved from the previous [0.0806535350500649] for 1 times.\n",
      "Iteration [5384]: Loss[0.08062833914243106] has not improved from the previous [0.08062600597686674] for 1 times.\n",
      "Iteration [5388]: Loss[0.0806307261313043] has not improved from the previous [0.08061817565101521] for 1 times.\n",
      "Iteration [5394]: Loss[0.08060094213489646] has not improved from the previous [0.08060068237745216] for 1 times.\n",
      "Iteration [5395]: Loss[0.08060779263450416] has not improved from the previous [0.08060094213489646] for 3 times.\n",
      "Iteration [5404]: Loss[0.0805755958799201] has not improved from the previous [0.08057509925905058] for 1 times.\n",
      "Iteration [5408]: Loss[0.08056813089357305] has not improved from the previous [0.08056535479377669] for 1 times.\n",
      "Iteration [5410]: Loss[0.08056383502743655] has not improved from the previous [0.08055988671500994] for 1 times.\n",
      "Iteration [5411]: Loss[0.08057227016954349] has not improved from the previous [0.08056383502743655] for 3 times.\n",
      "Iteration [5417]: Loss[0.0805451796934187] has not improved from the previous [0.0805423829326564] for 1 times.\n",
      "Iteration [5418]: Loss[0.08055484078325531] has not improved from the previous [0.0805451796934187] for 3 times.\n",
      "Iteration [5422]: Loss[0.08053285973305209] has not improved from the previous [0.0805328418414157] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5400 Loss 0.08058419201190499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [5424]: Loss[0.08052577767992] has not improved from the previous [0.08052552822772113] for 1 times.\n",
      "Iteration [5425]: Loss[0.0805375551545414] has not improved from the previous [0.08052577767992] for 3 times.\n",
      "Iteration [5432]: Loss[0.08050849553034778] has not improved from the previous [0.08050562934725361] for 1 times.\n",
      "Iteration [5434]: Loss[0.08050152552932972] has not improved from the previous [0.0805002332007942] for 1 times.\n",
      "Iteration [5435]: Loss[0.08051269004728573] has not improved from the previous [0.08050152552932972] for 3 times.\n",
      "Iteration [5442]: Loss[0.08048323609356492] has not improved from the previous [0.08048118796296837] for 1 times.\n",
      "Iteration [5444]: Loss[0.08047804107964221] has not improved from the previous [0.08047547319052742] for 1 times.\n",
      "Iteration [5445]: Loss[0.08048772152418404] has not improved from the previous [0.08047804107964221] for 3 times.\n",
      "Iteration [5450]: Loss[0.08046430381558209] has not improved from the previous [0.08046355872715659] for 1 times.\n",
      "Iteration [5457]: Loss[0.08044770939904952] has not improved from the previous [0.08044438591496694] for 1 times.\n",
      "Iteration [5460]: Loss[0.08043833792846065] has not improved from the previous [0.08043644748111042] for 1 times.\n",
      "Iteration [5461]: Loss[0.0804482602560066] has not improved from the previous [0.08043833792846065] for 3 times.\n",
      "Iteration [5471]: Loss[0.08041348933951147] has not improved from the previous [0.08041046990246985] for 1 times.\n",
      "Iteration [5474]: Loss[0.08040319838482753] has not improved from the previous [0.080402241976218] for 1 times.\n",
      "Iteration [5475]: Loss[0.0804087017813971] has not improved from the previous [0.08040319838482753] for 3 times.\n",
      "Iteration [5479]: Loss[0.08039293562508479] has not improved from the previous [0.08039127300307451] for 1 times.\n",
      "Iteration [5481]: Loss[0.08038524939966493] has not improved from the previous [0.08038512084438407] for 1 times.\n",
      "Iteration [5484]: Loss[0.0803793006221912] has not improved from the previous [0.08037720134929971] for 1 times.\n",
      "Iteration [5485]: Loss[0.08038942387890392] has not improved from the previous [0.0803793006221912] for 3 times.\n",
      "Iteration [5492]: Loss[0.08035946374462574] has not improved from the previous [0.08035798409415411] for 1 times.\n",
      "Iteration [5493]: Loss[0.08036286070242227] has not improved from the previous [0.08035946374462574] for 3 times.\n",
      "Iteration [5496]: Loss[0.0803512430310204] has not improved from the previous [0.08035034513603423] for 1 times.\n",
      "Iteration [5502]: Loss[0.08033811639055512] has not improved from the previous [0.08033481637207736] for 1 times.\n",
      "Iteration [5505]: Loss[0.08032781920423916] has not improved from the previous [0.08032707514256225] for 1 times.\n",
      "Iteration [5506]: Loss[0.08033894490219735] has not improved from the previous [0.08032781920423916] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5500 Loss 0.08033700252595698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [5511]: Loss[0.08031538872672808] has not improved from the previous [0.08031455362365103] for 1 times.\n",
      "Iteration [5513]: Loss[0.08030820293255526] has not improved from the previous [0.0803073645279162] for 1 times.\n",
      "Iteration [5514]: Loss[0.0803125374275648] has not improved from the previous [0.08030820293255526] for 3 times.\n",
      "Iteration [5517]: Loss[0.08030077606872603] has not improved from the previous [0.08029941398276215] for 1 times.\n",
      "Iteration [5520]: Loss[0.08029172247055487] has not improved from the previous [0.08029110510978664] for 1 times.\n",
      "Iteration [5521]: Loss[0.0803030231992319] has not improved from the previous [0.08029172247055487] for 3 times.\n",
      "Iteration [5526]: Loss[0.08027929086146028] has not improved from the previous [0.0802789581149639] for 1 times.\n",
      "Iteration [5528]: Loss[0.0802724393680431] has not improved from the previous [0.0802712830248797] for 1 times.\n",
      "Iteration [5531]: Loss[0.08026378350011658] has not improved from the previous [0.08026371465212798] for 1 times.\n",
      "Iteration [5534]: Loss[0.08026542363011552] has not improved from the previous [0.08025765416198512] for 1 times.\n",
      "Iteration [5538]: Loss[0.08025047902792744] has not improved from the previous [0.08024928565741724] for 1 times.\n",
      "Iteration [5540]: Loss[0.08024520777900009] has not improved from the previous [0.08024249347242395] for 1 times.\n",
      "Iteration [5541]: Loss[0.08025466748040601] has not improved from the previous [0.08024520777900009] for 3 times.\n",
      "Iteration [5545]: Loss[0.0802315571780721] has not improved from the previous [0.08023152260520726] for 1 times.\n",
      "Iteration [5552]: Loss[0.08021841048477395] has not improved from the previous [0.08021439574659273] for 1 times.\n",
      "Iteration [5556]: Loss[0.08020753185533329] has not improved from the previous [0.08020714804603461] for 1 times.\n",
      "Iteration [5559]: Loss[0.08020199451361872] has not improved from the previous [0.08019784904821496] for 1 times.\n",
      "Iteration [5560]: Loss[0.08020962765626748] has not improved from the previous [0.08020199451361872] for 3 times.\n",
      "Iteration [5564]: Loss[0.0801870482079494] has not improved from the previous [0.08018639454182272] for 1 times.\n",
      "Iteration [5571]: Loss[0.08017320092146706] has not improved from the previous [0.08016979650856837] for 1 times.\n",
      "Iteration [5578]: Loss[0.08015656488560224] has not improved from the previous [0.08015316241543512] for 1 times.\n",
      "Iteration [5585]: Loss[0.080137214965169] has not improved from the previous [0.0801363369784924] for 1 times.\n",
      "Iteration [5587]: Loss[0.08013402697425596] has not improved from the previous [0.08013285731988022] for 1 times.\n",
      "Iteration [5588]: Loss[0.08014398080666892] has not improved from the previous [0.08013402697425596] for 3 times.\n",
      "Iteration [5596]: Loss[0.0801130680960977] has not improved from the previous [0.08011077284467331] for 1 times.\n",
      "Iteration [5597]: Loss[0.08011565746016042] has not improved from the previous [0.0801130680960977] for 3 times.\n",
      "Iteration [5601]: Loss[0.08010219539028411] has not improved from the previous [0.08010197370158965] for 1 times.\n",
      "Iteration [5607]: Loss[0.08009330747748696] has not improved from the previous [0.08008807206247527] for 1 times.\n",
      "Iteration [5611]: Loss[0.08007917011001613] has not improved from the previous [0.0800782970644072] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5600 Loss 0.08010197370158965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [5614]: Loss[0.08007261679702503] has not improved from the previous [0.08006990833084586] for 1 times.\n",
      "Iteration [5615]: Loss[0.08008103676976012] has not improved from the previous [0.08007261679702503] for 3 times.\n",
      "Iteration [5623]: Loss[0.08005077542276197] has not improved from the previous [0.08004826044112956] for 1 times.\n",
      "Iteration [5624]: Loss[0.08005306786252404] has not improved from the previous [0.08005077542276197] for 3 times.\n",
      "Iteration [5627]: Loss[0.08004049529778129] has not improved from the previous [0.0800401272243683] for 1 times.\n",
      "Iteration [5631]: Loss[0.08003060033388328] has not improved from the previous [0.0800302957435563] for 1 times.\n",
      "Iteration [5634]: Loss[0.08002566168348295] has not improved from the previous [0.0800256125248157] for 1 times.\n",
      "Iteration [5637]: Loss[0.08002028958664771] has not improved from the previous [0.08001627566113516] for 1 times.\n",
      "Iteration [5638]: Loss[0.08002768958207244] has not improved from the previous [0.08002028958664771] for 3 times.\n",
      "Iteration [5646]: Loss[0.07999850014541544] has not improved from the previous [0.0799982506964697] for 1 times.\n",
      "Iteration [5649]: Loss[0.07999110008785344] has not improved from the previous [0.07998886707313925] for 1 times.\n",
      "Iteration [5650]: Loss[0.0799934446502397] has not improved from the previous [0.07999110008785344] for 3 times.\n",
      "Iteration [5653]: Loss[0.0799807210418072] has not improved from the previous [0.07998070843829617] for 1 times.\n",
      "Iteration [5657]: Loss[0.07997304253481954] has not improved from the previous [0.07997048832302998] for 1 times.\n",
      "Iteration [5658]: Loss[0.07997517901653294] has not improved from the previous [0.07997304253481954] for 3 times.\n",
      "Iteration [5661]: Loss[0.07996289200874347] has not improved from the previous [0.07996244854622234] for 1 times.\n",
      "Iteration [5665]: Loss[0.07995509996975154] has not improved from the previous [0.07995225889002719] for 1 times.\n",
      "Iteration [5666]: Loss[0.07995673668293816] has not improved from the previous [0.07995509996975154] for 3 times.\n",
      "Iteration [5676]: Loss[0.079930320210789] has not improved from the previous [0.07992758504168236] for 1 times.\n",
      "Iteration [5677]: Loss[0.07993208386578536] has not improved from the previous [0.079930320210789] for 3 times.\n",
      "Iteration [5687]: Loss[0.0799047376114924] has not improved from the previous [0.07990288175103408] for 1 times.\n",
      "Iteration [5688]: Loss[0.07990745828561208] has not improved from the previous [0.0799047376114924] for 3 times.\n",
      "Iteration [5692]: Loss[0.0798933067311612] has not improved from the previous [0.07989300939265913] for 1 times.\n",
      "Iteration [5696]: Loss[0.07988348235913569] has not improved from the previous [0.07988298996716701] for 1 times.\n",
      "Iteration [5699]: Loss[0.07988293176851964] has not improved from the previous [0.07987867172211753] for 1 times.\n",
      "Iteration [5703]: Loss[0.07986853054070389] has not improved from the previous [0.07986818077910303] for 1 times.\n",
      "Iteration [5707]: Loss[0.0798604055277344] has not improved from the previous [0.07985788360247985] for 1 times.\n",
      "Iteration [5708]: Loss[0.07986236414254427] has not improved from the previous [0.0798604055277344] for 3 times.\n",
      "Iteration [5712]: Loss[0.07984893750530513] has not improved from the previous [0.07984795979980049] for 1 times.\n",
      "Iteration [5716]: Loss[0.07984209401114453] has not improved from the previous [0.07983797197600256] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5700 Loss 0.0798753822625551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [5719]: Loss[0.07983490137000097] has not improved from the previous [0.07983150152126142] for 1 times.\n",
      "Iteration [5720]: Loss[0.07983548721231776] has not improved from the previous [0.07983490137000097] for 3 times.\n",
      "Iteration [5738]: Loss[0.07979030039583211] has not improved from the previous [0.0797896066230237] for 1 times.\n",
      "Iteration [5741]: Loss[0.07978616675544319] has not improved from the previous [0.07978366856902937] for 1 times.\n",
      "Iteration [5742]: Loss[0.0797873536256192] has not improved from the previous [0.07978616675544319] for 3 times.\n",
      "Iteration [5746]: Loss[0.07977336341329766] has not improved from the previous [0.07977277963795726] for 1 times.\n",
      "Iteration [5747]: Loss[0.07977660514508411] has not improved from the previous [0.07977336341329766] for 3 times.\n",
      "Iteration [5751]: Loss[0.07976243242730542] has not improved from the previous [0.07976213019120204] for 1 times.\n",
      "Iteration [5758]: Loss[0.07974671069138259] has not improved from the previous [0.07974629777307844] for 1 times.\n",
      "Iteration [5763]: Loss[0.07973571817236409] has not improved from the previous [0.079734912015579] for 1 times.\n",
      "Iteration [5766]: Loss[0.07973089808111049] has not improved from the previous [0.07972854021261766] for 1 times.\n",
      "Iteration [5767]: Loss[0.07973217919319783] has not improved from the previous [0.07973089808111049] for 3 times.\n",
      "Iteration [5771]: Loss[0.07971928294510586] has not improved from the previous [0.07971762761028656] for 1 times.\n",
      "Iteration [5779]: Loss[0.07970246150284922] has not improved from the previous [0.07970024187119522] for 1 times.\n",
      "Iteration [5780]: Loss[0.07970380135677194] has not improved from the previous [0.07970246150284922] for 3 times.\n",
      "Iteration [5784]: Loss[0.07969125723812304] has not improved from the previous [0.07968926974370188] for 1 times.\n",
      "Iteration [5792]: Loss[0.07967419205621507] has not improved from the previous [0.07967196792865071] for 1 times.\n",
      "Iteration [5793]: Loss[0.07967533831258618] has not improved from the previous [0.07967419205621507] for 3 times.\n",
      "Iteration [5797]: Loss[0.07966324750027712] has not improved from the previous [0.07966092031451788] for 1 times.\n",
      "Iteration [5802]: Loss[0.07965363058267504] has not improved from the previous [0.07965101918279827] for 1 times.\n",
      "Iteration [5805]: Loss[0.07964712256945983] has not improved from the previous [0.07964373953883574] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5800 Loss 0.07965291378067389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [5812]: Loss[0.07963034058616747] has not improved from the previous [0.07962906379443528] for 1 times.\n",
      "Iteration [5817]: Loss[0.07962143296873385] has not improved from the previous [0.07961836143098286] for 1 times.\n",
      "Iteration [5824]: Loss[0.07960572912743553] has not improved from the previous [0.07960318560311488] for 1 times.\n",
      "Iteration [5825]: Loss[0.07960652706869616] has not improved from the previous [0.07960572912743553] for 3 times.\n",
      "Iteration [5833]: Loss[0.07958534793696662] has not improved from the previous [0.07958450204756108] for 1 times.\n",
      "Iteration [5838]: Loss[0.07957654031813823] has not improved from the previous [0.07957321170431776] for 1 times.\n",
      "Iteration [5845]: Loss[0.07955984574826806] has not improved from the previous [0.07955829705156693] for 1 times.\n",
      "Iteration [5850]: Loss[0.07955066145896535] has not improved from the previous [0.07954789436210534] for 1 times.\n",
      "Iteration [5854]: Loss[0.07954034185875011] has not improved from the previous [0.07953938439559724] for 1 times.\n",
      "Iteration [5861]: Loss[0.07952646018524692] has not improved from the previous [0.0795246541687049] for 1 times.\n",
      "Iteration [5862]: Loss[0.07952772769397741] has not improved from the previous [0.07952646018524692] for 3 times.\n",
      "Iteration [5867]: Loss[0.07951381365778523] has not improved from the previous [0.07951181841876648] for 1 times.\n",
      "Iteration [5872]: Loss[0.07950409388838409] has not improved from the previous [0.07950156115001415] for 1 times.\n",
      "Iteration [5876]: Loss[0.07949642050984973] has not improved from the previous [0.07949285085212922] for 1 times.\n",
      "Iteration [5880]: Loss[0.07948832390605898] has not improved from the previous [0.07948468244343575] for 1 times.\n",
      "Iteration [5884]: Loss[0.07947699057697231] has not improved from the previous [0.07947613952528569] for 1 times.\n",
      "Iteration [5888]: Loss[0.07946852022882614] has not improved from the previous [0.07946843376552896] for 1 times.\n",
      "Iteration [5895]: Loss[0.07945411062872394] has not improved from the previous [0.07945332630931755] for 1 times.\n",
      "Iteration [5905]: Loss[0.07943544999004817] has not improved from the previous [0.07943266181374227] for 1 times.\n",
      "Iteration [5909]: Loss[0.07942774351857199] has not improved from the previous [0.07942387735649116] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5900 Loss 0.07944379526759521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [5922]: Loss[0.0793994218478289] has not improved from the previous [0.07939771115645464] for 1 times.\n",
      "Iteration [5923]: Loss[0.07940010019105212] has not improved from the previous [0.0793994218478289] for 3 times.\n",
      "Iteration [5933]: Loss[0.07937518427597048] has not improved from the previous [0.07937513047344445] for 1 times.\n",
      "Iteration [5937]: Loss[0.07936819627250341] has not improved from the previous [0.07936668136674008] for 1 times.\n",
      "Iteration [5941]: Loss[0.07935954014108378] has not improved from the previous [0.07935846532017127] for 1 times.\n",
      "Iteration [5945]: Loss[0.07935053321865249] has not improved from the previous [0.0793502895119696] for 1 times.\n",
      "Iteration [5949]: Loss[0.07934243984259591] has not improved from the previous [0.07934187904304174] for 1 times.\n",
      "Iteration [5954]: Loss[0.0793327215060056] has not improved from the previous [0.07933213976424554] for 1 times.\n",
      "Iteration [5958]: Loss[0.07932399990364843] has not improved from the previous [0.07932389828631123] for 1 times.\n",
      "Iteration [5962]: Loss[0.07931550688836742] has not improved from the previous [0.0793153286937474] for 1 times.\n",
      "Iteration [5967]: Loss[0.07930589599524494] has not improved from the previous [0.07930542213210545] for 1 times.\n",
      "Iteration [5971]: Loss[0.07929756485192041] has not improved from the previous [0.07929700478427415] for 1 times.\n",
      "Iteration [5976]: Loss[0.079287702899115] has not improved from the previous [0.0792870546193648] for 1 times.\n",
      "Iteration [5980]: Loss[0.07927902007805905] has not improved from the previous [0.07927880451215993] for 1 times.\n",
      "Iteration [5985]: Loss[0.07926953933195294] has not improved from the previous [0.07926875854684842] for 1 times.\n",
      "Iteration [5990]: Loss[0.07925973239392743] has not improved from the previous [0.0792586573432435] for 1 times.\n",
      "Iteration [5995]: Loss[0.07924971454996013] has not improved from the previous [0.07924848365387978] for 1 times.\n",
      "Iteration [6000]: Loss[0.07923998611580452] has not improved from the previous [0.07923838741196006] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6000 Loss 0.07923998611580452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [6024]: Loss[0.07919272112506963] has not improved from the previous [0.07918978533646301] for 1 times.\n",
      "Iteration [6029]: Loss[0.07918243248949026] has not improved from the previous [0.07918029655042957] for 1 times.\n",
      "Iteration [6034]: Loss[0.07917200065899958] has not improved from the previous [0.07917041414589457] for 1 times.\n",
      "Iteration [6039]: Loss[0.07916203230323832] has not improved from the previous [0.07916069132304954] for 1 times.\n",
      "Iteration [6044]: Loss[0.07915207132521682] has not improved from the previous [0.07915084031077721] for 1 times.\n",
      "Iteration [6049]: Loss[0.07914161982984588] has not improved from the previous [0.07914088888920952] for 1 times.\n",
      "Iteration [6054]: Loss[0.07913167200004424] has not improved from the previous [0.07913092952642974] for 1 times.\n",
      "Iteration [6059]: Loss[0.07912155982757232] has not improved from the previous [0.0791210212640615] for 1 times.\n",
      "Iteration [6064]: Loss[0.07911140949998549] has not improved from the previous [0.07911123644451658] for 1 times.\n",
      "Iteration [6069]: Loss[0.07910154889734956] has not improved from the previous [0.07910122679184116] for 1 times.\n",
      "Iteration [6073]: Loss[0.07909560660801954] has not improved from the previous [0.07909281223497289] for 1 times.\n",
      "Iteration [6078]: Loss[0.07908469606066088] has not improved from the previous [0.07908312198334616] for 1 times.\n",
      "Iteration [6083]: Loss[0.07907422685568215] has not improved from the previous [0.0790733600677946] for 1 times.\n",
      "Iteration [6088]: Loss[0.07906427575125309] has not improved from the previous [0.07906354327439864] for 1 times.\n",
      "Iteration [6098]: Loss[0.07904560695002691] has not improved from the previous [0.07904422870714903] for 1 times.\n",
      "Iteration [6103]: Loss[0.07903478674472182] has not improved from the previous [0.0790345262750715] for 1 times.\n",
      "Iteration [6108]: Loss[0.07902506362884466] has not improved from the previous [0.07902444881756468] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6100 Loss 0.07903855463136296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [6113]: Loss[0.07901708026752437] has not improved from the previous [0.07901493470211521] for 1 times.\n",
      "Iteration [6118]: Loss[0.07900592937564585] has not improved from the previous [0.07900484721465681] for 1 times.\n",
      "Iteration [6123]: Loss[0.07899613770493692] has not improved from the previous [0.07899502117631477] for 1 times.\n",
      "Iteration [6128]: Loss[0.07898777778069126] has not improved from the previous [0.07898557349293847] for 1 times.\n",
      "Iteration [6133]: Loss[0.07897637138187756] has not improved from the previous [0.07897588928810023] for 1 times.\n",
      "Iteration [6138]: Loss[0.07896861294162334] has not improved from the previous [0.07896634867201835] for 1 times.\n",
      "Iteration [6143]: Loss[0.07895708783189966] has not improved from the previous [0.07895656930594035] for 1 times.\n",
      "Iteration [6149]: Loss[0.07894552882986905] has not improved from the previous [0.0789453737751161] for 1 times.\n",
      "Iteration [6154]: Loss[0.07893710861885606] has not improved from the previous [0.078935909270336] for 1 times.\n",
      "Iteration [6159]: Loss[0.07892621527548255] has not improved from the previous [0.07892602101157721] for 1 times.\n",
      "Iteration [6164]: Loss[0.07891785809560724] has not improved from the previous [0.07891646983770868] for 1 times.\n",
      "Iteration [6169]: Loss[0.07890722502840784] has not improved from the previous [0.07890650498519129] for 1 times.\n",
      "Iteration [6174]: Loss[0.07889863134887322] has not improved from the previous [0.07889687906688587] for 1 times.\n",
      "Iteration [6185]: Loss[0.07887741902505538] has not improved from the previous [0.07887638714168592] for 1 times.\n",
      "Iteration [6190]: Loss[0.07886716299118936] has not improved from the previous [0.07886644743764482] for 1 times.\n",
      "Iteration [6195]: Loss[0.07885792420939486] has not improved from the previous [0.07885670960001011] for 1 times.\n",
      "Iteration [6201]: Loss[0.0788460099127248] has not improved from the previous [0.07884592962410629] for 1 times.\n",
      "Iteration [6206]: Loss[0.07883687266864312] has not improved from the previous [0.07883653799721686] for 1 times.\n",
      "Iteration [6211]: Loss[0.07882733247212939] has not improved from the previous [0.07882631694674946] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6200 Loss 0.07884592962410629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [6216]: Loss[0.07881776875565683] has not improved from the previous [0.07881687471153147] for 1 times.\n",
      "Iteration [6221]: Loss[0.07880898019731895] has not improved from the previous [0.0788073910185488] for 1 times.\n",
      "Iteration [6227]: Loss[0.07879691574929873] has not improved from the previous [0.07879640453266185] for 1 times.\n",
      "Iteration [6232]: Loss[0.07878743300383705] has not improved from the previous [0.07878665967546661] for 1 times.\n",
      "Iteration [6237]: Loss[0.0787787387620766] has not improved from the previous [0.07877718176889237] for 1 times.\n",
      "Iteration [6243]: Loss[0.0787668163671259] has not improved from the previous [0.07876634120722195] for 1 times.\n",
      "Iteration [6248]: Loss[0.0787574435211877] has not improved from the previous [0.07875671762022186] for 1 times.\n",
      "Iteration [6253]: Loss[0.078748157927788] has not improved from the previous [0.07874707814655126] for 1 times.\n",
      "Iteration [6258]: Loss[0.07873932172278567] has not improved from the previous [0.07873779952575817] for 1 times.\n",
      "Iteration [6265]: Loss[0.07872560097382882] has not improved from the previous [0.07872526299535512] for 1 times.\n",
      "Iteration [6270]: Loss[0.07871622762248152] has not improved from the previous [0.07871554191899253] for 1 times.\n",
      "Iteration [6275]: Loss[0.07870696963408692] has not improved from the previous [0.07870635917043241] for 1 times.\n",
      "Iteration [6280]: Loss[0.07869766558464879] has not improved from the previous [0.07869716198823092] for 1 times.\n",
      "Iteration [6286]: Loss[0.07868651795255914] has not improved from the previous [0.0786862900150329] for 1 times.\n",
      "Iteration [6292]: Loss[0.07867556032499852] has not improved from the previous [0.07867482766064748] for 1 times.\n",
      "Iteration [6297]: Loss[0.07866637251868393] has not improved from the previous [0.07866526903278001] for 1 times.\n",
      "Iteration [6303]: Loss[0.07865502345628042] has not improved from the previous [0.07865490157511675] for 1 times.\n",
      "Iteration [6308]: Loss[0.07864597439979548] has not improved from the previous [0.07864547754058482] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6300 Loss 0.07865792848358794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [6319]: Loss[0.07862556254130748] has not improved from the previous [0.07862505489017325] for 1 times.\n",
      "Iteration [6325]: Loss[0.07861455919109368] has not improved from the previous [0.07861429723924228] for 1 times.\n",
      "Iteration [6330]: Loss[0.07860549340893468] has not improved from the previous [0.07860470483902238] for 1 times.\n",
      "Iteration [6336]: Loss[0.07859457913108907] has not improved from the previous [0.0785940478932184] for 1 times.\n",
      "Iteration [6341]: Loss[0.07858558168447356] has not improved from the previous [0.07858424564715487] for 1 times.\n",
      "Iteration [6347]: Loss[0.07857441032417177] has not improved from the previous [0.07857389302191209] for 1 times.\n",
      "Iteration [6353]: Loss[0.07856332674768646] has not improved from the previous [0.07856296397583751] for 1 times.\n",
      "Iteration [6359]: Loss[0.07855256617634378] has not improved from the previous [0.07855210029135683] for 1 times.\n",
      "Iteration [6364]: Loss[0.07854364687434996] has not improved from the previous [0.07854237267060457] for 1 times.\n",
      "Iteration [6370]: Loss[0.07853253634237563] has not improved from the previous [0.07853203004628025] for 1 times.\n",
      "Iteration [6376]: Loss[0.0785217454264914] has not improved from the previous [0.07852125358724846] for 1 times.\n",
      "Iteration [6382]: Loss[0.07851091854123313] has not improved from the previous [0.07851043751522169] for 1 times.\n",
      "Iteration [6388]: Loss[0.07849993332812477] has not improved from the previous [0.07849983114482421] for 1 times.\n",
      "Iteration [6394]: Loss[0.07848924365197843] has not improved from the previous [0.07848901493797486] for 1 times.\n",
      "Iteration [6399]: Loss[0.0784803762911605] has not improved from the previous [0.07847930048176915] for 1 times.\n",
      "Iteration [6405]: Loss[0.07846951799976658] has not improved from the previous [0.07846855836718325] for 1 times.\n",
      "Iteration [6411]: Loss[0.07845852923441408] has not improved from the previous [0.07845802603966494] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6400 Loss 0.07847660129894138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [6417]: Loss[0.0784480178653294] has not improved from the previous [0.07844724632556503] for 1 times.\n",
      "Iteration [6423]: Loss[0.07843718284852655] has not improved from the previous [0.07843664041794629] for 1 times.\n",
      "Iteration [6429]: Loss[0.07842634384545326] has not improved from the previous [0.07842579527696741] for 1 times.\n",
      "Iteration [6435]: Loss[0.07841586192551493] has not improved from the previous [0.07841503332325847] for 1 times.\n",
      "Iteration [6441]: Loss[0.07840494294333335] has not improved from the previous [0.07840439935485315] for 1 times.\n",
      "Iteration [6447]: Loss[0.07839444335192176] has not improved from the previous [0.07839365251238123] for 1 times.\n",
      "Iteration [6453]: Loss[0.07838375252386345] has not improved from the previous [0.07838289494176477] for 1 times.\n",
      "Iteration [6459]: Loss[0.07837288760242879] has not improved from the previous [0.07837229593501009] for 1 times.\n",
      "Iteration [6465]: Loss[0.0783624282797097] has not improved from the previous [0.07836129884672279] for 1 times.\n",
      "Iteration [6471]: Loss[0.07835178539641727] has not improved from the previous [0.07835074911369207] for 1 times.\n",
      "Iteration [6478]: Loss[0.07833942596070216] has not improved from the previous [0.07833887992748487] for 1 times.\n",
      "Iteration [6484]: Loss[0.07832876326121703] has not improved from the previous [0.07832834146279717] for 1 times.\n",
      "Iteration [6490]: Loss[0.07831826357425828] has not improved from the previous [0.07831737993746112] for 1 times.\n",
      "Iteration [6496]: Loss[0.07830769595655487] has not improved from the previous [0.07830673624730117] for 1 times.\n",
      "Iteration [6502]: Loss[0.07829709176314044] has not improved from the previous [0.078296189412412] for 1 times.\n",
      "Iteration [6509]: Loss[0.07828484449000737] has not improved from the previous [0.07828457353147333] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6500 Loss 0.07829786117804532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [6515]: Loss[0.07827429961199757] has not improved from the previous [0.07827352392935796] for 1 times.\n",
      "Iteration [6521]: Loss[0.07826395459753059] has not improved from the previous [0.07826274999227872] for 1 times.\n",
      "Iteration [6528]: Loss[0.07825168434507387] has not improved from the previous [0.07825111941479514] for 1 times.\n",
      "Iteration [6534]: Loss[0.07824121697491478] has not improved from the previous [0.07824041604589847] for 1 times.\n",
      "Iteration [6540]: Loss[0.07823066604329017] has not improved from the previous [0.07822967466166016] for 1 times.\n",
      "Iteration [6547]: Loss[0.07821844437146148] has not improved from the previous [0.07821811623375718] for 1 times.\n",
      "Iteration [6553]: Loss[0.0782082125223706] has not improved from the previous [0.07820722087288404] for 1 times.\n",
      "Iteration [6560]: Loss[0.07819580108609327] has not improved from the previous [0.07819548192054167] for 1 times.\n",
      "Iteration [6566]: Loss[0.07818574376511027] has not improved from the previous [0.07818468873334808] for 1 times.\n",
      "Iteration [6573]: Loss[0.0781733594906194] has not improved from the previous [0.0781729087571165] for 1 times.\n",
      "Iteration [6580]: Loss[0.07816132489328907] has not improved from the previous [0.07816124795169929] for 1 times.\n",
      "Iteration [6586]: Loss[0.07815117492391294] has not improved from the previous [0.07815024843392186] for 1 times.\n",
      "Iteration [6593]: Loss[0.07813887512586282] has not improved from the previous [0.0781386463032939] for 1 times.\n",
      "Iteration [6599]: Loss[0.07812894277552555] has not improved from the previous [0.07812772477631821] for 1 times.\n",
      "Iteration [6606]: Loss[0.07811664456148351] has not improved from the previous [0.07811594677402392] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6600 Loss 0.07812514604396355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [6612]: Loss[0.07810667226480644] has not improved from the previous [0.07810535670948891] for 1 times.\n",
      "Iteration [6619]: Loss[0.07809450099194787] has not improved from the previous [0.07809348920586856] for 1 times.\n",
      "Iteration [6626]: Loss[0.07808246814281006] has not improved from the previous [0.07808170778937068] for 1 times.\n",
      "Iteration [6633]: Loss[0.0780704882913034] has not improved from the previous [0.0780701747377053] for 1 times.\n",
      "Iteration [6639]: Loss[0.07806045285495322] has not improved from the previous [0.0780591398802966] for 1 times.\n",
      "Iteration [6646]: Loss[0.07804840844765785] has not improved from the previous [0.07804766979308471] for 1 times.\n",
      "Iteration [6653]: Loss[0.07803647686265947] has not improved from the previous [0.0780357287794794] for 1 times.\n",
      "Iteration [6660]: Loss[0.07802469805723163] has not improved from the previous [0.07802387167942727] for 1 times.\n",
      "Iteration [6667]: Loss[0.07801281252994956] has not improved from the previous [0.07801223102063765] for 1 times.\n",
      "Iteration [6674]: Loss[0.07800088922636243] has not improved from the previous [0.07800014850313858] for 1 times.\n",
      "Iteration [6681]: Loss[0.07798912487661093] has not improved from the previous [0.07798831470277891] for 1 times.\n",
      "Iteration [6688]: Loss[0.07797716569407484] has not improved from the previous [0.07797665102548094] for 1 times.\n",
      "Iteration [6695]: Loss[0.07796545571539931] has not improved from the previous [0.07796485716303543] for 1 times.\n",
      "Iteration [6702]: Loss[0.07795368449633237] has not improved from the previous [0.07795269913439411] for 1 times.\n",
      "Iteration [6709]: Loss[0.07794172908717822] has not improved from the previous [0.07794119034883715] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6700 Loss 0.07795428028982468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [6716]: Loss[0.07793012056749893] has not improved from the previous [0.07792923015329747] for 1 times.\n",
      "Iteration [6723]: Loss[0.07791849509495762] has not improved from the previous [0.07791751015634335] for 1 times.\n",
      "Iteration [6730]: Loss[0.07790652620065358] has not improved from the previous [0.07790560850731806] for 1 times.\n",
      "Iteration [6737]: Loss[0.07789507800094378] has not improved from the previous [0.07789371941638232] for 1 times.\n",
      "Iteration [6745]: Loss[0.07788150852740248] has not improved from the previous [0.07788088808754792] for 1 times.\n",
      "Iteration [6752]: Loss[0.07786996657208899] has not improved from the previous [0.07786882140806162] for 1 times.\n",
      "Iteration [6759]: Loss[0.0778582148937816] has not improved from the previous [0.07785737630275] for 1 times.\n",
      "Iteration [6766]: Loss[0.07784676079222586] has not improved from the previous [0.07784553150937912] for 1 times.\n",
      "Iteration [6774]: Loss[0.07783336428642253] has not improved from the previous [0.07783267566809178] for 1 times.\n",
      "Iteration [6781]: Loss[0.07782198765932409] has not improved from the previous [0.07782058945253667] for 1 times.\n",
      "Iteration [6789]: Loss[0.07780864130580131] has not improved from the previous [0.07780784714711728] for 1 times.\n",
      "Iteration [6796]: Loss[0.07779715629024929] has not improved from the previous [0.07779589765149701] for 1 times.\n",
      "Iteration [6804]: Loss[0.07778396506569664] has not improved from the previous [0.07778301792573142] for 1 times.\n",
      "Iteration [6811]: Loss[0.07777234730686633] has not improved from the previous [0.07777123444887862] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6800 Loss 0.07778789642208302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [6819]: Loss[0.07775919286740962] has not improved from the previous [0.07775828794831126] for 1 times.\n",
      "Iteration [6827]: Loss[0.07774610644342998] has not improved from the previous [0.07774527444609178] for 1 times.\n",
      "Iteration [6834]: Loss[0.0777345883157046] has not improved from the previous [0.07773359341688084] for 1 times.\n",
      "Iteration [6842]: Loss[0.07772156700155616] has not improved from the previous [0.07772074578136116] for 1 times.\n",
      "Iteration [6850]: Loss[0.07770862399946153] has not improved from the previous [0.07770775020676773] for 1 times.\n",
      "Iteration [6858]: Loss[0.07769545110264384] has not improved from the previous [0.07769472409097604] for 1 times.\n",
      "Iteration [6866]: Loss[0.07768244428946762] has not improved from the previous [0.0776819310240594] for 1 times.\n",
      "Iteration [6874]: Loss[0.07766937298889397] has not improved from the previous [0.07766906288023391] for 1 times.\n",
      "Iteration [6881]: Loss[0.07765840424540038] has not improved from the previous [0.07765701859837319] for 1 times.\n",
      "Iteration [6889]: Loss[0.07764523651464293] has not improved from the previous [0.07764410202227857] for 1 times.\n",
      "Iteration [6897]: Loss[0.07763230992515052] has not improved from the previous [0.07763126208518017] for 1 times.\n",
      "Iteration [6905]: Loss[0.0776193542166] has not improved from the previous [0.07761854366717792] for 1 times.\n",
      "Iteration [6913]: Loss[0.07760651934228009] has not improved from the previous [0.07760551499607872] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6900 Loss 0.07762506021052112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [6921]: Loss[0.0775938287345468] has not improved from the previous [0.07759261878499488] for 1 times.\n",
      "Iteration [6929]: Loss[0.07758104319549038] has not improved from the previous [0.07757967783005137] for 1 times.\n",
      "Iteration [6937]: Loss[0.07756814204003702] has not improved from the previous [0.07756688037449996] for 1 times.\n",
      "Iteration [6946]: Loss[0.07755373343855369] has not improved from the previous [0.07755285059533266] for 1 times.\n",
      "Iteration [6954]: Loss[0.0775411649124696] has not improved from the previous [0.07754001772354861] for 1 times.\n",
      "Iteration [6963]: Loss[0.07752660879564367] has not improved from the previous [0.07752610637422856] for 1 times.\n",
      "Iteration [6971]: Loss[0.07751398163366603] has not improved from the previous [0.0775130004822973] for 1 times.\n",
      "Iteration [6980]: Loss[0.07749951370487344] has not improved from the previous [0.07749927122407843] for 1 times.\n",
      "Iteration [6988]: Loss[0.07748696610108317] has not improved from the previous [0.07748610702099168] for 1 times.\n",
      "Iteration [6996]: Loss[0.07747450841003489] has not improved from the previous [0.07747326560877611] for 1 times.\n",
      "Iteration [7005]: Loss[0.07746007054534987] has not improved from the previous [0.07745941842683143] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7000 Loss 0.07746557857771723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [7014]: Loss[0.07744606683178072] has not improved from the previous [0.07744538098387228] for 1 times.\n",
      "Iteration [7022]: Loss[0.07743338088513511] has not improved from the previous [0.07743229379148789] for 1 times.\n",
      "Iteration [7031]: Loss[0.07741939868731997] has not improved from the previous [0.07741825297930222] for 1 times.\n",
      "Iteration [7040]: Loss[0.07740525208690112] has not improved from the previous [0.07740443929837142] for 1 times.\n",
      "Iteration [7049]: Loss[0.07739107837045843] has not improved from the previous [0.07739043480480094] for 1 times.\n",
      "Iteration [7057]: Loss[0.07737874160051064] has not improved from the previous [0.07737754241814378] for 1 times.\n",
      "Iteration [7066]: Loss[0.07736478143628762] has not improved from the previous [0.07736355839403282] for 1 times.\n",
      "Iteration [7075]: Loss[0.07735071452004928] has not improved from the previous [0.07734941708364393] for 1 times.\n",
      "Iteration [7084]: Loss[0.07733658225309374] has not improved from the previous [0.07733553221639551] for 1 times.\n",
      "Iteration [7093]: Loss[0.07732250458271497] has not improved from the previous [0.0773216199330293] for 1 times.\n",
      "Iteration [7102]: Loss[0.07730869000783307] has not improved from the previous [0.07730749272146385] for 1 times.\n",
      "Iteration [7111]: Loss[0.0772948268036345] has not improved from the previous [0.07729342032552368] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7100 Loss 0.07730892322446224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [7121]: Loss[0.0772791914666473] has not improved from the previous [0.07727838184090995] for 1 times.\n",
      "Iteration [7130]: Loss[0.07726545612531122] has not improved from the previous [0.0772642283193923] for 1 times.\n",
      "Iteration [7140]: Loss[0.07724996760240037] has not improved from the previous [0.07724930366544497] for 1 times.\n",
      "Iteration [7149]: Loss[0.07723626888504638] has not improved from the previous [0.07723514255355526] for 1 times.\n",
      "Iteration [7158]: Loss[0.07722247564067516] has not improved from the previous [0.07722110152009214] for 1 times.\n",
      "Iteration [7168]: Loss[0.07720712483588756] has not improved from the previous [0.07720588849037534] for 1 times.\n",
      "Iteration [7178]: Loss[0.07719171714341035] has not improved from the previous [0.07719106087771506] for 1 times.\n",
      "Iteration [7187]: Loss[0.07717813847829247] has not improved from the previous [0.07717683412172022] for 1 times.\n",
      "Iteration [7197]: Loss[0.07716288125257846] has not improved from the previous [0.07716203989007739] for 1 times.\n",
      "Iteration [7207]: Loss[0.07714761469992872] has not improved from the previous [0.07714683756931313] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7200 Loss 0.0771557764329277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [7217]: Loss[0.07713261125394905] has not improved from the previous [0.0771316535700111] for 1 times.\n",
      "Iteration [7227]: Loss[0.07711752974214633] has not improved from the previous [0.07711646318502212] for 1 times.\n",
      "Iteration [7237]: Loss[0.07710236477440442] has not improved from the previous [0.07710152416836154] for 1 times.\n",
      "Iteration [7247]: Loss[0.07708726908070929] has not improved from the previous [0.07708663989932162] for 1 times.\n",
      "Iteration [7257]: Loss[0.07707226764031014] has not improved from the previous [0.07707136041683026] for 1 times.\n",
      "Iteration [7267]: Loss[0.07705741595037974] has not improved from the previous [0.0770562405452953] for 1 times.\n",
      "Iteration [7277]: Loss[0.0770424626889284] has not improved from the previous [0.07704140388827077] for 1 times.\n",
      "Iteration [7287]: Loss[0.0770274395133309] has not improved from the previous [0.07702638482131663] for 1 times.\n",
      "Iteration [7297]: Loss[0.07701262170174861] has not improved from the previous [0.07701139757403365] for 1 times.\n",
      "Iteration [7308]: Loss[0.07699621603903105] has not improved from the previous [0.07699522199327044] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7300 Loss 0.07700579816908348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [7318]: Loss[0.07698138538724168] has not improved from the previous [0.07698026520067888] for 1 times.\n",
      "Iteration [7329]: Loss[0.07696521285020903] has not improved from the previous [0.07696417887645646] for 1 times.\n",
      "Iteration [7339]: Loss[0.07695035835993445] has not improved from the previous [0.07694926762831628] for 1 times.\n",
      "Iteration [7350]: Loss[0.07693428881079552] has not improved from the previous [0.07693309378411452] for 1 times.\n",
      "Iteration [7361]: Loss[0.07691804097910357] has not improved from the previous [0.07691699313927879] for 1 times.\n",
      "Iteration [7372]: Loss[0.07690179178171692] has not improved from the previous [0.07690100474313201] for 1 times.\n",
      "Iteration [7383]: Loss[0.07688589797896359] has not improved from the previous [0.0768847392330543] for 1 times.\n",
      "Iteration [7394]: Loss[0.07686986267986906] has not improved from the previous [0.07686888181052713] for 1 times.\n",
      "Iteration [7405]: Loss[0.0768536966448833] has not improved from the previous [0.07685288697986288] for 1 times.\n",
      "Iteration [7416]: Loss[0.07683779280477389] has not improved from the previous [0.07683685245875534] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7400 Loss 0.0768583449088585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [7427]: Loss[0.07682187298758618] has not improved from the previous [0.07682100426838935] for 1 times.\n",
      "Iteration [7438]: Loss[0.07680601734201566] has not improved from the previous [0.07680493239731416] for 1 times.\n",
      "Iteration [7450]: Loss[0.0767888174190664] has not improved from the previous [0.07678782465747595] for 1 times.\n",
      "Iteration [7461]: Loss[0.07677304853365281] has not improved from the previous [0.07677174407586992] for 1 times.\n",
      "Iteration [7473]: Loss[0.07675579658153187] has not improved from the previous [0.07675485397549535] for 1 times.\n",
      "Iteration [7484]: Loss[0.07674000341578627] has not improved from the previous [0.0767388956252458] for 1 times.\n",
      "Iteration [7496]: Loss[0.07672283919791452] has not improved from the previous [0.0767218999429342] for 1 times.\n",
      "Iteration [7508]: Loss[0.07670590475031205] has not improved from the previous [0.07670483726410864] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7500 Loss 0.07671472343867036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [7519]: Loss[0.07669028812844049] has not improved from the previous [0.0766888286426215] for 1 times.\n",
      "Iteration [7531]: Loss[0.07667307719505133] has not improved from the previous [0.0766720858078086] for 1 times.\n",
      "Iteration [7543]: Loss[0.07665630256094925] has not improved from the previous [0.07665493950125245] for 1 times.\n",
      "Iteration [7555]: Loss[0.07663933631536272] has not improved from the previous [0.076638197719134] for 1 times.\n",
      "Iteration [7567]: Loss[0.07662236852463428] has not improved from the previous [0.07662119241772015] for 1 times.\n",
      "Iteration [7579]: Loss[0.07660570519651466] has not improved from the previous [0.0766042855956979] for 1 times.\n",
      "Iteration [7591]: Loss[0.07658871322182922] has not improved from the previous [0.07658750310359529] for 1 times.\n",
      "Iteration [7604]: Loss[0.07657057882648022] has not improved from the previous [0.07656943723165927] for 1 times.\n",
      "Iteration [7616]: Loss[0.07655385274015254] has not improved from the previous [0.07655256036421088] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7600 Loss 0.07657350396185746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [7629]: Loss[0.07653574534219228] has not improved from the previous [0.07653465060585814] for 1 times.\n",
      "Iteration [7642]: Loss[0.07651780780948318] has not improved from the previous [0.07651682362246905] for 1 times.\n",
      "Iteration [7654]: Loss[0.07650116083344262] has not improved from the previous [0.07649989653664675] for 1 times.\n",
      "Iteration [7668]: Loss[0.07648193303796495] has not improved from the previous [0.07648088656891842] for 1 times.\n",
      "Iteration [7680]: Loss[0.07646539276177186] has not improved from the previous [0.07646419958278187] for 1 times.\n",
      "Iteration [7693]: Loss[0.07644754911795398] has not improved from the previous [0.07644620141414905] for 1 times.\n",
      "Iteration [7706]: Loss[0.0764297367360047] has not improved from the previous [0.07642841112363016] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7700 Loss 0.07643523214275408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [7720]: Loss[0.076410592330814] has not improved from the previous [0.07640957459496328] for 1 times.\n",
      "Iteration [7733]: Loss[0.07639296965962156] has not improved from the previous [0.07639176084435387] for 1 times.\n",
      "Iteration [7746]: Loss[0.07637518605065029] has not improved from the previous [0.07637390843352133] for 1 times.\n",
      "Iteration [7759]: Loss[0.07635754316842469] has not improved from the previous [0.07635630794323416] for 1 times.\n",
      "Iteration [7773]: Loss[0.07633870296162497] has not improved from the previous [0.07633748737519575] for 1 times.\n",
      "Iteration [7787]: Loss[0.07631991250690613] has not improved from the previous [0.07631867500353255] for 1 times.\n",
      "Iteration [7800]: Loss[0.07630233869570548] has not improved from the previous [0.07630087272821856] for 1 times.\n",
      "Iteration [7814]: Loss[0.07628364871027904] has not improved from the previous [0.07628230391096291] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7800 Loss 0.07630233869570548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [7828]: Loss[0.07626485935142609] has not improved from the previous [0.07626369661988133] for 1 times.\n",
      "Iteration [7842]: Loss[0.07624610142935619] has not improved from the previous [0.07624498448104032] for 1 times.\n",
      "Iteration [7856]: Loss[0.07622748413176933] has not improved from the previous [0.07622619008656517] for 1 times.\n",
      "Iteration [7870]: Loss[0.07620902285947395] has not improved from the previous [0.07620746463323125] for 1 times.\n",
      "Iteration [7885]: Loss[0.0761890238447863] has not improved from the previous [0.07618803881574712] for 1 times.\n",
      "Iteration [7899]: Loss[0.0761706211941066] has not improved from the previous [0.07616927106656203] for 1 times.\n",
      "Iteration [7914]: Loss[0.07615084553990258] has not improved from the previous [0.07614988499158509] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7900 Loss 0.0761674028085154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [7928]: Loss[0.07613243253937008] has not improved from the previous [0.07613119282413534] for 1 times.\n",
      "Iteration [7943]: Loss[0.07611290267095853] has not improved from the previous [0.0761117913692011] for 1 times.\n",
      "Iteration [7958]: Loss[0.0760933822220365] has not improved from the previous [0.07609196875828679] for 1 times.\n",
      "Iteration [7972]: Loss[0.07607513478029143] has not improved from the previous [0.0760737272162733] for 1 times.\n",
      "Iteration [7988]: Loss[0.07605434229968311] has not improved from the previous [0.07605330524521721] for 1 times.\n",
      "Iteration [8002]: Loss[0.0760360871366233] has not improved from the previous [0.07603477586399904] for 1 times.\n",
      "Iteration [8017]: Loss[0.07601690320642407] has not improved from the previous [0.0760154512038119] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8000 Loss 0.07603607205490696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [8033]: Loss[0.07599621145815105] has not improved from the previous [0.0759951370658661] for 1 times.\n",
      "Iteration [8048]: Loss[0.07597712685710048] has not improved from the previous [0.07597563518466886] for 1 times.\n",
      "Iteration [8063]: Loss[0.075957774707611] has not improved from the previous [0.07595647426789237] for 1 times.\n",
      "Iteration [8079]: Loss[0.07593740980648189] has not improved from the previous [0.0759361395659157] for 1 times.\n",
      "Iteration [8094]: Loss[0.07591827263617679] has not improved from the previous [0.07591703036500626] for 1 times.\n",
      "Iteration [8110]: Loss[0.0758979581384666] has not improved from the previous [0.07589674665450956] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8100 Loss 0.07590803281226483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [8125]: Loss[0.07587910187246667] has not improved from the previous [0.07587756513367698] for 1 times.\n",
      "Iteration [8141]: Loss[0.07585892292336566] has not improved from the previous [0.07585740748885558] for 1 times.\n",
      "Iteration [8157]: Loss[0.07583874150278047] has not improved from the previous [0.07583725273079438] for 1 times.\n",
      "Iteration [8174]: Loss[0.0758173231943654] has not improved from the previous [0.07581622722838599] for 1 times.\n",
      "Iteration [8190]: Loss[0.07579736481277576] has not improved from the previous [0.07579604975465684] for 1 times.\n",
      "Iteration [8206]: Loss[0.0757775005663862] has not improved from the previous [0.0757761250009658] for 1 times.\n",
      "Iteration [8222]: Loss[0.07575766515302748] has not improved from the previous [0.07575605488310239] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8200 Loss 0.07578220031285952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [8239]: Loss[0.07573657756757404] has not improved from the previous [0.07573527042238175] for 1 times.\n",
      "Iteration [8255]: Loss[0.07571674570748307] has not improved from the previous [0.0757153226976436] for 1 times.\n",
      "Iteration [8272]: Loss[0.07569589629759432] has not improved from the previous [0.07569436565874037] for 1 times.\n",
      "Iteration [8289]: Loss[0.07567500954912518] has not improved from the previous [0.07567368057958687] for 1 times.\n",
      "Iteration [8306]: Loss[0.07565419488486205] has not improved from the previous [0.07565282886905053] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8300 Loss 0.07565873355653577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [8323]: Loss[0.07563346246003277] has not improved from the previous [0.07563218436058684] for 1 times.\n",
      "Iteration [8340]: Loss[0.07561274398319486] has not improved from the previous [0.07561122524222398] for 1 times.\n",
      "Iteration [8357]: Loss[0.07559214698251888] has not improved from the previous [0.0755906326099813] for 1 times.\n",
      "Iteration [8375]: Loss[0.07557044396867715] has not improved from the previous [0.07556910882760193] for 1 times.\n",
      "Iteration [8392]: Loss[0.07554987612939264] has not improved from the previous [0.0755484808685431] for 1 times.\n",
      "Iteration [8409]: Loss[0.07552954184488544] has not improved from the previous [0.0755279362482503] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8400 Loss 0.07553757779779212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [8427]: Loss[0.07550795702900134] has not improved from the previous [0.07550637353373371] for 1 times.\n",
      "Iteration [8445]: Loss[0.07548653037707397] has not improved from the previous [0.07548499590066848] for 1 times.\n",
      "Iteration [8463]: Loss[0.07546508605517455] has not improved from the previous [0.07546346527460332] for 1 times.\n",
      "Iteration [8481]: Loss[0.07544377127037828] has not improved from the previous [0.07544217796361444] for 1 times.\n",
      "Iteration [8499]: Loss[0.07542241556774971] has not improved from the previous [0.07542077177939888] for 1 times.\n",
      "Iteration [8517]: Loss[0.07540123121504456] has not improved from the previous [0.07539957998797314] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8500 Loss 0.07541920180464996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [8536]: Loss[0.07537888959626848] has not improved from the previous [0.07537748240896641] for 1 times.\n",
      "Iteration [8554]: Loss[0.0753577791576542] has not improved from the previous [0.07535614664820765] for 1 times.\n",
      "Iteration [8573]: Loss[0.0753354920947971] has not improved from the previous [0.07533406876236824] for 1 times.\n",
      "Iteration [8592]: Loss[0.07531355017685337] has not improved from the previous [0.07531190584130769] for 1 times.\n",
      "Iteration [8610]: Loss[0.07529273004232424] has not improved from the previous [0.07529086794470208] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8600 Loss 0.07530157093535922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [8629]: Loss[0.075270580476477] has not improved from the previous [0.07526908984016696] for 1 times.\n",
      "Iteration [8649]: Loss[0.0752476736123614] has not improved from the previous [0.07524624976993266] for 1 times.\n",
      "Iteration [8660]: Loss[0.07523502073564434] has not improved from the previous [0.07523353428767143] for 1 times.\n",
      "Iteration [8681]: Loss[0.0752113594016203] has not improved from the previous [0.07520951093677578] for 1 times.\n",
      "Iteration [8694]: Loss[0.07519616762314085] has not improved from the previous [0.07519468135113713] for 1 times.\n",
      "Iteration [8716]: Loss[0.07517289783746421] has not improved from the previous [0.0751696265042123] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8700 Loss 0.07518679704755628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [8730]: Loss[0.07515603529201742] has not improved from the previous [0.0751536849554016] for 1 times.\n",
      "Iteration [8745]: Loss[0.07513865268454689] has not improved from the previous [0.07513680670866126] for 1 times.\n",
      "Iteration [8761]: Loss[0.07512070047121303] has not improved from the previous [0.07511887587255907] for 1 times.\n",
      "Iteration [8778]: Loss[0.07510181610971378] has not improved from the previous [0.07509976795578593] for 1 times.\n",
      "Iteration [8796]: Loss[0.07508230530236755] has not improved from the previous [0.07507951394473587] for 1 times.\n",
      "Iteration [8815]: Loss[0.07506179221675018] has not improved from the previous [0.07505826727292352] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8800 Loss 0.07507402731496093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [8834]: Loss[0.07504160999110436] has not improved from the previous [0.07503693660216272] for 1 times.\n",
      "Iteration [8855]: Loss[0.07501958728741517] has not improved from the previous [0.07501377363625726] for 1 times.\n",
      "Iteration [8874]: Loss[0.07499951159715686] has not improved from the previous [0.07499256472577258] for 1 times.\n",
      "Iteration [8895]: Loss[0.07497783002511989] has not improved from the previous [0.07496936353577763] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8900 Loss 0.07496301068195237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [8915]: Loss[0.07495718011532655] has not improved from the previous [0.07494733308022809] for 1 times.\n",
      "Iteration [8927]: Loss[0.07494104794272456] has not improved from the previous [0.0749343543401064] for 1 times.\n",
      "Iteration [8930]: Loss[0.07493463080606784] has not improved from the previous [0.07493234312937103] for 1 times.\n",
      "Iteration [8961]: Loss[0.0749044991124445] has not improved from the previous [0.07489737831094462] for 1 times.\n",
      "Iteration [8964]: Loss[0.07489763219180788] has not improved from the previous [0.07489568218568055] for 1 times.\n",
      "Iteration [8991]: Loss[0.0748710783634051] has not improved from the previous [0.07486492350658618] for 1 times.\n",
      "Iteration [9012]: Loss[0.07484932184839932] has not improved from the previous [0.07484235606370543] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9000 Loss 0.07485437096999925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [9015]: Loss[0.07484287177213181] has not improved from the previous [0.07484044353752908] for 1 times.\n",
      "Iteration [9037]: Loss[0.07482062430246265] has not improved from the previous [0.07481567024212027] for 1 times.\n",
      "Iteration [9056]: Loss[0.0748007717896787] has not improved from the previous [0.07479540777259669] for 1 times.\n",
      "Iteration [9073]: Loss[0.07478265915711661] has not improved from the previous [0.07477724110650934] for 1 times.\n",
      "Iteration [9090]: Loss[0.07476444857065882] has not improved from the previous [0.07475936669144873] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9100 Loss 0.07474784816613579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [9106]: Loss[0.0747473091625394] has not improved from the previous [0.07474247770971192] for 1 times.\n",
      "Iteration [9122]: Loss[0.07472983572102102] has not improved from the previous [0.0747260044157213] for 1 times.\n",
      "Iteration [9137]: Loss[0.07471365841122975] has not improved from the previous [0.07471021031429106] for 1 times.\n",
      "Iteration [9151]: Loss[0.07469854822486163] has not improved from the previous [0.07469536359369414] for 1 times.\n",
      "Iteration [9165]: Loss[0.07468319398100483] has not improved from the previous [0.07468080618993371] for 1 times.\n",
      "Iteration [9166]: Loss[0.07468343229397811] has not improved from the previous [0.07468319398100483] for 3 times.\n",
      "Iteration [9179]: Loss[0.07466777131535034] has not improved from the previous [0.07466652324422234] for 1 times.\n",
      "Iteration [9180]: Loss[0.07466909708930554] has not improved from the previous [0.07466777131535034] for 3 times.\n",
      "Iteration [9192]: Loss[0.07465349217976935] has not improved from the previous [0.07465293039375552] for 1 times.\n",
      "Iteration [9193]: Loss[0.07465559585637917] has not improved from the previous [0.07465349217976935] for 3 times.\n",
      "Iteration [9205]: Loss[0.07463970978706351] has not improved from the previous [0.07463953337968077] for 1 times.\n",
      "Iteration [9214]: Loss[0.07463194662570376] has not improved from the previous [0.0746300486203961] for 1 times.\n",
      "Iteration [9215]: Loss[0.07463279843911519] has not improved from the previous [0.07463194662570376] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9200 Loss 0.07464374672282548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [9227]: Loss[0.07461742628653979] has not improved from the previous [0.07461688425890978] for 1 times.\n",
      "Iteration [9228]: Loss[0.07461964323228952] has not improved from the previous [0.07461742628653979] for 3 times.\n",
      "Iteration [9239]: Loss[0.07460457166329774] has not improved from the previous [0.07460455560783721] for 1 times.\n",
      "Iteration [9248]: Loss[0.07459666958856333] has not improved from the previous [0.07459538682060844] for 1 times.\n",
      "Iteration [9249]: Loss[0.07459809110868651] has not improved from the previous [0.07459666958856333] for 3 times.\n",
      "Iteration [9259]: Loss[0.07458420114014293] has not improved from the previous [0.0745838014141113] for 1 times.\n",
      "Iteration [9260]: Loss[0.0745866523600739] has not improved from the previous [0.07458420114014293] for 3 times.\n",
      "Iteration [9279]: Loss[0.07456437349102692] has not improved from the previous [0.07456347558151784] for 1 times.\n",
      "Iteration [9280]: Loss[0.07456637947953774] has not improved from the previous [0.07456437349102692] for 3 times.\n",
      "Iteration [9290]: Loss[0.07455253528968796] has not improved from the previous [0.07455248227639097] for 1 times.\n",
      "Iteration [9298]: Loss[0.07454520315421712] has not improved from the previous [0.07454426414305448] for 1 times.\n",
      "Iteration [9299]: Loss[0.07454721702105449] has not improved from the previous [0.07454520315421712] for 3 times.\n",
      "Iteration [9309]: Loss[0.07453328416748228] has not improved from the previous [0.07453318086291312] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9300 Loss 0.07454189489385177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [9316]: Loss[0.07452735497144734] has not improved from the previous [0.07452566516018802] for 1 times.\n",
      "Iteration [9317]: Loss[0.07452870082000175] has not improved from the previous [0.07452735497144734] for 3 times.\n",
      "Iteration [9334]: Loss[0.07450915639962032] has not improved from the previous [0.07450759301915709] for 1 times.\n",
      "Iteration [9335]: Loss[0.07451055291916237] has not improved from the previous [0.07450915639962032] for 3 times.\n",
      "Iteration [9344]: Loss[0.07449794206032835] has not improved from the previous [0.07449757923839756] for 1 times.\n",
      "Iteration [9352]: Loss[0.0744907613971685] has not improved from the previous [0.07448978811598359] for 1 times.\n",
      "Iteration [9353]: Loss[0.07449282665280989] has not improved from the previous [0.0744907613971685] for 3 times.\n",
      "Iteration [9361]: Loss[0.07448090276834045] has not improved from the previous [0.07448038695926273] for 1 times.\n",
      "Iteration [9369]: Loss[0.07447350086626596] has not improved from the previous [0.07447281251079786] for 1 times.\n",
      "Iteration [9370]: Loss[0.07447581874646148] has not improved from the previous [0.07447350086626596] for 3 times.\n",
      "Iteration [9378]: Loss[0.07446389952028469] has not improved from the previous [0.07446338389223331] for 1 times.\n",
      "Iteration [9385]: Loss[0.07445750636776664] has not improved from the previous [0.0744563141615984] for 1 times.\n",
      "Iteration [9386]: Loss[0.07445950932081652] has not improved from the previous [0.07445750636776664] for 3 times.\n",
      "Iteration [9395]: Loss[0.07444684458947144] has not improved from the previous [0.0744467605889792] for 1 times.\n",
      "Iteration [9402]: Loss[0.07444001332993495] has not improved from the previous [0.07443977461235833] for 1 times.\n",
      "Iteration [9403]: Loss[0.07444297842851526] has not improved from the previous [0.07444001332993495] for 3 times.\n",
      "Iteration [9411]: Loss[0.07443087597799924] has not improved from the previous [0.07443085053905202] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9400 Loss 0.07444033058977076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [9424]: Loss[0.07441940965223132] has not improved from the previous [0.07441745403215679] for 1 times.\n",
      "Iteration [9425]: Loss[0.07442077507730878] has not improved from the previous [0.07441940965223132] for 3 times.\n",
      "Iteration [9433]: Loss[0.07440902012847539] has not improved from the previous [0.07440884172540399] for 1 times.\n",
      "Iteration [9440]: Loss[0.07440271713771529] has not improved from the previous [0.07440208445634483] for 1 times.\n",
      "Iteration [9441]: Loss[0.07440527174015057] has not improved from the previous [0.07440271713771529] for 3 times.\n",
      "Iteration [9448]: Loss[0.0743942441649589] has not improved from the previous [0.07439382537120343] for 1 times.\n",
      "Iteration [9455]: Loss[0.07438731278587063] has not improved from the previous [0.0743872541319367] for 1 times.\n",
      "Iteration [9456]: Loss[0.07439046688122991] has not improved from the previous [0.07438731278587063] for 3 times.\n",
      "Iteration [9463]: Loss[0.07437946092456467] has not improved from the previous [0.07437907006553868] for 1 times.\n",
      "Iteration [9476]: Loss[0.07436743748155501] has not improved from the previous [0.07436636391758479] for 1 times.\n",
      "Iteration [9477]: Loss[0.07436978016437445] has not improved from the previous [0.07436743748155501] for 3 times.\n",
      "Iteration [9484]: Loss[0.07435889197810953] has not improved from the previous [0.07435833627217654] for 1 times.\n",
      "Iteration [9490]: Loss[0.07435301295712413] has not improved from the previous [0.07435239619082423] for 1 times.\n",
      "Iteration [9491]: Loss[0.07435584214000816] has not improved from the previous [0.07435301295712413] for 3 times.\n",
      "Iteration [9511]: Loss[0.07433287808632866] has not improved from the previous [0.07433217707671277] for 1 times.\n",
      "Iteration [9512]: Loss[0.07433556252859011] has not improved from the previous [0.07433287808632866] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9500 Loss 0.07434215759436266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [9525]: Loss[0.07431863113895346] has not improved from the previous [0.0743185972865101] for 1 times.\n",
      "Iteration [9531]: Loss[0.07431363022039529] has not improved from the previous [0.07431280680686671] for 1 times.\n",
      "Iteration [9532]: Loss[0.07431623145748886] has not improved from the previous [0.07431363022039529] for 3 times.\n",
      "Iteration [9538]: Loss[0.07430618402158791] has not improved from the previous [0.07430548549769829] for 1 times.\n",
      "Iteration [9544]: Loss[0.07430021124919978] has not improved from the previous [0.07429976353954539] for 1 times.\n",
      "Iteration [9545]: Loss[0.07430332858744314] has not improved from the previous [0.07430021124919978] for 3 times.\n",
      "Iteration [9552]: Loss[0.07429255973367181] has not improved from the previous [0.07429228591284004] for 1 times.\n",
      "Iteration [9558]: Loss[0.0742866136398998] has not improved from the previous [0.07428659434730932] for 1 times.\n",
      "Iteration [9563]: Loss[0.07428219444592513] has not improved from the previous [0.07428125925992936] for 1 times.\n",
      "Iteration [9564]: Loss[0.0742848076117589] has not improved from the previous [0.07428219444592513] for 3 times.\n",
      "Iteration [9571]: Loss[0.07427409923292354] has not improved from the previous [0.0742739050957611] for 1 times.\n",
      "Iteration [9582]: Loss[0.07426398438874574] has not improved from the previous [0.07426291692865836] for 1 times.\n",
      "Iteration [9583]: Loss[0.07426651568759696] has not improved from the previous [0.07426398438874574] for 3 times.\n",
      "Iteration [9595]: Loss[0.07425093957243047] has not improved from the previous [0.07425055886736849] for 1 times.\n",
      "Iteration [9601]: Loss[0.0742454139740073] has not improved from the previous [0.07424503690599829] for 1 times.\n",
      "Iteration [9602]: Loss[0.07424871774048897] has not improved from the previous [0.0742454139740073] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9600 Loss 0.07424503690599829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [9608]: Loss[0.0742385510876257] has not improved from the previous [0.07423814146326123] for 1 times.\n",
      "Iteration [9613]: Loss[0.07423375094855895] has not improved from the previous [0.07423303879961331] for 1 times.\n",
      "Iteration [9619]: Loss[0.07422796283289322] has not improved from the previous [0.07422790388060295] for 1 times.\n",
      "Iteration [9620]: Loss[0.0742314992480742] has not improved from the previous [0.07422796283289322] for 3 times.\n",
      "Iteration [9626]: Loss[0.07422132571546652] has not improved from the previous [0.07422100460165508] for 1 times.\n",
      "Iteration [9631]: Loss[0.07421654617761606] has not improved from the previous [0.07421587141128412] for 1 times.\n",
      "Iteration [9636]: Loss[0.07421207618466549] has not improved from the previous [0.07421098373756234] for 1 times.\n",
      "Iteration [9637]: Loss[0.07421469809448993] has not improved from the previous [0.07421207618466549] for 3 times.\n",
      "Iteration [9649]: Loss[0.07419919878744903] has not improved from the previous [0.07419908684470003] for 1 times.\n",
      "Iteration [9654]: Loss[0.07419452446582794] has not improved from the previous [0.07419409399792606] for 1 times.\n",
      "Iteration [9655]: Loss[0.07419793511717283] has not improved from the previous [0.07419452446582794] for 3 times.\n",
      "Iteration [9661]: Loss[0.07418792333343494] has not improved from the previous [0.07418762694675705] for 1 times.\n",
      "Iteration [9666]: Loss[0.07418309798725793] has not improved from the previous [0.07418274891185855] for 1 times.\n",
      "Iteration [9671]: Loss[0.07417830573259585] has not improved from the previous [0.07417783543038885] for 1 times.\n",
      "Iteration [9676]: Loss[0.07417457634905561] has not improved from the previous [0.07417297409789285] for 1 times.\n",
      "Iteration [9677]: Loss[0.07417688203237105] has not improved from the previous [0.07417457634905561] for 3 times.\n",
      "Iteration [9683]: Loss[0.07416701312406768] has not improved from the previous [0.07416666979845382] for 1 times.\n",
      "Iteration [9688]: Loss[0.07416218886706596] has not improved from the previous [0.07416189301004653] for 1 times.\n",
      "Iteration [9693]: Loss[0.07415792028218984] has not improved from the previous [0.07415707496283508] for 1 times.\n",
      "Iteration [9694]: Loss[0.07416099805888954] has not improved from the previous [0.07415792028218984] for 3 times.\n",
      "Iteration [9700]: Loss[0.07415091273615336] has not improved from the previous [0.07415084245473275] for 1 times.\n",
      "Iteration [9705]: Loss[0.07414612791157178] has not improved from the previous [0.07414599862046839] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9700 Loss 0.07415091273615336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [9710]: Loss[0.07414135765428354] has not improved from the previous [0.07414124543959252] for 1 times.\n",
      "Iteration [9711]: Loss[0.07414516934496163] has not improved from the previous [0.07414135765428354] for 3 times.\n",
      "Iteration [9726]: Loss[0.07412653669988496] has not improved from the previous [0.07412564483376362] for 1 times.\n",
      "Iteration [9731]: Loss[0.07412245939930973] has not improved from the previous [0.07412119102358289] for 1 times.\n",
      "Iteration [9732]: Loss[0.07412510804413308] has not improved from the previous [0.07412245939930973] for 3 times.\n",
      "Iteration [9738]: Loss[0.07411519311992433] has not improved from the previous [0.07411511434567812] for 1 times.\n",
      "Iteration [9752]: Loss[0.07410343326368331] has not improved from the previous [0.07410126256406051] for 1 times.\n",
      "Iteration [9753]: Loss[0.07410543610617758] has not improved from the previous [0.07410343326368331] for 3 times.\n",
      "Iteration [9768]: Loss[0.07408781745994696] has not improved from the previous [0.07408644722026532] for 1 times.\n",
      "Iteration [9769]: Loss[0.07409053055669197] has not improved from the previous [0.07408781745994696] for 3 times.\n",
      "Iteration [9779]: Loss[0.07407697448933649] has not improved from the previous [0.07407613178181939] for 1 times.\n",
      "Iteration [9784]: Loss[0.0740721089057165] has not improved from the previous [0.0740719778220067] for 1 times.\n",
      "Iteration [9789]: Loss[0.07406801532835945] has not improved from the previous [0.07406753768700775] for 1 times.\n",
      "Iteration [9790]: Loss[0.0740716396060884] has not improved from the previous [0.07406801532835945] for 3 times.\n",
      "Iteration [9799]: Loss[0.0740584367654698] has not improved from the previous [0.07405741239718829] for 1 times.\n",
      "Iteration [9804]: Loss[0.07405366908226964] has not improved from the previous [0.07405328450533004] for 1 times.\n",
      "Iteration [9805]: Loss[0.07405735167793408] has not improved from the previous [0.07405366908226964] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9800 Loss 0.07405632994966153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [9810]: Loss[0.07404812920274013] has not improved from the previous [0.07404773241708114] for 1 times.\n",
      "Iteration [9819]: Loss[0.07403980824300774] has not improved from the previous [0.07403906627734391] for 1 times.\n",
      "Iteration [9824]: Loss[0.07403523017117393] has not improved from the previous [0.07403500757940669] for 1 times.\n",
      "Iteration [9825]: Loss[0.0740390667942694] has not improved from the previous [0.07403523017117393] for 3 times.\n",
      "Iteration [9830]: Loss[0.0740296044181679] has not improved from the previous [0.07402940475868741] for 1 times.\n",
      "Iteration [9834]: Loss[0.0740259960296272] has not improved from the previous [0.0740251931057823] for 1 times.\n",
      "Iteration [9843]: Loss[0.07401841052461512] has not improved from the previous [0.0740168273283257] for 1 times.\n",
      "Iteration [9844]: Loss[0.0740210695172015] has not improved from the previous [0.07401841052461512] for 3 times.\n",
      "Iteration [9849]: Loss[0.07401217451239372] has not improved from the previous [0.07401152424067059] for 1 times.\n",
      "Iteration [9858]: Loss[0.07400383334925814] has not improved from the previous [0.07400319019716448] for 1 times.\n",
      "Iteration [9862]: Loss[0.07399927433906538] has not improved from the previous [0.0739991597455218] for 1 times.\n",
      "Iteration [9863]: Loss[0.0739993830556549] has not improved from the previous [0.07399927433906538] for 3 times.\n",
      "Iteration [9864]: Loss[0.07400340349231142] has not improved from the previous [0.0739993830556549] for 5 times.\n",
      "Iteration [9873]: Loss[0.0739900177321752] has not improved from the previous [0.07398970587956945] for 1 times.\n",
      "Iteration [9877]: Loss[0.07398647250889066] has not improved from the previous [0.07398563013197548] for 1 times.\n",
      "Iteration [9881]: Loss[0.07398179895714121] has not improved from the previous [0.07398179589432842] for 1 times.\n",
      "Iteration [9882]: Loss[0.0739820290014447] has not improved from the previous [0.07398179895714121] for 3 times.\n",
      "Iteration [9883]: Loss[0.07398597474059367] has not improved from the previous [0.0739820290014447] for 5 times.\n",
      "Iteration [9892]: Loss[0.0739726808463451] has not improved from the previous [0.0739723222390366] for 1 times.\n",
      "Iteration [9896]: Loss[0.07396910751831628] has not improved from the previous [0.07396829995132385] for 1 times.\n",
      "Iteration [9900]: Loss[0.07396631210960519] has not improved from the previous [0.07396448407269081] for 1 times.\n",
      "Iteration [9901]: Loss[0.07396892942403512] has not improved from the previous [0.07396631210960519] for 3 times.\n",
      "Iteration [9906]: Loss[0.07396006902248904] has not improved from the previous [0.07395945919990274] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9900 Loss 0.07396631210960519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [9910]: Loss[0.07395638664494768] has not improved from the previous [0.07395546717409174] for 1 times.\n",
      "Iteration [9919]: Loss[0.07394848144799025] has not improved from the previous [0.0739477409274882] for 1 times.\n",
      "Iteration [9920]: Loss[0.07395209959445272] has not improved from the previous [0.07394848144799025] for 3 times.\n",
      "Iteration [9925]: Loss[0.0739426396437052] has not improved from the previous [0.07394258869729199] for 1 times.\n",
      "Iteration [9929]: Loss[0.0739389676265988] has not improved from the previous [0.07393867339601849] for 1 times.\n",
      "Iteration [9933]: Loss[0.07393542391793001] has not improved from the previous [0.0739347819472868] for 1 times.\n",
      "Iteration [9937]: Loss[0.07393239165120918] has not improved from the previous [0.07393096799942664] for 1 times.\n",
      "Iteration [9938]: Loss[0.0739354715969379] has not improved from the previous [0.07393239165120918] for 3 times.\n",
      "Iteration [9943]: Loss[0.07392639199722802] has not improved from the previous [0.07392611741444396] for 1 times.\n",
      "Iteration [9947]: Loss[0.0739228339081821] has not improved from the previous [0.07392210239434736] for 1 times.\n",
      "Iteration [9951]: Loss[0.07391909916689661] has not improved from the previous [0.07391841271295906] for 1 times.\n",
      "Iteration [9955]: Loss[0.07391601220058458] has not improved from the previous [0.07391465436327295] for 1 times.\n",
      "Iteration [9956]: Loss[0.07391909118653736] has not improved from the previous [0.07391601220058458] for 3 times.\n",
      "Iteration [9961]: Loss[0.07391006995508456] has not improved from the previous [0.07390992487859044] for 1 times.\n",
      "Iteration [9965]: Loss[0.07390650108634056] has not improved from the previous [0.07390606564991624] for 1 times.\n",
      "Iteration [9969]: Loss[0.07390293521702579] has not improved from the previous [0.07390223789118239] for 1 times.\n",
      "Iteration [9973]: Loss[0.07389931485973023] has not improved from the previous [0.07389862477471777] for 1 times.\n",
      "Iteration [9974]: Loss[0.07390316661653894] has not improved from the previous [0.07389931485973023] for 3 times.\n",
      "Iteration [9979]: Loss[0.07389390675938935] has not improved from the previous [0.07389380290818877] for 1 times.\n",
      "Iteration [9983]: Loss[0.0738902433628425] has not improved from the previous [0.07388999026526191] for 1 times.\n",
      "Iteration [9987]: Loss[0.07388661359970451] has not improved from the previous [0.073886369650821] for 1 times.\n",
      "Iteration [9991]: Loss[0.07388305580491389] has not improved from the previous [0.07388261022143203] for 1 times.\n",
      "Iteration [9995]: Loss[0.07388007453326102] has not improved from the previous [0.07387891795677365] for 1 times.\n",
      "Iteration [9996]: Loss[0.07388351436037162] has not improved from the previous [0.07388007453326102] for 3 times.\n",
      "Iteration [10000]: Loss[0.0738742354353082] has not improved from the previous [0.07387420616292331] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10000 Loss 0.0738742354353082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [10013]: Loss[0.07386327185001386] has not improved from the previous [0.0738632016486478] for 1 times.\n",
      "Iteration [10014]: Loss[0.07386772182646144] has not improved from the previous [0.07386327185001386] for 3 times.\n",
      "Iteration [10018]: Loss[0.07385909284378735] has not improved from the previous [0.07385838667118616] for 1 times.\n",
      "Iteration [10022]: Loss[0.07385539478069487] has not improved from the previous [0.07385483769938493] for 1 times.\n",
      "Iteration [10026]: Loss[0.07385170601350784] has not improved from the previous [0.07385137171774059] for 1 times.\n",
      "Iteration [10030]: Loss[0.0738481398376727] has not improved from the previous [0.07384776551064214] for 1 times.\n",
      "Iteration [10034]: Loss[0.07384512186548543] has not improved from the previous [0.0738442464101924] for 1 times.\n",
      "Iteration [10035]: Loss[0.0738488947713645] has not improved from the previous [0.07384512186548543] for 3 times.\n",
      "Iteration [10039]: Loss[0.0738403723329886] has not improved from the previous [0.07383949868829313] for 1 times.\n",
      "Iteration [10043]: Loss[0.07383663897410332] has not improved from the previous [0.0738361122112986] for 1 times.\n",
      "Iteration [10047]: Loss[0.0738330584446305] has not improved from the previous [0.07383256950772939] for 1 times.\n",
      "Iteration [10051]: Loss[0.07382968422083881] has not improved from the previous [0.07382900513724829] for 1 times.\n",
      "Iteration [10052]: Loss[0.0738336860486462] has not improved from the previous [0.07382968422083881] for 3 times.\n",
      "Iteration [10056]: Loss[0.07382532122023897] has not improved from the previous [0.07382433483899187] for 1 times.\n",
      "Iteration [10060]: Loss[0.07382157978671228] has not improved from the previous [0.07382099882527007] for 1 times.\n",
      "Iteration [10064]: Loss[0.0738179987581268] has not improved from the previous [0.0738175080008752] for 1 times.\n",
      "Iteration [10068]: Loss[0.07381430871187912] has not improved from the previous [0.07381408585579544] for 1 times.\n",
      "Iteration [10072]: Loss[0.07381106381515268] has not improved from the previous [0.0738107302789194] for 1 times.\n",
      "Iteration [10073]: Loss[0.07381537020156193] has not improved from the previous [0.07381106381515268] for 3 times.\n",
      "Iteration [10077]: Loss[0.0738065043647271] has not improved from the previous [0.07380610232864032] for 1 times.\n",
      "Iteration [10081]: Loss[0.07380290792831067] has not improved from the previous [0.07380266664486097] for 1 times.\n",
      "Iteration [10085]: Loss[0.07379926317315572] has not improved from the previous [0.0737991826413619] for 1 times.\n",
      "Iteration [10088]: Loss[0.07379587785305092] has not improved from the previous [0.07379571752129362] for 1 times.\n",
      "Iteration [10092]: Loss[0.0737924214163634] has not improved from the previous [0.07379212621010618] for 1 times.\n",
      "Iteration [10093]: Loss[0.07379251893005193] has not improved from the previous [0.0737924214163634] for 3 times.\n",
      "Iteration [10094]: Loss[0.07379707004296239] has not improved from the previous [0.07379251893005193] for 5 times.\n",
      "Iteration [10098]: Loss[0.07378788229298104] has not improved from the previous [0.07378786748355727] for 1 times.\n",
      "Iteration [10101]: Loss[0.0737845125658628] has not improved from the previous [0.07378440376746999] for 1 times.\n",
      "Iteration [10105]: Loss[0.07378113254647176] has not improved from the previous [0.07378066728603516] for 1 times.\n",
      "Iteration [10109]: Loss[0.07377886940155083] has not improved from the previous [0.07377733242075458] for 1 times.\n",
      "Iteration [10110]: Loss[0.07378232489166532] has not improved from the previous [0.07377886940155083] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10100 Loss 0.07378440376746999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [10114]: Loss[0.07377403241911408] has not improved from the previous [0.07377321352917932] for 1 times.\n",
      "Iteration [10118]: Loss[0.07377032401098109] has not improved from the previous [0.07377003027851468] for 1 times.\n",
      "Iteration [10122]: Loss[0.07376672583617254] has not improved from the previous [0.07376664565845706] for 1 times.\n",
      "Iteration [10125]: Loss[0.07376340515957151] has not improved from the previous [0.07376320208088459] for 1 times.\n",
      "Iteration [10129]: Loss[0.073761788210332] has not improved from the previous [0.07375951799563996] for 1 times.\n",
      "Iteration [10130]: Loss[0.07376457307821649] has not improved from the previous [0.073761788210332] for 3 times.\n",
      "Iteration [10134]: Loss[0.07375657677447163] has not improved from the previous [0.07375552010111971] for 1 times.\n",
      "Iteration [10138]: Loss[0.07375277439250555] has not improved from the previous [0.07375239723549401] for 1 times.\n",
      "Iteration [10145]: Loss[0.07374587244811433] has not improved from the previous [0.07374565360899543] for 1 times.\n",
      "Iteration [10149]: Loss[0.07374428712384413] has not improved from the previous [0.07374214787510036] for 1 times.\n",
      "Iteration [10150]: Loss[0.0737471460157261] has not improved from the previous [0.07374428712384413] for 3 times.\n",
      "Iteration [10154]: Loss[0.07373897202742734] has not improved from the previous [0.0737382397695439] for 1 times.\n",
      "Iteration [10158]: Loss[0.07373530280451394] has not improved from the previous [0.07373501636092578] for 1 times.\n",
      "Iteration [10161]: Loss[0.07373191736112865] has not improved from the previous [0.07373171081171317] for 1 times.\n",
      "Iteration [10165]: Loss[0.07372969194054946] has not improved from the previous [0.07372811723637289] for 1 times.\n",
      "Iteration [10166]: Loss[0.07373321177382655] has not improved from the previous [0.07372969194054946] for 3 times.\n",
      "Iteration [10170]: Loss[0.07372510387266894] has not improved from the previous [0.07372421675988228] for 1 times.\n",
      "Iteration [10174]: Loss[0.07372131442580955] has not improved from the previous [0.07372119189713058] for 1 times.\n",
      "Iteration [10177]: Loss[0.07371800402960133] has not improved from the previous [0.07371780332505062] for 1 times.\n",
      "Iteration [10181]: Loss[0.07371539712672588] has not improved from the previous [0.07371435222630245] for 1 times.\n",
      "Iteration [10185]: Loss[0.07371166854765057] has not improved from the previous [0.07371152569637106] for 1 times.\n",
      "Iteration [10188]: Loss[0.0737084508371449] has not improved from the previous [0.0737081292920316] for 1 times.\n",
      "Iteration [10190]: Loss[0.07371332272732306] has not improved from the previous [0.07370832473584542] for 1 times.\n",
      "Iteration [10193]: Loss[0.07370419723840792] has not improved from the previous [0.07370402017250056] for 1 times.\n",
      "Iteration [10197]: Loss[0.0737016011903684] has not improved from the previous [0.07370050262448267] for 1 times.\n",
      "Iteration [10204]: Loss[0.07369581229472065] has not improved from the previous [0.07369427077610306] for 1 times.\n",
      "Iteration [10205]: Loss[0.07369942486472598] has not improved from the previous [0.07369581229472065] for 3 times.\n",
      "Iteration [10209]: Loss[0.0736912231723429] has not improved from the previous [0.07369048604395145] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10200 Loss 0.07369784675847209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [10216]: Loss[0.07368441482855122] has not improved from the previous [0.07368387833937595] for 1 times.\n",
      "Iteration [10220]: Loss[0.07368144060082732] has not improved from the previous [0.07368090082825025] for 1 times.\n",
      "Iteration [10223]: Loss[0.07367816946108086] has not improved from the previous [0.0736778152093799] for 1 times.\n",
      "Iteration [10227]: Loss[0.07367645233002229] has not improved from the previous [0.0736746267448437] for 1 times.\n",
      "Iteration [10228]: Loss[0.07367976826449611] has not improved from the previous [0.07367645233002229] for 3 times.\n",
      "Iteration [10232]: Loss[0.07367112224183159] has not improved from the previous [0.0736709256004568] for 1 times.\n",
      "Iteration [10235]: Loss[0.07366790734692823] has not improved from the previous [0.07366756095046811] for 1 times.\n",
      "Iteration [10239]: Loss[0.07366517427006578] has not improved from the previous [0.07366437900220406] for 1 times.\n",
      "Iteration [10242]: Loss[0.07366167915049454] has not improved from the previous [0.0736615030833228] for 1 times.\n",
      "Iteration [10246]: Loss[0.07366005740514776] has not improved from the previous [0.07365820168789197] for 1 times.\n",
      "Iteration [10247]: Loss[0.07366334806097168] has not improved from the previous [0.07366005740514776] for 3 times.\n",
      "Iteration [10251]: Loss[0.07365482166634675] has not improved from the previous [0.07365460137395011] for 1 times.\n",
      "Iteration [10254]: Loss[0.07365158739466839] has not improved from the previous [0.07365123787312398] for 1 times.\n",
      "Iteration [10258]: Loss[0.07364881889100669] has not improved from the previous [0.07364818547619902] for 1 times.\n",
      "Iteration [10261]: Loss[0.07364557092838428] has not improved from the previous [0.07364516151672208] for 1 times.\n",
      "Iteration [10265]: Loss[0.07364344745838616] has not improved from the previous [0.07364204299925155] for 1 times.\n",
      "Iteration [10266]: Loss[0.07364723348924827] has not improved from the previous [0.07364344745838616] for 3 times.\n",
      "Iteration [10273]: Loss[0.07363610684933782] has not improved from the previous [0.07363516013418656] for 1 times.\n",
      "Iteration [10276]: Loss[0.0736325313713319] has not improved from the previous [0.07363242676838437] for 1 times.\n",
      "Iteration [10280]: Loss[0.07363002549739178] has not improved from the previous [0.07362912082670077] for 1 times.\n",
      "Iteration [10283]: Loss[0.0736265641775963] has not improved from the previous [0.07362634264172792] for 1 times.\n",
      "Iteration [10287]: Loss[0.0736252578309788] has not improved from the previous [0.07362312942819753] for 1 times.\n",
      "Iteration [10288]: Loss[0.07362833977505907] has not improved from the previous [0.0736252578309788] for 3 times.\n",
      "Iteration [10295]: Loss[0.07361731286454132] has not improved from the previous [0.07361625462580199] for 1 times.\n",
      "Iteration [10298]: Loss[0.07361373145246018] has not improved from the previous [0.07361359482002927] for 1 times.\n",
      "Iteration [10302]: Loss[0.07361153369546122] has not improved from the previous [0.07361036547437391] for 1 times.\n",
      "Iteration [10303]: Loss[0.07361557742553303] has not improved from the previous [0.07361153369546122] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10300 Loss 0.07361130094658594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [10310]: Loss[0.0736045440890799] has not improved from the previous [0.07360358165054359] for 1 times.\n",
      "Iteration [10313]: Loss[0.07360114583634597] has not improved from the previous [0.07360081436595826] for 1 times.\n",
      "Iteration [10317]: Loss[0.0735984331740536] has not improved from the previous [0.07359774274592776] for 1 times.\n",
      "Iteration [10320]: Loss[0.0735963371149859] has not improved from the previous [0.07359473111496713] for 1 times.\n",
      "Iteration [10321]: Loss[0.07360007504299493] has not improved from the previous [0.0735963371149859] for 3 times.\n",
      "Iteration [10325]: Loss[0.07359174790417915] has not improved from the previous [0.07359136525555347] for 1 times.\n",
      "Iteration [10328]: Loss[0.07358944773249448] has not improved from the previous [0.07358808180388375] for 1 times.\n",
      "Iteration [10335]: Loss[0.07358335388427273] has not improved from the previous [0.07358223266987467] for 1 times.\n",
      "Iteration [10338]: Loss[0.07357994551805713] has not improved from the previous [0.07357953951537999] for 1 times.\n",
      "Iteration [10342]: Loss[0.07357764521825028] has not improved from the previous [0.07357652567760499] for 1 times.\n",
      "Iteration [10343]: Loss[0.07358187739059224] has not improved from the previous [0.07357764521825028] for 3 times.\n",
      "Iteration [10346]: Loss[0.07357317640327567] has not improved from the previous [0.07357300650276388] for 1 times.\n",
      "Iteration [10350]: Loss[0.07357057433046306] has not improved from the previous [0.07356995106435665] for 1 times.\n",
      "Iteration [10353]: Loss[0.07356810226046821] has not improved from the previous [0.07356709337808795] for 1 times.\n",
      "Iteration [10356]: Loss[0.0735646053110644] has not improved from the previous [0.07356439315138692] for 1 times.\n",
      "Iteration [10360]: Loss[0.07356211215577288] has not improved from the previous [0.07356136704126907] for 1 times.\n",
      "Iteration [10361]: Loss[0.0735666528341665] has not improved from the previous [0.07356211215577288] for 3 times.\n",
      "Iteration [10364]: Loss[0.07355809924536087] has not improved from the previous [0.07355773911756226] for 1 times.\n",
      "Iteration [10368]: Loss[0.07355530651943555] has not improved from the previous [0.07355486439531052] for 1 times.\n",
      "Iteration [10371]: Loss[0.07355293374734806] has not improved from the previous [0.0735519286946359] for 1 times.\n",
      "Iteration [10374]: Loss[0.07354963974535157] has not improved from the previous [0.0735491342541822] for 1 times.\n",
      "Iteration [10378]: Loss[0.0735468036951053] has not improved from the previous [0.07354630004350855] for 1 times.\n",
      "Iteration [10381]: Loss[0.07354503904372323] has not improved from the previous [0.07354339804889483] for 1 times.\n",
      "Iteration [10382]: Loss[0.07354882985373068] has not improved from the previous [0.07354503904372323] for 3 times.\n",
      "Iteration [10385]: Loss[0.07354021437397655] has not improved from the previous [0.07354016808267973] for 1 times.\n",
      "Iteration [10389]: Loss[0.07353777279600408] has not improved from the previous [0.07353700468751778] for 1 times.\n",
      "Iteration [10392]: Loss[0.07353527967854055] has not improved from the previous [0.07353421363666851] for 1 times.\n",
      "Iteration [10395]: Loss[0.0735319535319009] has not improved from the previous [0.07353151148430265] for 1 times.\n",
      "Iteration [10399]: Loss[0.07352913499670849] has not improved from the previous [0.0735287588843458] for 1 times.\n",
      "Iteration [10402]: Loss[0.07352764592779244] has not improved from the previous [0.07352580176464524] for 1 times.\n",
      "Iteration [10403]: Loss[0.07353127496509111] has not improved from the previous [0.07352764592779244] for 3 times.\n",
      "Iteration [10406]: Loss[0.07352269623546198] has not improved from the previous [0.07352252713350058] for 1 times.\n",
      "Iteration [10410]: Loss[0.07352010262478134] has not improved from the previous [0.07351951666404696] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10400 Loss 0.07352689164011113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [10413]: Loss[0.07351771738220321] has not improved from the previous [0.07351662487267185] for 1 times.\n",
      "Iteration [10416]: Loss[0.07351521279895026] has not improved from the previous [0.07351395847445706] for 1 times.\n",
      "Iteration [10419]: Loss[0.07351173442595584] has not improved from the previous [0.07351142238370625] for 1 times.\n",
      "Iteration [10423]: Loss[0.07350990477542654] has not improved from the previous [0.0735086018224133] for 1 times.\n",
      "Iteration [10424]: Loss[0.07351398626034966] has not improved from the previous [0.07350990477542654] for 3 times.\n",
      "Iteration [10427]: Loss[0.07350620233891951] has not improved from the previous [0.0735048465345631] for 1 times.\n",
      "Iteration [10430]: Loss[0.07350277165421644] has not improved from the previous [0.0735023029960558] for 1 times.\n",
      "Iteration [10434]: Loss[0.07349993432052335] has not improved from the previous [0.07349954040303784] for 1 times.\n",
      "Iteration [10437]: Loss[0.07349759744652877] has not improved from the previous [0.07349665192937842] for 1 times.\n",
      "Iteration [10440]: Loss[0.07349568828107785] has not improved from the previous [0.07349403483499993] for 1 times.\n",
      "Iteration [10441]: Loss[0.0734994725593483] has not improved from the previous [0.07349568828107785] for 3 times.\n",
      "Iteration [10444]: Loss[0.07349105568366346] has not improved from the previous [0.07349075832256981] for 1 times.\n",
      "Iteration [10448]: Loss[0.0734883548512805] has not improved from the previous [0.07348786151765238] for 1 times.\n",
      "Iteration [10451]: Loss[0.07348601908113728] has not improved from the previous [0.07348499102113162] for 1 times.\n",
      "Iteration [10454]: Loss[0.07348359949207153] has not improved from the previous [0.07348230128916547] for 1 times.\n",
      "Iteration [10457]: Loss[0.07348028743780684] has not improved from the previous [0.07347974583370127] for 1 times.\n",
      "Iteration [10461]: Loss[0.0734777854685376] has not improved from the previous [0.0734770912408539] for 1 times.\n",
      "Iteration [10462]: Loss[0.07348258769681475] has not improved from the previous [0.0734777854685376] for 3 times.\n",
      "Iteration [10465]: Loss[0.07347441451155767] has not improved from the previous [0.07347356615168696] for 1 times.\n",
      "Iteration [10468]: Loss[0.07347206194925382] has not improved from the previous [0.0734707089354244] for 1 times.\n",
      "Iteration [10471]: Loss[0.07346879336644897] has not improved from the previous [0.07346813256769279] for 1 times.\n",
      "Iteration [10475]: Loss[0.07346578315081723] has not improved from the previous [0.07346562204702174] for 1 times.\n",
      "Iteration [10478]: Loss[0.07346345873283337] has not improved from the previous [0.07346278546854583] for 1 times.\n",
      "Iteration [10481]: Loss[0.07346169773079132] has not improved from the previous [0.07346023097419248] for 1 times.\n",
      "Iteration [10482]: Loss[0.07346571434565388] has not improved from the previous [0.07346169773079132] for 3 times.\n",
      "Iteration [10485]: Loss[0.07345807530103374] has not improved from the previous [0.07345666523392394] for 1 times.\n",
      "Iteration [10488]: Loss[0.07345472593942508] has not improved from the previous [0.0734541348015924] for 1 times.\n",
      "Iteration [10492]: Loss[0.07345176445079385] has not improved from the previous [0.07345159626757165] for 1 times.\n",
      "Iteration [10495]: Loss[0.07344943102631303] has not improved from the previous [0.07344879513629998] for 1 times.\n",
      "Iteration [10498]: Loss[0.07344715045203427] has not improved from the previous [0.07344616350939125] for 1 times.\n",
      "Iteration [10499]: Loss[0.07345168127953688] has not improved from the previous [0.07344715045203427] for 3 times.\n",
      "Iteration [10502]: Loss[0.07344408485217355] has not improved from the previous [0.0734426582257544] for 1 times.\n",
      "Iteration [10505]: Loss[0.07344075090558193] has not improved from the previous [0.0734401262604651] for 1 times.\n",
      "Iteration [10509]: Loss[0.0734377652220408] has not improved from the previous [0.07343764738574743] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10500 Loss 0.07344424426565795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [10512]: Loss[0.07343541986148709] has not improved from the previous [0.07343488030549716] for 1 times.\n",
      "Iteration [10515]: Loss[0.07343290183081186] has not improved from the previous [0.07343237702922972] for 1 times.\n",
      "Iteration [10518]: Loss[0.07343073827307064] has not improved from the previous [0.07342968670822725] for 1 times.\n",
      "Iteration [10519]: Loss[0.07343530044657987] has not improved from the previous [0.07343073827307064] for 3 times.\n",
      "Iteration [10522]: Loss[0.07342758877949274] has not improved from the previous [0.07342627466112124] for 1 times.\n",
      "Iteration [10525]: Loss[0.07342504592684777] has not improved from the previous [0.07342373453578441] for 1 times.\n",
      "Iteration [10528]: Loss[0.07342177065847759] has not improved from the previous [0.07342117983534834] for 1 times.\n",
      "Iteration [10535]: Loss[0.07341634721572203] has not improved from the previous [0.07341610391155864] for 1 times.\n",
      "Iteration [10538]: Loss[0.07341418606749615] has not improved from the previous [0.07341340452100065] for 1 times.\n",
      "Iteration [10539]: Loss[0.07341905213616916] has not improved from the previous [0.07341418606749615] for 3 times.\n",
      "Iteration [10542]: Loss[0.07341101479991921] has not improved from the previous [0.07341012159233118] for 1 times.\n",
      "Iteration [10545]: Loss[0.07340854314621574] has not improved from the previous [0.07340751550808426] for 1 times.\n",
      "Iteration [10548]: Loss[0.07340616982227498] has not improved from the previous [0.07340485270261189] for 1 times.\n",
      "Iteration [10551]: Loss[0.07340366140084309] has not improved from the previous [0.07340240716352903] for 1 times.\n",
      "Iteration [10554]: Loss[0.07340117691881332] has not improved from the previous [0.073399864484948] for 1 times.\n",
      "Iteration [10557]: Loss[0.07339790594326899] has not improved from the previous [0.07339731714654302] for 1 times.\n",
      "Iteration [10560]: Loss[0.07339501670343426] has not improved from the previous [0.07339499629745865] for 1 times.\n",
      "Iteration [10561]: Loss[0.07339549945004405] has not improved from the previous [0.07339501670343426] for 3 times.\n",
      "Iteration [10562]: Loss[0.07340056947726753] has not improved from the previous [0.07339549945004405] for 5 times.\n",
      "Iteration [10565]: Loss[0.07339209956029767] has not improved from the previous [0.07339155228435283] for 1 times.\n",
      "Iteration [10568]: Loss[0.07338962090280746] has not improved from the previous [0.07338896947669371] for 1 times.\n",
      "Iteration [10571]: Loss[0.07338707960855251] has not improved from the previous [0.07338653676153666] for 1 times.\n",
      "Iteration [10574]: Loss[0.07338466910828663] has not improved from the previous [0.07338390532732673] for 1 times.\n",
      "Iteration [10577]: Loss[0.07338237458388165] has not improved from the previous [0.07338140553955481] for 1 times.\n",
      "Iteration [10578]: Loss[0.07338709677764178] has not improved from the previous [0.07338237458388165] for 3 times.\n",
      "Iteration [10581]: Loss[0.07337921463931192] has not improved from the previous [0.07337819950297542] for 1 times.\n",
      "Iteration [10584]: Loss[0.07337681106286484] has not improved from the previous [0.07337556296750035] for 1 times.\n",
      "Iteration [10587]: Loss[0.07337436034935947] has not improved from the previous [0.07337311004401027] for 1 times.\n",
      "Iteration [10590]: Loss[0.07337186862336265] has not improved from the previous [0.07337064166415891] for 1 times.\n",
      "Iteration [10593]: Loss[0.07336936980055646] has not improved from the previous [0.07336813425415074] for 1 times.\n",
      "Iteration [10596]: Loss[0.0733672018110687] has not improved from the previous [0.07336568432573268] for 1 times.\n",
      "Iteration [10597]: Loss[0.07337140576494146] has not improved from the previous [0.0733672018110687] for 3 times.\n",
      "Iteration [10600]: Loss[0.0733631108271917] has not improved from the previous [0.07336258040294934] for 1 times.\n",
      "Iteration [10607]: Loss[0.07335769925891351] has not improved from the previous [0.07335768162547089] for 1 times.\n",
      "Iteration [10610]: Loss[0.07335521716008002] has not improved from the previous [0.07335514803768253] for 1 times.\n",
      "Iteration [10613]: Loss[0.07335283615436605] has not improved from the previous [0.07335258403120985] for 1 times.\n",
      "Iteration [10616]: Loss[0.07335039082632276] has not improved from the previous [0.07335015330177229] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10600 Loss 0.0733631108271917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [10619]: Loss[0.07334833022166654] has not improved from the previous [0.07334770247841285] for 1 times.\n",
      "Iteration [10620]: Loss[0.07335336673446215] has not improved from the previous [0.07334833022166654] for 3 times.\n",
      "Iteration [10623]: Loss[0.07334501297059869] has not improved from the previous [0.0733444417339415] for 1 times.\n",
      "Iteration [10626]: Loss[0.07334254150960637] has not improved from the previous [0.07334193888452241] for 1 times.\n",
      "Iteration [10629]: Loss[0.07333998864577335] has not improved from the previous [0.07333962948387399] for 1 times.\n",
      "Iteration [10632]: Loss[0.07333752755030398] has not improved from the previous [0.07333709444551069] for 1 times.\n",
      "Iteration [10635]: Loss[0.07333515006065572] has not improved from the previous [0.0733345750471582] for 1 times.\n",
      "Iteration [10638]: Loss[0.0733329330833534] has not improved from the previous [0.07333223856342358] for 1 times.\n",
      "Iteration [10639]: Loss[0.07333792439688944] has not improved from the previous [0.0733329330833534] for 3 times.\n",
      "Iteration [10642]: Loss[0.07332966733410179] has not improved from the previous [0.07332908770214014] for 1 times.\n",
      "Iteration [10645]: Loss[0.07332726868090086] has not improved from the previous [0.07332653606425299] for 1 times.\n",
      "Iteration [10648]: Loss[0.07332475845290469] has not improved from the previous [0.07332416740974242] for 1 times.\n",
      "Iteration [10651]: Loss[0.07332220156416482] has not improved from the previous [0.07332185545006757] for 1 times.\n",
      "Iteration [10654]: Loss[0.07331978597589284] has not improved from the previous [0.07331935006920101] for 1 times.\n",
      "Iteration [10657]: Loss[0.07331754072585224] has not improved from the previous [0.0733168690110334] for 1 times.\n",
      "Iteration [10658]: Loss[0.07332265035352803] has not improved from the previous [0.07331754072585224] for 3 times.\n",
      "Iteration [10661]: Loss[0.07331435055581885] has not improved from the previous [0.07331384891166398] for 1 times.\n",
      "Iteration [10664]: Loss[0.07331194158427398] has not improved from the previous [0.07331132717074236] for 1 times.\n",
      "Iteration [10667]: Loss[0.0733094380908666] has not improved from the previous [0.07330902013787725] for 1 times.\n",
      "Iteration [10670]: Loss[0.07330694280265773] has not improved from the previous [0.07330664887989402] for 1 times.\n",
      "Iteration [10673]: Loss[0.07330444869287374] has not improved from the previous [0.0733042193218568] for 1 times.\n",
      "Iteration [10676]: Loss[0.07330201240412] has not improved from the previous [0.07330186425251888] for 1 times.\n",
      "Iteration [10679]: Loss[0.07330000465652939] has not improved from the previous [0.0732994451427615] for 1 times.\n",
      "Iteration [10680]: Loss[0.07330520599768922] has not improved from the previous [0.07330000465652939] for 3 times.\n",
      "Iteration [10683]: Loss[0.07329661867830345] has not improved from the previous [0.07329631214085365] for 1 times.\n",
      "Iteration [10686]: Loss[0.07329414766076464] has not improved from the previous [0.07329394171888512] for 1 times.\n",
      "Iteration [10689]: Loss[0.0732916717663119] has not improved from the previous [0.0732915696293326] for 1 times.\n",
      "Iteration [10691]: Loss[0.07328924402788307] has not improved from the previous [0.07328921845313546] for 1 times.\n",
      "Iteration [10694]: Loss[0.07328691230577983] has not improved from the previous [0.07328682864367858] for 1 times.\n",
      "Iteration [10697]: Loss[0.073284459014515] has not improved from the previous [0.07328435947359734] for 1 times.\n",
      "Iteration [10700]: Loss[0.07328211527581525] has not improved from the previous [0.07328185640450305] for 1 times.\n",
      "Iteration [10701]: Loss[0.07328242313309416] has not improved from the previous [0.07328211527581525] for 3 times.\n",
      "Iteration [10702]: Loss[0.07328787794780878] has not improved from the previous [0.07328242313309416] for 5 times.\n",
      "Iteration [10704]: Loss[0.07327908240564258] has not improved from the previous [0.07327900849769202] for 1 times.\n",
      "Iteration [10707]: Loss[0.07327663417826884] has not improved from the previous [0.07327645353049507] for 1 times.\n",
      "Iteration [10710]: Loss[0.07327433186377504] has not improved from the previous [0.07327393359241437] for 1 times.\n",
      "Iteration [10713]: Loss[0.07327207486300372] has not improved from the previous [0.07327148003671236] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10700 Loss 0.07328211527581525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [10716]: Loss[0.07326964080859187] has not improved from the previous [0.07326905660570315] for 1 times.\n",
      "Iteration [10719]: Loss[0.07326723456704905] has not improved from the previous [0.07326658579187188] for 1 times.\n",
      "Iteration [10721]: Loss[0.07327306702524135] has not improved from the previous [0.07326664294896168] for 1 times.\n",
      "Iteration [10723]: Loss[0.07326430012943227] has not improved from the previous [0.07326368374460612] for 1 times.\n",
      "Iteration [10726]: Loss[0.07326184058395283] has not improved from the previous [0.073261187552063] for 1 times.\n",
      "Iteration [10729]: Loss[0.07325949633021663] has not improved from the previous [0.07325867352263829] for 1 times.\n",
      "Iteration [10732]: Loss[0.07325730906124406] has not improved from the previous [0.0732561664441409] for 1 times.\n",
      "Iteration [10735]: Loss[0.07325588827297609] has not improved from the previous [0.0732537919975031] for 1 times.\n",
      "Iteration [10736]: Loss[0.07326004182696905] has not improved from the previous [0.07325588827297609] for 3 times.\n",
      "Iteration [10739]: Loss[0.07325211352353717] has not improved from the previous [0.07325155250530739] for 1 times.\n",
      "Iteration [10742]: Loss[0.07324958054694161] has not improved from the previous [0.07324933028566007] for 1 times.\n",
      "Iteration [10747]: Loss[0.0732447772603654] has not improved from the previous [0.07324464283186992] for 1 times.\n",
      "Iteration [10750]: Loss[0.07324243178150686] has not improved from the previous [0.07324223318214435] for 1 times.\n",
      "Iteration [10753]: Loss[0.07324020345404776] has not improved from the previous [0.07323973377755513] for 1 times.\n",
      "Iteration [10756]: Loss[0.07323794869826372] has not improved from the previous [0.07323718500648935] for 1 times.\n",
      "Iteration [10759]: Loss[0.0732356428228637] has not improved from the previous [0.07323474187305093] for 1 times.\n",
      "Iteration [10761]: Loss[0.073241523738431] has not improved from the previous [0.07323478395742615] for 1 times.\n",
      "Iteration [10763]: Loss[0.0732326038195426] has not improved from the previous [0.07323182301520541] for 1 times.\n",
      "Iteration [10766]: Loss[0.07323041520588494] has not improved from the previous [0.07322930324660386] for 1 times.\n",
      "Iteration [10769]: Loss[0.07322848572631539] has not improved from the previous [0.07322695616057984] for 1 times.\n",
      "Iteration [10772]: Loss[0.07322564265314353] has not improved from the previous [0.07322547963682745] for 1 times.\n",
      "Iteration [10777]: Loss[0.07322096127981609] has not improved from the previous [0.07322078836247241] for 1 times.\n",
      "Iteration [10779]: Loss[0.07322689638767482] has not improved from the previous [0.0732208171648856] for 1 times.\n",
      "Iteration [10781]: Loss[0.07321817997870858] has not improved from the previous [0.0732178430813821] for 1 times.\n",
      "Iteration [10784]: Loss[0.07321581921270046] has not improved from the previous [0.07321534717265683] for 1 times.\n",
      "Iteration [10787]: Loss[0.07321355759234102] has not improved from the previous [0.0732128125783756] for 1 times.\n",
      "Iteration [10790]: Loss[0.07321136665693467] has not improved from the previous [0.07321028408968018] for 1 times.\n",
      "Iteration [10793]: Loss[0.07320954728529068] has not improved from the previous [0.07320794103219352] for 1 times.\n",
      "Iteration [10794]: Loss[0.07321434274581008] has not improved from the previous [0.07320954728529068] for 3 times.\n",
      "Iteration [10797]: Loss[0.07320619288366735] has not improved from the previous [0.07320591353575483] for 1 times.\n",
      "Iteration [10800]: Loss[0.0732037236632543] has not improved from the previous [0.07320364115029199] for 1 times.\n",
      "Iteration [10802]: Loss[0.07320159682849418] has not improved from the previous [0.07320121804493358] for 1 times.\n",
      "Iteration [10805]: Loss[0.07319936962290449] has not improved from the previous [0.07319869646721411] for 1 times.\n",
      "Iteration [10808]: Loss[0.07319709510084838] has not improved from the previous [0.07319627990433225] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10800 Loss 0.0732037236632543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [10811]: Loss[0.07319482388035277] has not improved from the previous [0.07319378968274912] for 1 times.\n",
      "Iteration [10814]: Loss[0.07319343532113648] has not improved from the previous [0.07319148797873266] for 1 times.\n",
      "Iteration [10815]: Loss[0.07319790301567748] has not improved from the previous [0.07319343532113648] for 3 times.\n",
      "Iteration [10818]: Loss[0.07318961982290319] has not improved from the previous [0.07318949430680351] for 1 times.\n",
      "Iteration [10820]: Loss[0.07318735245273708] has not improved from the previous [0.07318724866920928] for 1 times.\n",
      "Iteration [10823]: Loss[0.07318510692345705] has not improved from the previous [0.07318470051052862] for 1 times.\n",
      "Iteration [10826]: Loss[0.07318295111236009] has not improved from the previous [0.0731821652751202] for 1 times.\n",
      "Iteration [10829]: Loss[0.0731808051202335] has not improved from the previous [0.07317964379380316] for 1 times.\n",
      "Iteration [10832]: Loss[0.07317895502228751] has not improved from the previous [0.07317744413147122] for 1 times.\n",
      "Iteration [10833]: Loss[0.07318379615658979] has not improved from the previous [0.07317895502228751] for 3 times.\n",
      "Iteration [10836]: Loss[0.07317561861243753] has not improved from the previous [0.07317535788562009] for 1 times.\n",
      "Iteration [10838]: Loss[0.07317330595967432] has not improved from the previous [0.07317312438977586] for 1 times.\n",
      "Iteration [10841]: Loss[0.07317118962251264] has not improved from the previous [0.07317053006926907] for 1 times.\n",
      "Iteration [10844]: Loss[0.07316898690920239] has not improved from the previous [0.07316807984579977] for 1 times.\n",
      "Iteration [10847]: Loss[0.07316722548633434] has not improved from the previous [0.07316569711931616] for 1 times.\n",
      "Iteration [10850]: Loss[0.07316442581343219] has not improved from the previous [0.07316427185577788] for 1 times.\n",
      "Iteration [10852]: Loss[0.07316215123580526] has not improved from the previous [0.07316201840060137] for 1 times.\n",
      "Iteration [10855]: Loss[0.07316007697245715] has not improved from the previous [0.07315946467471342] for 1 times.\n",
      "Iteration [10858]: Loss[0.07315797836399829] has not improved from the previous [0.07315694249999187] for 1 times.\n",
      "Iteration [10860]: Loss[0.07316393193073618] has not improved from the previous [0.07315701111376854] for 1 times.\n",
      "Iteration [10862]: Loss[0.07315567099503226] has not improved from the previous [0.07315405147713216] for 1 times.\n",
      "Iteration [10865]: Loss[0.07315289373419398] has not improved from the previous [0.0731525465757267] for 1 times.\n",
      "Iteration [10867]: Loss[0.07315062045554938] has not improved from the previous [0.07315038171923251] for 1 times.\n",
      "Iteration [10870]: Loss[0.07314855214565728] has not improved from the previous [0.07314778661478157] for 1 times.\n",
      "Iteration [10873]: Loss[0.07314640026874093] has not improved from the previous [0.07314532588786811] for 1 times.\n",
      "Iteration [10876]: Loss[0.07314544378316908] has not improved from the previous [0.07314314739443589] for 1 times.\n",
      "Iteration [10877]: Loss[0.07314958738768836] has not improved from the previous [0.07314544378316908] for 3 times.\n",
      "Iteration [10880]: Loss[0.07314122381557543] has not improved from the previous [0.07314115283737312] for 1 times.\n",
      "Iteration [10882]: Loss[0.07313909269673338] has not improved from the previous [0.07313869582390277] for 1 times.\n",
      "Iteration [10885]: Loss[0.07313700121832228] has not improved from the previous [0.07313618081939402] for 1 times.\n",
      "Iteration [10888]: Loss[0.07313536509842754] has not improved from the previous [0.07313374776005559] for 1 times.\n",
      "Iteration [10891]: Loss[0.07313250134838051] has not improved from the previous [0.07313240743795435] for 1 times.\n",
      "Iteration [10893]: Loss[0.07313034730199239] has not improved from the previous [0.07313011615792149] for 1 times.\n",
      "Iteration [10896]: Loss[0.07312829526576443] has not improved from the previous [0.0731275472316206] for 1 times.\n",
      "Iteration [10899]: Loss[0.07312859650481404] has not improved from the previous [0.07312501752714455] for 1 times.\n",
      "Iteration [10900]: Loss[0.07313156361062506] has not improved from the previous [0.07312859650481404] for 3 times.\n",
      "Iteration [10903]: Loss[0.0731233479190087] has not improved from the previous [0.07312321687322212] for 1 times.\n",
      "Iteration [10905]: Loss[0.07312110538602329] has not improved from the previous [0.07312097184192683] for 1 times.\n",
      "Iteration [10908]: Loss[0.07311903916031888] has not improved from the previous [0.07311839015909634] for 1 times.\n",
      "Iteration [10911]: Loss[0.07311703325218057] has not improved from the previous [0.0731158245319533] for 1 times.\n",
      "Iteration [10914]: Loss[0.07311550036176477] has not improved from the previous [0.07311374252765818] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10900 Loss 0.07313156361062506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [10915]: Loss[0.07312021471576895] has not improved from the previous [0.07311550036176477] for 3 times.\n",
      "Iteration [10920]: Loss[0.07310988479510977] has not improved from the previous [0.0731092714535126] for 1 times.\n",
      "Iteration [10923]: Loss[0.07310836520110814] has not improved from the previous [0.07310667756619454] for 1 times.\n",
      "Iteration [10926]: Loss[0.07310556235628154] has not improved from the previous [0.07310538156444887] for 1 times.\n",
      "Iteration [10928]: Loss[0.0731034876865953] has not improved from the previous [0.073103008445078] for 1 times.\n",
      "Iteration [10931]: Loss[0.07310146986607526] has not improved from the previous [0.07310048712088955] for 1 times.\n",
      "Iteration [10934]: Loss[0.07310029786586421] has not improved from the previous [0.07309826473016269] for 1 times.\n",
      "Iteration [10935]: Loss[0.07310474853557462] has not improved from the previous [0.07310029786586421] for 3 times.\n",
      "Iteration [10938]: Loss[0.07309638790366643] has not improved from the previous [0.07309636689289427] for 1 times.\n",
      "Iteration [10940]: Loss[0.07309439548299265] has not improved from the previous [0.07309384240289674] for 1 times.\n",
      "Iteration [10943]: Loss[0.0730924061651194] has not improved from the previous [0.07309128805309895] for 1 times.\n",
      "Iteration [10946]: Loss[0.07309049050961768] has not improved from the previous [0.07308918868067715] for 1 times.\n",
      "Iteration [10948]: Loss[0.07308794116374634] has not improved from the previous [0.0730877792698254] for 1 times.\n",
      "Iteration [10951]: Loss[0.07308595882843594] has not improved from the previous [0.0730852159995488] for 1 times.\n",
      "Iteration [10954]: Loss[0.07308502756242657] has not improved from the previous [0.07308280288312706] for 1 times.\n",
      "Iteration [10955]: Loss[0.07308938432271872] has not improved from the previous [0.07308502756242657] for 3 times.\n",
      "Iteration [10960]: Loss[0.07307905890389997] has not improved from the previous [0.0730784763625187] for 1 times.\n",
      "Iteration [10963]: Loss[0.07307703588935803] has not improved from the previous [0.07307597854233333] for 1 times.\n",
      "Iteration [10966]: Loss[0.07307506989815189] has not improved from the previous [0.07307387830839696] for 1 times.\n",
      "Iteration [10968]: Loss[0.07307261913426044] has not improved from the previous [0.07307245757289615] for 1 times.\n",
      "Iteration [10971]: Loss[0.07307068413157858] has not improved from the previous [0.07306979723496672] for 1 times.\n",
      "Iteration [10974]: Loss[0.07306963233934727] has not improved from the previous [0.07306752012052808] for 1 times.\n",
      "Iteration [10975]: Loss[0.07307412756130004] has not improved from the previous [0.07306963233934727] for 3 times.\n",
      "Iteration [10977]: Loss[0.07306575969452248] has not improved from the previous [0.07306573070881313] for 1 times.\n",
      "Iteration [10980]: Loss[0.07306377198232346] has not improved from the previous [0.07306317162044902] for 1 times.\n",
      "Iteration [10983]: Loss[0.07306225194282215] has not improved from the previous [0.07306065152879249] for 1 times.\n",
      "Iteration [10988]: Loss[0.07305753380938969] has not improved from the previous [0.07305693122602235] for 1 times.\n",
      "Iteration [10991]: Loss[0.073056278865245] has not improved from the previous [0.07305439167326513] for 1 times.\n",
      "Iteration [10992]: Loss[0.07306101522467562] has not improved from the previous [0.073056278865245] for 3 times.\n",
      "Iteration [10997]: Loss[0.07305077829798289] has not improved from the previous [0.07305025668668177] for 1 times.\n",
      "Iteration [11000]: Loss[0.0730488027940401] has not improved from the previous [0.07304767455459864] for 1 times.\n",
      "Iteration [11003]: Loss[0.07304682550806112] has not improved from the previous [0.07304568240319018] for 1 times.\n",
      "Iteration [11005]: Loss[0.0730444882815912] has not improved from the previous [0.07304412540579362] for 1 times.\n",
      "Iteration [11008]: Loss[0.07304268885862393] has not improved from the previous [0.0730414296063685] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11000 Loss 0.0730488027940401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [11011]: Loss[0.0730406581125319] has not improved from the previous [0.07303951142862469] for 1 times.\n",
      "Iteration [11013]: Loss[0.07303833236205859] has not improved from the previous [0.0730378663783295] for 1 times.\n",
      "Iteration [11016]: Loss[0.07303645988247838] has not improved from the previous [0.07303531843382577] for 1 times.\n",
      "Iteration [11019]: Loss[0.07303613031886277] has not improved from the previous [0.07303337184179531] for 1 times.\n",
      "Iteration [11020]: Loss[0.07304000765192269] has not improved from the previous [0.07303613031886277] for 3 times.\n",
      "Iteration [11022]: Loss[0.07303167730105445] has not improved from the previous [0.07303123910023587] for 1 times.\n",
      "Iteration [11025]: Loss[0.07302980253893358] has not improved from the previous [0.07302855062655988] for 1 times.\n",
      "Iteration [11028]: Loss[0.07302777923881076] has not improved from the previous [0.07302663785384396] for 1 times.\n",
      "Iteration [11030]: Loss[0.07302556756248337] has not improved from the previous [0.0730249735046926] for 1 times.\n",
      "Iteration [11033]: Loss[0.07302467157564589] has not improved from the previous [0.0730224696575359] for 1 times.\n",
      "Iteration [11034]: Loss[0.07302908868940024] has not improved from the previous [0.07302467157564589] for 3 times.\n",
      "Iteration [11039]: Loss[0.0730189543186563] has not improved from the previous [0.07301819516431311] for 1 times.\n",
      "Iteration [11042]: Loss[0.07301736762932938] has not improved from the previous [0.07301584863018647] for 1 times.\n",
      "Iteration [11044]: Loss[0.07301467989837525] has not improved from the previous [0.07301459261802269] for 1 times.\n",
      "Iteration [11047]: Loss[0.07301295938945229] has not improved from the previous [0.07301191312069213] for 1 times.\n",
      "Iteration [11050]: Loss[0.07301114624911889] has not improved from the previous [0.07300982669797788] for 1 times.\n",
      "Iteration [11052]: Loss[0.0730087709033647] has not improved from the previous [0.07300835784354029] for 1 times.\n",
      "Iteration [11055]: Loss[0.0730069327378802] has not improved from the previous [0.07300575930240599] for 1 times.\n",
      "Iteration [11058]: Loss[0.07300625285593472] has not improved from the previous [0.07300387922313621] for 1 times.\n",
      "Iteration [11059]: Loss[0.07301057629234925] has not improved from the previous [0.07300625285593472] for 3 times.\n",
      "Iteration [11061]: Loss[0.07300218344752854] has not improved from the previous [0.0730016361084302] for 1 times.\n",
      "Iteration [11064]: Loss[0.07300082514665968] has not improved from the previous [0.07299906009412276] for 1 times.\n",
      "Iteration [11066]: Loss[0.07299806965182959] has not improved from the previous [0.07299798237716934] for 1 times.\n",
      "Iteration [11069]: Loss[0.07299632436196435] has not improved from the previous [0.07299528523889377] for 1 times.\n",
      "Iteration [11072]: Loss[0.07299458807013082] has not improved from the previous [0.07299325302224811] for 1 times.\n",
      "Iteration [11073]: Loss[0.07299988620916861] has not improved from the previous [0.07299458807013082] for 3 times.\n",
      "Iteration [11075]: Loss[0.07299162368411702] has not improved from the previous [0.07299125926468453] for 1 times.\n",
      "Iteration [11078]: Loss[0.07299030005059788] has not improved from the previous [0.07298857282296685] for 1 times.\n",
      "Iteration [11081]: Loss[0.07298747130990983] has not improved from the previous [0.07298740357105382] for 1 times.\n",
      "Iteration [11083]: Loss[0.072985730462532] has not improved from the previous [0.07298486033735903] for 1 times.\n",
      "Iteration [11086]: Loss[0.07298408216103985] has not improved from the previous [0.07298264176217371] for 1 times.\n",
      "Iteration [11088]: Loss[0.07298166712465018] has not improved from the previous [0.07298124745626434] for 1 times.\n",
      "Iteration [11091]: Loss[0.07298066360795505] has not improved from the previous [0.07297863025894467] for 1 times.\n",
      "Iteration [11092]: Loss[0.07298530090790314] has not improved from the previous [0.07298066360795505] for 3 times.\n",
      "Iteration [11097]: Loss[0.07297524071675339] has not improved from the previous [0.07297444179342227] for 1 times.\n",
      "Iteration [11100]: Loss[0.07297360555558945] has not improved from the previous [0.0729722062757624] for 1 times.\n",
      "Iteration [11102]: Loss[0.07297118934188716] has not improved from the previous [0.07297082239823754] for 1 times.\n",
      "Iteration [11105]: Loss[0.07296993138869388] has not improved from the previous [0.07296818322544484] for 1 times.\n",
      "Iteration [11107]: Loss[0.07296713899975656] has not improved from the previous [0.07296711052846976] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11100 Loss 0.07297360555558945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [11110]: Loss[0.07296549568868796] has not improved from the previous [0.07296442021244143] for 1 times.\n",
      "Iteration [11113]: Loss[0.07296409419149663] has not improved from the previous [0.07296245253453046] for 1 times.\n",
      "Iteration [11114]: Loss[0.07296912857697764] has not improved from the previous [0.07296409419149663] for 3 times.\n",
      "Iteration [11116]: Loss[0.07296090733648242] has not improved from the previous [0.0729603824878589] for 1 times.\n",
      "Iteration [11119]: Loss[0.0729594163773798] has not improved from the previous [0.07295789838671082] for 1 times.\n",
      "Iteration [11121]: Loss[0.07295677781809036] has not improved from the previous [0.07295667923195859] for 1 times.\n",
      "Iteration [11124]: Loss[0.07295514178330781] has not improved from the previous [0.07295396759130479] for 1 times.\n",
      "Iteration [11127]: Loss[0.07295318543046654] has not improved from the previous [0.07295208846976356] for 1 times.\n",
      "Iteration [11129]: Loss[0.07295106697364422] has not improved from the previous [0.0729503455612236] for 1 times.\n",
      "Iteration [11132]: Loss[0.07294989089363066] has not improved from the previous [0.07294808853664232] for 1 times.\n",
      "Iteration [11133]: Loss[0.07295481365148622] has not improved from the previous [0.07294989089363066] for 3 times.\n",
      "Iteration [11135]: Loss[0.07294667768622319] has not improved from the previous [0.07294624894831217] for 1 times.\n",
      "Iteration [11138]: Loss[0.07294523093357001] has not improved from the previous [0.07294368735148313] for 1 times.\n",
      "Iteration [11140]: Loss[0.0729426869786392] has not improved from the previous [0.07294252116126868] for 1 times.\n",
      "Iteration [11143]: Loss[0.0729409764127232] has not improved from the previous [0.0729397891083468] for 1 times.\n",
      "Iteration [11146]: Loss[0.07293893653234033] has not improved from the previous [0.0729379969626015] for 1 times.\n",
      "Iteration [11148]: Loss[0.07293693531340206] has not improved from the previous [0.07293616343780407] for 1 times.\n",
      "Iteration [11151]: Loss[0.07293538856067831] has not improved from the previous [0.072933996537839] for 1 times.\n",
      "Iteration [11152]: Loss[0.07294079312617062] has not improved from the previous [0.07293538856067831] for 3 times.\n",
      "Iteration [11154]: Loss[0.07293257426525206] has not improved from the previous [0.07293197355623514] for 1 times.\n",
      "Iteration [11157]: Loss[0.07293107493787683] has not improved from the previous [0.07292961299928373] for 1 times.\n",
      "Iteration [11159]: Loss[0.07292864669499492] has not improved from the previous [0.07292828150156547] for 1 times.\n",
      "Iteration [11162]: Loss[0.07292731530751224] has not improved from the previous [0.07292575512313325] for 1 times.\n",
      "Iteration [11164]: Loss[0.07292470354254979] has not improved from the previous [0.07292455965411708] for 1 times.\n",
      "Iteration [11167]: Loss[0.07292303878574984] has not improved from the previous [0.07292189168858053] for 1 times.\n",
      "Iteration [11170]: Loss[0.07292101452817502] has not improved from the previous [0.07292009938499791] for 1 times.\n",
      "Iteration [11172]: Loss[0.0729190360996591] has not improved from the previous [0.07291826014787894] for 1 times.\n",
      "Iteration [11175]: Loss[0.0729183581513712] has not improved from the previous [0.07291615419485084] for 1 times.\n",
      "Iteration [11176]: Loss[0.0729229915208171] has not improved from the previous [0.0729183581513712] for 3 times.\n",
      "Iteration [11178]: Loss[0.07291486253690076] has not improved from the previous [0.07291401952521734] for 1 times.\n",
      "Iteration [11181]: Loss[0.07291310930020399] has not improved from the previous [0.07291187041410752] for 1 times.\n",
      "Iteration [11183]: Loss[0.07291087525259389] has not improved from the previous [0.07291028400216863] for 1 times.\n",
      "Iteration [11186]: Loss[0.07290939735107062] has not improved from the previous [0.0729079980256016] for 1 times.\n",
      "Iteration [11188]: Loss[0.07290709517299676] has not improved from the previous [0.07290658689345571] for 1 times.\n",
      "Iteration [11191]: Loss[0.07290586738296832] has not improved from the previous [0.07290422912038819] for 1 times.\n",
      "Iteration [11192]: Loss[0.07291101176722307] has not improved from the previous [0.07290586738296832] for 3 times.\n",
      "Iteration [11194]: Loss[0.07290274773252345] has not improved from the previous [0.07290235217829628] for 1 times.\n",
      "Iteration [11197]: Loss[0.07290146724807332] has not improved from the previous [0.07289980205643547] for 1 times.\n",
      "Iteration [11199]: Loss[0.07289892228670053] has not improved from the previous [0.07289862960870662] for 1 times.\n",
      "Iteration [11202]: Loss[0.07289770551523564] has not improved from the previous [0.07289602619803219] for 1 times.\n",
      "Iteration [11204]: Loss[0.07289509604407643] has not improved from the previous [0.07289486121405456] for 1 times.\n",
      "Iteration [11207]: Loss[0.07289400929041834] has not improved from the previous [0.07289220423319585] for 1 times.\n",
      "Iteration [11209]: Loss[0.07289132678466884] has not improved from the previous [0.07289111866043332] for 1 times.\n",
      "Iteration [11212]: Loss[0.07289099430453448] has not improved from the previous [0.0728884388624542] for 1 times.\n",
      "Iteration [11213]: Loss[0.07289526885486323] has not improved from the previous [0.07289099430453448] for 3 times.\n",
      "Iteration [11215]: Loss[0.0728871908128401] has not improved from the previous [0.07288694142632321] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11200 Loss 0.07289848943818178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [11218]: Loss[0.07288591934191938] has not improved from the previous [0.07288428055686454] for 1 times.\n",
      "Iteration [11220]: Loss[0.07288330273099398] has not improved from the previous [0.07288317154559527] for 1 times.\n",
      "Iteration [11223]: Loss[0.07288217647564965] has not improved from the previous [0.0728804926834109] for 1 times.\n",
      "Iteration [11225]: Loss[0.07287953124872602] has not improved from the previous [0.07287944247847869] for 1 times.\n",
      "Iteration [11228]: Loss[0.07287846202262621] has not improved from the previous [0.07287672585748473] for 1 times.\n",
      "Iteration [11230]: Loss[0.07287585431260968] has not improved from the previous [0.07287569971385918] for 1 times.\n",
      "Iteration [11233]: Loss[0.0728755883799616] has not improved from the previous [0.0728730384418945] for 1 times.\n",
      "Iteration [11234]: Loss[0.07287993373439704] has not improved from the previous [0.0728755883799616] for 3 times.\n",
      "Iteration [11236]: Loss[0.07287175927142962] has not improved from the previous [0.07287133174417226] for 1 times.\n",
      "Iteration [11239]: Loss[0.07287048878717989] has not improved from the previous [0.072868802277452] for 1 times.\n",
      "Iteration [11241]: Loss[0.07286794566253336] has not improved from the previous [0.07286758542489077] for 1 times.\n",
      "Iteration [11244]: Loss[0.07286673303378473] has not improved from the previous [0.07286507820346366] for 1 times.\n",
      "Iteration [11246]: Loss[0.07286422377497506] has not improved from the previous [0.07286384357060968] for 1 times.\n",
      "Iteration [11249]: Loss[0.07286297682709336] has not improved from the previous [0.07286139008423576] for 1 times.\n",
      "Iteration [11251]: Loss[0.07286058668350208] has not improved from the previous [0.07286012481140479] for 1 times.\n",
      "Iteration [11254]: Loss[0.07286016232769858] has not improved from the previous [0.07285777117211145] for 1 times.\n",
      "Iteration [11255]: Loss[0.07286461804704811] has not improved from the previous [0.07286016232769858] for 3 times.\n",
      "Iteration [11257]: Loss[0.07285639560802797] has not improved from the previous [0.07285586315618124] for 1 times.\n",
      "Iteration [11260]: Loss[0.07285494674346096] has not improved from the previous [0.0728535276830754] for 1 times.\n",
      "Iteration [11262]: Loss[0.07285262305726554] has not improved from the previous [0.07285210784690123] for 1 times.\n",
      "Iteration [11265]: Loss[0.07285120707004002] has not improved from the previous [0.07284981926087862] for 1 times.\n",
      "Iteration [11267]: Loss[0.07284895198214508] has not improved from the previous [0.07284834547156543] for 1 times.\n",
      "Iteration [11270]: Loss[0.07284747975961832] has not improved from the previous [0.07284612467181403] for 1 times.\n",
      "Iteration [11272]: Loss[0.07284529198534663] has not improved from the previous [0.07284459106683801] for 1 times.\n",
      "Iteration [11275]: Loss[0.07284477670014079] has not improved from the previous [0.07284245759827376] for 1 times.\n",
      "Iteration [11276]: Loss[0.07284937529165916] has not improved from the previous [0.07284477670014079] for 3 times.\n",
      "Iteration [11278]: Loss[0.07284132539275223] has not improved from the previous [0.07284035874702809] for 1 times.\n",
      "Iteration [11281]: Loss[0.07283935939455355] has not improved from the previous [0.07283846228919855] for 1 times.\n",
      "Iteration [11283]: Loss[0.07283754311109693] has not improved from the previous [0.07283657852891191] for 1 times.\n",
      "Iteration [11286]: Loss[0.07283560112398038] has not improved from the previous [0.07283477897515168] for 1 times.\n",
      "Iteration [11288]: Loss[0.07283387310945043] has not improved from the previous [0.07283283881697629] for 1 times.\n",
      "Iteration [11291]: Loss[0.07283185429248651] has not improved from the previous [0.0728311114638704] for 1 times.\n",
      "Iteration [11293]: Loss[0.07283020669062497] has not improved from the previous [0.07282910770176347] for 1 times.\n",
      "Iteration [11296]: Loss[0.07282908264359167] has not improved from the previous [0.07282745371545718] for 1 times.\n",
      "Iteration [11297]: Loss[0.0728343973835276] has not improved from the previous [0.07282908264359167] for 3 times.\n",
      "Iteration [11299]: Loss[0.07282664283344138] has not improved from the previous [0.0728249117777606] for 1 times.\n",
      "Iteration [11301]: Loss[0.07282411613560387] has not improved from the previous [0.0728237404685028] for 1 times.\n",
      "Iteration [11304]: Loss[0.07282277955881057] has not improved from the previous [0.07282131981605765] for 1 times.\n",
      "Iteration [11306]: Loss[0.07282052020338504] has not improved from the previous [0.07281988200906186] for 1 times.\n",
      "Iteration [11309]: Loss[0.07281901567940868] has not improved from the previous [0.07281771948944418] for 1 times.\n",
      "Iteration [11311]: Loss[0.07281693852159303] has not improved from the previous [0.07281610533686184] for 1 times.\n",
      "Iteration [11314]: Loss[0.07281593630920072] has not improved from the previous [0.07281413870846176] for 1 times.\n",
      "Iteration [11315]: Loss[0.07282107994446792] has not improved from the previous [0.07281593630920072] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11300 Loss 0.0728237404685028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [11317]: Loss[0.07281297476510065] has not improved from the previous [0.07281185893812442] for 1 times.\n",
      "Iteration [11320]: Loss[0.0728108932227805] has not improved from the previous [0.07281016393220865] for 1 times.\n",
      "Iteration [11322]: Loss[0.07280937582930597] has not improved from the previous [0.07280809469024993] for 1 times.\n",
      "Iteration [11325]: Loss[0.0728070682661547] has not improved from the previous [0.07280662057523167] for 1 times.\n",
      "Iteration [11327]: Loss[0.0728061061084508] has not improved from the previous [0.07280441969510672] for 1 times.\n",
      "Iteration [11329]: Loss[0.07280380383292201] has not improved from the previous [0.07280319436661481] for 1 times.\n",
      "Iteration [11332]: Loss[0.07280253459037457] has not improved from the previous [0.07280108463804831] for 1 times.\n",
      "Iteration [11333]: Loss[0.07280801962771682] has not improved from the previous [0.07280253459037457] for 3 times.\n",
      "Iteration [11335]: Loss[0.07279985669002845] has not improved from the previous [0.0727988420807504] for 1 times.\n",
      "Iteration [11338]: Loss[0.07279794353279985] has not improved from the previous [0.07279701669625382] for 1 times.\n",
      "Iteration [11340]: Loss[0.07279622046408529] has not improved from the previous [0.07279506192123508] for 1 times.\n",
      "Iteration [11343]: Loss[0.07279417055531841] has not improved from the previous [0.07279345012293265] for 1 times.\n",
      "Iteration [11345]: Loss[0.07279313620375583] has not improved from the previous [0.07279129096744977] for 1 times.\n",
      "Iteration [11347]: Loss[0.07279069307113421] has not improved from the previous [0.0727902265922683] for 1 times.\n",
      "Iteration [11350]: Loss[0.07278924900569488] has not improved from the previous [0.07278797414773112] for 1 times.\n",
      "Iteration [11352]: Loss[0.07278721264796167] has not improved from the previous [0.07278636925891234] for 1 times.\n",
      "Iteration [11355]: Loss[0.07278641342818729] has not improved from the previous [0.0727844631568688] for 1 times.\n",
      "Iteration [11356]: Loss[0.07279147846882991] has not improved from the previous [0.07278641342818729] for 3 times.\n",
      "Iteration [11358]: Loss[0.07278389484079989] has not improved from the previous [0.07278208679600418] for 1 times.\n",
      "Iteration [11360]: Loss[0.072781312500787] has not improved from the previous [0.07278093404037173] for 1 times.\n",
      "Iteration [11363]: Loss[0.07278003680800292] has not improved from the previous [0.07277856182077083] for 1 times.\n",
      "Iteration [11365]: Loss[0.07277785990138282] has not improved from the previous [0.07277709145991954] for 1 times.\n",
      "Iteration [11368]: Loss[0.07277620755363988] has not improved from the previous [0.07277512066764456] for 1 times.\n",
      "Iteration [11370]: Loss[0.07277437455111853] has not improved from the previous [0.07277330953461955] for 1 times.\n",
      "Iteration [11373]: Loss[0.0727727581472819] has not improved from the previous [0.07277166473262602] for 1 times.\n",
      "Iteration [11374]: Loss[0.07277861011424365] has not improved from the previous [0.0727727581472819] for 3 times.\n",
      "Iteration [11376]: Loss[0.07277080846667093] has not improved from the previous [0.07276925042271257] for 1 times.\n",
      "Iteration [11378]: Loss[0.07276849940312165] has not improved from the previous [0.07276799041498859] for 1 times.\n",
      "Iteration [11381]: Loss[0.07276689834050144] has not improved from the previous [0.07276581662087371] for 1 times.\n",
      "Iteration [11383]: Loss[0.07276503291995762] has not improved from the previous [0.07276410689329466] for 1 times.\n",
      "Iteration [11386]: Loss[0.07276309388177798] has not improved from the previous [0.0727623592099359] for 1 times.\n",
      "Iteration [11388]: Loss[0.07276156220137597] has not improved from the previous [0.07276028488866847] for 1 times.\n",
      "Iteration [11391]: Loss[0.07275929924878399] has not improved from the previous [0.07275887896958945] for 1 times.\n",
      "Iteration [11393]: Loss[0.07275917823450505] has not improved from the previous [0.07275674748934698] for 1 times.\n",
      "Iteration [11394]: Loss[0.0727638669736871] has not improved from the previous [0.07275917823450505] for 3 times.\n",
      "Iteration [11396]: Loss[0.07275578261926242] has not improved from the previous [0.07275480410651632] for 1 times.\n",
      "Iteration [11399]: Loss[0.0727539195438105] has not improved from the previous [0.07275296776475439] for 1 times.\n",
      "Iteration [11401]: Loss[0.07275227469874886] has not improved from the previous [0.07275095679704169] for 1 times.\n",
      "Iteration [11404]: Loss[0.07275007347759738] has not improved from the previous [0.07274952986554913] for 1 times.\n",
      "Iteration [11406]: Loss[0.07274906417336117] has not improved from the previous [0.07274737280314418] for 1 times.\n",
      "Iteration [11408]: Loss[0.07274688027463676] has not improved from the previous [0.07274607729904066] for 1 times.\n",
      "Iteration [11411]: Loss[0.0727451952593763] has not improved from the previous [0.07274415459876775] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11400 Loss 0.07275095679704169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [11412]: Loss[0.0727511877527746] has not improved from the previous [0.0727451952593763] for 3 times.\n",
      "Iteration [11414]: Loss[0.07274351574616689] has not improved from the previous [0.0727418603206436] for 1 times.\n",
      "Iteration [11416]: Loss[0.07274111070516698] has not improved from the previous [0.07274054999654347] for 1 times.\n",
      "Iteration [11419]: Loss[0.07273960805971524] has not improved from the previous [0.07273842108078701] for 1 times.\n",
      "Iteration [11421]: Loss[0.07273772456032561] has not improved from the previous [0.07273667814354082] for 1 times.\n",
      "Iteration [11424]: Loss[0.07273574883699324] has not improved from the previous [0.07273504396955097] for 1 times.\n",
      "Iteration [11426]: Loss[0.07273476782807183] has not improved from the previous [0.07273287655498768] for 1 times.\n",
      "Iteration [11428]: Loss[0.07273244252681318] has not improved from the previous [0.07273173741905023] for 1 times.\n",
      "Iteration [11431]: Loss[0.07273102765193835] has not improved from the previous [0.07272974641193032] for 1 times.\n",
      "Iteration [11432]: Loss[0.07273674355604831] has not improved from the previous [0.07273102765193835] for 3 times.\n",
      "Iteration [11434]: Loss[0.0727291584584759] has not improved from the previous [0.07272742982577042] for 1 times.\n",
      "Iteration [11436]: Loss[0.07272665502717564] has not improved from the previous [0.07272628993870094] for 1 times.\n",
      "Iteration [11439]: Loss[0.07272524127235955] has not improved from the previous [0.07272403508506156] for 1 times.\n",
      "Iteration [11441]: Loss[0.0727233171397272] has not improved from the previous [0.0727223993190647] for 1 times.\n",
      "Iteration [11444]: Loss[0.07272136957441366] has not improved from the previous [0.07272068155123303] for 1 times.\n",
      "Iteration [11446]: Loss[0.0727203818773358] has not improved from the previous [0.07271859503458007] for 1 times.\n",
      "Iteration [11448]: Loss[0.07271801893202487] has not improved from the previous [0.07271743220093879] for 1 times.\n",
      "Iteration [11451]: Loss[0.07271676935389569] has not improved from the previous [0.07271541269957993] for 1 times.\n",
      "Iteration [11452]: Loss[0.07272246570399143] has not improved from the previous [0.07271676935389569] for 3 times.\n",
      "Iteration [11454]: Loss[0.07271485543703275] has not improved from the previous [0.07271307594319464] for 1 times.\n",
      "Iteration [11456]: Loss[0.07271240521516624] has not improved from the previous [0.07271195425207866] for 1 times.\n",
      "Iteration [11459]: Loss[0.07271088060949722] has not improved from the previous [0.07270978287879279] for 1 times.\n",
      "Iteration [11461]: Loss[0.07270908055957061] has not improved from the previous [0.07270801630140966] for 1 times.\n",
      "Iteration [11464]: Loss[0.0727070190050205] has not improved from the previous [0.07270646054334085] for 1 times.\n",
      "Iteration [11466]: Loss[0.07270597070779662] has not improved from the previous [0.07270439331840416] for 1 times.\n",
      "Iteration [11468]: Loss[0.07270382236401807] has not improved from the previous [0.072703089603323] for 1 times.\n",
      "Iteration [11471]: Loss[0.07270229736076142] has not improved from the previous [0.07270123427208705] for 1 times.\n",
      "Iteration [11472]: Loss[0.07270833791429361] has not improved from the previous [0.07270229736076142] for 3 times.\n",
      "Iteration [11474]: Loss[0.07270050608103998] has not improved from the previous [0.07269891638440391] for 1 times.\n",
      "Iteration [11476]: Loss[0.07269822911086582] has not improved from the previous [0.07269753578448164] for 1 times.\n",
      "Iteration [11479]: Loss[0.07269657434209527] has not improved from the previous [0.07269559044552526] for 1 times.\n",
      "Iteration [11481]: Loss[0.07269496990782094] has not improved from the previous [0.0726936235782769] for 1 times.\n",
      "Iteration [11484]: Loss[0.07269270448268966] has not improved from the previous [0.07269230236500143] for 1 times.\n",
      "Iteration [11486]: Loss[0.07269165122589658] has not improved from the previous [0.07269025221573412] for 1 times.\n",
      "Iteration [11488]: Loss[0.07268971777928719] has not improved from the previous [0.07268869349741774] for 1 times.\n",
      "Iteration [11491]: Loss[0.07268786961429825] has not improved from the previous [0.07268711789872091] for 1 times.\n",
      "Iteration [11492]: Loss[0.07269423745198951] has not improved from the previous [0.07268786961429825] for 3 times.\n",
      "Iteration [11494]: Loss[0.0726861346873304] has not improved from the previous [0.07268487507390059] for 1 times.\n",
      "Iteration [11496]: Loss[0.07268422976287008] has not improved from the previous [0.07268313019251818] for 1 times.\n",
      "Iteration [11499]: Loss[0.07268219910961765] has not improved from the previous [0.07268157553194073] for 1 times.\n",
      "Iteration [11501]: Loss[0.07268118079135356] has not improved from the previous [0.07267949206874244] for 1 times.\n",
      "Iteration [11503]: Loss[0.07267904039102362] has not improved from the previous [0.0726781417513304] for 1 times.\n",
      "Iteration [11506]: Loss[0.07267721772096154] has not improved from the previous [0.07267641309854157] for 1 times.\n",
      "Iteration [11508]: Loss[0.07267639442456342] has not improved from the previous [0.07267438017862095] for 1 times.\n",
      "Iteration [11509]: Loss[0.07268161324495381] has not improved from the previous [0.07267639442456342] for 3 times.\n",
      "Iteration [11511]: Loss[0.07267367677794616] has not improved from the previous [0.07267257955813493] for 1 times.\n",
      "Iteration [11514]: Loss[0.07267162861985332] has not improved from the previous [0.07267099827305439] for 1 times.\n",
      "Iteration [11516]: Loss[0.07267062546318183] has not improved from the previous [0.07266889682584082] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11500 Loss 0.07267949206874244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [11518]: Loss[0.07266856061952721] has not improved from the previous [0.07266755228914813] for 1 times.\n",
      "Iteration [11521]: Loss[0.07266658132929531] has not improved from the previous [0.07266591722936121] for 1 times.\n",
      "Iteration [11523]: Loss[0.07266558366203087] has not improved from the previous [0.07266383901823967] for 1 times.\n",
      "Iteration [11525]: Loss[0.07266343697945152] has not improved from the previous [0.07266252239258698] for 1 times.\n",
      "Iteration [11528]: Loss[0.07266158824777728] has not improved from the previous [0.07266082565392422] for 1 times.\n",
      "Iteration [11530]: Loss[0.07266136231104439] has not improved from the previous [0.07265878451849643] for 1 times.\n",
      "Iteration [11531]: Loss[0.07266604656319127] has not improved from the previous [0.07266136231104439] for 3 times.\n",
      "Iteration [11533]: Loss[0.07265811636185378] has not improved from the previous [0.07265696568637792] for 1 times.\n",
      "Iteration [11536]: Loss[0.07265598274575916] has not improved from the previous [0.07265547428837209] for 1 times.\n",
      "Iteration [11538]: Loss[0.07265497804773943] has not improved from the previous [0.07265339034813789] for 1 times.\n",
      "Iteration [11540]: Loss[0.07265299553780644] has not improved from the previous [0.07265190795642634] for 1 times.\n",
      "Iteration [11543]: Loss[0.07265097183138232] has not improved from the previous [0.0726503901044617] for 1 times.\n",
      "Iteration [11545]: Loss[0.07264996993807117] has not improved from the previous [0.07264833339937382] for 1 times.\n",
      "Iteration [11547]: Loss[0.07264795514058937] has not improved from the previous [0.07264689820087443] for 1 times.\n",
      "Iteration [11550]: Loss[0.07264611026103056] has not improved from the previous [0.07264536088846063] for 1 times.\n",
      "Iteration [11551]: Loss[0.07265248283532555] has not improved from the previous [0.07264611026103056] for 3 times.\n",
      "Iteration [11553]: Loss[0.07264431808148732] has not improved from the previous [0.07264323358262254] for 1 times.\n",
      "Iteration [11555]: Loss[0.07264259205622126] has not improved from the previous [0.07264135008905971] for 1 times.\n",
      "Iteration [11558]: Loss[0.07264034552751185] has not improved from the previous [0.07264004237071352] for 1 times.\n",
      "Iteration [11560]: Loss[0.07263933939492746] has not improved from the previous [0.07263798552205215] for 1 times.\n",
      "Iteration [11562]: Loss[0.07263761294665715] has not improved from the previous [0.07263626600663628] for 1 times.\n",
      "Iteration [11565]: Loss[0.0726353218030376] has not improved from the previous [0.07263503239218742] for 1 times.\n",
      "Iteration [11567]: Loss[0.07263432012248576] has not improved from the previous [0.07263298821214995] for 1 times.\n",
      "Iteration [11569]: Loss[0.07263262581227316] has not improved from the previous [0.07263124644671254] for 1 times.\n",
      "Iteration [11572]: Loss[0.0726308018007474] has not improved from the previous [0.07263004463543377] for 1 times.\n",
      "Iteration [11573]: Loss[0.07263717424856588] has not improved from the previous [0.0726308018007474] for 3 times.\n",
      "Iteration [11575]: Loss[0.07262866117365978] has not improved from the previous [0.0726279396899468] for 1 times.\n",
      "Iteration [11577]: Loss[0.07262763868069341] has not improved from the previous [0.07262583265149974] for 1 times.\n",
      "Iteration [11579]: Loss[0.07262548593803035] has not improved from the previous [0.07262452879740644] for 1 times.\n",
      "Iteration [11582]: Loss[0.07262357694779462] has not improved from the previous [0.07262291932525217] for 1 times.\n",
      "Iteration [11584]: Loss[0.07262255970707476] has not improved from the previous [0.07262089897285154] for 1 times.\n",
      "Iteration [11586]: Loss[0.07262056498222581] has not improved from the previous [0.07261946763748944] for 1 times.\n",
      "Iteration [11589]: Loss[0.0726185257824861] has not improved from the previous [0.0726180019385703] for 1 times.\n",
      "Iteration [11591]: Loss[0.07261851317951022] has not improved from the previous [0.07261600573004291] for 1 times.\n",
      "Iteration [11592]: Loss[0.07262332124433454] has not improved from the previous [0.07261851317951022] for 3 times.\n",
      "Iteration [11594]: Loss[0.07261578830344205] has not improved from the previous [0.07261404932331651] for 1 times.\n",
      "Iteration [11596]: Loss[0.07261362073721661] has not improved from the previous [0.07261271769365714] for 1 times.\n",
      "Iteration [11599]: Loss[0.07261171110664162] has not improved from the previous [0.07261103536871087] for 1 times.\n",
      "Iteration [11601]: Loss[0.07261068491602447] has not improved from the previous [0.07260902766814979] for 1 times.\n",
      "Iteration [11603]: Loss[0.07260869469887925] has not improved from the previous [0.07260760963891096] for 1 times.\n",
      "Iteration [11606]: Loss[0.07260664559124912] has not improved from the previous [0.07260614750759041] for 1 times.\n",
      "Iteration [11608]: Loss[0.07260593433938363] has not improved from the previous [0.07260418976342391] for 1 times.\n",
      "Iteration [11609]: Loss[0.07261150789661935] has not improved from the previous [0.07260593433938363] for 3 times.\n",
      "Iteration [11611]: Loss[0.07260390721581537] has not improved from the previous [0.07260222648767724] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11600 Loss 0.07260902766814979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [11613]: Loss[0.07260173941835874] has not improved from the previous [0.07260083848495649] for 1 times.\n",
      "Iteration [11616]: Loss[0.07259986144253301] has not improved from the previous [0.07259918450734523] for 1 times.\n",
      "Iteration [11618]: Loss[0.07259881955117105] has not improved from the previous [0.0725972082452793] for 1 times.\n",
      "Iteration [11620]: Loss[0.07259687668009762] has not improved from the previous [0.07259575176765536] for 1 times.\n",
      "Iteration [11623]: Loss[0.07259478324004262] has not improved from the previous [0.07259434160845457] for 1 times.\n",
      "Iteration [11625]: Loss[0.07259371296683842] has not improved from the previous [0.07259240609184153] for 1 times.\n",
      "Iteration [11627]: Loss[0.07259204856242632] has not improved from the previous [0.07259068537490028] for 1 times.\n",
      "Iteration [11630]: Loss[0.07258984365618207] has not improved from the previous [0.07258954319799851] for 1 times.\n",
      "Iteration [11631]: Loss[0.07259678419313212] has not improved from the previous [0.07258984365618207] for 3 times.\n",
      "Iteration [11633]: Loss[0.07258805593686952] has not improved from the previous [0.07258747297488914] for 1 times.\n",
      "Iteration [11635]: Loss[0.07258700151939668] has not improved from the previous [0.07258543611571644] for 1 times.\n",
      "Iteration [11637]: Loss[0.07258512049114789] has not improved from the previous [0.0725839142998875] for 1 times.\n",
      "Iteration [11640]: Loss[0.07258291655523992] has not improved from the previous [0.0725826125807982] for 1 times.\n",
      "Iteration [11642]: Loss[0.07258182767748593] has not improved from the previous [0.07258069975011786] for 1 times.\n",
      "Iteration [11644]: Loss[0.07258069575843211] has not improved from the previous [0.07257890192021632] for 1 times.\n",
      "Iteration [11646]: Loss[0.07257856863684123] has not improved from the previous [0.07257771644457395] for 1 times.\n",
      "Iteration [11649]: Loss[0.07257686599594122] has not improved from the previous [0.07257613141519707] for 1 times.\n",
      "Iteration [11650]: Loss[0.07258339523258996] has not improved from the previous [0.07257686599594122] for 3 times.\n",
      "Iteration [11652]: Loss[0.07257502450652856] has not improved from the previous [0.07257404020883597] for 1 times.\n",
      "Iteration [11654]: Loss[0.07257390954349992] has not improved from the previous [0.07257208222078808] for 1 times.\n",
      "Iteration [11656]: Loss[0.0725717381818068] has not improved from the previous [0.07257089032579564] for 1 times.\n",
      "Iteration [11659]: Loss[0.07256979960567576] has not improved from the previous [0.07256930736931735] for 1 times.\n",
      "Iteration [11661]: Loss[0.07256872399870025] has not improved from the previous [0.07256741041541451] for 1 times.\n",
      "Iteration [11663]: Loss[0.07256706046033704] has not improved from the previous [0.07256575792607753] for 1 times.\n",
      "Iteration [11666]: Loss[0.07256465374050465] has not improved from the previous [0.07256461910318372] for 1 times.\n",
      "Iteration [11668]: Loss[0.07256400490396607] has not improved from the previous [0.07256271994743756] for 1 times.\n",
      "Iteration [11669]: Loss[0.07257002899780822] has not improved from the previous [0.07256400490396607] for 3 times.\n",
      "Iteration [11671]: Loss[0.07256191434289933] has not improved from the previous [0.07256077596261018] for 1 times.\n",
      "Iteration [11673]: Loss[0.07256026371746173] has not improved from the previous [0.07255895889426732] for 1 times.\n",
      "Iteration [11676]: Loss[0.07255784695597414] has not improved from the previous [0.07255782878533157] for 1 times.\n",
      "Iteration [11678]: Loss[0.07255678652904803] has not improved from the previous [0.07255593708277241] for 1 times.\n",
      "Iteration [11680]: Loss[0.07255570906087665] has not improved from the previous [0.07255412854370445] for 1 times.\n",
      "Iteration [11682]: Loss[0.07255386567160106] has not improved from the previous [0.07255263346838403] for 1 times.\n",
      "Iteration [11685]: Loss[0.07255159280111313] has not improved from the previous [0.07255143726447988] for 1 times.\n",
      "Iteration [11687]: Loss[0.072550865714436] has not improved from the previous [0.07254949170610032] for 1 times.\n",
      "Iteration [11688]: Loss[0.07255680944851307] has not improved from the previous [0.072550865714436] for 3 times.\n",
      "Iteration [11690]: Loss[0.07254881525554825] has not improved from the previous [0.07254764657636334] for 1 times.\n",
      "Iteration [11692]: Loss[0.07254716018783411] has not improved from the previous [0.07254582723170024] for 1 times.\n",
      "Iteration [11697]: Loss[0.07254369765593113] has not improved from the previous [0.0725428161437995] for 1 times.\n",
      "Iteration [11699]: Loss[0.0725426179982084] has not improved from the previous [0.07254099160716854] for 1 times.\n",
      "Iteration [11701]: Loss[0.07254080241234968] has not improved from the previous [0.0725394800560346] for 1 times.\n",
      "Iteration [11704]: Loss[0.07253848442837185] has not improved from the previous [0.07253833133260437] for 1 times.\n",
      "Iteration [11706]: Loss[0.07253756649614272] has not improved from the previous [0.072536405268248] for 1 times.\n",
      "Iteration [11707]: Loss[0.07254378502139355] has not improved from the previous [0.07253756649614272] for 3 times.\n",
      "Iteration [11709]: Loss[0.07253570222112855] has not improved from the previous [0.0725346002889111] for 1 times.\n",
      "Iteration [11711]: Loss[0.07253418027600672] has not improved from the previous [0.07253263452386714] for 1 times.\n",
      "Iteration [11716]: Loss[0.07253057571443153] has not improved from the previous [0.07252978628178675] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11700 Loss 0.0725394800560346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [11718]: Loss[0.07252945479164453] has not improved from the previous [0.07252800898708148] for 1 times.\n",
      "Iteration [11720]: Loss[0.0725277984251787] has not improved from the previous [0.07252635564862793] for 1 times.\n",
      "Iteration [11725]: Loss[0.07252424328656898] has not improved from the previous [0.07252350389167764] for 1 times.\n",
      "Iteration [11727]: Loss[0.07252381356043669] has not improved from the previous [0.0725217655335236] for 1 times.\n",
      "Iteration [11728]: Loss[0.07252920598113677] has not improved from the previous [0.07252381356043669] for 3 times.\n",
      "Iteration [11730]: Loss[0.07252139475627767] has not improved from the previous [0.0725199329012685] for 1 times.\n",
      "Iteration [11732]: Loss[0.07251951459093169] has not improved from the previous [0.07251834874502737] for 1 times.\n",
      "Iteration [11735]: Loss[0.07251727236526219] has not improved from the previous [0.0725171091341727] for 1 times.\n",
      "Iteration [11737]: Loss[0.07251616726517252] has not improved from the previous [0.0725152690837532] for 1 times.\n",
      "Iteration [11739]: Loss[0.07251504650741099] has not improved from the previous [0.07251354322057868] for 1 times.\n",
      "Iteration [11741]: Loss[0.07251331178544795] has not improved from the previous [0.07251201343633375] for 1 times.\n",
      "Iteration [11746]: Loss[0.07251005812347218] has not improved from the previous [0.07250908205223026] for 1 times.\n",
      "Iteration [11747]: Loss[0.07251644270053824] has not improved from the previous [0.07251005812347218] for 3 times.\n",
      "Iteration [11749]: Loss[0.07250809392185267] has not improved from the previous [0.07250724550540821] for 1 times.\n",
      "Iteration [11751]: Loss[0.07250702795294464] has not improved from the previous [0.07250537152102629] for 1 times.\n",
      "Iteration [11753]: Loss[0.07250514161355136] has not improved from the previous [0.07250393242544402] for 1 times.\n",
      "Iteration [11756]: Loss[0.07250283619165582] has not improved from the previous [0.07250277673422634] for 1 times.\n",
      "Iteration [11758]: Loss[0.0725017998926979] has not improved from the previous [0.07250088358017626] for 1 times.\n",
      "Iteration [11760]: Loss[0.07250070437890428] has not improved from the previous [0.07249911189507287] for 1 times.\n",
      "Iteration [11762]: Loss[0.07249899184887797] has not improved from the previous [0.07249754836848948] for 1 times.\n",
      "Iteration [11767]: Loss[0.07249593656744227] has not improved from the previous [0.07249470414055867] for 1 times.\n",
      "Iteration [11768]: Loss[0.07250215204196676] has not improved from the previous [0.07249593656744227] for 3 times.\n",
      "Iteration [11770]: Loss[0.0724937146856293] has not improved from the previous [0.0724929412128115] for 1 times.\n",
      "Iteration [11772]: Loss[0.07249263307123337] has not improved from the previous [0.07249102241470876] for 1 times.\n",
      "Iteration [11774]: Loss[0.07249086218447601] has not improved from the previous [0.07248949158733398] for 1 times.\n",
      "Iteration [11779]: Loss[0.07248732772710514] has not improved from the previous [0.07248664707162547] for 1 times.\n",
      "Iteration [11781]: Loss[0.07248617180240804] has not improved from the previous [0.0724849497286176] for 1 times.\n",
      "Iteration [11783]: Loss[0.0724853058157335] has not improved from the previous [0.07248329094680464] for 1 times.\n",
      "Iteration [11784]: Loss[0.0724907432167033] has not improved from the previous [0.0724853058157335] for 3 times.\n",
      "Iteration [11786]: Loss[0.07248326912273834] has not improved from the previous [0.07248154169709514] for 1 times.\n",
      "Iteration [11788]: Loss[0.07248117249141196] has not improved from the previous [0.07248024760960324] for 1 times.\n",
      "Iteration [11791]: Loss[0.0724790815531678] has not improved from the previous [0.07247883186890368] for 1 times.\n",
      "Iteration [11793]: Loss[0.07247799835583768] has not improved from the previous [0.0724770340473064] for 1 times.\n",
      "Iteration [11795]: Loss[0.07247689619338772] has not improved from the previous [0.07247532068253572] for 1 times.\n",
      "Iteration [11797]: Loss[0.07247517574847821] has not improved from the previous [0.07247378709810834] for 1 times.\n",
      "Iteration [11802]: Loss[0.07247169257620575] has not improved from the previous [0.07247091776216737] for 1 times.\n",
      "Iteration [11804]: Loss[0.07247095114453474] has not improved from the previous [0.0724691832023533] for 1 times.\n",
      "Iteration [11805]: Loss[0.07247666615624966] has not improved from the previous [0.07247095114453474] for 3 times.\n",
      "Iteration [11807]: Loss[0.07246877948998853] has not improved from the previous [0.07246753223148224] for 1 times.\n",
      "Iteration [11809]: Loss[0.0724672084001707] has not improved from the previous [0.07246568167542149] for 1 times.\n",
      "Iteration [11814]: Loss[0.07246355107047697] has not improved from the previous [0.07246298034304811] for 1 times.\n",
      "Iteration [11816]: Loss[0.07246242245222637] has not improved from the previous [0.07246126700901744] for 1 times.\n",
      "Iteration [11818]: Loss[0.0724612382780048] has not improved from the previous [0.07245963242403079] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11800 Loss 0.07247274396336521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [11820]: Loss[0.07245949096597697] has not improved from the previous [0.07245814531613252] for 1 times.\n",
      "Iteration [11825]: Loss[0.07245615594860838] has not improved from the previous [0.0724554016852225] for 1 times.\n",
      "Iteration [11826]: Loss[0.07246288755601615] has not improved from the previous [0.07245615594860838] for 3 times.\n",
      "Iteration [11828]: Loss[0.07245420783139964] has not improved from the previous [0.07245363484710217] for 1 times.\n",
      "Iteration [11830]: Loss[0.07245305487506548] has not improved from the previous [0.07245182313108967] for 1 times.\n",
      "Iteration [11832]: Loss[0.072451901196063] has not improved from the previous [0.0724501823944868] for 1 times.\n",
      "Iteration [11834]: Loss[0.07245006141130532] has not improved from the previous [0.07244880147754615] for 1 times.\n",
      "Iteration [11839]: Loss[0.07244659068948675] has not improved from the previous [0.07244596063508608] for 1 times.\n",
      "Iteration [11841]: Loss[0.07244548114113032] has not improved from the previous [0.07244424535960703] for 1 times.\n",
      "Iteration [11843]: Loss[0.07244486830515208] has not improved from the previous [0.07244258381098562] for 1 times.\n",
      "Iteration [11844]: Loss[0.07245014086233835] has not improved from the previous [0.07244486830515208] for 3 times.\n",
      "Iteration [11846]: Loss[0.07244252530747128] has not improved from the previous [0.0724409709787951] for 1 times.\n",
      "Iteration [11848]: Loss[0.0724407267073726] has not improved from the previous [0.07243937915170919] for 1 times.\n",
      "Iteration [11851]: Loss[0.07243835667754904] has not improved from the previous [0.07243828069438037] for 1 times.\n",
      "Iteration [11853]: Loss[0.07243728384345449] has not improved from the previous [0.07243648160340523] for 1 times.\n",
      "Iteration [11855]: Loss[0.07243610559287379] has not improved from the previous [0.07243484552638964] for 1 times.\n",
      "Iteration [11857]: Loss[0.07243491232544896] has not improved from the previous [0.07243324084517573] for 1 times.\n",
      "Iteration [11859]: Loss[0.07243313849020984] has not improved from the previous [0.07243184194098877] for 1 times.\n",
      "Iteration [11864]: Loss[0.07242975802252079] has not improved from the previous [0.07242910832071274] for 1 times.\n",
      "Iteration [11865]: Loss[0.07243660411355846] has not improved from the previous [0.07242975802252079] for 3 times.\n",
      "Iteration [11867]: Loss[0.07242782697448759] has not improved from the previous [0.07242738780953255] for 1 times.\n",
      "Iteration [11869]: Loss[0.07242668658866144] has not improved from the previous [0.07242560027187696] for 1 times.\n",
      "Iteration [11871]: Loss[0.07242554739946497] has not improved from the previous [0.07242396380197569] for 1 times.\n",
      "Iteration [11873]: Loss[0.07242390067068773] has not improved from the previous [0.07242239413821712] for 1 times.\n",
      "Iteration [11878]: Loss[0.072420254052928] has not improved from the previous [0.0724197519325986] for 1 times.\n",
      "Iteration [11880]: Loss[0.0724191379194632] has not improved from the previous [0.07241807296332425] for 1 times.\n",
      "Iteration [11882]: Loss[0.0724183003265914] has not improved from the previous [0.07241647392291015] for 1 times.\n",
      "Iteration [11883]: Loss[0.07242405529512395] has not improved from the previous [0.0724183003265914] for 3 times.\n",
      "Iteration [11885]: Loss[0.07241615627019285] has not improved from the previous [0.07241487017476575] for 1 times.\n",
      "Iteration [11887]: Loss[0.07241501340669534] has not improved from the previous [0.07241307919447522] for 1 times.\n",
      "Iteration [11889]: Loss[0.07241302015073622] has not improved from the previous [0.07241182853670507] for 1 times.\n",
      "Iteration [11894]: Loss[0.07240957854964136] has not improved from the previous [0.07240900308539472] for 1 times.\n",
      "Iteration [11896]: Loss[0.0724084166101588] has not improved from the previous [0.07240739155566643] for 1 times.\n",
      "Iteration [11898]: Loss[0.072407279826163] has not improved from the previous [0.07240579652128272] for 1 times.\n",
      "Iteration [11900]: Loss[0.0724064326844718] has not improved from the previous [0.07240417489235101] for 1 times.\n",
      "Iteration [11901]: Loss[0.07241171985195523] has not improved from the previous [0.0724064326844718] for 3 times.\n",
      "Iteration [11903]: Loss[0.07240427008601737] has not improved from the previous [0.07240263838960627] for 1 times.\n",
      "Iteration [11905]: Loss[0.07240238895426106] has not improved from the previous [0.07240117441404925] for 1 times.\n",
      "Iteration [11910]: Loss[0.07239892277049913] has not improved from the previous [0.07239836125879476] for 1 times.\n",
      "Iteration [11912]: Loss[0.07239779910128252] has not improved from the previous [0.0723966945138608] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11900 Loss 0.0724064326844718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [11914]: Loss[0.07239661899112415] has not improved from the previous [0.0723951010582783] for 1 times.\n",
      "Iteration [11916]: Loss[0.07239540736806815] has not improved from the previous [0.07239355251385199] for 1 times.\n",
      "Iteration [11918]: Loss[0.07239351676960429] has not improved from the previous [0.07239228313572861] for 1 times.\n",
      "Iteration [11923]: Loss[0.07239036812144353] has not improved from the previous [0.07238952402500833] for 1 times.\n",
      "Iteration [11924]: Loss[0.07239709122618962] has not improved from the previous [0.07239036812144353] for 3 times.\n",
      "Iteration [11926]: Loss[0.07238827271483399] has not improved from the previous [0.0723878708755576] for 1 times.\n",
      "Iteration [11928]: Loss[0.07238710244386395] has not improved from the previous [0.07238612914803184] for 1 times.\n",
      "Iteration [11930]: Loss[0.07238594234028628] has not improved from the previous [0.0723845571714538] for 1 times.\n",
      "Iteration [11932]: Loss[0.07238478097238267] has not improved from the previous [0.07238296050885686] for 1 times.\n",
      "Iteration [11934]: Loss[0.07238300100087132] has not improved from the previous [0.07238155950365546] for 1 times.\n",
      "Iteration [11939]: Loss[0.07237941892636279] has not improved from the previous [0.07237891655796196] for 1 times.\n",
      "Iteration [11941]: Loss[0.0723783985011811] has not improved from the previous [0.07237730662359085] for 1 times.\n",
      "Iteration [11942]: Loss[0.07238492012608128] has not improved from the previous [0.0723783985011811] for 3 times.\n",
      "Iteration [11944]: Loss[0.07237644086198798] has not improved from the previous [0.07237576284620498] for 1 times.\n",
      "Iteration [11946]: Loss[0.07237530254966795] has not improved from the previous [0.07237398944332676] for 1 times.\n",
      "Iteration [11948]: Loss[0.0723740727538903] has not improved from the previous [0.07237243615186435] for 1 times.\n",
      "Iteration [11950]: Loss[0.07237242550374252] has not improved from the previous [0.07237092504217747] for 1 times.\n",
      "Iteration [11955]: Loss[0.0723686089950524] has not improved from the previous [0.07236851867820183] for 1 times.\n",
      "Iteration [11957]: Loss[0.0723674840760223] has not improved from the previous [0.07236693483109014] for 1 times.\n",
      "Iteration [11959]: Loss[0.07236633788510582] has not improved from the previous [0.07236533270589932] for 1 times.\n",
      "Iteration [11961]: Loss[0.07236573912600008] has not improved from the previous [0.0723637534064453] for 1 times.\n",
      "Iteration [11962]: Loss[0.07237138509039556] has not improved from the previous [0.07236573912600008] for 3 times.\n",
      "Iteration [11964]: Loss[0.07236330117884857] has not improved from the previous [0.07236228825906081] for 1 times.\n",
      "Iteration [11966]: Loss[0.07236218033305959] has not improved from the previous [0.07236051362057482] for 1 times.\n",
      "Iteration [11968]: Loss[0.07236096314200073] has not improved from the previous [0.07235895760026262] for 1 times.\n",
      "Iteration [11970]: Loss[0.0723590064256188] has not improved from the previous [0.07235775214189488] for 1 times.\n",
      "Iteration [11975]: Loss[0.07235549812358448] has not improved from the previous [0.07235505219341756] for 1 times.\n",
      "Iteration [11977]: Loss[0.07235432125279377] has not improved from the previous [0.0723535015619663] for 1 times.\n",
      "Iteration [11979]: Loss[0.07235329007316399] has not improved from the previous [0.0723519619054567] for 1 times.\n",
      "Iteration [11980]: Loss[0.07235954975594813] has not improved from the previous [0.07235329007316399] for 3 times.\n",
      "Iteration [11982]: Loss[0.0723512924330084] has not improved from the previous [0.07235044116582634] for 1 times.\n",
      "Iteration [11984]: Loss[0.07235017056701595] has not improved from the previous [0.07234874180842758] for 1 times.\n",
      "Iteration [11986]: Loss[0.0723489980162673] has not improved from the previous [0.07234714695503279] for 1 times.\n",
      "Iteration [11988]: Loss[0.0723472514481881] has not improved from the previous [0.0723457326386432] for 1 times.\n",
      "Iteration [11993]: Loss[0.07234357237836636] has not improved from the previous [0.07234322053451646] for 1 times.\n",
      "Iteration [11995]: Loss[0.07234238180250924] has not improved from the previous [0.07234166828450128] for 1 times.\n",
      "Iteration [11997]: Loss[0.07234116666905593] has not improved from the previous [0.07234016214505075] for 1 times.\n",
      "Iteration [11999]: Loss[0.07234031255352782] has not improved from the previous [0.07233866522774753] for 1 times.\n",
      "Iteration [12000]: Loss[0.07234631224373332] has not improved from the previous [0.07234031255352782] for 3 times.\n",
      "Iteration [12002]: Loss[0.07233812600942939] has not improved from the previous [0.07233715462341768] for 1 times.\n",
      "Iteration [12004]: Loss[0.0723369511422824] has not improved from the previous [0.07233547863815569] for 1 times.\n",
      "Iteration [12006]: Loss[0.07233577308177186] has not improved from the previous [0.07233394677372894] for 1 times.\n",
      "Iteration [12008]: Loss[0.07233402226370185] has not improved from the previous [0.07233254323700561] for 1 times.\n",
      "Iteration [12013]: Loss[0.07233032750635107] has not improved from the previous [0.07233006390100388] for 1 times.\n",
      "Iteration [12015]: Loss[0.0723291806500691] has not improved from the previous [0.07232847966071695] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 12000 Loss 0.07234631224373332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [12017]: Loss[0.07232795861573843] has not improved from the previous [0.0723269845145641] for 1 times.\n",
      "Iteration [12019]: Loss[0.0723271550599218] has not improved from the previous [0.07232550274043549] for 1 times.\n",
      "Iteration [12020]: Loss[0.07233318967801056] has not improved from the previous [0.0723271550599218] for 3 times.\n",
      "Iteration [12022]: Loss[0.07232490968622371] has not improved from the previous [0.07232400837692389] for 1 times.\n",
      "Iteration [12024]: Loss[0.07232370493892237] has not improved from the previous [0.07232234880572527] for 1 times.\n",
      "Iteration [12026]: Loss[0.07232250075506563] has not improved from the previous [0.07232085398787416] for 1 times.\n",
      "Iteration [12028]: Loss[0.07232131278983074] has not improved from the previous [0.07231934213562123] for 1 times.\n",
      "Iteration [12030]: Loss[0.07231946470412039] has not improved from the previous [0.07231805756147316] for 1 times.\n",
      "Iteration [12035]: Loss[0.07231584611245383] has not improved from the previous [0.0723155260663956] for 1 times.\n",
      "Iteration [12037]: Loss[0.0723146749307428] has not improved from the previous [0.07231397210390139] for 1 times.\n",
      "Iteration [12039]: Loss[0.07231381115938706] has not improved from the previous [0.07231249655962167] for 1 times.\n",
      "Iteration [12040]: Loss[0.07232019023585087] has not improved from the previous [0.07231381115938706] for 3 times.\n",
      "Iteration [12042]: Loss[0.07231162579822552] has not improved from the previous [0.07231101661048916] for 1 times.\n",
      "Iteration [12044]: Loss[0.07231041593492858] has not improved from the previous [0.0723093652348721] for 1 times.\n",
      "Iteration [12046]: Loss[0.07230918295610272] has not improved from the previous [0.07230787968959819] for 1 times.\n",
      "Iteration [12048]: Loss[0.07230799469741173] has not improved from the previous [0.07230640843037298] for 1 times.\n",
      "Iteration [12050]: Loss[0.07230680380016476] has not improved from the previous [0.07230489661564886] for 1 times.\n",
      "Iteration [12052]: Loss[0.07230507135215954] has not improved from the previous [0.07230351096945264] for 1 times.\n",
      "Iteration [12057]: Loss[0.07230130876316963] has not improved from the previous [0.07230113988656639] for 1 times.\n",
      "Iteration [12059]: Loss[0.07230030341184587] has not improved from the previous [0.07229961994403739] for 1 times.\n",
      "Iteration [12060]: Loss[0.07230731723774679] has not improved from the previous [0.07230030341184587] for 3 times.\n",
      "Iteration [12062]: Loss[0.07229826638588323] has not improved from the previous [0.07229818340194623] for 1 times.\n",
      "Iteration [12064]: Loss[0.0722971008602784] has not improved from the previous [0.07229649478430043] for 1 times.\n",
      "Iteration [12066]: Loss[0.07229585098531137] has not improved from the previous [0.07229501603387442] for 1 times.\n",
      "Iteration [12068]: Loss[0.07229463061575947] has not improved from the previous [0.07229356609356183] for 1 times.\n",
      "Iteration [12070]: Loss[0.07229343843102141] has not improved from the previous [0.07229209378328691] for 1 times.\n",
      "Iteration [12072]: Loss[0.0722922489047333] has not improved from the previous [0.07229059718060858] for 1 times.\n",
      "Iteration [12074]: Loss[0.07229103691281748] has not improved from the previous [0.07228912210475238] for 1 times.\n",
      "Iteration [12076]: Loss[0.07228930210288448] has not improved from the previous [0.07228775828666437] for 1 times.\n",
      "Iteration [12081]: Loss[0.07228600527659192] has not improved from the previous [0.0722854661891659] for 1 times.\n",
      "Iteration [12082]: Loss[0.07229315977679897] has not improved from the previous [0.07228600527659192] for 3 times.\n",
      "Iteration [12086]: Loss[0.07228246807163294] has not improved from the previous [0.07228233178107517] for 1 times.\n",
      "Iteration [12088]: Loss[0.07228122271172072] has not improved from the previous [0.0722808617484274] for 1 times.\n",
      "Iteration [12090]: Loss[0.07228002808387111] has not improved from the previous [0.0722794192331514] for 1 times.\n",
      "Iteration [12092]: Loss[0.072278837619795] has not improved from the previous [0.07227793091895246] for 1 times.\n",
      "Iteration [12094]: Loss[0.0722776421557464] has not improved from the previous [0.07227644415897809] for 1 times.\n",
      "Iteration [12096]: Loss[0.07227662767890428] has not improved from the previous [0.07227500321655676] for 1 times.\n",
      "Iteration [12097]: Loss[0.07228276030294549] has not improved from the previous [0.07227662767890428] for 3 times.\n",
      "Iteration [12099]: Loss[0.07227452370777421] has not improved from the previous [0.07227363710134456] for 1 times.\n",
      "Iteration [12101]: Loss[0.0722733273707169] has not improved from the previous [0.0722719921137457] for 1 times.\n",
      "Iteration [12103]: Loss[0.07227206046595167] has not improved from the previous [0.07227055004108507] for 1 times.\n",
      "Iteration [12105]: Loss[0.07227082596221288] has not improved from the previous [0.07226913358530582] for 1 times.\n",
      "Iteration [12107]: Loss[0.07226961680500624] has not improved from the previous [0.07226769139729952] for 1 times.\n",
      "Iteration [12109]: Loss[0.07226787461102041] has not improved from the previous [0.0722663522745191] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 12100 Loss 0.0722719921137457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [12114]: Loss[0.07226406907225565] has not improved from the previous [0.07226405017221708] for 1 times.\n",
      "Iteration [12116]: Loss[0.0722628923933592] has not improved from the previous [0.07226253703387382] for 1 times.\n",
      "Iteration [12118]: Loss[0.07226207172333077] has not improved from the previous [0.07226111115966316] for 1 times.\n",
      "Iteration [12119]: Loss[0.0722688604465807] has not improved from the previous [0.07226207172333077] for 3 times.\n",
      "Iteration [12121]: Loss[0.07225979555685011] has not improved from the previous [0.07225971342827772] for 1 times.\n",
      "Iteration [12123]: Loss[0.07225858245520395] has not improved from the previous [0.07225809829393276] for 1 times.\n",
      "Iteration [12125]: Loss[0.07225732207189521] has not improved from the previous [0.07225666076486977] for 1 times.\n",
      "Iteration [12127]: Loss[0.0722561130834333] has not improved from the previous [0.07225524856575855] for 1 times.\n",
      "Iteration [12129]: Loss[0.07225490775447406] has not improved from the previous [0.07225379039947855] for 1 times.\n",
      "Iteration [12131]: Loss[0.07225370542979218] has not improved from the previous [0.07225233207841235] for 1 times.\n",
      "Iteration [12133]: Loss[0.07225245902380487] has not improved from the previous [0.07225091600736855] for 1 times.\n",
      "Iteration [12135]: Loss[0.07225168939466625] has not improved from the previous [0.07224952961405726] for 1 times.\n",
      "Iteration [12136]: Loss[0.07225731942863663] has not improved from the previous [0.07225168939466625] for 3 times.\n",
      "Iteration [12138]: Loss[0.07224930904942461] has not improved from the previous [0.07224817236970182] for 1 times.\n",
      "Iteration [12140]: Loss[0.07224807088013951] has not improved from the previous [0.07224659383773356] for 1 times.\n",
      "Iteration [12142]: Loss[0.07224682381926789] has not improved from the previous [0.07224517981564321] for 1 times.\n",
      "Iteration [12144]: Loss[0.07224560601992273] has not improved from the previous [0.07224375838601489] for 1 times.\n",
      "Iteration [12146]: Loss[0.07224397314521132] has not improved from the previous [0.07224233660411822] for 1 times.\n",
      "Iteration [12153]: Loss[0.07223887633601236] has not improved from the previous [0.07223865558007611] for 1 times.\n",
      "Iteration [12155]: Loss[0.07223762657249579] has not improved from the previous [0.07223725374690874] for 1 times.\n",
      "Iteration [12157]: Loss[0.07223686515658014] has not improved from the previous [0.07223585858828842] for 1 times.\n",
      "Iteration [12158]: Loss[0.07224364191035205] has not improved from the previous [0.07223686515658014] for 3 times.\n",
      "Iteration [12160]: Loss[0.07223450307981331] has not improved from the previous [0.07223449690996973] for 1 times.\n",
      "Iteration [12162]: Loss[0.07223332508500484] has not improved from the previous [0.07223285261322737] for 1 times.\n",
      "Iteration [12164]: Loss[0.0722321072025397] has not improved from the previous [0.07223145407507953] for 1 times.\n",
      "Iteration [12166]: Loss[0.07223089384536774] has not improved from the previous [0.07223001439444804] for 1 times.\n",
      "Iteration [12168]: Loss[0.07222967987531216] has not improved from the previous [0.07222857520664956] for 1 times.\n",
      "Iteration [12170]: Loss[0.07222841794265017] has not improved from the previous [0.07222718640576525] for 1 times.\n",
      "Iteration [12172]: Loss[0.07222714430852362] has not improved from the previous [0.0722258205933596] for 1 times.\n",
      "Iteration [12174]: Loss[0.0722263411101343] has not improved from the previous [0.07222445345183985] for 1 times.\n",
      "Iteration [12175]: Loss[0.07223224288933262] has not improved from the previous [0.0722263411101343] for 3 times.\n",
      "Iteration [12177]: Loss[0.07222397202607209] has not improved from the previous [0.07222313303197841] for 1 times.\n",
      "Iteration [12179]: Loss[0.07222276170775994] has not improved from the previous [0.07222157377300112] for 1 times.\n",
      "Iteration [12181]: Loss[0.0722215408734349] has not improved from the previous [0.07222014904785688] for 1 times.\n",
      "Iteration [12183]: Loss[0.07222031272654288] has not improved from the previous [0.07221872030028964] for 1 times.\n",
      "Iteration [12185]: Loss[0.07221906038369526] has not improved from the previous [0.0722173297396387] for 1 times.\n",
      "Iteration [12187]: Loss[0.07221777367835575] has not improved from the previous [0.07221597699317993] for 1 times.\n",
      "Iteration [12189]: Loss[0.07221649981896204] has not improved from the previous [0.07221462672849854] for 1 times.\n",
      "Iteration [12191]: Loss[0.07221484907761463] has not improved from the previous [0.07221329246287567] for 1 times.\n",
      "Iteration [12197]: Loss[0.07221877386319084] has not improved from the previous [0.07221102767907457] for 1 times.\n",
      "Iteration [12205]: Loss[0.07220545939824816] has not improved from the previous [0.07220527473900108] for 1 times.\n",
      "Iteration [12207]: Loss[0.07220418832578857] has not improved from the previous [0.07220390419312502] for 1 times.\n",
      "Iteration [12209]: Loss[0.07220291597235197] has not improved from the previous [0.07220254830142644] for 1 times.\n",
      "Iteration [12211]: Loss[0.0722016790249568] has not improved from the previous [0.0722011907061299] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 12200 Loss 0.07220817062060166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [12213]: Loss[0.0722004704441651] has not improved from the previous [0.07219980179238646] for 1 times.\n",
      "Iteration [12214]: Loss[0.07220756880792382] has not improved from the previous [0.0722004704441651] for 3 times.\n",
      "Iteration [12218]: Loss[0.07219732188867097] has not improved from the previous [0.07219696380887498] for 1 times.\n",
      "Iteration [12220]: Loss[0.07219610373246584] has not improved from the previous [0.07219551639712887] for 1 times.\n",
      "Iteration [12222]: Loss[0.07219484372390468] has not improved from the previous [0.07219413390750826] for 1 times.\n",
      "Iteration [12224]: Loss[0.07219355620635377] has not improved from the previous [0.0721927913977499] for 1 times.\n",
      "Iteration [12226]: Loss[0.07219229143062333] has not improved from the previous [0.07219145056673615] for 1 times.\n",
      "Iteration [12228]: Loss[0.07219106755536532] has not improved from the previous [0.07219009064021319] for 1 times.\n",
      "Iteration [12230]: Loss[0.07218983949362473] has not improved from the previous [0.07218869524591387] for 1 times.\n",
      "Iteration [12232]: Loss[0.07218902936635103] has not improved from the previous [0.07218725222461089] for 1 times.\n",
      "Iteration [12233]: Loss[0.07219512143011758] has not improved from the previous [0.07218902936635103] for 3 times.\n",
      "Iteration [12235]: Loss[0.07218667117100007] has not improved from the previous [0.07218606155252505] for 1 times.\n",
      "Iteration [12237]: Loss[0.07218547040293195] has not improved from the previous [0.07218447946412174] for 1 times.\n",
      "Iteration [12239]: Loss[0.07218417277995323] has not improved from the previous [0.07218312399220463] for 1 times.\n",
      "Iteration [12241]: Loss[0.07218288641465097] has not improved from the previous [0.07218179191242545] for 1 times.\n",
      "Iteration [12243]: Loss[0.07218164835757881] has not improved from the previous [0.07218045446089923] for 1 times.\n",
      "Iteration [12245]: Loss[0.07218041450954737] has not improved from the previous [0.07217907167654827] for 1 times.\n",
      "Iteration [12247]: Loss[0.07217917493982173] has not improved from the previous [0.07217769487395591] for 1 times.\n",
      "Iteration [12249]: Loss[0.07217787999616473] has not improved from the previous [0.07217637514920124] for 1 times.\n",
      "Iteration [12251]: Loss[0.07217715066172725] has not improved from the previous [0.07217500188805762] for 1 times.\n",
      "Iteration [12252]: Loss[0.0721828848887722] has not improved from the previous [0.07217715066172725] for 3 times.\n",
      "Iteration [12254]: Loss[0.07217469901405078] has not improved from the previous [0.07217377017642174] for 1 times.\n",
      "Iteration [12256]: Loss[0.0721734346746174] has not improved from the previous [0.07217226568218528] for 1 times.\n",
      "Iteration [12258]: Loss[0.07217217140442357] has not improved from the previous [0.07217092338870183] for 1 times.\n",
      "Iteration [12260]: Loss[0.07217092905378855] has not improved from the previous [0.07216956442250033] for 1 times.\n",
      "Iteration [12262]: Loss[0.07216968506642822] has not improved from the previous [0.0721681908839539] for 1 times.\n",
      "Iteration [12264]: Loss[0.07216840280900605] has not improved from the previous [0.07216685885475999] for 1 times.\n",
      "Iteration [12266]: Loss[0.07216715563413134] has not improved from the previous [0.07216549474435037] for 1 times.\n",
      "Iteration [12268]: Loss[0.0721658661860951] has not improved from the previous [0.07216420523861179] for 1 times.\n",
      "Iteration [12270]: Loss[0.07216510426178835] has not improved from the previous [0.07216288906166668] for 1 times.\n",
      "Iteration [12271]: Loss[0.07217072319072633] has not improved from the previous [0.07216510426178835] for 3 times.\n",
      "Iteration [12273]: Loss[0.07216260779194612] has not improved from the previous [0.07216168356531578] for 1 times.\n",
      "Iteration [12275]: Loss[0.07216145966063008] has not improved from the previous [0.072160111267849] for 1 times.\n",
      "Iteration [12277]: Loss[0.07216020820149179] has not improved from the previous [0.07215872860917362] for 1 times.\n",
      "Iteration [12279]: Loss[0.07215893309028405] has not improved from the previous [0.0721573864670477] for 1 times.\n",
      "Iteration [12281]: Loss[0.07215761807649465] has not improved from the previous [0.0721560937436896] for 1 times.\n",
      "Iteration [12283]: Loss[0.07215632317462109] has not improved from the previous [0.07215479974693899] for 1 times.\n",
      "Iteration [12285]: Loss[0.07215507167742836] has not improved from the previous [0.0721534942115962] for 1 times.\n",
      "Iteration [12287]: Loss[0.07215382286370851] has not improved from the previous [0.0721521457869067] for 1 times.\n",
      "Iteration [12289]: Loss[0.07215294916143548] has not improved from the previous [0.07215080181103035] for 1 times.\n",
      "Iteration [12290]: Loss[0.07215869681376455] has not improved from the previous [0.07215294916143548] for 3 times.\n",
      "Iteration [12292]: Loss[0.07215055404043116] has not improved from the previous [0.0721496717526091] for 1 times.\n",
      "Iteration [12294]: Loss[0.07214939644805508] has not improved from the previous [0.07214805332199614] for 1 times.\n",
      "Iteration [12296]: Loss[0.07214808185779335] has not improved from the previous [0.07214673840703585] for 1 times.\n",
      "Iteration [12298]: Loss[0.07214676419909684] has not improved from the previous [0.07214545353786672] for 1 times.\n",
      "Iteration [12300]: Loss[0.07214549817148781] has not improved from the previous [0.07214416577481204] for 1 times.\n",
      "Iteration [12302]: Loss[0.07214424290153565] has not improved from the previous [0.07214283422666543] for 1 times.\n",
      "Iteration [12304]: Loss[0.07214304584321607] has not improved from the previous [0.07214143029455404] for 1 times.\n",
      "Iteration [12306]: Loss[0.07214174734433738] has not improved from the previous [0.07214013970337263] for 1 times.\n",
      "Iteration [12308]: Loss[0.07214068011282367] has not improved from the previous [0.07213886896797396] for 1 times.\n",
      "Iteration [12309]: Loss[0.07214677919400622] has not improved from the previous [0.07214068011282367] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 12300 Loss 0.07214549817148781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [12311]: Loss[0.07213847820011059] has not improved from the previous [0.07213767473693246] for 1 times.\n",
      "Iteration [12313]: Loss[0.07213725295916633] has not improved from the previous [0.07213614020949162] for 1 times.\n",
      "Iteration [12315]: Loss[0.0721359604444239] has not improved from the previous [0.072134845735803] for 1 times.\n",
      "Iteration [12317]: Loss[0.07213470110845434] has not improved from the previous [0.07213353064104724] for 1 times.\n",
      "Iteration [12319]: Loss[0.07213344503687703] has not improved from the previous [0.0721321939606425] for 1 times.\n",
      "Iteration [12321]: Loss[0.07213215786022768] has not improved from the previous [0.07213088614263906] for 1 times.\n",
      "Iteration [12323]: Loss[0.07213084415741385] has not improved from the previous [0.07212961623328944] for 1 times.\n",
      "Iteration [12325]: Loss[0.07212954663011299] has not improved from the previous [0.07212834298419254] for 1 times.\n",
      "Iteration [12327]: Loss[0.0721284120387833] has not improved from the previous [0.0721269962465889] for 1 times.\n",
      "Iteration [12328]: Loss[0.07213486473222668] has not improved from the previous [0.0721284120387833] for 3 times.\n",
      "Iteration [12330]: Loss[0.07212631098817304] has not improved from the previous [0.07212584544580072] for 1 times.\n",
      "Iteration [12332]: Loss[0.0721250977669826] has not improved from the previous [0.07212437135410138] for 1 times.\n",
      "Iteration [12334]: Loss[0.07212384513914183] has not improved from the previous [0.07212300182576208] for 1 times.\n",
      "Iteration [12336]: Loss[0.07212261830448259] has not improved from the previous [0.07212161931013694] for 1 times.\n",
      "Iteration [12338]: Loss[0.07212129466445753] has not improved from the previous [0.07212036664473706] for 1 times.\n",
      "Iteration [12340]: Loss[0.07211998684488631] has not improved from the previous [0.07211909983654562] for 1 times.\n",
      "Iteration [12342]: Loss[0.07211872667048315] has not improved from the previous [0.07211782524164827] for 1 times.\n",
      "Iteration [12344]: Loss[0.07211747018185985] has not improved from the previous [0.0721165029566682] for 1 times.\n",
      "Iteration [12346]: Loss[0.07211620833259878] has not improved from the previous [0.07211518563077153] for 1 times.\n",
      "Iteration [12348]: Loss[0.0721152834209692] has not improved from the previous [0.07211385777605499] for 1 times.\n",
      "Iteration [12349]: Loss[0.07212180061234702] has not improved from the previous [0.0721152834209692] for 3 times.\n",
      "Iteration [12351]: Loss[0.07211295971073194] has not improved from the previous [0.07211273148343626] for 1 times.\n",
      "Iteration [12353]: Loss[0.07211175435828694] has not improved from the previous [0.07211119119529086] for 1 times.\n",
      "Iteration [12355]: Loss[0.0721104217385444] has not improved from the previous [0.07210991662771075] for 1 times.\n",
      "Iteration [12357]: Loss[0.07210914764063531] has not improved from the previous [0.0721086551222802] for 1 times.\n",
      "Iteration [12359]: Loss[0.07210788733655069] has not improved from the previous [0.07210734567482652] for 1 times.\n",
      "Iteration [12361]: Loss[0.07210662518143802] has not improved from the previous [0.07210602734684661] for 1 times.\n",
      "Iteration [12363]: Loss[0.07210532961170868] has not improved from the previous [0.07210474921056269] for 1 times.\n",
      "Iteration [12365]: Loss[0.07210401092383796] has not improved from the previous [0.0721034951033731] for 1 times.\n",
      "Iteration [12367]: Loss[0.07210278158870302] has not improved from the previous [0.0721021798780901] for 1 times.\n",
      "Iteration [12368]: Loss[0.07211009470082481] has not improved from the previous [0.07210278158870302] for 3 times.\n",
      "Iteration [12372]: Loss[0.07209958198350194] has not improved from the previous [0.07209952718125312] for 1 times.\n",
      "Iteration [12374]: Loss[0.07209831486977626] has not improved from the previous [0.07209822300453746] for 1 times.\n",
      "Iteration [12376]: Loss[0.07209705395310184] has not improved from the previous [0.07209690365988652] for 1 times.\n",
      "Iteration [12378]: Loss[0.07209576139339707] has not improved from the previous [0.07209561817236892] for 1 times.\n",
      "Iteration [12380]: Loss[0.07209444003818846] has not improved from the previous [0.07209436728206085] for 1 times.\n",
      "Iteration [12382]: Loss[0.07209314653422749] has not improved from the previous [0.07209311547659325] for 1 times.\n",
      "Iteration [12384]: Loss[0.07209188852886242] has not improved from the previous [0.07209184252987363] for 1 times.\n",
      "Iteration [12386]: Loss[0.07209068405607508] has not improved from the previous [0.07209047419752394] for 1 times.\n",
      "Iteration [12388]: Loss[0.07208956172175135] has not improved from the previous [0.07208919350549259] for 1 times.\n",
      "Iteration [12389]: Loss[0.07209714493140837] has not improved from the previous [0.07208956172175135] for 3 times.\n",
      "Iteration [12410]: Loss[0.07208429356295008] has not improved from the previous [0.07207619534455431] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 12400 Loss 0.0720814641942226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [12431]: Loss[0.07207145680917194] has not improved from the previous [0.07206298864231285] for 1 times.\n",
      "Iteration [12434]: Loss[0.07206088702256] has not improved from the previous [0.07206059801945226] for 1 times.\n",
      "Iteration [12436]: Loss[0.07205963611254768] has not improved from the previous [0.07205939528439993] for 1 times.\n",
      "Iteration [12438]: Loss[0.07205839613277709] has not improved from the previous [0.07205806940583795] for 1 times.\n",
      "Iteration [12440]: Loss[0.07205711458601875] has not improved from the previous [0.07205679302042578] for 1 times.\n",
      "Iteration [12442]: Loss[0.07205581966661258] has not improved from the previous [0.07205553233731425] for 1 times.\n",
      "Iteration [12444]: Loss[0.07205450192623306] has not improved from the previous [0.0720542773745985] for 1 times.\n",
      "Iteration [12446]: Loss[0.07205328607650907] has not improved from the previous [0.07205303549879835] for 1 times.\n",
      "Iteration [12448]: Loss[0.07205205671158825] has not improved from the previous [0.07205171564570831] for 1 times.\n",
      "Iteration [12450]: Loss[0.07205080254077703] has not improved from the previous [0.07205042390094317] for 1 times.\n",
      "Iteration [12452]: Loss[0.07205871094438603] has not improved from the previous [0.07204958011911393] for 1 times.\n",
      "Iteration [12455]: Loss[0.07204820053443609] has not improved from the previous [0.07204714214812262] for 1 times.\n",
      "Iteration [12457]: Loss[0.07204688348576872] has not improved from the previous [0.07204598376255533] for 1 times.\n",
      "Iteration [12459]: Loss[0.07204562287949012] has not improved from the previous [0.07204472126039688] for 1 times.\n",
      "Iteration [12461]: Loss[0.07204433024465617] has not improved from the previous [0.07204342982555695] for 1 times.\n",
      "Iteration [12463]: Loss[0.07204311320231196] has not improved from the previous [0.07204216626587205] for 1 times.\n",
      "Iteration [12465]: Loss[0.0720418580241398] has not improved from the previous [0.07204087287528005] for 1 times.\n",
      "Iteration [12467]: Loss[0.07204057182700717] has not improved from the previous [0.07203961861746777] for 1 times.\n",
      "Iteration [12469]: Loss[0.07203924693922441] has not improved from the previous [0.07203836112037372] for 1 times.\n",
      "Iteration [12471]: Loss[0.07204722580576024] has not improved from the previous [0.07203716010625148] for 1 times.\n",
      "Iteration [12474]: Loss[0.07203667666689316] has not improved from the previous [0.07203512362368135] for 1 times.\n",
      "Iteration [12476]: Loss[0.07203543097868219] has not improved from the previous [0.07203394554792271] for 1 times.\n",
      "Iteration [12478]: Loss[0.07203420148946718] has not improved from the previous [0.07203262219638702] for 1 times.\n",
      "Iteration [12480]: Loss[0.07203294238743611] has not improved from the previous [0.07203133520941059] for 1 times.\n",
      "Iteration [12482]: Loss[0.07203165457362247] has not improved from the previous [0.07203008121016552] for 1 times.\n",
      "Iteration [12484]: Loss[0.07203039030781494] has not improved from the previous [0.07202883143062824] for 1 times.\n",
      "Iteration [12486]: Loss[0.07202910718678777] has not improved from the previous [0.07202755698765498] for 1 times.\n",
      "Iteration [12488]: Loss[0.0720278975562646] has not improved from the previous [0.07202629666598044] for 1 times.\n",
      "Iteration [12490]: Loss[0.07202666500604198] has not improved from the previous [0.07202499636716182] for 1 times.\n",
      "Iteration [12492]: Loss[0.0720345772582872] has not improved from the previous [0.07202379745035689] for 1 times.\n",
      "Iteration [12495]: Loss[0.07202356905687034] has not improved from the previous [0.07202199818292832] for 1 times.\n",
      "Iteration [12508]: Loss[0.07202399534882793] has not improved from the previous [0.072015007950053] for 1 times.\n",
      "Iteration [12511]: Loss[0.07201354347056152] has not improved from the previous [0.07201275292599622] for 1 times.\n",
      "Iteration [12513]: Loss[0.0720123148697105] has not improved from the previous [0.0720114533636228] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 12500 Loss 0.07201961605227201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [12515]: Loss[0.07201102120514252] has not improved from the previous [0.0720101479967016] for 1 times.\n",
      "Iteration [12517]: Loss[0.07200977883086795] has not improved from the previous [0.07200892553745522] for 1 times.\n",
      "Iteration [12519]: Loss[0.07200854936099353] has not improved from the previous [0.07200764615455925] for 1 times.\n",
      "Iteration [12521]: Loss[0.07200735531202514] has not improved from the previous [0.07200634678944609] for 1 times.\n",
      "Iteration [12523]: Loss[0.07200609847984546] has not improved from the previous [0.07200501699717642] for 1 times.\n",
      "Iteration [12525]: Loss[0.07200490126739614] has not improved from the previous [0.07200376155394193] for 1 times.\n",
      "Iteration [12527]: Loss[0.07200365284049934] has not improved from the previous [0.07200248343942357] for 1 times.\n",
      "Iteration [12529]: Loss[0.07201160220371632] has not improved from the previous [0.0720012312623806] for 1 times.\n",
      "Iteration [12532]: Loss[0.07200114083504197] has not improved from the previous [0.07199916905445607] for 1 times.\n",
      "Iteration [12534]: Loss[0.07199988229801556] has not improved from the previous [0.07199799752165903] for 1 times.\n",
      "Iteration [12536]: Loss[0.07199862135543872] has not improved from the previous [0.07199670776064562] for 1 times.\n",
      "Iteration [12538]: Loss[0.07199743623742576] has not improved from the previous [0.07199542792696864] for 1 times.\n",
      "Iteration [12540]: Loss[0.07199621547595829] has not improved from the previous [0.07199412271715015] for 1 times.\n",
      "Iteration [12542]: Loss[0.07199496072764582] has not improved from the previous [0.07199285770589174] for 1 times.\n",
      "Iteration [12544]: Loss[0.07199563614187977] has not improved from the previous [0.07199165545506601] for 1 times.\n",
      "Iteration [12545]: Loss[0.07200019104705606] has not improved from the previous [0.07199563614187977] for 3 times.\n",
      "Iteration [12548]: Loss[0.07199052615951895] has not improved from the previous [0.07199044749062429] for 1 times.\n",
      "Iteration [12550]: Loss[0.07198934692204671] has not improved from the previous [0.07198907130452055] for 1 times.\n",
      "Iteration [12552]: Loss[0.07198812175606745] has not improved from the previous [0.07198775753599303] for 1 times.\n",
      "Iteration [12554]: Loss[0.07198691523818268] has not improved from the previous [0.07198645516726447] for 1 times.\n",
      "Iteration [12556]: Loss[0.0719857490917481] has not improved from the previous [0.07198513952405451] for 1 times.\n",
      "Iteration [12558]: Loss[0.07198457812266902] has not improved from the previous [0.07198379340121402] for 1 times.\n",
      "Iteration [12560]: Loss[0.07198339534509417] has not improved from the previous [0.07198246029900812] for 1 times.\n",
      "Iteration [12562]: Loss[0.07198216675860386] has not improved from the previous [0.07198117809371492] for 1 times.\n",
      "Iteration [12564]: Loss[0.07198088608865864] has not improved from the previous [0.07197989565103818] for 1 times.\n",
      "Iteration [12566]: Loss[0.07197973373296412] has not improved from the previous [0.07197865919341345] for 1 times.\n",
      "Iteration [12568]: Loss[0.07198776672654338] has not improved from the previous [0.07197758379134235] for 1 times.\n",
      "Iteration [12571]: Loss[0.07197729259926085] has not improved from the previous [0.07197530192373414] for 1 times.\n",
      "Iteration [12573]: Loss[0.07197603867700046] has not improved from the previous [0.07197404733163934] for 1 times.\n",
      "Iteration [12575]: Loss[0.07197485100885212] has not improved from the previous [0.07197278042186858] for 1 times.\n",
      "Iteration [12577]: Loss[0.07197403463288948] has not improved from the previous [0.07197151029342513] for 1 times.\n",
      "Iteration [12578]: Loss[0.07198012575258944] has not improved from the previous [0.07197403463288948] for 3 times.\n",
      "Iteration [12581]: Loss[0.07197046826583324] has not improved from the previous [0.07197035387937324] for 1 times.\n",
      "Iteration [12583]: Loss[0.07196931922830681] has not improved from the previous [0.07196894879330755] for 1 times.\n",
      "Iteration [12585]: Loss[0.07196814041912515] has not improved from the previous [0.07196759135785646] for 1 times.\n",
      "Iteration [12587]: Loss[0.07196693079353202] has not improved from the previous [0.07196627704310414] for 1 times.\n",
      "Iteration [12589]: Loss[0.07196574426562512] has not improved from the previous [0.07196497658251982] for 1 times.\n",
      "Iteration [12591]: Loss[0.07196459207231072] has not improved from the previous [0.07196365190802789] for 1 times.\n",
      "Iteration [12593]: Loss[0.07196337638535986] has not improved from the previous [0.07196229990682669] for 1 times.\n",
      "Iteration [12595]: Loss[0.07196221677647402] has not improved from the previous [0.07196102323383974] for 1 times.\n",
      "Iteration [12597]: Loss[0.0719610064075326] has not improved from the previous [0.07195972906989799] for 1 times.\n",
      "Iteration [12599]: Loss[0.07195974614843947] has not improved from the previous [0.07195843841286417] for 1 times.\n",
      "Iteration [12601]: Loss[0.07195860816469046] has not improved from the previous [0.07195719352501144] for 1 times.\n",
      "Iteration [12603]: Loss[0.07195745818862474] has not improved from the previous [0.07195583847838657] for 1 times.\n",
      "Iteration [12605]: Loss[0.0719562421900901] has not improved from the previous [0.07195450101202608] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 12600 Loss 0.07195719352501144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [12607]: Loss[0.07196423923483192] has not improved from the previous [0.07195352571884242] for 1 times.\n",
      "Iteration [12610]: Loss[0.07195305038188568] has not improved from the previous [0.07195173430326157] for 1 times.\n",
      "Iteration [12615]: Loss[0.07194954831328057] has not improved from the previous [0.0719494469037166] for 1 times.\n",
      "Iteration [12617]: Loss[0.07194838294430765] has not improved from the previous [0.07194808714104488] for 1 times.\n",
      "Iteration [12619]: Loss[0.071947218932943] has not improved from the previous [0.07194676328754507] for 1 times.\n",
      "Iteration [12621]: Loss[0.07194609548671842] has not improved from the previous [0.0719454285045267] for 1 times.\n",
      "Iteration [12623]: Loss[0.07194496443494376] has not improved from the previous [0.07194406213285706] for 1 times.\n",
      "Iteration [12625]: Loss[0.07195302952714484] has not improved from the previous [0.07194308049532687] for 1 times.\n",
      "Iteration [12628]: Loss[0.0719426108551873] has not improved from the previous [0.0719406562189501] for 1 times.\n",
      "Iteration [12630]: Loss[0.07194136105171144] has not improved from the previous [0.07193941528246695] for 1 times.\n",
      "Iteration [12632]: Loss[0.07194016708459475] has not improved from the previous [0.07193817136529845] for 1 times.\n",
      "Iteration [12634]: Loss[0.0719393350385417] has not improved from the previous [0.07193692115869148] for 1 times.\n",
      "Iteration [12635]: Loss[0.07194553884973379] has not improved from the previous [0.0719393350385417] for 3 times.\n",
      "Iteration [12638]: Loss[0.07193596134236613] has not improved from the previous [0.0719356312850904] for 1 times.\n",
      "Iteration [12640]: Loss[0.07193480443173877] has not improved from the previous [0.07193424775698735] for 1 times.\n",
      "Iteration [12642]: Loss[0.071933624595942] has not improved from the previous [0.07193290729113398] for 1 times.\n",
      "Iteration [12644]: Loss[0.07193248422681371] has not improved from the previous [0.07193158047720194] for 1 times.\n",
      "Iteration [12646]: Loss[0.07193136081179413] has not improved from the previous [0.07193022115146717] for 1 times.\n",
      "Iteration [12648]: Loss[0.0719301740904831] has not improved from the previous [0.07192885853184842] for 1 times.\n",
      "Iteration [12650]: Loss[0.07192902829457484] has not improved from the previous [0.0719275770932879] for 1 times.\n",
      "Iteration [12652]: Loss[0.071927788093805] has not improved from the previous [0.07192626756729725] for 1 times.\n",
      "Iteration [12654]: Loss[0.0719266394988107] has not improved from the previous [0.07192501912970113] for 1 times.\n",
      "Iteration [12656]: Loss[0.07192551800379846] has not improved from the previous [0.07192368584473327] for 1 times.\n",
      "Iteration [12658]: Loss[0.07192433160671334] has not improved from the previous [0.07192233234949837] for 1 times.\n",
      "Iteration [12660]: Loss[0.07192320253994999] has not improved from the previous [0.07192104063009236] for 1 times.\n",
      "Iteration [12662]: Loss[0.07192491707258895] has not improved from the previous [0.07191988662698363] for 1 times.\n",
      "Iteration [12663]: Loss[0.07192853397364223] has not improved from the previous [0.07192491707258895] for 3 times.\n",
      "Iteration [12666]: Loss[0.0719189338685171] has not improved from the previous [0.07191859348049048] for 1 times.\n",
      "Iteration [12668]: Loss[0.0719178387620283] has not improved from the previous [0.07191716853105205] for 1 times.\n",
      "Iteration [12670]: Loss[0.0719167116017768] has not improved from the previous [0.07191579030062596] for 1 times.\n",
      "Iteration [12672]: Loss[0.0719155481655931] has not improved from the previous [0.07191445816283237] for 1 times.\n",
      "Iteration [12674]: Loss[0.07191441535144999] has not improved from the previous [0.07191313287326896] for 1 times.\n",
      "Iteration [12676]: Loss[0.07191330773786452] has not improved from the previous [0.07191178419266749] for 1 times.\n",
      "Iteration [12678]: Loss[0.07191213822227306] has not improved from the previous [0.07191041372978721] for 1 times.\n",
      "Iteration [12680]: Loss[0.07191100930568424] has not improved from the previous [0.07190912913409274] for 1 times.\n",
      "Iteration [12682]: Loss[0.07190978345672142] has not improved from the previous [0.07190781419839772] for 1 times.\n",
      "Iteration [12684]: Loss[0.07191019609163495] has not improved from the previous [0.07190681841230588] for 1 times.\n",
      "Iteration [12686]: Loss[0.07190709789309226] has not improved from the previous [0.0719061041326017] for 1 times.\n",
      "Iteration [12688]: Loss[0.07190573701081225] has not improved from the previous [0.07190513535438954] for 1 times.\n",
      "Iteration [12690]: Loss[0.0719045676403719] has not improved from the previous [0.07190385506754737] for 1 times.\n",
      "Iteration [12692]: Loss[0.07190340896596864] has not improved from the previous [0.07190252814991092] for 1 times.\n",
      "Iteration [12694]: Loss[0.07190229982510628] has not improved from the previous [0.07190119993239319] for 1 times.\n",
      "Iteration [12696]: Loss[0.07190114797243477] has not improved from the previous [0.07189982565199014] for 1 times.\n",
      "Iteration [12698]: Loss[0.071900063503159] has not improved from the previous [0.07189850047260747] for 1 times.\n",
      "Iteration [12700]: Loss[0.07189892463379306] has not improved from the previous [0.07189715845127546] for 1 times.\n",
      "Iteration [12702]: Loss[0.07190689650282013] has not improved from the previous [0.07189590802128085] for 1 times.\n",
      "Iteration [12705]: Loss[0.07189635448657417] has not improved from the previous [0.07189451581772266] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 12700 Loss 0.07189892463379306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [12706]: Loss[0.07189687252282125] has not improved from the previous [0.07189635448657417] for 3 times.\n",
      "Iteration [12708]: Loss[0.07189446875437577] has not improved from the previous [0.07189271720446459] for 1 times.\n",
      "Iteration [12710]: Loss[0.07189293926150121] has not improved from the previous [0.0718917023193917] for 1 times.\n",
      "Iteration [12712]: Loss[0.0718916242248384] has not improved from the previous [0.07189045952092353] for 1 times.\n",
      "Iteration [12714]: Loss[0.07189035510058087] has not improved from the previous [0.0718892376418093] for 1 times.\n",
      "Iteration [12716]: Loss[0.07188924512326669] has not improved from the previous [0.07188796499689845] for 1 times.\n",
      "Iteration [12718]: Loss[0.07188812647732264] has not improved from the previous [0.07188658718100656] for 1 times.\n",
      "Iteration [12720]: Loss[0.07189616602973033] has not improved from the previous [0.0718854445575269] for 1 times.\n",
      "Iteration [12723]: Loss[0.07188501537766524] has not improved from the previous [0.0718837127487948] for 1 times.\n",
      "Iteration [12728]: Loss[0.07188164016199794] has not improved from the previous [0.07188140187945105] for 1 times.\n",
      "Iteration [12730]: Loss[0.07188053983132922] has not improved from the previous [0.0718800161734266] for 1 times.\n",
      "Iteration [12732]: Loss[0.07187947265808108] has not improved from the previous [0.07187864989448778] for 1 times.\n",
      "Iteration [12734]: Loss[0.07187839542133781] has not improved from the previous [0.07187725495073922] for 1 times.\n",
      "Iteration [12736]: Loss[0.07187731042115253] has not improved from the previous [0.07187587007989396] for 1 times.\n",
      "Iteration [12738]: Loss[0.071876173728211] has not improved from the previous [0.07187454182404575] for 1 times.\n",
      "Iteration [12740]: Loss[0.07188425300629497] has not improved from the previous [0.07187340645618223] for 1 times.\n",
      "Iteration [12743]: Loss[0.07187350183119853] has not improved from the previous [0.07187185491944277] for 1 times.\n",
      "Iteration [12744]: Loss[0.07187428262909334] has not improved from the previous [0.07187350183119853] for 3 times.\n",
      "Iteration [12746]: Loss[0.07187189137452778] has not improved from the previous [0.07187006495209823] for 1 times.\n",
      "Iteration [12748]: Loss[0.07187036487906447] has not improved from the previous [0.0718690364041176] for 1 times.\n",
      "Iteration [12750]: Loss[0.07186907167661093] has not improved from the previous [0.0718677793597674] for 1 times.\n",
      "Iteration [12752]: Loss[0.07186779949091897] has not improved from the previous [0.07186654447761681] for 1 times.\n",
      "Iteration [12754]: Loss[0.07186669993568895] has not improved from the previous [0.07186527366029126] for 1 times.\n",
      "Iteration [12756]: Loss[0.07186560166829653] has not improved from the previous [0.07186388256979907] for 1 times.\n",
      "Iteration [12758]: Loss[0.07187359737244288] has not improved from the previous [0.07186260027065569] for 1 times.\n",
      "Iteration [12761]: Loss[0.07186235283223369] has not improved from the previous [0.07186116323514105] for 1 times.\n",
      "Iteration [12766]: Loss[0.07185918443896692] has not improved from the previous [0.07185867214087577] for 1 times.\n",
      "Iteration [12768]: Loss[0.07185808932812517] has not improved from the previous [0.07185727795869186] for 1 times.\n",
      "Iteration [12770]: Loss[0.07185698514164938] has not improved from the previous [0.07185591414532275] for 1 times.\n",
      "Iteration [12772]: Loss[0.07185591535554636] has not improved from the previous [0.07185456476499233] for 1 times.\n",
      "Iteration [12774]: Loss[0.07185485140235745] has not improved from the previous [0.07185317638998999] for 1 times.\n",
      "Iteration [12776]: Loss[0.07185378196975904] has not improved from the previous [0.07185178557424744] for 1 times.\n",
      "Iteration [12778]: Loss[0.07185698281210806] has not improved from the previous [0.071850493789068] for 1 times.\n",
      "Iteration [12779]: Loss[0.0718632423078115] has not improved from the previous [0.07185698281210806] for 3 times.\n",
      "Iteration [12782]: Loss[0.07185054596088973] has not improved from the previous [0.07184870377259585] for 1 times.\n",
      "Iteration [12784]: Loss[0.07184909505265569] has not improved from the previous [0.07184759418182404] for 1 times.\n",
      "Iteration [12786]: Loss[0.07184785968321604] has not improved from the previous [0.07184637772239041] for 1 times.\n",
      "Iteration [12788]: Loss[0.07184669760144002] has not improved from the previous [0.07184503864925432] for 1 times.\n",
      "Iteration [12790]: Loss[0.07184556936032124] has not improved from the previous [0.0718437351176463] for 1 times.\n",
      "Iteration [12792]: Loss[0.07184442563713173] has not improved from the previous [0.07184240334914704] for 1 times.\n",
      "Iteration [12794]: Loss[0.07184600548649067] has not improved from the previous [0.07184122562080086] for 1 times.\n",
      "Iteration [12795]: Loss[0.07184988165521762] has not improved from the previous [0.07184600548649067] for 3 times.\n",
      "Iteration [12798]: Loss[0.07184039671186808] has not improved from the previous [0.07183980032508987] for 1 times.\n",
      "Iteration [12800]: Loss[0.07183929340487472] has not improved from the previous [0.0718384084647384] for 1 times.\n",
      "Iteration [12802]: Loss[0.07183820256929983] has not improved from the previous [0.07183703130332712] for 1 times.\n",
      "Iteration [12804]: Loss[0.07183715295844498] has not improved from the previous [0.07183565736108538] for 1 times.\n",
      "Iteration [12806]: Loss[0.07183899013775763] has not improved from the previous [0.0718342957879108] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 12800 Loss 0.07183929340487472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [12808]: Loss[0.07183502610835854] has not improved from the previous [0.07183425783829404] for 1 times.\n",
      "Iteration [12809]: Loss[0.07184250162999622] has not improved from the previous [0.07183502610835854] for 3 times.\n",
      "Iteration [12812]: Loss[0.07183246324088623] has not improved from the previous [0.07183151988506588] for 1 times.\n",
      "Iteration [12814]: Loss[0.07183126407928646] has not improved from the previous [0.07183025143157253] for 1 times.\n",
      "Iteration [12816]: Loss[0.07183017285459763] has not improved from the previous [0.07182890363653051] for 1 times.\n",
      "Iteration [12818]: Loss[0.07182909760387378] has not improved from the previous [0.07182749037216936] for 1 times.\n",
      "Iteration [12820]: Loss[0.07182794870005778] has not improved from the previous [0.07182610592435827] for 1 times.\n",
      "Iteration [12822]: Loss[0.07182684081070076] has not improved from the previous [0.07182481312866901] for 1 times.\n",
      "Iteration [12824]: Loss[0.07182635353694974] has not improved from the previous [0.07182362976828226] for 1 times.\n",
      "Iteration [12825]: Loss[0.0718364630008202] has not improved from the previous [0.07182635353694974] for 3 times.\n",
      "Iteration [12828]: Loss[0.071823708251365] has not improved from the previous [0.07182175677890042] for 1 times.\n",
      "Iteration [12830]: Loss[0.07182229252066061] has not improved from the previous [0.07182067271642197] for 1 times.\n",
      "Iteration [12832]: Loss[0.0718210750688785] has not improved from the previous [0.07181936497409638] for 1 times.\n",
      "Iteration [12834]: Loss[0.071819858169665] has not improved from the previous [0.07181806133760167] for 1 times.\n",
      "Iteration [12836]: Loss[0.07181873401166163] has not improved from the previous [0.07181677074285746] for 1 times.\n",
      "Iteration [12838]: Loss[0.07181713783827377] has not improved from the previous [0.07181552899815964] for 1 times.\n",
      "Iteration [12843]: Loss[0.07181362992728851] has not improved from the previous [0.071813508402078] for 1 times.\n",
      "Iteration [12845]: Loss[0.07181261621911814] has not improved from the previous [0.07181204338994579] for 1 times.\n",
      "Iteration [12847]: Loss[0.07181158887884943] has not improved from the previous [0.07181061142009369] for 1 times.\n",
      "Iteration [12849]: Loss[0.07181053556193338] has not improved from the previous [0.07180920987972784] for 1 times.\n",
      "Iteration [12851]: Loss[0.07181243321716795] has not improved from the previous [0.07180811122635217] for 1 times.\n",
      "Iteration [12853]: Loss[0.07181088718820217] has not improved from the previous [0.0718077162120458] for 1 times.\n",
      "Iteration [12854]: Loss[0.07181593828340492] has not improved from the previous [0.07181088718820217] for 3 times.\n",
      "Iteration [12857]: Loss[0.071805890355641] has not improved from the previous [0.07180512174523655] for 1 times.\n",
      "Iteration [12859]: Loss[0.07180479769597413] has not improved from the previous [0.07180381726814143] for 1 times.\n",
      "Iteration [12861]: Loss[0.07180372548518184] has not improved from the previous [0.07180237642495592] for 1 times.\n",
      "Iteration [12863]: Loss[0.07180260571571058] has not improved from the previous [0.071801010590563] for 1 times.\n",
      "Iteration [12865]: Loss[0.07180145683863422] has not improved from the previous [0.07179965252798745] for 1 times.\n",
      "Iteration [12867]: Loss[0.07180042423421856] has not improved from the previous [0.07179832573571442] for 1 times.\n",
      "Iteration [12869]: Loss[0.07180265657339391] has not improved from the previous [0.07179719889415531] for 1 times.\n",
      "Iteration [12870]: Loss[0.07181006394106011] has not improved from the previous [0.07180265657339391] for 3 times.\n",
      "Iteration [12873]: Loss[0.07179706495727467] has not improved from the previous [0.07179537960491314] for 1 times.\n",
      "Iteration [12875]: Loss[0.07179512000691773] has not improved from the previous [0.07179495387765594] for 1 times.\n",
      "Iteration [12882]: Loss[0.07179080837800893] has not improved from the previous [0.07179071620915668] for 1 times.\n",
      "Iteration [12884]: Loss[0.07178978242186919] has not improved from the previous [0.07178937292028455] for 1 times.\n",
      "Iteration [12886]: Loss[0.07178875974227711] has not improved from the previous [0.0717879408386695] for 1 times.\n",
      "Iteration [12888]: Loss[0.07178773492063666] has not improved from the previous [0.07178650680548658] for 1 times.\n",
      "Iteration [12890]: Loss[0.0718002015692591] has not improved from the previous [0.07178556544555859] for 1 times.\n",
      "Iteration [12896]: Loss[0.07178302509650314] has not improved from the previous [0.07178260832650596] for 1 times.\n",
      "Iteration [12898]: Loss[0.07178198259508468] has not improved from the previous [0.07178117456805821] for 1 times.\n",
      "Iteration [12900]: Loss[0.07178088235590027] has not improved from the previous [0.07177972274101974] for 1 times.\n",
      "Iteration [12902]: Loss[0.07177985032682525] has not improved from the previous [0.07177833372105176] for 1 times.\n",
      "Iteration [12904]: Loss[0.07177876358478842] has not improved from the previous [0.07177695111339957] for 1 times.\n",
      "Iteration [12906]: Loss[0.07177768734097462] has not improved from the previous [0.071775573714841] for 1 times.\n",
      "Iteration [12908]: Loss[0.07177507979380884] has not improved from the previous [0.07177488562018797] for 1 times.\n",
      "Iteration [12909]: Loss[0.071785564449175] has not improved from the previous [0.07177507979380884] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 12900 Loss 0.07178088235590027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [12912]: Loss[0.07177445441159121] has not improved from the previous [0.07177269751996553] for 1 times.\n",
      "Iteration [12914]: Loss[0.07177320564377862] has not improved from the previous [0.07177147498466396] for 1 times.\n",
      "Iteration [12916]: Loss[0.0717720473423673] has not improved from the previous [0.07177009251502339] for 1 times.\n",
      "Iteration [12918]: Loss[0.07177165082897621] has not improved from the previous [0.07176878322516919] for 1 times.\n",
      "Iteration [12919]: Loss[0.07177757884231976] has not improved from the previous [0.07177165082897621] for 3 times.\n",
      "Iteration [12922]: Loss[0.0717710871611762] has not improved from the previous [0.07176766743792266] for 1 times.\n",
      "Iteration [12924]: Loss[0.07176735331029659] has not improved from the previous [0.07176639570296472] for 1 times.\n",
      "Iteration [12931]: Loss[0.07176259581101387] has not improved from the previous [0.0717625376122799] for 1 times.\n",
      "Iteration [12933]: Loss[0.07176155011960118] has not improved from the previous [0.07176108367232836] for 1 times.\n",
      "Iteration [12935]: Loss[0.07176041608652167] has not improved from the previous [0.07175967875673303] for 1 times.\n",
      "Iteration [12937]: Loss[0.07175936886499094] has not improved from the previous [0.07175832706446535] for 1 times.\n",
      "Iteration [12939]: Loss[0.0717614420500483] has not improved from the previous [0.07175704355208035] for 1 times.\n",
      "Iteration [12941]: Loss[0.07175862811458823] has not improved from the previous [0.0717567372800318] for 1 times.\n",
      "Iteration [12942]: Loss[0.07176499120424645] has not improved from the previous [0.07175862811458823] for 3 times.\n",
      "Iteration [12945]: Loss[0.07175497441989251] has not improved from the previous [0.07175419895484562] for 1 times.\n",
      "Iteration [12947]: Loss[0.07175384075452768] has not improved from the previous [0.07175278950520443] for 1 times.\n",
      "Iteration [12949]: Loss[0.071752756072899] has not improved from the previous [0.07175141759632052] for 1 times.\n",
      "Iteration [12951]: Loss[0.07175167057907732] has not improved from the previous [0.0717500101697473] for 1 times.\n",
      "Iteration [12953]: Loss[0.07175063499935709] has not improved from the previous [0.07174861539707303] for 1 times.\n",
      "Iteration [12956]: Loss[0.07174914000610874] has not improved from the previous [0.07174781446762697] for 1 times.\n",
      "Iteration [12958]: Loss[0.07174757441937758] has not improved from the previous [0.0717465107243744] for 1 times.\n",
      "Iteration [12960]: Loss[0.07174625883923734] has not improved from the previous [0.07174518807683757] for 1 times.\n",
      "Iteration [12962]: Loss[0.07174511982414221] has not improved from the previous [0.07174387357963835] for 1 times.\n",
      "Iteration [12964]: Loss[0.07175331392059081] has not improved from the previous [0.0717426684860355] for 1 times.\n",
      "Iteration [12967]: Loss[0.07174307471505319] has not improved from the previous [0.07174091841356682] for 1 times.\n",
      "Iteration [12968]: Loss[0.07174371238925702] has not improved from the previous [0.07174307471505319] for 3 times.\n",
      "Iteration [12970]: Loss[0.07174131322685713] has not improved from the previous [0.0717393968094] for 1 times.\n",
      "Iteration [12972]: Loss[0.07173983719443015] has not improved from the previous [0.07173818377114706] for 1 times.\n",
      "Iteration [12974]: Loss[0.07173852076349055] has not improved from the previous [0.07173690541044349] for 1 times.\n",
      "Iteration [12976]: Loss[0.07173742821802909] has not improved from the previous [0.07173558284082908] for 1 times.\n",
      "Iteration [12978]: Loss[0.07173828288293578] has not improved from the previous [0.07173419195667416] for 1 times.\n",
      "Iteration [12979]: Loss[0.0717429654392524] has not improved from the previous [0.07173828288293578] for 3 times.\n",
      "Iteration [12982]: Loss[0.0717366101465487] has not improved from the previous [0.07173330616988] for 1 times.\n",
      "Iteration [12984]: Loss[0.07173271338907304] has not improved from the previous [0.0717319358398178] for 1 times.\n",
      "Iteration [12991]: Loss[0.0717281278550784] has not improved from the previous [0.0717278178037627] for 1 times.\n",
      "Iteration [12993]: Loss[0.07172708053033115] has not improved from the previous [0.07172639033120182] for 1 times.\n",
      "Iteration [12995]: Loss[0.0717260710410911] has not improved from the previous [0.07172493935577866] for 1 times.\n",
      "Iteration [12997]: Loss[0.07172820290476813] has not improved from the previous [0.07172396012403572] for 1 times.\n",
      "Iteration [12999]: Loss[0.07172570159292695] has not improved from the previous [0.07172348350410453] for 1 times.\n",
      "Iteration [13000]: Loss[0.07173179664806495] has not improved from the previous [0.07172570159292695] for 3 times.\n",
      "Iteration [13003]: Loss[0.07172177856526456] has not improved from the previous [0.07172072417500312] for 1 times.\n",
      "Iteration [13005]: Loss[0.07172059717647522] has not improved from the previous [0.0717193529165015] for 1 times.\n",
      "Iteration [13007]: Loss[0.07171953458053293] has not improved from the previous [0.07171797075899496] for 1 times.\n",
      "Iteration [13009]: Loss[0.07171851586454615] has not improved from the previous [0.07171652724684882] for 1 times.\n",
      "Iteration [13012]: Loss[0.07171709397232967] has not improved from the previous [0.07171577322772342] for 1 times.\n",
      "Iteration [13014]: Loss[0.07171553660671971] has not improved from the previous [0.07171441508505288] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 13000 Loss 0.07173179664806495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [13016]: Loss[0.07171421351340627] has not improved from the previous [0.07171308508736497] for 1 times.\n",
      "Iteration [13018]: Loss[0.0717131016715574] has not improved from the previous [0.07171174457086842] for 1 times.\n",
      "Iteration [13020]: Loss[0.07172131527652663] has not improved from the previous [0.07171044577147902] for 1 times.\n",
      "Iteration [13024]: Loss[0.0717106140754441] has not improved from the previous [0.07170882066615805] for 1 times.\n",
      "Iteration [13026]: Loss[0.07170902367862081] has not improved from the previous [0.07170751250668303] for 1 times.\n",
      "Iteration [13028]: Loss[0.07170772530234713] has not improved from the previous [0.07170611957653479] for 1 times.\n",
      "Iteration [13030]: Loss[0.071706561752135] has not improved from the previous [0.0717047754932195] for 1 times.\n",
      "Iteration [13032]: Loss[0.0717071852687206] has not improved from the previous [0.07170337620390194] for 1 times.\n",
      "Iteration [13033]: Loss[0.0717165588353274] has not improved from the previous [0.0717071852687206] for 3 times.\n",
      "Iteration [13036]: Loss[0.07170357586848503] has not improved from the previous [0.07170176348269218] for 1 times.\n",
      "Iteration [13038]: Loss[0.071701531483931] has not improved from the previous [0.07170136154932087] for 1 times.\n",
      "Iteration [13043]: Loss[0.07169843156852075] has not improved from the previous [0.0716983646309449] for 1 times.\n",
      "Iteration [13045]: Loss[0.07169739467611928] has not improved from the previous [0.07169688847484595] for 1 times.\n",
      "Iteration [13047]: Loss[0.07169960082423918] has not improved from the previous [0.07169588824724438] for 1 times.\n",
      "Iteration [13049]: Loss[0.07169654904153286] has not improved from the previous [0.07169490068257121] for 1 times.\n",
      "Iteration [13050]: Loss[0.07170314028249913] has not improved from the previous [0.07169654904153286] for 3 times.\n",
      "Iteration [13053]: Loss[0.07169313608228069] has not improved from the previous [0.07169265569807674] for 1 times.\n",
      "Iteration [13055]: Loss[0.07169208116264732] has not improved from the previous [0.07169126371220376] for 1 times.\n",
      "Iteration [13057]: Loss[0.07169105814551943] has not improved from the previous [0.07168974501827244] for 1 times.\n",
      "Iteration [13059]: Loss[0.07169330397842598] has not improved from the previous [0.07168872809684541] for 1 times.\n",
      "Iteration [13064]: Loss[0.07168677565240278] has not improved from the previous [0.07168646431590708] for 1 times.\n",
      "Iteration [13066]: Loss[0.07168573694876045] has not improved from the previous [0.07168490867596133] for 1 times.\n",
      "Iteration [13068]: Loss[0.07168470785207204] has not improved from the previous [0.07168339135142539] for 1 times.\n",
      "Iteration [13070]: Loss[0.07168359888025222] has not improved from the previous [0.07168193324285986] for 1 times.\n",
      "Iteration [13072]: Loss[0.07168582255903364] has not improved from the previous [0.07168149355431064] for 1 times.\n",
      "Iteration [13074]: Loss[0.07168279722243943] has not improved from the previous [0.07168112774701453] for 1 times.\n",
      "Iteration [13075]: Loss[0.07168937609954196] has not improved from the previous [0.07168279722243943] for 3 times.\n",
      "Iteration [13078]: Loss[0.07167935448114215] has not improved from the previous [0.0716777699774671] for 1 times.\n",
      "Iteration [13080]: Loss[0.07167829669081552] has not improved from the previous [0.07167638294892745] for 1 times.\n",
      "Iteration [13082]: Loss[0.07167571696712277] has not improved from the previous [0.07167517844085529] for 1 times.\n",
      "Iteration [13083]: Loss[0.0716769855294491] has not improved from the previous [0.07167571696712277] for 3 times.\n",
      "Iteration [13085]: Loss[0.07167539763310887] has not improved from the previous [0.0716742739790936] for 1 times.\n",
      "Iteration [13087]: Loss[0.0716740567967556] has not improved from the previous [0.07167291249683377] for 1 times.\n",
      "Iteration [13089]: Loss[0.07167296855321867] has not improved from the previous [0.07167151777035675] for 1 times.\n",
      "Iteration [13091]: Loss[0.07167192309303223] has not improved from the previous [0.07167001135891238] for 1 times.\n",
      "Iteration [13093]: Loss[0.07166988274520827] has not improved from the previous [0.071668698677741] for 1 times.\n",
      "Iteration [13094]: Loss[0.07168006851669967] has not improved from the previous [0.07166988274520827] for 3 times.\n",
      "Iteration [13097]: Loss[0.07166895532008578] has not improved from the previous [0.0716671223081059] for 1 times.\n",
      "Iteration [13099]: Loss[0.07166766290903381] has not improved from the previous [0.07166578136678022] for 1 times.\n",
      "Iteration [13101]: Loss[0.07166678732661523] has not improved from the previous [0.07166445658802197] for 1 times.\n",
      "Iteration [13102]: Loss[0.07167777319429415] has not improved from the previous [0.07166678732661523] for 3 times.\n",
      "Iteration [13105]: Loss[0.07166461903018939] has not improved from the previous [0.07166292017004969] for 1 times.\n",
      "Iteration [13112]: Loss[0.07165958523343931] has not improved from the previous [0.071659324156726] for 1 times.\n",
      "Iteration [13114]: Loss[0.0716619322074324] has not improved from the previous [0.07165822319705671] for 1 times.\n",
      "Iteration [13116]: Loss[0.07165783005057158] has not improved from the previous [0.07165715117426294] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 13100 Loss 0.07166445658802197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [13121]: Loss[0.0716543935950483] has not improved from the previous [0.07165437147707117] for 1 times.\n",
      "Iteration [13123]: Loss[0.07165338301560943] has not improved from the previous [0.07165281162838791] for 1 times.\n",
      "Iteration [13125]: Loss[0.07165573892123484] has not improved from the previous [0.07165171942783162] for 1 times.\n",
      "Iteration [13127]: Loss[0.07165366254073151] has not improved from the previous [0.07165095953375163] for 1 times.\n",
      "Iteration [13128]: Loss[0.07165927020824374] has not improved from the previous [0.07165366254073151] for 3 times.\n",
      "Iteration [13131]: Loss[0.0716492487522185] has not improved from the previous [0.07164848130852552] for 1 times.\n",
      "Iteration [13133]: Loss[0.0716481440349686] has not improved from the previous [0.07164705786292953] for 1 times.\n",
      "Iteration [13135]: Loss[0.07165049468196451] has not improved from the previous [0.07164584988058254] for 1 times.\n",
      "Iteration [13140]: Loss[0.07164404183908168] has not improved from the previous [0.07164365409346515] for 1 times.\n",
      "Iteration [13142]: Loss[0.0716429436186339] has not improved from the previous [0.07164208462727244] for 1 times.\n",
      "Iteration [13144]: Loss[0.07164189681414308] has not improved from the previous [0.07164058194631812] for 1 times.\n",
      "Iteration [13146]: Loss[0.07164424122354633] has not improved from the previous [0.07163971927584585] for 1 times.\n",
      "Iteration [13148]: Loss[0.07164162070106939] has not improved from the previous [0.07163950667473594] for 1 times.\n",
      "Iteration [13149]: Loss[0.07164783476671897] has not improved from the previous [0.07164162070106939] for 3 times.\n",
      "Iteration [13152]: Loss[0.07163779990295624] has not improved from the previous [0.0716362625119098] for 1 times.\n",
      "Iteration [13154]: Loss[0.07163673728451386] has not improved from the previous [0.07163477762913467] for 1 times.\n",
      "Iteration [13157]: Loss[0.07163550773034917] has not improved from the previous [0.07163418848984757] for 1 times.\n",
      "Iteration [13159]: Loss[0.07163386693817451] has not improved from the previous [0.07163271772459778] for 1 times.\n",
      "Iteration [13161]: Loss[0.07163260480885363] has not improved from the previous [0.07163130848829755] for 1 times.\n",
      "Iteration [13163]: Loss[0.07163152832745018] has not improved from the previous [0.07162981704484306] for 1 times.\n",
      "Iteration [13165]: Loss[0.07163045800919175] has not improved from the previous [0.07162832259057882] for 1 times.\n",
      "Iteration [13168]: Loss[0.07163870813611595] has not improved from the previous [0.07162811248756407] for 1 times.\n",
      "Iteration [13171]: Loss[0.07162724839090964] has not improved from the previous [0.07162550362245933] for 1 times.\n",
      "Iteration [13176]: Loss[0.07162692023586299] has not improved from the previous [0.07162417518142195] for 1 times.\n",
      "Iteration [13178]: Loss[0.07162484222846481] has not improved from the previous [0.0716222175739836] for 1 times.\n",
      "Iteration [13179]: Loss[0.07163058675776517] has not improved from the previous [0.07162484222846481] for 3 times.\n",
      "Iteration [13182]: Loss[0.07162053074258865] has not improved from the previous [0.071620396264172] for 1 times.\n",
      "Iteration [13184]: Loss[0.07161949160274227] has not improved from the previous [0.07161892835626259] for 1 times.\n",
      "Iteration [13186]: Loss[0.07162187542309059] has not improved from the previous [0.07161839134202182] for 1 times.\n",
      "Iteration [13188]: Loss[0.07161740347539927] has not improved from the previous [0.07161713651689036] for 1 times.\n",
      "Iteration [13193]: Loss[0.07161436510619419] has not improved from the previous [0.07161386042503756] for 1 times.\n",
      "Iteration [13195]: Loss[0.07161679654708701] has not improved from the previous [0.07161269442684308] for 1 times.\n",
      "Iteration [13197]: Loss[0.07161374036273681] has not improved from the previous [0.07161204331205696] for 1 times.\n",
      "Iteration [13198]: Loss[0.07162035286247494] has not improved from the previous [0.07161374036273681] for 3 times.\n",
      "Iteration [13201]: Loss[0.07161032243272192] has not improved from the previous [0.07160944557802633] for 1 times.\n",
      "Iteration [13203]: Loss[0.07160925128822467] has not improved from the previous [0.07160794409970467] for 1 times.\n",
      "Iteration [13205]: Loss[0.0716116338263617] has not improved from the previous [0.07160749127708246] for 1 times.\n",
      "Iteration [13210]: Loss[0.07160517461815448] has not improved from the previous [0.07160454301252259] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 13200 Loss 0.07160944557802633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [13212]: Loss[0.07160414832126565] has not improved from the previous [0.0716028799619903] for 1 times.\n",
      "Iteration [13214]: Loss[0.0716065854130086] has not improved from the previous [0.07160192489664532] for 1 times.\n",
      "Iteration [13216]: Loss[0.07160264411562048] has not improved from the previous [0.07160181482730772] for 1 times.\n",
      "Iteration [13217]: Loss[0.07161006434361383] has not improved from the previous [0.07160264411562048] for 3 times.\n",
      "Iteration [13220]: Loss[0.07160003766316135] has not improved from the previous [0.0715985216700183] for 1 times.\n",
      "Iteration [13222]: Loss[0.07159895460278143] has not improved from the previous [0.07159705286103511] for 1 times.\n",
      "Iteration [13225]: Loss[0.07159780346368919] has not improved from the previous [0.07159641411143307] for 1 times.\n",
      "Iteration [13227]: Loss[0.07159622902242706] has not improved from the previous [0.07159493363483006] for 1 times.\n",
      "Iteration [13229]: Loss[0.07159494765932443] has not improved from the previous [0.07159344027377801] for 1 times.\n",
      "Iteration [13231]: Loss[0.0715938253978337] has not improved from the previous [0.07159198673274388] for 1 times.\n",
      "Iteration [13234]: Loss[0.07159270696333882] has not improved from the previous [0.07159141327060954] for 1 times.\n",
      "Iteration [13236]: Loss[0.07159109493422831] has not improved from the previous [0.07158992868928869] for 1 times.\n",
      "Iteration [13238]: Loss[0.07158989770245765] has not improved from the previous [0.07158842180653957] for 1 times.\n",
      "Iteration [13240]: Loss[0.07159807131612228] has not improved from the previous [0.07158708780991055] for 1 times.\n",
      "Iteration [13244]: Loss[0.07158717548889892] has not improved from the previous [0.07158576282482347] for 1 times.\n",
      "Iteration [13251]: Loss[0.07158548742336313] has not improved from the previous [0.0715826557729843] for 1 times.\n",
      "Iteration [13253]: Loss[0.07158397277048281] has not improved from the previous [0.07158075557176533] for 1 times.\n",
      "Iteration [13254]: Loss[0.07158912059753716] has not improved from the previous [0.07158397277048281] for 3 times.\n",
      "Iteration [13257]: Loss[0.07157909431783585] has not improved from the previous [0.07157876248490169] for 1 times.\n",
      "Iteration [13259]: Loss[0.07158157966916467] has not improved from the previous [0.07157764793397084] for 1 times.\n",
      "Iteration [13261]: Loss[0.07157729926583814] has not improved from the previous [0.07157680826513374] for 1 times.\n",
      "Iteration [13266]: Loss[0.07157394718016735] has not improved from the previous [0.07157365291663456] for 1 times.\n",
      "Iteration [13268]: Loss[0.07157645941468367] has not improved from the previous [0.07157287192234389] for 1 times.\n",
      "Iteration [13270]: Loss[0.07157364056445774] has not improved from the previous [0.07157170702405372] for 1 times.\n",
      "Iteration [13271]: Loss[0.07158003050366685] has not improved from the previous [0.07157364056445774] for 3 times.\n",
      "Iteration [13274]: Loss[0.07156999351516936] has not improved from the previous [0.07156912253362527] for 1 times.\n",
      "Iteration [13276]: Loss[0.0715725206472147] has not improved from the previous [0.07156815772773296] for 1 times.\n",
      "Iteration [13281]: Loss[0.07156593250805003] has not improved from the previous [0.07156569330292652] for 1 times.\n",
      "Iteration [13283]: Loss[0.0715649035607808] has not improved from the previous [0.0715640010761083] for 1 times.\n",
      "Iteration [13285]: Loss[0.07156740737104411] has not improved from the previous [0.0715633628630185] for 1 times.\n",
      "Iteration [13287]: Loss[0.07156334789808012] has not improved from the previous [0.07156265898745624] for 1 times.\n",
      "Iteration [13288]: Loss[0.07157095673814579] has not improved from the previous [0.07156334789808012] for 3 times.\n",
      "Iteration [13291]: Loss[0.07156092867193332] has not improved from the previous [0.07155947056294355] for 1 times.\n",
      "Iteration [13293]: Loss[0.07156340555800063] has not improved from the previous [0.07155880828181956] for 1 times.\n",
      "Iteration [13298]: Loss[0.07155687144621392] has not improved from the previous [0.07155607099641506] for 1 times.\n",
      "Iteration [13300]: Loss[0.07155584028037344] has not improved from the previous [0.07155435485369942] for 1 times.\n",
      "Iteration [13302]: Loss[0.07155835619773043] has not improved from the previous [0.071554059579865] for 1 times.\n",
      "Iteration [13305]: Loss[0.07156184118334345] has not improved from the previous [0.07155314633238354] for 1 times.\n",
      "Iteration [13308]: Loss[0.07155188735550098] has not improved from the previous [0.07154987300851001] for 1 times.\n",
      "Iteration [13311]: Loss[0.071550820501206] has not improved from the previous [0.07154935276090181] for 1 times.\n",
      "Iteration [13313]: Loss[0.07154915152701465] has not improved from the previous [0.07154780010611488] for 1 times.\n",
      "Iteration [13315]: Loss[0.07154787853094526] has not improved from the previous [0.07154625845181735] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 13300 Loss 0.07155584028037344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [13317]: Loss[0.07154678584430592] has not improved from the previous [0.07154470662531388] for 1 times.\n",
      "Iteration [13320]: Loss[0.07154571670326026] has not improved from the previous [0.0715441976248831] for 1 times.\n",
      "Iteration [13322]: Loss[0.07154407496928607] has not improved from the previous [0.07154261345292146] for 1 times.\n",
      "Iteration [13324]: Loss[0.07154286271745927] has not improved from the previous [0.07154112481741418] for 1 times.\n",
      "Iteration [13326]: Loss[0.07154066737532415] has not improved from the previous [0.0715399366271123] for 1 times.\n",
      "Iteration [13327]: Loss[0.07154180053411294] has not improved from the previous [0.07154066737532415] for 3 times.\n",
      "Iteration [13329]: Loss[0.07154954366856699] has not improved from the previous [0.07153939921213454] for 1 times.\n",
      "Iteration [13332]: Loss[0.07153947470081042] has not improved from the previous [0.07153674893233217] for 1 times.\n",
      "Iteration [13333]: Loss[0.07154006616109207] has not improved from the previous [0.07153947470081042] for 3 times.\n",
      "Iteration [13335]: Loss[0.07153759117590318] has not improved from the previous [0.07153594244096965] for 1 times.\n",
      "Iteration [13337]: Loss[0.07153601370487013] has not improved from the previous [0.07153451863736351] for 1 times.\n",
      "Iteration [13339]: Loss[0.07153483944081508] has not improved from the previous [0.07153298005810263] for 1 times.\n",
      "Iteration [13341]: Loss[0.07153256214862613] has not improved from the previous [0.07153174237124568] for 1 times.\n",
      "Iteration [13342]: Loss[0.07153379248941194] has not improved from the previous [0.07153256214862613] for 3 times.\n",
      "Iteration [13344]: Loss[0.07153212493525267] has not improved from the previous [0.07153087804826591] for 1 times.\n",
      "Iteration [13346]: Loss[0.07153085683049687] has not improved from the previous [0.07152935118984526] for 1 times.\n",
      "Iteration [13348]: Loss[0.07153907345558914] has not improved from the previous [0.07152776857920651] for 1 times.\n",
      "Iteration [13352]: Loss[0.07152791863994094] has not improved from the previous [0.07152689532608134] for 1 times.\n",
      "Iteration [13357]: Loss[0.07152782702451253] has not improved from the previous [0.0715242852813025] for 1 times.\n",
      "Iteration [13359]: Loss[0.07152563716551055] has not improved from the previous [0.07152303787659513] for 1 times.\n",
      "Iteration [13360]: Loss[0.07153140632840993] has not improved from the previous [0.07152563716551055] for 3 times.\n",
      "Iteration [13363]: Loss[0.07152134207015097] has not improved from the previous [0.0715209142085618] for 1 times.\n",
      "Iteration [13365]: Loss[0.07152391438495503] has not improved from the previous [0.07152020416960495] for 1 times.\n",
      "Iteration [13367]: Loss[0.07151936050381326] has not improved from the previous [0.0715191444049314] for 1 times.\n",
      "Iteration [13370]: Loss[0.07151734668554029] has not improved from the previous [0.07151728503290453] for 1 times.\n",
      "Iteration [13372]: Loss[0.07152001715331358] has not improved from the previous [0.0715158109214956] for 1 times.\n",
      "Iteration [13374]: Loss[0.07151615757847274] has not improved from the previous [0.07151521908557025] for 1 times.\n",
      "Iteration [13375]: Loss[0.07152349952492651] has not improved from the previous [0.07151615757847274] for 3 times.\n",
      "Iteration [13378]: Loss[0.07151342875864182] has not improved from the previous [0.07151254958091836] for 1 times.\n",
      "Iteration [13380]: Loss[0.07151605275778165] has not improved from the previous [0.07151202649035586] for 1 times.\n",
      "Iteration [13385]: Loss[0.0715094457463298] has not improved from the previous [0.07150898454443079] for 1 times.\n",
      "Iteration [13387]: Loss[0.07151211981120827] has not improved from the previous [0.07150767993146008] for 1 times.\n",
      "Iteration [13389]: Loss[0.07150738277778873] has not improved from the previous [0.0715073214326478] for 1 times.\n",
      "Iteration [13392]: Loss[0.07150554229646695] has not improved from the previous [0.07150525487646527] for 1 times.\n",
      "Iteration [13394]: Loss[0.07150821288933822] has not improved from the previous [0.07150357333593166] for 1 times.\n",
      "Iteration [13396]: Loss[0.07150529186219798] has not improved from the previous [0.071503447215033] for 1 times.\n",
      "Iteration [13397]: Loss[0.07151167637165445] has not improved from the previous [0.07150529186219798] for 3 times.\n",
      "Iteration [13400]: Loss[0.07150163332194047] has not improved from the previous [0.07150050271292356] for 1 times.\n",
      "Iteration [13402]: Loss[0.07150424386999128] has not improved from the previous [0.0714998478209653] for 1 times.\n",
      "Iteration [13407]: Loss[0.07149769915865188] has not improved from the previous [0.07149684813757229] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 13400 Loss 0.07150163332194047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [13409]: Loss[0.07150036564608926] has not improved from the previous [0.07149563798123391] for 1 times.\n",
      "Iteration [13411]: Loss[0.07149565370765083] has not improved from the previous [0.071495604780456] for 1 times.\n",
      "Iteration [13412]: Loss[0.07150381477441776] has not improved from the previous [0.07149565370765083] for 3 times.\n",
      "Iteration [13415]: Loss[0.0714937316184168] has not improved from the previous [0.07149211423760286] for 1 times.\n",
      "Iteration [13418]: Loss[0.07149276108310965] has not improved from the previous [0.07149161653701418] for 1 times.\n",
      "Iteration [13420]: Loss[0.07149115031110213] has not improved from the previous [0.07148997425037268] for 1 times.\n",
      "Iteration [13422]: Loss[0.07148992272904942] has not improved from the previous [0.07148833677800233] for 1 times.\n",
      "Iteration [13424]: Loss[0.07149254981556981] has not improved from the previous [0.07148765366501579] for 1 times.\n",
      "Iteration [13427]: Loss[0.07148691535593713] has not improved from the previous [0.07148689585338086] for 1 times.\n",
      "Iteration [13429]: Loss[0.07148591449550405] has not improved from the previous [0.07148480035601071] for 1 times.\n",
      "Iteration [13431]: Loss[0.07148860921652751] has not improved from the previous [0.07148373938966626] for 1 times.\n",
      "Iteration [13433]: Loss[0.07148459587121875] has not improved from the previous [0.07148381671319722] for 1 times.\n",
      "Iteration [13434]: Loss[0.07149210511473822] has not improved from the previous [0.07148459587121875] for 3 times.\n",
      "Iteration [13437]: Loss[0.0714820705177341] has not improved from the previous [0.071479976942371] for 1 times.\n",
      "Iteration [13440]: Loss[0.07148108897752559] has not improved from the previous [0.0714795196618833] for 1 times.\n",
      "Iteration [13442]: Loss[0.0714794220994623] has not improved from the previous [0.07147781308942358] for 1 times.\n",
      "Iteration [13444]: Loss[0.07147818401327265] has not improved from the previous [0.07147622417928064] for 1 times.\n",
      "Iteration [13447]: Loss[0.07147721820735811] has not improved from the previous [0.07147577253232994] for 1 times.\n",
      "Iteration [13449]: Loss[0.07147557746544578] has not improved from the previous [0.07147407600664923] for 1 times.\n",
      "Iteration [13451]: Loss[0.07147425489669748] has not improved from the previous [0.07147244659098338] for 1 times.\n",
      "Iteration [13454]: Loss[0.07148284532649737] has not improved from the previous [0.07147212943141064] for 1 times.\n",
      "Iteration [13457]: Loss[0.07147125305763578] has not improved from the previous [0.0714695148821256] for 1 times.\n",
      "Iteration [13458]: Loss[0.07147299821199193] has not improved from the previous [0.07147125305763578] for 3 times.\n",
      "Iteration [13460]: Loss[0.07147032593682294] has not improved from the previous [0.07146887979451762] for 1 times.\n",
      "Iteration [13462]: Loss[0.07146873129822895] has not improved from the previous [0.07146730627166097] for 1 times.\n",
      "Iteration [13464]: Loss[0.07146749109229666] has not improved from the previous [0.07146565301996043] for 1 times.\n",
      "Iteration [13466]: Loss[0.07146520842149319] has not improved from the previous [0.07146500779401356] for 1 times.\n",
      "Iteration [13467]: Loss[0.07146657863096072] has not improved from the previous [0.07146520842149319] for 3 times.\n",
      "Iteration [13469]: Loss[0.07146486277607043] has not improved from the previous [0.07146350392746192] for 1 times.\n",
      "Iteration [13471]: Loss[0.07146362441939019] has not improved from the previous [0.07146186092843047] for 1 times.\n",
      "Iteration [13473]: Loss[0.07146211597819974] has not improved from the previous [0.07146107245863742] for 1 times.\n",
      "Iteration [13474]: Loss[0.07147220878564314] has not improved from the previous [0.07146211597819974] for 3 times.\n",
      "Iteration [13477]: Loss[0.07146051136369823] has not improved from the previous [0.07145889298306347] for 1 times.\n",
      "Iteration [13479]: Loss[0.07145971204669899] has not improved from the previous [0.07145860123728567] for 1 times.\n",
      "Iteration [13480]: Loss[0.0714615203815927] has not improved from the previous [0.07145971204669899] for 3 times.\n",
      "Iteration [13482]: Loss[0.0714590909511893] has not improved from the previous [0.07145682956379885] for 1 times.\n",
      "Iteration [13483]: Loss[0.07146527053530949] has not improved from the previous [0.0714590909511893] for 3 times.\n",
      "Iteration [13486]: Loss[0.07145905140793507] has not improved from the previous [0.07145525948940838] for 1 times.\n",
      "Iteration [13488]: Loss[0.0714548673293241] has not improved from the previous [0.07145421683761176] for 1 times.\n",
      "Iteration [13493]: Loss[0.07145517012878082] has not improved from the previous [0.07145160850396669] for 1 times.\n",
      "Iteration [13495]: Loss[0.07145095894549974] has not improved from the previous [0.07145037344818107] for 1 times.\n",
      "Iteration [13500]: Loss[0.07145136475015851] has not improved from the previous [0.07144788577049337] for 1 times.\n",
      "Iteration [13502]: Loss[0.07144854100240004] has not improved from the previous [0.07144655202027356] for 1 times.\n",
      "Iteration [13503]: Loss[0.07145481693918478] has not improved from the previous [0.07144854100240004] for 3 times.\n",
      "Iteration [13506]: Loss[0.0714486720846672] has not improved from the previous [0.07144413792259774] for 1 times.\n",
      "Iteration [13508]: Loss[0.07144394473326868] has not improved from the previous [0.07144382207617844] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 13500 Loss 0.07145136475015851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [13511]: Loss[0.07144193768637024] has not improved from the previous [0.07144173996574703] for 1 times.\n",
      "Iteration [13513]: Loss[0.07144471778471669] has not improved from the previous [0.07144074956793985] for 1 times.\n",
      "Iteration [13515]: Loss[0.07144000189607269] has not improved from the previous [0.07143991272657181] for 1 times.\n",
      "Iteration [13518]: Loss[0.07143806757875541] has not improved from the previous [0.07143781404851957] for 1 times.\n",
      "Iteration [13520]: Loss[0.07144085355186172] has not improved from the previous [0.07143725090719552] for 1 times.\n",
      "Iteration [13522]: Loss[0.07143800544730979] has not improved from the previous [0.07143607517842118] for 1 times.\n",
      "Iteration [13523]: Loss[0.07144435704096395] has not improved from the previous [0.07143800544730979] for 3 times.\n",
      "Iteration [13526]: Loss[0.07143817743599087] has not improved from the previous [0.07143353529797791] for 1 times.\n",
      "Iteration [13531]: Loss[0.07143149101774204] has not improved from the previous [0.07143075769802144] for 1 times.\n",
      "Iteration [13533]: Loss[0.07143426145234354] has not improved from the previous [0.0714301802488422] for 1 times.\n",
      "Iteration [13536]: Loss[0.07143768533039972] has not improved from the previous [0.07142926732944879] for 1 times.\n",
      "Iteration [13539]: Loss[0.07143148068657072] has not improved from the previous [0.07142661807292638] for 1 times.\n",
      "Iteration [13544]: Loss[0.07142478877387665] has not improved from the previous [0.07142378561134095] for 1 times.\n",
      "Iteration [13546]: Loss[0.07142756766919989] has not improved from the previous [0.07142338098000958] for 1 times.\n",
      "Iteration [13551]: Loss[0.0714209159443766] has not improved from the previous [0.07141986755850495] for 1 times.\n",
      "Iteration [13553]: Loss[0.07142370884282338] has not improved from the previous [0.0714199594679489] for 1 times.\n",
      "Iteration [13556]: Loss[0.0714271948756648] has not improved from the previous [0.07141890070622696] for 1 times.\n",
      "Iteration [13560]: Loss[0.07141740810444255] has not improved from the previous [0.07141617399882154] for 1 times.\n",
      "Iteration [13562]: Loss[0.07141570739968198] has not improved from the previous [0.07141440782034997] for 1 times.\n",
      "Iteration [13564]: Loss[0.07141446044042325] has not improved from the previous [0.07141264775918278] for 1 times.\n",
      "Iteration [13567]: Loss[0.0714135731668673] has not improved from the previous [0.07141225529836691] for 1 times.\n",
      "Iteration [13569]: Loss[0.07141186635411545] has not improved from the previous [0.07141043001753088] for 1 times.\n",
      "Iteration [13571]: Loss[0.07141060762839535] has not improved from the previous [0.07140875054577753] for 1 times.\n",
      "Iteration [13574]: Loss[0.07140973762568352] has not improved from the previous [0.07140828844871339] for 1 times.\n",
      "Iteration [13576]: Loss[0.07140808924376095] has not improved from the previous [0.07140651080541481] for 1 times.\n",
      "Iteration [13578]: Loss[0.07141090459534645] has not improved from the previous [0.07140501494352958] for 1 times.\n",
      "Iteration [13580]: Loss[0.07140789231960831] has not improved from the previous [0.07140595370768091] for 1 times.\n",
      "Iteration [13581]: Loss[0.07141414663844357] has not improved from the previous [0.07140789231960831] for 3 times.\n",
      "Iteration [13584]: Loss[0.0714030680455345] has not improved from the previous [0.07140253436691846] for 1 times.\n",
      "Iteration [13585]: Loss[0.07140431691908548] has not improved from the previous [0.0714030680455345] for 3 times.\n",
      "Iteration [13587]: Loss[0.0714025450437688] has not improved from the previous [0.07140121983446975] for 1 times.\n",
      "Iteration [13589]: Loss[0.0714012713423392] has not improved from the previous [0.07139946115222688] for 1 times.\n",
      "Iteration [13592]: Loss[0.07140041173305302] has not improved from the previous [0.07139906621302235] for 1 times.\n",
      "Iteration [13594]: Loss[0.07139871998516628] has not improved from the previous [0.07139717332894353] for 1 times.\n",
      "Iteration [13596]: Loss[0.07139742285765316] has not improved from the previous [0.07139546328458223] for 1 times.\n",
      "Iteration [13599]: Loss[0.07140612229102998] has not improved from the previous [0.07139578722850795] for 1 times.\n",
      "Iteration [13602]: Loss[0.07139567867219278] has not improved from the previous [0.07139281486348426] for 1 times.\n",
      "Iteration [13603]: Loss[0.07139638064311898] has not improved from the previous [0.07139567867219278] for 3 times.\n",
      "Iteration [13605]: Loss[0.0713943173978309] has not improved from the previous [0.07139189010165528] for 1 times.\n",
      "Iteration [13606]: Loss[0.07140029014001446] has not improved from the previous [0.0713943173978309] for 3 times.\n",
      "Iteration [13609]: Loss[0.07139420707472877] has not improved from the previous [0.07139100364564815] for 1 times.\n",
      "Iteration [13611]: Loss[0.07138995280303169] has not improved from the previous [0.07138935050024549] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 13600 Loss 0.0713965040054294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [13615]: Loss[0.07138781616985426] has not improved from the previous [0.07138749997146603] for 1 times.\n",
      "Iteration [13616]: Loss[0.07139033809097431] has not improved from the previous [0.07138781616985426] for 3 times.\n",
      "Iteration [13618]: Loss[0.07138578013980043] has not improved from the previous [0.07138554252812686] for 1 times.\n",
      "Iteration [13621]: Loss[0.07138778944195999] has not improved from the previous [0.07138392217345273] for 1 times.\n",
      "Iteration [13623]: Loss[0.07138457440963383] has not improved from the previous [0.07138294562562625] for 1 times.\n",
      "Iteration [13624]: Loss[0.07139113355059219] has not improved from the previous [0.07138457440963383] for 3 times.\n",
      "Iteration [13627]: Loss[0.07138501698315354] has not improved from the previous [0.07138102643681099] for 1 times.\n",
      "Iteration [13629]: Loss[0.07138034369002103] has not improved from the previous [0.07138019006439834] for 1 times.\n",
      "Iteration [13632]: Loss[0.07137827114972284] has not improved from the previous [0.07137801440907596] for 1 times.\n",
      "Iteration [13634]: Loss[0.07138115977197049] has not improved from the previous [0.07137810076376996] for 1 times.\n",
      "Iteration [13639]: Loss[0.07137852672131607] has not improved from the previous [0.0713743784479397] for 1 times.\n",
      "Iteration [13641]: Loss[0.0713749593612939] has not improved from the previous [0.07137371144752612] for 1 times.\n",
      "Iteration [13642]: Loss[0.07138189149731736] has not improved from the previous [0.0713749593612939] for 3 times.\n",
      "Iteration [13645]: Loss[0.07137577568052818] has not improved from the previous [0.07137151778144206] for 1 times.\n",
      "Iteration [13650]: Loss[0.07136902703224164] has not improved from the previous [0.07136838163379298] for 1 times.\n",
      "Iteration [13652]: Loss[0.07137189981554821] has not improved from the previous [0.0713686814662063] for 1 times.\n",
      "Iteration [13657]: Loss[0.07136928939613957] has not improved from the previous [0.07136497896794679] for 1 times.\n",
      "Iteration [13659]: Loss[0.07136524613412198] has not improved from the previous [0.07136445856677452] for 1 times.\n",
      "Iteration [13660]: Loss[0.07137265351045906] has not improved from the previous [0.07136524613412198] for 3 times.\n",
      "Iteration [13663]: Loss[0.07136653997983167] has not improved from the previous [0.07136218140961106] for 1 times.\n",
      "Iteration [13668]: Loss[0.07135979316610579] has not improved from the previous [0.07135873134664851] for 1 times.\n",
      "Iteration [13670]: Loss[0.0713626574811768] has not improved from the previous [0.07135937276925013] for 1 times.\n",
      "Iteration [13673]: Loss[0.0713570230954967] has not improved from the previous [0.07135693046772638] for 1 times.\n",
      "Iteration [13675]: Loss[0.07136006071675784] has not improved from the previous [0.0713556969897176] for 1 times.\n",
      "Iteration [13677]: Loss[0.07135550046666711] has not improved from the previous [0.07135522705334547] for 1 times.\n",
      "Iteration [13678]: Loss[0.07136342639934402] has not improved from the previous [0.07135550046666711] for 3 times.\n",
      "Iteration [13681]: Loss[0.07135731551043846] has not improved from the previous [0.07135292623962596] for 1 times.\n",
      "Iteration [13684]: Loss[0.07135161016618276] has not improved from the previous [0.07135144796481302] for 1 times.\n",
      "Iteration [13686]: Loss[0.0713547101475251] has not improved from the previous [0.07134945008981106] for 1 times.\n",
      "Iteration [13691]: Loss[0.07134778617729444] has not improved from the previous [0.07134684604515117] for 1 times.\n",
      "Iteration [13693]: Loss[0.07135068721118527] has not improved from the previous [0.07134712878816822] for 1 times.\n",
      "Iteration [13696]: Loss[0.07135422129346186] has not improved from the previous [0.0713456342362144] for 1 times.\n",
      "Iteration [13700]: Loss[0.07134438392881273] has not improved from the previous [0.07134292337807684] for 1 times.\n",
      "Iteration [13702]: Loss[0.07134269339990858] has not improved from the previous [0.07134102891845859] for 1 times.\n",
      "Iteration [13704]: Loss[0.07134076998894918] has not improved from the previous [0.071340269797806] for 1 times.\n",
      "Iteration [13705]: Loss[0.07134183047130015] has not improved from the previous [0.07134076998894918] for 3 times.\n",
      "Iteration [13707]: Loss[0.07134001329361658] has not improved from the previous [0.07133875608145578] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 13700 Loss 0.07134438392881273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [13709]: Loss[0.07133869754492599] has not improved from the previous [0.0713369478528296] for 1 times.\n",
      "Iteration [13712]: Loss[0.07133791762917559] has not improved from the previous [0.07133647348411498] for 1 times.\n",
      "Iteration [13714]: Loss[0.07133623651816622] has not improved from the previous [0.0713345763585651] for 1 times.\n",
      "Iteration [13716]: Loss[0.07133433090077784] has not improved from the previous [0.07133414567950798] for 1 times.\n",
      "Iteration [13717]: Loss[0.0713353882514996] has not improved from the previous [0.07133433090077784] for 3 times.\n",
      "Iteration [13719]: Loss[0.07134303485067389] has not improved from the previous [0.07133288706622896] for 1 times.\n",
      "Iteration [13723]: Loss[0.07133244645786188] has not improved from the previous [0.07133096978801003] for 1 times.\n",
      "Iteration [13727]: Loss[0.07133040138552889] has not improved from the previous [0.07132894431012111] for 1 times.\n",
      "Iteration [13728]: Loss[0.07133189667963784] has not improved from the previous [0.07133040138552889] for 3 times.\n",
      "Iteration [13730]: Loss[0.0713295758612288] has not improved from the previous [0.07132705462543154] for 1 times.\n",
      "Iteration [13731]: Loss[0.07133541667188291] has not improved from the previous [0.0713295758612288] for 3 times.\n",
      "Iteration [13733]: Loss[0.07132733186011248] has not improved from the previous [0.07132657419146385] for 1 times.\n",
      "Iteration [13734]: Loss[0.07132932583522889] has not improved from the previous [0.07132733186011248] for 3 times.\n",
      "Iteration [13736]: Loss[0.07132447575764793] has not improved from the previous [0.07132447228719432] for 1 times.\n",
      "Iteration [13738]: Loss[0.07132379356543918] has not improved from the previous [0.07132367738467886] for 1 times.\n",
      "Iteration [13739]: Loss[0.07132675720287297] has not improved from the previous [0.07132379356543918] for 3 times.\n",
      "Iteration [13741]: Loss[0.07132215866008759] has not improved from the previous [0.07132190019774702] for 1 times.\n",
      "Iteration [13744]: Loss[0.07132415546617576] has not improved from the previous [0.07132055932002557] for 1 times.\n",
      "Iteration [13746]: Loss[0.07132098691149087] has not improved from the previous [0.07131927806368847] for 1 times.\n",
      "Iteration [13747]: Loss[0.07132748171412767] has not improved from the previous [0.07132098691149087] for 3 times.\n",
      "Iteration [13750]: Loss[0.07132142322387965] has not improved from the previous [0.07131813024488873] for 1 times.\n",
      "Iteration [13755]: Loss[0.07131876187478331] has not improved from the previous [0.07131495367076793] for 1 times.\n",
      "Iteration [13757]: Loss[0.07131402977771698] has not improved from the previous [0.07131388791692685] for 1 times.\n",
      "Iteration [13760]: Loss[0.07131617314771503] has not improved from the previous [0.07131187860551069] for 1 times.\n",
      "Iteration [13762]: Loss[0.07131242539345557] has not improved from the previous [0.0713112342673109] for 1 times.\n",
      "Iteration [13763]: Loss[0.0713194131641367] has not improved from the previous [0.07131242539345557] for 3 times.\n",
      "Iteration [13766]: Loss[0.07131337088863661] has not improved from the previous [0.07130961049901167] for 1 times.\n",
      "Iteration [13771]: Loss[0.0713107042733753] has not improved from the previous [0.07130652018327788] for 1 times.\n",
      "Iteration [13776]: Loss[0.0713080744318969] has not improved from the previous [0.07130351835405452] for 1 times.\n",
      "Iteration [13778]: Loss[0.0713037504173966] has not improved from the previous [0.07130317841771298] for 1 times.\n",
      "Iteration [13779]: Loss[0.07131133902037363] has not improved from the previous [0.0713037504173966] for 3 times.\n",
      "Iteration [13782]: Loss[0.07130530621694336] has not improved from the previous [0.07130131509802301] for 1 times.\n",
      "Iteration [13787]: Loss[0.07130267239928582] has not improved from the previous [0.0712981835534021] for 1 times.\n",
      "Iteration [13792]: Loss[0.07130003056972914] has not improved from the previous [0.07129526814945629] for 1 times.\n",
      "Iteration [13797]: Loss[0.0712930645845068] has not improved from the previous [0.07129267857226906] for 1 times.\n",
      "Iteration [13798]: Loss[0.07129341380178995] has not improved from the previous [0.0712930645845068] for 3 times.\n",
      "Iteration [13799]: Loss[0.07129604518396666] has not improved from the previous [0.07129341380178995] for 5 times.\n",
      "Iteration [13801]: Loss[0.0712922045471827] has not improved from the previous [0.07129125003613053] for 1 times.\n",
      "Iteration [13802]: Loss[0.0712995065021236] has not improved from the previous [0.0712922045471827] for 3 times.\n",
      "Iteration [13804]: Loss[0.07129075070150151] has not improved from the previous [0.07129074707417743] for 1 times.\n",
      "Iteration [13806]: Loss[0.07128921754010012] has not improved from the previous [0.07128898511223268] for 1 times.\n",
      "Iteration [13808]: Loss[0.07128791520748237] has not improved from the previous [0.0712867377586852] for 1 times.\n",
      "Iteration [13810]: Loss[0.07129087272897994] has not improved from the previous [0.07128737373523579] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 13800 Loss 0.07129125003613053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [13813]: Loss[0.07128515230226101] has not improved from the previous [0.07128482886011707] for 1 times.\n",
      "Iteration [13815]: Loss[0.0712882752402518] has not improved from the previous [0.07128434499058285] for 1 times.\n",
      "Iteration [13818]: Loss[0.07129157842404876] has not improved from the previous [0.07128334447074262] for 1 times.\n",
      "Iteration [13822]: Loss[0.07128189094464599] has not improved from the previous [0.0712803598725776] for 1 times.\n",
      "Iteration [13824]: Loss[0.07128011558878036] has not improved from the previous [0.07127833034754134] for 1 times.\n",
      "Iteration [13827]: Loss[0.07127932932681061] has not improved from the previous [0.07127801297581371] for 1 times.\n",
      "Iteration [13829]: Loss[0.07127752124290031] has not improved from the previous [0.07127596633883468] for 1 times.\n",
      "Iteration [13831]: Loss[0.07127565939291125] has not improved from the previous [0.07127556262881259] for 1 times.\n",
      "Iteration [13832]: Loss[0.07127672423737182] has not improved from the previous [0.07127565939291125] for 3 times.\n",
      "Iteration [13834]: Loss[0.07127489637968375] has not improved from the previous [0.07127354318147891] for 1 times.\n",
      "Iteration [13836]: Loss[0.07127784036644749] has not improved from the previous [0.07127266391363693] for 1 times.\n",
      "Iteration [13838]: Loss[0.07127377117245738] has not improved from the previous [0.07127289779486594] for 1 times.\n",
      "Iteration [13839]: Loss[0.07128110920226279] has not improved from the previous [0.07127377117245738] for 3 times.\n",
      "Iteration [13843]: Loss[0.07127115361344712] has not improved from the previous [0.07126962480626539] for 1 times.\n",
      "Iteration [13845]: Loss[0.07126902913788428] has not improved from the previous [0.0712687353472905] for 1 times.\n",
      "Iteration [13846]: Loss[0.07127194414751971] has not improved from the previous [0.07126902913788428] for 3 times.\n",
      "Iteration [13848]: Loss[0.07126969489421123] has not improved from the previous [0.07126701557016238] for 1 times.\n",
      "Iteration [13849]: Loss[0.07127525815954983] has not improved from the previous [0.07126969489421123] for 3 times.\n",
      "Iteration [13851]: Loss[0.07126701924086734] has not improved from the previous [0.0712663489639166] for 1 times.\n",
      "Iteration [13852]: Loss[0.07126916684097126] has not improved from the previous [0.07126701924086734] for 3 times.\n",
      "Iteration [13854]: Loss[0.07126478032753743] has not improved from the previous [0.07126431958018974] for 1 times.\n",
      "Iteration [13856]: Loss[0.07126391362046616] has not improved from the previous [0.07126345870208593] for 1 times.\n",
      "Iteration [13857]: Loss[0.07126663006790611] has not improved from the previous [0.07126391362046616] for 3 times.\n",
      "Iteration [13859]: Loss[0.0712622533796521] has not improved from the previous [0.07126174235702509] for 1 times.\n",
      "Iteration [13861]: Loss[0.0712611177348535] has not improved from the previous [0.071260855879051] for 1 times.\n",
      "Iteration [13862]: Loss[0.07126403914546539] has not improved from the previous [0.0712611177348535] for 3 times.\n",
      "Iteration [13864]: Loss[0.07126024328439366] has not improved from the previous [0.07125914702407958] for 1 times.\n",
      "Iteration [13865]: Loss[0.07126730489833108] has not improved from the previous [0.07126024328439366] for 3 times.\n",
      "Iteration [13867]: Loss[0.07125898771061681] has not improved from the previous [0.0712584684319732] for 1 times.\n",
      "Iteration [13868]: Loss[0.07126130606609082] has not improved from the previous [0.07125898771061681] for 3 times.\n",
      "Iteration [13872]: Loss[0.07125601458580706] has not improved from the previous [0.07125553486042775] for 1 times.\n",
      "Iteration [13873]: Loss[0.07125871046241132] has not improved from the previous [0.07125601458580706] for 3 times.\n",
      "Iteration [13877]: Loss[0.07125312187667342] has not improved from the previous [0.07125294706699327] for 1 times.\n",
      "Iteration [13878]: Loss[0.0712561318974691] has not improved from the previous [0.07125312187667342] for 3 times.\n",
      "Iteration [13882]: Loss[0.07125042822698964] has not improved from the previous [0.07125034514557699] for 1 times.\n",
      "Iteration [13883]: Loss[0.07125351543484511] has not improved from the previous [0.07125042822698964] for 3 times.\n",
      "Iteration [13885]: Loss[0.07125011027683674] has not improved from the previous [0.07124865301258637] for 1 times.\n",
      "Iteration [13886]: Loss[0.07125681824860901] has not improved from the previous [0.07125011027683674] for 3 times.\n",
      "Iteration [13888]: Loss[0.07124833586782835] has not improved from the previous [0.0712479573996372] for 1 times.\n",
      "Iteration [13889]: Loss[0.07125078144519097] has not improved from the previous [0.07124833586782835] for 3 times.\n",
      "Iteration [13893]: Loss[0.07124535323524456] has not improved from the previous [0.0712450369494913] for 1 times.\n",
      "Iteration [13894]: Loss[0.07124821177252501] has not improved from the previous [0.07124535323524456] for 3 times.\n",
      "Iteration [13898]: Loss[0.07124254295446321] has not improved from the previous [0.0712424584491062] for 1 times.\n",
      "Iteration [13899]: Loss[0.0712456574551082] has not improved from the previous [0.07124254295446321] for 3 times.\n",
      "Iteration [13902]: Loss[0.07124893839728633] has not improved from the previous [0.07124066064070318] for 1 times.\n",
      "Iteration [13906]: Loss[0.07123867839778951] has not improved from the previous [0.07123837089963676] for 1 times.\n",
      "Iteration [13908]: Loss[0.07124177780331413] has not improved from the previous [0.07123635217730373] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 13900 Loss 0.07124075631727261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [13913]: Loss[0.07123891952360373] has not improved from the previous [0.07123439621900746] for 1 times.\n",
      "Iteration [13918]: Loss[0.07123627854471905] has not improved from the previous [0.07123193774053738] for 1 times.\n",
      "Iteration [13920]: Loss[0.07123238898584199] has not improved from the previous [0.07123137131593213] for 1 times.\n",
      "Iteration [13921]: Loss[0.07123950083116548] has not improved from the previous [0.07123238898584199] for 3 times.\n",
      "Iteration [13924]: Loss[0.07123346687348873] has not improved from the previous [0.0712301591346878] for 1 times.\n",
      "Iteration [13927]: Loss[0.0712276770536458] has not improved from the previous [0.07122731645007907] for 1 times.\n",
      "Iteration [13929]: Loss[0.07123087270882023] has not improved from the previous [0.07122735799222879] for 1 times.\n",
      "Iteration [13932]: Loss[0.07122508568869439] has not improved from the previous [0.07122481509524423] for 1 times.\n",
      "Iteration [13934]: Loss[0.07122830441751217] has not improved from the previous [0.07122469029187968] for 1 times.\n",
      "Iteration [13937]: Loss[0.0712316425257311] has not improved from the previous [0.07122274304105285] for 1 times.\n",
      "Iteration [13941]: Loss[0.07122134471719899] has not improved from the previous [0.07122046972344062] for 1 times.\n",
      "Iteration [13943]: Loss[0.07122440137311474] has not improved from the previous [0.07121859886122342] for 1 times.\n",
      "Iteration [13946]: Loss[0.0712183184696043] has not improved from the previous [0.07121822501817547] for 1 times.\n",
      "Iteration [13948]: Loss[0.0712215470053488] has not improved from the previous [0.07121683871666228] for 1 times.\n",
      "Iteration [13951]: Loss[0.07121570180570404] has not improved from the previous [0.07121566351433321] for 1 times.\n",
      "Iteration [13953]: Loss[0.0712189211176713] has not improved from the previous [0.07121443706884904] for 1 times.\n",
      "Iteration [13955]: Loss[0.07121431314856626] has not improved from the previous [0.07121399885180232] for 1 times.\n",
      "Iteration [13956]: Loss[0.0712221300204165] has not improved from the previous [0.07121431314856626] for 3 times.\n",
      "Iteration [13958]: Loss[0.07121328764212949] has not improved from the previous [0.0712132563078991] for 1 times.\n",
      "Iteration [13960]: Loss[0.07121245762574373] has not improved from the previous [0.07121081757208662] for 1 times.\n",
      "Iteration [13962]: Loss[0.0712126692807777] has not improved from the previous [0.07120873769000455] for 1 times.\n",
      "Iteration [13965]: Loss[0.07120970083081463] has not improved from the previous [0.07120804298700491] for 1 times.\n",
      "Iteration [13967]: Loss[0.07121148258127563] has not improved from the previous [0.07120605882855188] for 1 times.\n",
      "Iteration [13968]: Loss[0.07122026805618167] has not improved from the previous [0.07121148258127563] for 3 times.\n",
      "Iteration [13971]: Loss[0.07120710418573135] has not improved from the previous [0.07120489042945811] for 1 times.\n",
      "Iteration [13972]: Loss[0.07120870626604156] has not improved from the previous [0.07120710418573135] for 3 times.\n",
      "Iteration [13974]: Loss[0.07120537517056728] has not improved from the previous [0.07120398717125212] for 1 times.\n",
      "Iteration [13976]: Loss[0.07120382917322401] has not improved from the previous [0.07120306882292271] for 1 times.\n",
      "Iteration [13977]: Loss[0.07120634892916536] has not improved from the previous [0.07120382917322401] for 3 times.\n",
      "Iteration [13979]: Loss[0.07120259375720676] has not improved from the previous [0.07120141973377998] for 1 times.\n",
      "Iteration [13981]: Loss[0.07120134695307931] has not improved from the previous [0.07120053146177785] for 1 times.\n",
      "Iteration [13982]: Loss[0.07120380327051526] has not improved from the previous [0.07120134695307931] for 3 times.\n",
      "Iteration [13984]: Loss[0.0712007810915759] has not improved from the previous [0.07119886548043067] for 1 times.\n",
      "Iteration [13985]: Loss[0.07120710323488129] has not improved from the previous [0.0712007810915759] for 3 times.\n",
      "Iteration [13987]: Loss[0.07119949669726808] has not improved from the previous [0.07119817640696476] for 1 times.\n",
      "Iteration [13988]: Loss[0.07120110001850219] has not improved from the previous [0.07119949669726808] for 3 times.\n",
      "Iteration [13992]: Loss[0.07119678696902375] has not improved from the previous [0.07119530196513031] for 1 times.\n",
      "Iteration [13993]: Loss[0.07119851200407] has not improved from the previous [0.07119678696902375] for 3 times.\n",
      "Iteration [13997]: Loss[0.07119412852998651] has not improved from the previous [0.07119273180133248] for 1 times.\n",
      "Iteration [13998]: Loss[0.07119596956202762] has not improved from the previous [0.07119412852998651] for 3 times.\n",
      "Iteration [14002]: Loss[0.07119144262509179] has not improved from the previous [0.07119023824648547] for 1 times.\n",
      "Iteration [14003]: Loss[0.07119349890023473] has not improved from the previous [0.07119144262509179] for 3 times.\n",
      "Iteration [14005]: Loss[0.07118934750795845] has not improved from the previous [0.07118857255388701] for 1 times.\n",
      "Iteration [14006]: Loss[0.0711967950285733] has not improved from the previous [0.07118934750795845] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14000 Loss 0.07119081539013189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [14010]: Loss[0.07118654320626373] has not improved from the previous [0.07118631047316441] for 1 times.\n",
      "Iteration [14012]: Loss[0.07118966670261216] has not improved from the previous [0.07118551792210637] for 1 times.\n",
      "Iteration [14016]: Loss[0.07118382989040951] has not improved from the previous [0.0711835829655192] for 1 times.\n",
      "Iteration [14017]: Loss[0.07118683428291903] has not improved from the previous [0.07118382989040951] for 3 times.\n",
      "Iteration [14020]: Loss[0.07119014911857606] has not improved from the previous [0.07118147086383239] for 1 times.\n",
      "Iteration [14024]: Loss[0.0711798393453528] has not improved from the previous [0.07117940868581112] for 1 times.\n",
      "Iteration [14026]: Loss[0.07118295155929753] has not improved from the previous [0.07117818477565907] for 1 times.\n",
      "Iteration [14031]: Loss[0.07118018571955267] has not improved from the previous [0.07117644829177336] for 1 times.\n",
      "Iteration [14035]: Loss[0.07117430813214445] has not improved from the previous [0.0711742325465804] for 1 times.\n",
      "Iteration [14036]: Loss[0.07117750869434857] has not improved from the previous [0.07117430813214445] for 3 times.\n",
      "Iteration [14039]: Loss[0.07118076624886256] has not improved from the previous [0.07117237919771381] for 1 times.\n",
      "Iteration [14043]: Loss[0.07117049311473983] has not improved from the previous [0.07116980018861056] for 1 times.\n",
      "Iteration [14045]: Loss[0.07117367400034262] has not improved from the previous [0.07116874444374496] for 1 times.\n",
      "Iteration [14048]: Loss[0.07116759473113066] has not improved from the previous [0.07116737235129546] for 1 times.\n",
      "Iteration [14050]: Loss[0.07117084826165994] has not improved from the previous [0.07116705016092872] for 1 times.\n",
      "Iteration [14053]: Loss[0.07116494947657546] has not improved from the previous [0.07116472558315894] for 1 times.\n",
      "Iteration [14055]: Loss[0.07116821782862597] has not improved from the previous [0.07116483185735019] for 1 times.\n",
      "Iteration [14058]: Loss[0.07117147724048566] has not improved from the previous [0.07116308784776829] for 1 times.\n",
      "Iteration [14062]: Loss[0.07116120042174111] has not improved from the previous [0.07116011147359934] for 1 times.\n",
      "Iteration [14065]: Loss[0.0711605637646129] has not improved from the previous [0.07115939768444726] for 1 times.\n",
      "Iteration [14067]: Loss[0.07115862986140788] has not improved from the previous [0.071157013230517] for 1 times.\n",
      "Iteration [14070]: Loss[0.07115791454940186] has not improved from the previous [0.07115665041581663] for 1 times.\n",
      "Iteration [14072]: Loss[0.07115597726997212] has not improved from the previous [0.07115431324728193] for 1 times.\n",
      "Iteration [14075]: Loss[0.07115529625040024] has not improved from the previous [0.07115392406725811] for 1 times.\n",
      "Iteration [14077]: Loss[0.07115339973755656] has not improved from the previous [0.0711516437027783] for 1 times.\n",
      "Iteration [14080]: Loss[0.07116233674869275] has not improved from the previous [0.07115195133854209] for 1 times.\n",
      "Iteration [14084]: Loss[0.07115126608950426] has not improved from the previous [0.07114984447056322] for 1 times.\n",
      "Iteration [14085]: Loss[0.07115784495316133] has not improved from the previous [0.07115126608950426] for 3 times.\n",
      "Iteration [14087]: Loss[0.07115024673311156] has not improved from the previous [0.0711488251577694] for 1 times.\n",
      "Iteration [14088]: Loss[0.07115180154572505] has not improved from the previous [0.07115024673311156] for 3 times.\n",
      "Iteration [14090]: Loss[0.071146979156423] has not improved from the previous [0.07114683028452272] for 1 times.\n",
      "Iteration [14091]: Loss[0.07115057712251978] has not improved from the previous [0.071146979156423] for 3 times.\n",
      "Iteration [14093]: Loss[0.07114632517685067] has not improved from the previous [0.07114564309540518] for 1 times.\n",
      "Iteration [14095]: Loss[0.07114495086815746] has not improved from the previous [0.07114455544125156] for 1 times.\n",
      "Iteration [14096]: Loss[0.07114787789391004] has not improved from the previous [0.07114495086815746] for 3 times.\n",
      "Iteration [14098]: Loss[0.07114339501732603] has not improved from the previous [0.07114290014734746] for 1 times.\n",
      "Iteration [14100]: Loss[0.07114298266030666] has not improved from the previous [0.07114193192197783] for 1 times.\n",
      "Iteration [14101]: Loss[0.07114521695995586] has not improved from the previous [0.07114298266030666] for 3 times.\n",
      "Iteration [14103]: Loss[0.07114116429432789] has not improved from the previous [0.0711403043208293] for 1 times.\n",
      "Iteration [14104]: Loss[0.07114849808681543] has not improved from the previous [0.07114116429432789] for 3 times.\n",
      "Iteration [14110]: Loss[0.07114143285620005] has not improved from the previous [0.07113767445645122] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14100 Loss 0.07114298266030666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [14114]: Loss[0.0711360539898606] has not improved from the previous [0.07113532274331955] for 1 times.\n",
      "Iteration [14115]: Loss[0.07113862704495051] has not improved from the previous [0.0711360539898606] for 3 times.\n",
      "Iteration [14119]: Loss[0.07113397225829401] has not improved from the previous [0.07113272029810924] for 1 times.\n",
      "Iteration [14120]: Loss[0.07113601344543512] has not improved from the previous [0.07113397225829401] for 3 times.\n",
      "Iteration [14122]: Loss[0.07113133451956269] has not improved from the previous [0.07113110356973401] for 1 times.\n",
      "Iteration [14123]: Loss[0.07114540816387342] has not improved from the previous [0.07113133451956269] for 3 times.\n",
      "Iteration [14127]: Loss[0.07113311761851791] has not improved from the previous [0.07112913315392734] for 1 times.\n",
      "Iteration [14132]: Loss[0.07113055545114] has not improved from the previous [0.0711268823525931] for 1 times.\n",
      "Iteration [14136]: Loss[0.07112499937709162] has not improved from the previous [0.07112456757445525] for 1 times.\n",
      "Iteration [14137]: Loss[0.07112789400519473] has not improved from the previous [0.07112499937709162] for 3 times.\n",
      "Iteration [14139]: Loss[0.07112336647985408] has not improved from the previous [0.0711229509045193] for 1 times.\n",
      "Iteration [14140]: Loss[0.07113107779726273] has not improved from the previous [0.07112336647985408] for 3 times.\n",
      "Iteration [14144]: Loss[0.07112084778571111] has not improved from the previous [0.07112048198302028] for 1 times.\n",
      "Iteration [14146]: Loss[0.07112406758982764] has not improved from the previous [0.0711198997337851] for 1 times.\n",
      "Iteration [14149]: Loss[0.07111796103990174] has not improved from the previous [0.0711179280646609] for 1 times.\n",
      "Iteration [14150]: Loss[0.07111830702473258] has not improved from the previous [0.07111796103990174] for 3 times.\n",
      "Iteration [14151]: Loss[0.0711212639037211] has not improved from the previous [0.07111830702473258] for 5 times.\n",
      "Iteration [14154]: Loss[0.07112006394008814] has not improved from the previous [0.07111519475812879] for 1 times.\n",
      "Iteration [14156]: Loss[0.07111574283985006] has not improved from the previous [0.07111511230054568] for 1 times.\n",
      "Iteration [14157]: Loss[0.0711231360162572] has not improved from the previous [0.07111574283985006] for 3 times.\n",
      "Iteration [14159]: Loss[0.07111453547590815] has not improved from the previous [0.07111414688944692] for 1 times.\n",
      "Iteration [14161]: Loss[0.07111279912497566] has not improved from the previous [0.07111259833250226] for 1 times.\n",
      "Iteration [14163]: Loss[0.07111606934671363] has not improved from the previous [0.07111078975706765] for 1 times.\n",
      "Iteration [14168]: Loss[0.0711131292367692] has not improved from the previous [0.07110956960746961] for 1 times.\n",
      "Iteration [14171]: Loss[0.07110719282584464] has not improved from the previous [0.07110711511932753] for 1 times.\n",
      "Iteration [14172]: Loss[0.07110757863591377] has not improved from the previous [0.07110719282584464] for 3 times.\n",
      "Iteration [14173]: Loss[0.07111050822542056] has not improved from the previous [0.07110757863591377] for 5 times.\n",
      "Iteration [14176]: Loss[0.07111984501771663] has not improved from the previous [0.0711054223559512] for 1 times.\n",
      "Iteration [14180]: Loss[0.07110755800461062] has not improved from the previous [0.07110309326568161] for 1 times.\n",
      "Iteration [14185]: Loss[0.07110498389278319] has not improved from the previous [0.07110098582886935] for 1 times.\n",
      "Iteration [14189]: Loss[0.07109908830290988] has not improved from the previous [0.07109901655199549] for 1 times.\n",
      "Iteration [14190]: Loss[0.07110234702061544] has not improved from the previous [0.07109908830290988] for 3 times.\n",
      "Iteration [14193]: Loss[0.07110563714880946] has not improved from the previous [0.07109721031508179] for 1 times.\n",
      "Iteration [14197]: Loss[0.07109532580186859] has not improved from the previous [0.0710940329312958] for 1 times.\n",
      "Iteration [14200]: Loss[0.07109473489443176] has not improved from the previous [0.07109325925008238] for 1 times.\n",
      "Iteration [14202]: Loss[0.07109576184980416] has not improved from the previous [0.07109091743487066] for 1 times.\n",
      "Iteration [14203]: Loss[0.07110531677974476] has not improved from the previous [0.07109576184980416] for 3 times.\n",
      "Iteration [14206]: Loss[0.07109274356175002] has not improved from the previous [0.07108970824892266] for 1 times.\n",
      "Iteration [14207]: Loss[0.0710935846484058] has not improved from the previous [0.07109274356175002] for 3 times.\n",
      "Iteration [14209]: Loss[0.07108972406747197] has not improved from the previous [0.0710888591116308] for 1 times.\n",
      "Iteration [14211]: Loss[0.07108996230847056] has not improved from the previous [0.07108786543559949] for 1 times.\n",
      "Iteration [14212]: Loss[0.07109123191604734] has not improved from the previous [0.07108996230847056] for 3 times.\n",
      "Iteration [14214]: Loss[0.07108685895428755] has not improved from the previous [0.07108628680543345] for 1 times.\n",
      "Iteration [14215]: Loss[0.07109013023124403] has not improved from the previous [0.07108685895428755] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14200 Loss 0.07109473489443176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [14217]: Loss[0.071086167875481] has not improved from the previous [0.07108515784443466] for 1 times.\n",
      "Iteration [14218]: Loss[0.07109315025630872] has not improved from the previous [0.071086167875481] for 3 times.\n",
      "Iteration [14220]: Loss[0.07108457005879613] has not improved from the previous [0.0710845142850394] for 1 times.\n",
      "Iteration [14224]: Loss[0.07108613275355605] has not improved from the previous [0.071082567063156] for 1 times.\n",
      "Iteration [14226]: Loss[0.07108118709568352] has not improved from the previous [0.07108104470701793] for 1 times.\n",
      "Iteration [14228]: Loss[0.071081237350988] has not improved from the previous [0.07107993740020184] for 1 times.\n",
      "Iteration [14229]: Loss[0.07108329248619896] has not improved from the previous [0.071081237350988] for 3 times.\n",
      "Iteration [14232]: Loss[0.07108210345318478] has not improved from the previous [0.07107826238535349] for 1 times.\n",
      "Iteration [14234]: Loss[0.07107770731775254] has not improved from the previous [0.0710771308903636] for 1 times.\n",
      "Iteration [14235]: Loss[0.07108512706069642] has not improved from the previous [0.07107770731775254] for 3 times.\n",
      "Iteration [14237]: Loss[0.07107656814953703] has not improved from the previous [0.07107643447452493] for 1 times.\n",
      "Iteration [14241]: Loss[0.07107813444490349] has not improved from the previous [0.0710741370428147] for 1 times.\n",
      "Iteration [14245]: Loss[0.07107289975668361] has not improved from the previous [0.07107191065016427] for 1 times.\n",
      "Iteration [14246]: Loss[0.07107524939545985] has not improved from the previous [0.07107289975668361] for 3 times.\n",
      "Iteration [14249]: Loss[0.0710740751021247] has not improved from the previous [0.07106990408745233] for 1 times.\n",
      "Iteration [14251]: Loss[0.0710692868773118] has not improved from the previous [0.07106911117343535] for 1 times.\n",
      "Iteration [14253]: Loss[0.07106847843519963] has not improved from the previous [0.07106792970270984] for 1 times.\n",
      "Iteration [14254]: Loss[0.07107132609500001] has not improved from the previous [0.07106847843519963] for 3 times.\n",
      "Iteration [14256]: Loss[0.07106732453439775] has not improved from the previous [0.07106633539239661] for 1 times.\n",
      "Iteration [14257]: Loss[0.07107452633115961] has not improved from the previous [0.07106732453439775] for 3 times.\n",
      "Iteration [14261]: Loss[0.07106420594851999] has not improved from the previous [0.071064026137454] for 1 times.\n",
      "Iteration [14263]: Loss[0.07106748359701294] has not improved from the previous [0.07106392067948779] for 1 times.\n",
      "Iteration [14266]: Loss[0.0710662052808459] has not improved from the previous [0.07106131349002053] for 1 times.\n",
      "Iteration [14270]: Loss[0.0710601615484282] has not improved from the previous [0.07105992929591426] for 1 times.\n",
      "Iteration [14271]: Loss[0.07106333270750106] has not improved from the previous [0.0710601615484282] for 3 times.\n",
      "Iteration [14273]: Loss[0.0710586299398019] has not improved from the previous [0.07105833143135386] for 1 times.\n",
      "Iteration [14274]: Loss[0.07106649715682434] has not improved from the previous [0.0710586299398019] for 3 times.\n",
      "Iteration [14278]: Loss[0.07105617614159708] has not improved from the previous [0.07105553745954483] for 1 times.\n",
      "Iteration [14280]: Loss[0.0710594528159387] has not improved from the previous [0.0710557151746141] for 1 times.\n",
      "Iteration [14283]: Loss[0.07105814682212118] has not improved from the previous [0.07105314063188635] for 1 times.\n",
      "Iteration [14287]: Loss[0.0710519757306363] has not improved from the previous [0.07105192926016146] for 1 times.\n",
      "Iteration [14288]: Loss[0.0710552992575823] has not improved from the previous [0.0710519757306363] for 3 times.\n",
      "Iteration [14291]: Loss[0.07105844936737123] has not improved from the previous [0.0710499277181728] for 1 times.\n",
      "Iteration [14295]: Loss[0.07104817040519312] has not improved from the previous [0.07104702368706788] for 1 times.\n",
      "Iteration [14298]: Loss[0.07104761585503204] has not improved from the previous [0.07104620004209873] for 1 times.\n",
      "Iteration [14300]: Loss[0.07104580243076455] has not improved from the previous [0.07104382354931701] for 1 times.\n",
      "Iteration [14301]: Loss[0.07104653061711737] has not improved from the previous [0.07104580243076455] for 3 times.\n",
      "Iteration [14303]: Loss[0.07104431328181085] has not improved from the previous [0.07104309268895433] for 1 times.\n",
      "Iteration [14306]: Loss[0.0710436369063755] has not improved from the previous [0.07104259746571066] for 1 times.\n",
      "Iteration [14308]: Loss[0.07104161508528052] has not improved from the previous [0.07103999805831529] for 1 times.\n",
      "Iteration [14309]: Loss[0.07104229545741648] has not improved from the previous [0.07104161508528052] for 3 times.\n",
      "Iteration [14311]: Loss[0.07104099182245512] has not improved from the previous [0.07103951919756984] for 1 times.\n",
      "Iteration [14313]: Loss[0.07104607940645773] has not improved from the previous [0.07103808656058608] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14300 Loss 0.07104580243076455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [14317]: Loss[0.07103953051145435] has not improved from the previous [0.07103699009378389] for 1 times.\n",
      "Iteration [14318]: Loss[0.07104026934445719] has not improved from the previous [0.07103953051145435] for 3 times.\n",
      "Iteration [14320]: Loss[0.07103692654975413] has not improved from the previous [0.0710352672272325] for 1 times.\n",
      "Iteration [14321]: Loss[0.07104972601643034] has not improved from the previous [0.07103692654975413] for 3 times.\n",
      "Iteration [14324]: Loss[0.0710355237834576] has not improved from the previous [0.07103348666424766] for 1 times.\n",
      "Iteration [14325]: Loss[0.0710374287503692] has not improved from the previous [0.0710355237834576] for 3 times.\n",
      "Iteration [14327]: Loss[0.07103302421809869] has not improved from the previous [0.07103258316547428] for 1 times.\n",
      "Iteration [14329]: Loss[0.07103347157401592] has not improved from the previous [0.07103145574695563] for 1 times.\n",
      "Iteration [14330]: Loss[0.07103485338212334] has not improved from the previous [0.07103347157401592] for 3 times.\n",
      "Iteration [14332]: Loss[0.07103056220426222] has not improved from the previous [0.07102991852317495] for 1 times.\n",
      "Iteration [14333]: Loss[0.07103377295426] has not improved from the previous [0.07103056220426222] for 3 times.\n",
      "Iteration [14335]: Loss[0.07102947195620972] has not improved from the previous [0.07102878057264889] for 1 times.\n",
      "Iteration [14336]: Loss[0.07103674395465254] has not improved from the previous [0.07102947195620972] for 3 times.\n",
      "Iteration [14341]: Loss[0.07102666188659734] has not improved from the previous [0.07102642831468527] for 1 times.\n",
      "Iteration [14342]: Loss[0.07102974226417699] has not improved from the previous [0.07102666188659734] for 3 times.\n",
      "Iteration [14345]: Loss[0.07102844088385457] has not improved from the previous [0.07102422511555795] for 1 times.\n",
      "Iteration [14347]: Loss[0.0710235094850221] has not improved from the previous [0.07102340805823275] for 1 times.\n",
      "Iteration [14349]: Loss[0.071023202932848] has not improved from the previous [0.07102216426207106] for 1 times.\n",
      "Iteration [14350]: Loss[0.07102557044944445] has not improved from the previous [0.071023202932848] for 3 times.\n",
      "Iteration [14353]: Loss[0.07102445173493474] has not improved from the previous [0.07102046544431072] for 1 times.\n",
      "Iteration [14355]: Loss[0.0710210543512606] has not improved from the previous [0.07101943749585667] for 1 times.\n",
      "Iteration [14356]: Loss[0.0710273796892555] has not improved from the previous [0.0710210543512606] for 3 times.\n",
      "Iteration [14362]: Loss[0.07102035752509395] has not improved from the previous [0.0710167950482281] for 1 times.\n",
      "Iteration [14366]: Loss[0.07101498625658863] has not improved from the previous [0.0710143021918048] for 1 times.\n",
      "Iteration [14370]: Loss[0.07101650822743688] has not improved from the previous [0.07101257394400705] for 1 times.\n",
      "Iteration [14373]: Loss[0.07101934906582495] has not improved from the previous [0.0710113363379713] for 1 times.\n",
      "Iteration [14377]: Loss[0.07100907166337882] has not improved from the previous [0.07100840344520265] for 1 times.\n",
      "Iteration [14378]: Loss[0.07100915335907117] has not improved from the previous [0.07100907166337882] for 3 times.\n",
      "Iteration [14380]: Loss[0.07100852337384385] has not improved from the previous [0.07100754052215152] for 1 times.\n",
      "Iteration [14382]: Loss[0.07101155051879732] has not improved from the previous [0.07100536539067043] for 1 times.\n",
      "Iteration [14386]: Loss[0.0710053920516361] has not improved from the previous [0.0710049271627693] for 1 times.\n",
      "Iteration [14387]: Loss[0.07100832094177896] has not improved from the previous [0.0710053920516361] for 3 times.\n",
      "Iteration [14390]: Loss[0.07101773956865406] has not improved from the previous [0.07100298849021507] for 1 times.\n",
      "Iteration [14393]: Loss[0.07100180011895753] has not improved from the previous [0.07100145224636663] for 1 times.\n",
      "Iteration [14394]: Loss[0.07100534279248794] has not improved from the previous [0.07100180011895753] for 3 times.\n",
      "Iteration [14397]: Loss[0.07099935897397762] has not improved from the previous [0.07099914920474877] for 1 times.\n",
      "Iteration [14398]: Loss[0.07100000284313769] has not improved from the previous [0.07099935897397762] for 3 times.\n",
      "Iteration [14400]: Loss[0.07099901609116857] has not improved from the previous [0.07099791883366327] for 1 times.\n",
      "Iteration [14402]: Loss[0.07100208935139693] has not improved from the previous [0.07099589543347265] for 1 times.\n",
      "Iteration [14405]: Loss[0.07099544741278473] has not improved from the previous [0.07099528602981993] for 1 times.\n",
      "Iteration [14406]: Loss[0.07099594729770377] has not improved from the previous [0.07099544741278473] for 3 times.\n",
      "Iteration [14407]: Loss[0.07099885767651186] has not improved from the previous [0.07099594729770377] for 5 times.\n",
      "Iteration [14409]: Loss[0.07099456037970062] has not improved from the previous [0.07099384180882229] for 1 times.\n",
      "Iteration [14410]: Loss[0.07100831078987968] has not improved from the previous [0.07099456037970062] for 3 times.\n",
      "Iteration [14413]: Loss[0.0709923593994155] has not improved from the previous [0.07099200746782723] for 1 times.\n",
      "Iteration [14414]: Loss[0.0709959112703101] has not improved from the previous [0.0709923593994155] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14400 Loss 0.07099901609116857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [14417]: Loss[0.07098994017465465] has not improved from the previous [0.07098949079789448] for 1 times.\n",
      "Iteration [14418]: Loss[0.07099081560440727] has not improved from the previous [0.07098994017465465] for 3 times.\n",
      "Iteration [14420]: Loss[0.07098896433916395] has not improved from the previous [0.07098863085560785] for 1 times.\n",
      "Iteration [14422]: Loss[0.07099230496968811] has not improved from the previous [0.07098753116407557] for 1 times.\n",
      "Iteration [14425]: Loss[0.07099518777017245] has not improved from the previous [0.07098641739175626] for 1 times.\n",
      "Iteration [14429]: Loss[0.07098484209792376] has not improved from the previous [0.07098321011093327] for 1 times.\n",
      "Iteration [14430]: Loss[0.07098531005322407] has not improved from the previous [0.07098484209792376] for 3 times.\n",
      "Iteration [14432]: Loss[0.07098364408490414] has not improved from the previous [0.0709828235102793] for 1 times.\n",
      "Iteration [14434]: Loss[0.07098435867732582] has not improved from the previous [0.07098157466682159] for 1 times.\n",
      "Iteration [14435]: Loss[0.07098506068190638] has not improved from the previous [0.07098435867732582] for 3 times.\n",
      "Iteration [14437]: Loss[0.07098248342755044] has not improved from the previous [0.0709799915554013] for 1 times.\n",
      "Iteration [14438]: Loss[0.07099451787940954] has not improved from the previous [0.07098248342755044] for 3 times.\n",
      "Iteration [14441]: Loss[0.07098080649913889] has not improved from the previous [0.07097817873463227] for 1 times.\n",
      "Iteration [14442]: Loss[0.07098216006347385] has not improved from the previous [0.07098080649913889] for 3 times.\n",
      "Iteration [14444]: Loss[0.07097758301843315] has not improved from the previous [0.07097730596047228] for 1 times.\n",
      "Iteration [14445]: Loss[0.07098125478059142] has not improved from the previous [0.07097758301843315] for 3 times.\n",
      "Iteration [14447]: Loss[0.07097668870172061] has not improved from the previous [0.070976197704582] for 1 times.\n",
      "Iteration [14449]: Loss[0.07097654192018496] has not improved from the previous [0.07097487821147015] for 1 times.\n",
      "Iteration [14450]: Loss[0.0709783474900501] has not improved from the previous [0.07097654192018496] for 3 times.\n",
      "Iteration [14452]: Loss[0.07097419479590783] has not improved from the previous [0.07097331041387746] for 1 times.\n",
      "Iteration [14453]: Loss[0.07098782109885741] has not improved from the previous [0.07097419479590783] for 3 times.\n",
      "Iteration [14456]: Loss[0.07097309593759138] has not improved from the previous [0.07097147015588606] for 1 times.\n",
      "Iteration [14457]: Loss[0.07097540409151087] has not improved from the previous [0.07097309593759138] for 3 times.\n",
      "Iteration [14461]: Loss[0.07097136605338139] has not improved from the previous [0.07096938186892081] for 1 times.\n",
      "Iteration [14462]: Loss[0.07097284073981659] has not improved from the previous [0.07097136605338139] for 3 times.\n",
      "Iteration [14464]: Loss[0.0709686494136136] has not improved from the previous [0.07096784410052547] for 1 times.\n",
      "Iteration [14465]: Loss[0.07097176559828576] has not improved from the previous [0.0709686494136136] for 3 times.\n",
      "Iteration [14469]: Loss[0.07096749173754689] has not improved from the previous [0.07096548454472514] for 1 times.\n",
      "Iteration [14470]: Loss[0.0709689446381533] has not improved from the previous [0.07096749173754689] for 3 times.\n",
      "Iteration [14472]: Loss[0.07096572303637823] has not improved from the previous [0.07096395569998115] for 1 times.\n",
      "Iteration [14473]: Loss[0.07097844794047566] has not improved from the previous [0.07096572303637823] for 3 times.\n",
      "Iteration [14476]: Loss[0.07096384244566416] has not improved from the previous [0.07096217235150355] for 1 times.\n",
      "Iteration [14477]: Loss[0.07096606872058964] has not improved from the previous [0.07096384244566416] for 3 times.\n",
      "Iteration [14480]: Loss[0.0709651091359517] has not improved from the previous [0.07096079191832479] for 1 times.\n",
      "Iteration [14484]: Loss[0.0709598484009044] has not improved from the previous [0.07095877696196198] for 1 times.\n",
      "Iteration [14485]: Loss[0.07096225781262608] has not improved from the previous [0.0709598484009044] for 3 times.\n",
      "Iteration [14487]: Loss[0.07095766280805024] has not improved from the previous [0.07095720560623373] for 1 times.\n",
      "Iteration [14488]: Loss[0.07097172668945168] has not improved from the previous [0.07095766280805024] for 3 times.\n",
      "Iteration [14491]: Loss[0.07095645558754897] has not improved from the previous [0.07095536370822657] for 1 times.\n",
      "Iteration [14492]: Loss[0.07095932388339694] has not improved from the previous [0.07095645558754897] for 3 times.\n",
      "Iteration [14495]: Loss[0.07095832343687081] has not improved from the previous [0.07095346633512865] for 1 times.\n",
      "Iteration [14499]: Loss[0.07095264726808428] has not improved from the previous [0.07095191710998411] for 1 times.\n",
      "Iteration [14500]: Loss[0.07095539304275554] has not improved from the previous [0.07095264726808428] for 3 times.\n",
      "Iteration [14503]: Loss[0.070954276813863] has not improved from the previous [0.07095016032849942] for 1 times.\n",
      "Iteration [14505]: Loss[0.07094988040027837] has not improved from the previous [0.0709492311422534] for 1 times.\n",
      "Iteration [14506]: Loss[0.07095716232639497] has not improved from the previous [0.07094988040027837] for 3 times.\n",
      "Iteration [14510]: Loss[0.07094679654758558] has not improved from the previous [0.07094614410536418] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14500 Loss 0.07095539304275554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [14511]: Loss[0.0709470418240827] has not improved from the previous [0.07094679654758558] for 3 times.\n",
      "Iteration [14513]: Loss[0.07094627045974934] has not improved from the previous [0.07094516785115947] for 1 times.\n",
      "Iteration [14515]: Loss[0.07094461668162051] has not improved from the previous [0.07094336743941927] for 1 times.\n",
      "Iteration [14516]: Loss[0.07094524960359024] has not improved from the previous [0.07094461668162051] for 3 times.\n",
      "Iteration [14518]: Loss[0.0709429577131525] has not improved from the previous [0.07094166081663396] for 1 times.\n",
      "Iteration [14519]: Loss[0.0709436470353344] has not improved from the previous [0.0709429577131525] for 3 times.\n",
      "Iteration [14521]: Loss[0.07094233615402308] has not improved from the previous [0.07094106187778101] for 1 times.\n",
      "Iteration [14523]: Loss[0.07094052237932494] has not improved from the previous [0.07093990508144747] for 1 times.\n",
      "Iteration [14524]: Loss[0.07094136804064476] has not improved from the previous [0.07094052237932494] for 3 times.\n",
      "Iteration [14526]: Loss[0.07093917599099353] has not improved from the previous [0.07093761221141677] for 1 times.\n",
      "Iteration [14527]: Loss[0.07093992459113906] has not improved from the previous [0.07093917599099353] for 3 times.\n",
      "Iteration [14529]: Loss[0.07094721992356473] has not improved from the previous [0.07093962031391192] for 1 times.\n",
      "Iteration [14533]: Loss[0.07093631852078772] has not improved from the previous [0.0709351660782914] for 1 times.\n",
      "Iteration [14536]: Loss[0.07093703716835875] has not improved from the previous [0.07093435648685095] for 1 times.\n",
      "Iteration [14537]: Loss[0.07094222882048987] has not improved from the previous [0.07093703716835875] for 3 times.\n",
      "Iteration [14542]: Loss[0.07093375230629995] has not improved from the previous [0.0709318702330574] for 1 times.\n",
      "Iteration [14543]: Loss[0.07093524541900752] has not improved from the previous [0.07093375230629995] for 3 times.\n",
      "Iteration [14545]: Loss[0.0709315892419829] has not improved from the previous [0.0709300897634884] for 1 times.\n",
      "Iteration [14546]: Loss[0.07093400529740121] has not improved from the previous [0.0709315892419829] for 3 times.\n",
      "Iteration [14548]: Loss[0.07092917621085264] has not improved from the previous [0.0709289357259288] for 1 times.\n",
      "Iteration [14549]: Loss[0.07093285081160336] has not improved from the previous [0.07092917621085264] for 3 times.\n",
      "Iteration [14551]: Loss[0.07092886304245462] has not improved from the previous [0.07092774942837952] for 1 times.\n",
      "Iteration [14552]: Loss[0.07093565137191105] has not improved from the previous [0.07092886304245462] for 3 times.\n",
      "Iteration [14557]: Loss[0.07092630963416383] has not improved from the previous [0.07092522434009045] for 1 times.\n",
      "Iteration [14558]: Loss[0.07092860324190103] has not improved from the previous [0.07092630963416383] for 3 times.\n",
      "Iteration [14560]: Loss[0.07092414410102903] has not improved from the previous [0.07092347153969464] for 1 times.\n",
      "Iteration [14561]: Loss[0.07092738963995632] has not improved from the previous [0.07092414410102903] for 3 times.\n",
      "Iteration [14564]: Loss[0.07092618792942114] has not improved from the previous [0.07092173887695832] for 1 times.\n",
      "Iteration [14567]: Loss[0.07092888659596004] has not improved from the previous [0.07092092792212039] for 1 times.\n",
      "Iteration [14571]: Loss[0.07091851492267426] has not improved from the previous [0.07091832986436879] for 1 times.\n",
      "Iteration [14572]: Loss[0.07091910898122518] has not improved from the previous [0.07091851492267426] for 3 times.\n",
      "Iteration [14573]: Loss[0.07092190659882137] has not improved from the previous [0.07091910898122518] for 5 times.\n",
      "Iteration [14575]: Loss[0.070916982399635] has not improved from the previous [0.07091675061576015] for 1 times.\n",
      "Iteration [14576]: Loss[0.07092067272029545] has not improved from the previous [0.070916982399635] for 3 times.\n",
      "Iteration [14579]: Loss[0.07091945172467598] has not improved from the previous [0.0709146341719955] for 1 times.\n",
      "Iteration [14583]: Loss[0.07091414482624266] has not improved from the previous [0.0709129353164926] for 1 times.\n",
      "Iteration [14584]: Loss[0.0709164301464986] has not improved from the previous [0.07091414482624266] for 3 times.\n",
      "Iteration [14586]: Loss[0.07091223532629268] has not improved from the previous [0.07091136858655007] for 1 times.\n",
      "Iteration [14587]: Loss[0.07092591596660544] has not improved from the previous [0.07091223532629268] for 3 times.\n",
      "Iteration [14590]: Loss[0.07091106358321743] has not improved from the previous [0.0709095072965093] for 1 times.\n",
      "Iteration [14591]: Loss[0.07091352055808334] has not improved from the previous [0.07091106358321743] for 3 times.\n",
      "Iteration [14594]: Loss[0.07091254419589373] has not improved from the previous [0.07090806105327131] for 1 times.\n",
      "Iteration [14598]: Loss[0.07090735645013435] has not improved from the previous [0.07090613729837705] for 1 times.\n",
      "Iteration [14599]: Loss[0.07090961306550325] has not improved from the previous [0.07090735645013435] for 3 times.\n",
      "Iteration [14601]: Loss[0.07090491829929674] has not improved from the previous [0.07090459619738748] for 1 times.\n",
      "Iteration [14602]: Loss[0.070908518157053] has not improved from the previous [0.07090491829929674] for 3 times.\n",
      "Iteration [14604]: Loss[0.0709035370899101] has not improved from the previous [0.07090349400350764] for 1 times.\n",
      "Iteration [14605]: Loss[0.07091795551150962] has not improved from the previous [0.0709035370899101] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14600 Loss 0.07090459619738748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [14608]: Loss[0.07090211641394675] has not improved from the previous [0.07090148533962418] for 1 times.\n",
      "Iteration [14609]: Loss[0.07090542124259244] has not improved from the previous [0.07090211641394675] for 3 times.\n",
      "Iteration [14612]: Loss[0.0709044256220243] has not improved from the previous [0.07089936101846388] for 1 times.\n",
      "Iteration [14616]: Loss[0.07089873761512663] has not improved from the previous [0.07089795649847616] for 1 times.\n",
      "Iteration [14617]: Loss[0.07090147170635139] has not improved from the previous [0.07089873761512663] for 3 times.\n",
      "Iteration [14619]: Loss[0.07089662650631877] has not improved from the previous [0.07089638925360721] for 1 times.\n",
      "Iteration [14620]: Loss[0.07091096161881667] has not improved from the previous [0.07089662650631877] for 3 times.\n",
      "Iteration [14623]: Loss[0.07089570670369975] has not improved from the previous [0.07089452871384741] for 1 times.\n",
      "Iteration [14624]: Loss[0.07089854462615426] has not improved from the previous [0.07089570670369975] for 3 times.\n",
      "Iteration [14627]: Loss[0.07089759162667707] has not improved from the previous [0.07089273183735767] for 1 times.\n",
      "Iteration [14630]: Loss[0.07089127501463988] has not improved from the previous [0.07089086329876837] for 1 times.\n",
      "Iteration [14631]: Loss[0.07089219212239681] has not improved from the previous [0.07089127501463988] for 3 times.\n",
      "Iteration [14633]: Loss[0.07089015898022534] has not improved from the previous [0.0708898911476465] for 1 times.\n",
      "Iteration [14635]: Loss[0.0708935816511476] has not improved from the previous [0.07088944449430769] for 1 times.\n",
      "Iteration [14638]: Loss[0.07090295385647923] has not improved from the previous [0.07088830844014338] for 1 times.\n",
      "Iteration [14641]: Loss[0.07088708241974989] has not improved from the previous [0.07088643339046505] for 1 times.\n",
      "Iteration [14642]: Loss[0.07089044146959358] has not improved from the previous [0.07088708241974989] for 3 times.\n",
      "Iteration [14645]: Loss[0.07088944556969998] has not improved from the previous [0.0708842193366476] for 1 times.\n",
      "Iteration [14648]: Loss[0.07088298380088265] has not improved from the previous [0.07088262097285697] for 1 times.\n",
      "Iteration [14649]: Loss[0.07088396055272929] has not improved from the previous [0.07088298380088265] for 3 times.\n",
      "Iteration [14651]: Loss[0.07088264542459781] has not improved from the previous [0.07088125201814564] for 1 times.\n",
      "Iteration [14653]: Loss[0.07088059823677124] has not improved from the previous [0.07088043116820278] for 1 times.\n",
      "Iteration [14654]: Loss[0.07088168520685242] has not improved from the previous [0.07088059823677124] for 3 times.\n",
      "Iteration [14656]: Loss[0.07088654045961294] has not improved from the previous [0.07087765548623111] for 1 times.\n",
      "Iteration [14657]: Loss[0.07089236434580493] has not improved from the previous [0.07088654045961294] for 3 times.\n",
      "Iteration [14660]: Loss[0.07087792122031882] has not improved from the previous [0.07087707612865031] for 1 times.\n",
      "Iteration [14661]: Loss[0.07087850471882622] has not improved from the previous [0.07087792122031882] for 3 times.\n",
      "Iteration [14663]: Loss[0.07087768698931982] has not improved from the previous [0.07087617694772433] for 1 times.\n",
      "Iteration [14664]: Loss[0.07087968084256811] has not improved from the previous [0.07087768698931982] for 3 times.\n",
      "Iteration [14666]: Loss[0.07087638611581583] has not improved from the previous [0.07087449379243671] for 1 times.\n",
      "Iteration [14667]: Loss[0.07088911808302405] has not improved from the previous [0.07087638611581583] for 3 times.\n",
      "Iteration [14670]: Loss[0.07087527844466654] has not improved from the previous [0.07087255230114353] for 1 times.\n",
      "Iteration [14671]: Loss[0.07087659857475194] has not improved from the previous [0.07087527844466654] for 3 times.\n",
      "Iteration [14673]: Loss[0.07087238438176474] has not improved from the previous [0.07087167513674568] for 1 times.\n",
      "Iteration [14674]: Loss[0.07087570158112666] has not improved from the previous [0.07087238438176474] for 3 times.\n",
      "Iteration [14678]: Loss[0.07087031646111216] has not improved from the previous [0.07086931075869765] for 1 times.\n",
      "Iteration [14681]: Loss[0.07086901847633062] has not improved from the previous [0.0708682640300472] for 1 times.\n",
      "Iteration [14682]: Loss[0.07087171284413132] has not improved from the previous [0.07086901847633062] for 3 times.\n",
      "Iteration [14684]: Loss[0.07086811552706267] has not improved from the previous [0.0708665059968744] for 1 times.\n",
      "Iteration [14685]: Loss[0.07088111756649254] has not improved from the previous [0.07086811552706267] for 3 times.\n",
      "Iteration [14688]: Loss[0.07086671060335102] has not improved from the previous [0.07086456595575752] for 1 times.\n",
      "Iteration [14689]: Loss[0.07086854059979679] has not improved from the previous [0.07086671060335102] for 3 times.\n",
      "Iteration [14691]: Loss[0.07086394623319048] has not improved from the previous [0.07086363743102408] for 1 times.\n",
      "Iteration [14692]: Loss[0.07086758088329752] has not improved from the previous [0.07086394623319048] for 3 times.\n",
      "Iteration [14696]: Loss[0.07086219963039861] has not improved from the previous [0.07086152659916217] for 1 times.\n",
      "Iteration [14699]: Loss[0.07086069104419203] has not improved from the previous [0.07086018450980272] for 1 times.\n",
      "Iteration [14700]: Loss[0.07086362679674092] has not improved from the previous [0.07086069104419203] for 3 times.\n",
      "Iteration [14702]: Loss[0.0708600708064591] has not improved from the previous [0.07085841752158274] for 1 times.\n",
      "Iteration [14703]: Loss[0.07087302920033578] has not improved from the previous [0.0708600708064591] for 3 times.\n",
      "Iteration [14706]: Loss[0.07085839274956673] has not improved from the previous [0.07085646218233944] for 1 times.\n",
      "Iteration [14707]: Loss[0.07086053149396514] has not improved from the previous [0.07085839274956673] for 3 times.\n",
      "Iteration [14710]: Loss[0.07085953153294659] has not improved from the previous [0.07085557100005636] for 1 times.\n",
      "Iteration [14713]: Loss[0.07085834949927576] has not improved from the previous [0.0708534131893432] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14700 Loss 0.07086362679674092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [14716]: Loss[0.07086095013840761] has not improved from the previous [0.07085293435687284] for 1 times.\n",
      "Iteration [14720]: Loss[0.07085067027741343] has not improved from the previous [0.07084965947570568] for 1 times.\n",
      "Iteration [14721]: Loss[0.07085157752513406] has not improved from the previous [0.07085067027741343] for 3 times.\n",
      "Iteration [14723]: Loss[0.0708493633381197] has not improved from the previous [0.07084893395334654] for 1 times.\n",
      "Iteration [14726]: Loss[0.07084878870841875] has not improved from the previous [0.07084778051026637] for 1 times.\n",
      "Iteration [14728]: Loss[0.07084702548810633] has not improved from the previous [0.07084609524402528] for 1 times.\n",
      "Iteration [14729]: Loss[0.07084777542796807] has not improved from the previous [0.07084702548810633] for 3 times.\n",
      "Iteration [14731]: Loss[0.07084600243997403] has not improved from the previous [0.07084385850191614] for 1 times.\n",
      "Iteration [14732]: Loss[0.07084628708993757] has not improved from the previous [0.07084600243997403] for 3 times.\n",
      "Iteration [14734]: Loss[0.07084412070981291] has not improved from the previous [0.07084350973522717] for 1 times.\n",
      "Iteration [14735]: Loss[0.07084427517039633] has not improved from the previous [0.07084412070981291] for 3 times.\n",
      "Iteration [14737]: Loss[0.0708435478641095] has not improved from the previous [0.07084241454880146] for 1 times.\n",
      "Iteration [14739]: Loss[0.07084884447601122] has not improved from the previous [0.07084129489735828] for 1 times.\n",
      "Iteration [14741]: Loss[0.07084179669367392] has not improved from the previous [0.0708411057560452] for 1 times.\n",
      "Iteration [14742]: Loss[0.07084467259791671] has not improved from the previous [0.07084179669367392] for 3 times.\n",
      "Iteration [14744]: Loss[0.07084019265277057] has not improved from the previous [0.07083938061043953] for 1 times.\n",
      "Iteration [14745]: Loss[0.07084333838996391] has not improved from the previous [0.07084019265277057] for 3 times.\n",
      "Iteration [14747]: Loss[0.07084047068470722] has not improved from the previous [0.07083812329201163] for 1 times.\n",
      "Iteration [14748]: Loss[0.07084599404287283] has not improved from the previous [0.07084047068470722] for 3 times.\n",
      "Iteration [14753]: Loss[0.0708366316686673] has not improved from the previous [0.07083577375728242] for 1 times.\n",
      "Iteration [14756]: Loss[0.07083560422114478] has not improved from the previous [0.0708343874117268] for 1 times.\n",
      "Iteration [14757]: Loss[0.07083781914528514] has not improved from the previous [0.07083560422114478] for 3 times.\n",
      "Iteration [14759]: Loss[0.07083389326974072] has not improved from the previous [0.07083257616232133] for 1 times.\n",
      "Iteration [14760]: Loss[0.070836542487227] has not improved from the previous [0.07083389326974072] for 3 times.\n",
      "Iteration [14762]: Loss[0.07083191044809894] has not improved from the previous [0.07083140649806709] for 1 times.\n",
      "Iteration [14763]: Loss[0.07084597215106658] has not improved from the previous [0.07083191044809894] for 3 times.\n",
      "Iteration [14766]: Loss[0.07083166777670588] has not improved from the previous [0.0708293321957963] for 1 times.\n",
      "Iteration [14767]: Loss[0.07083339883182907] has not improved from the previous [0.07083166777670588] for 3 times.\n",
      "Iteration [14769]: Loss[0.07082893416099435] has not improved from the previous [0.07082844805130124] for 1 times.\n",
      "Iteration [14770]: Loss[0.07083242098380807] has not improved from the previous [0.07082893416099435] for 3 times.\n",
      "Iteration [14773]: Loss[0.07083122555464132] has not improved from the previous [0.07082687296857285] for 1 times.\n",
      "Iteration [14777]: Loss[0.07082669475340674] has not improved from the previous [0.0708245392114528] for 1 times.\n",
      "Iteration [14778]: Loss[0.07082808788215715] has not improved from the previous [0.07082669475340674] for 3 times.\n",
      "Iteration [14780]: Loss[0.07082480795228836] has not improved from the previous [0.07082298345928657] for 1 times.\n",
      "Iteration [14781]: Loss[0.0708376112016605] has not improved from the previous [0.07082480795228836] for 3 times.\n",
      "Iteration [14784]: Loss[0.07082278792356043] has not improved from the previous [0.07082140428044532] for 1 times.\n",
      "Iteration [14787]: Loss[0.07082087624953139] has not improved from the previous [0.07082082029729407] for 1 times.\n",
      "Iteration [14788]: Loss[0.07082427784215174] has not improved from the previous [0.07082087624953139] for 3 times.\n",
      "Iteration [14790]: Loss[0.07081923899523716] has not improved from the previous [0.07081906102788192] for 1 times.\n",
      "Iteration [14791]: Loss[0.0708230350187767] has not improved from the previous [0.07081923899523716] for 3 times.\n",
      "Iteration [14794]: Loss[0.07082181230248058] has not improved from the previous [0.07081720746658578] for 1 times.\n",
      "Iteration [14797]: Loss[0.07082436130136421] has not improved from the previous [0.0708165014571853] for 1 times.\n",
      "Iteration [14801]: Loss[0.07081479734671489] has not improved from the previous [0.07081300114512781] for 1 times.\n",
      "Iteration [14802]: Loss[0.07081499365564967] has not improved from the previous [0.07081479734671489] for 3 times.\n",
      "Iteration [14804]: Loss[0.07081279303884487] has not improved from the previous [0.0708122132264422] for 1 times.\n",
      "Iteration [14805]: Loss[0.0708133767509684] has not improved from the previous [0.07081279303884487] for 3 times.\n",
      "Iteration [14807]: Loss[0.07081222177015364] has not improved from the previous [0.07081102734139916] for 1 times.\n",
      "Iteration [14810]: Loss[0.07081122624883401] has not improved from the previous [0.07081021974811562] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14800 Loss 0.07081300114512781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [14812]: Loss[0.07080941602354017] has not improved from the previous [0.07080791562427936] for 1 times.\n",
      "Iteration [14813]: Loss[0.07081004756484575] has not improved from the previous [0.07080941602354017] for 3 times.\n",
      "Iteration [14815]: Loss[0.07080817656173097] has not improved from the previous [0.07080610381823867] for 1 times.\n",
      "Iteration [14816]: Loss[0.07080854049050939] has not improved from the previous [0.07080817656173097] for 3 times.\n",
      "Iteration [14818]: Loss[0.07081568617436751] has not improved from the previous [0.07080672708953668] for 1 times.\n",
      "Iteration [14822]: Loss[0.07080820997089639] has not improved from the previous [0.0708031691743887] for 1 times.\n",
      "Iteration [14825]: Loss[0.07080522865864688] has not improved from the previous [0.07080248353511481] for 1 times.\n",
      "Iteration [14826]: Loss[0.07081719967590938] has not improved from the previous [0.07080522865864688] for 3 times.\n",
      "Iteration [14829]: Loss[0.07080232591968208] has not improved from the previous [0.07080062994081955] for 1 times.\n",
      "Iteration [14830]: Loss[0.07080257014390373] has not improved from the previous [0.07080232591968208] for 3 times.\n",
      "Iteration [14832]: Loss[0.0708015923012212] has not improved from the previous [0.07080037165389759] for 1 times.\n",
      "Iteration [14833]: Loss[0.07080390339516944] has not improved from the previous [0.0708015923012212] for 3 times.\n",
      "Iteration [14835]: Loss[0.07079995588392711] has not improved from the previous [0.0707986317921319] for 1 times.\n",
      "Iteration [14836]: Loss[0.07080261756191088] has not improved from the previous [0.07079995588392711] for 3 times.\n",
      "Iteration [14838]: Loss[0.07079800131702617] has not improved from the previous [0.07079744506858292] for 1 times.\n",
      "Iteration [14839]: Loss[0.07080143164126518] has not improved from the previous [0.07079800131702617] for 3 times.\n",
      "Iteration [14841]: Loss[0.07079666175051952] has not improved from the previous [0.07079622098985218] for 1 times.\n",
      "Iteration [14842]: Loss[0.07080400789970343] has not improved from the previous [0.07079666175051952] for 3 times.\n",
      "Iteration [14846]: Loss[0.07079910639059406] has not improved from the previous [0.07079366495343518] for 1 times.\n",
      "Iteration [14850]: Loss[0.07079327504284909] has not improved from the previous [0.0707925769068492] for 1 times.\n",
      "Iteration [14853]: Loss[0.07079210227663926] has not improved from the previous [0.07079105891501436] for 1 times.\n",
      "Iteration [14854]: Loss[0.07079452195220243] has not improved from the previous [0.07079210227663926] for 3 times.\n",
      "Iteration [14856]: Loss[0.07079046639187304] has not improved from the previous [0.07078927247457774] for 1 times.\n",
      "Iteration [14857]: Loss[0.07079325809757013] has not improved from the previous [0.07079046639187304] for 3 times.\n",
      "Iteration [14859]: Loss[0.07078846077583832] has not improved from the previous [0.07078810882037838] for 1 times.\n",
      "Iteration [14860]: Loss[0.0708026739966279] has not improved from the previous [0.07078846077583832] for 3 times.\n",
      "Iteration [14863]: Loss[0.07078840830326148] has not improved from the previous [0.07078602119281396] for 1 times.\n",
      "Iteration [14864]: Loss[0.07079010005064366] has not improved from the previous [0.07078840830326148] for 3 times.\n",
      "Iteration [14866]: Loss[0.07078575748793466] has not improved from the previous [0.07078513789310596] for 1 times.\n",
      "Iteration [14867]: Loss[0.07078910804080918] has not improved from the previous [0.07078575748793466] for 3 times.\n",
      "Iteration [14870]: Loss[0.07078795725960706] has not improved from the previous [0.07078374684296809] for 1 times.\n",
      "Iteration [14873]: Loss[0.07078669006753656] has not improved from the previous [0.0707819720732832] for 1 times.\n",
      "Iteration [14876]: Loss[0.07078921343533749] has not improved from the previous [0.07078092921592205] for 1 times.\n",
      "Iteration [14880]: Loss[0.07078432801397865] has not improved from the previous [0.07077789616289423] for 1 times.\n",
      "Iteration [14883]: Loss[0.07077793353060814] has not improved from the previous [0.07077700589051568] for 1 times.\n",
      "Iteration [14884]: Loss[0.07077842387560558] has not improved from the previous [0.07077793353060814] for 3 times.\n",
      "Iteration [14886]: Loss[0.07077617215240248] has not improved from the previous [0.0707758738054707] for 1 times.\n",
      "Iteration [14887]: Loss[0.07077678044568082] has not improved from the previous [0.07077617215240248] for 3 times.\n",
      "Iteration [14889]: Loss[0.07077562779479778] has not improved from the previous [0.07077456717343397] for 1 times.\n",
      "Iteration [14892]: Loss[0.0707746366370486] has not improved from the previous [0.07077369426873102] for 1 times.\n",
      "Iteration [14894]: Loss[0.07077283888339844] has not improved from the previous [0.07077161975654155] for 1 times.\n",
      "Iteration [14895]: Loss[0.07077346077219289] has not improved from the previous [0.07077283888339844] for 3 times.\n",
      "Iteration [14897]: Loss[0.07077655046067295] has not improved from the previous [0.07076956059103458] for 1 times.\n",
      "Iteration [14899]: Loss[0.07077148546865344] has not improved from the previous [0.07077095645792836] for 1 times.\n",
      "Iteration [14900]: Loss[0.07077866329374198] has not improved from the previous [0.07077148546865344] for 3 times.\n",
      "Iteration [14904]: Loss[0.07076849637062411] has not improved from the previous [0.07076724029915989] for 1 times.\n",
      "Iteration [14905]: Loss[0.07076942236231992] has not improved from the previous [0.07076849637062411] for 3 times.\n",
      "Iteration [14907]: Loss[0.07077136509923491] has not improved from the previous [0.07076527779869635] for 1 times.\n",
      "Iteration [14908]: Loss[0.07078004702040011] has not improved from the previous [0.07077136509923491] for 3 times.\n",
      "Iteration [14911]: Loss[0.07076553396303409] has not improved from the previous [0.07076545147904381] for 1 times.\n",
      "Iteration [14914]: Loss[0.07076603583596812] has not improved from the previous [0.0707636353928769] for 1 times.\n",
      "Iteration [14915]: Loss[0.07076717654336218] has not improved from the previous [0.07076603583596812] for 3 times.\n",
      "Iteration [14917]: Loss[0.0707643474459759] has not improved from the previous [0.07076194088799921] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14900 Loss 0.07077866329374198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [14918]: Loss[0.07076594658413073] has not improved from the previous [0.0707643474459759] for 3 times.\n",
      "Iteration [14920]: Loss[0.07076235646694962] has not improved from the previous [0.07076079009259444] for 1 times.\n",
      "Iteration [14921]: Loss[0.07076480776867586] has not improved from the previous [0.07076235646694962] for 3 times.\n",
      "Iteration [14923]: Loss[0.07076090573013863] has not improved from the previous [0.07075961159151124] for 1 times.\n",
      "Iteration [14924]: Loss[0.070774319640456] has not improved from the previous [0.07076090573013863] for 3 times.\n",
      "Iteration [14927]: Loss[0.07076047955327029] has not improved from the previous [0.07075750602206222] for 1 times.\n",
      "Iteration [14928]: Loss[0.07076160407272625] has not improved from the previous [0.07076047955327029] for 3 times.\n",
      "Iteration [14930]: Loss[0.07075790710962353] has not improved from the previous [0.07075661754189257] for 1 times.\n",
      "Iteration [14931]: Loss[0.07076062510474772] has not improved from the previous [0.07075790710962353] for 3 times.\n",
      "Iteration [14933]: Loss[0.0707559841791416] has not improved from the previous [0.07075546705602628] for 1 times.\n",
      "Iteration [14934]: Loss[0.07075949087827423] has not improved from the previous [0.0707559841791416] for 3 times.\n",
      "Iteration [14937]: Loss[0.07075821597112569] has not improved from the previous [0.07075416771347727] for 1 times.\n",
      "Iteration [14940]: Loss[0.07076760120810548] has not improved from the previous [0.0707529895751452] for 1 times.\n",
      "Iteration [14943]: Loss[0.07075273914401412] has not improved from the previous [0.07075079044320347] for 1 times.\n",
      "Iteration [14944]: Loss[0.07075486786656851] has not improved from the previous [0.07075273914401412] for 3 times.\n",
      "Iteration [14946]: Loss[0.07075028448664225] has not improved from the previous [0.07074985882626426] for 1 times.\n",
      "Iteration [14947]: Loss[0.07075383683368783] has not improved from the previous [0.07075028448664225] for 3 times.\n",
      "Iteration [14950]: Loss[0.07075264686177996] has not improved from the previous [0.07074842691629353] for 1 times.\n",
      "Iteration [14953]: Loss[0.07075143746729251] has not improved from the previous [0.07074671908662307] for 1 times.\n",
      "Iteration [14956]: Loss[0.07075381371338418] has not improved from the previous [0.07074600251694474] for 1 times.\n",
      "Iteration [14960]: Loss[0.07074894285332366] has not improved from the previous [0.07074289603017796] for 1 times.\n",
      "Iteration [14963]: Loss[0.0707474136080003] has not improved from the previous [0.07074200007943932] for 1 times.\n",
      "Iteration [14966]: Loss[0.0707410370258348] has not improved from the previous [0.07074073483358063] for 1 times.\n",
      "Iteration [14967]: Loss[0.07074167745165508] has not improved from the previous [0.0707410370258348] for 3 times.\n",
      "Iteration [14970]: Loss[0.07074015065013395] has not improved from the previous [0.07073944439400177] for 1 times.\n",
      "Iteration [14971]: Loss[0.07074291397444561] has not improved from the previous [0.07074015065013395] for 3 times.\n",
      "Iteration [14973]: Loss[0.07073859800798714] has not improved from the previous [0.07073768285175831] for 1 times.\n",
      "Iteration [14974]: Loss[0.07074165909462658] has not improved from the previous [0.07073859800798714] for 3 times.\n",
      "Iteration [14976]: Loss[0.07073791364680555] has not improved from the previous [0.07073651450319747] for 1 times.\n",
      "Iteration [14977]: Loss[0.070751182039214] has not improved from the previous [0.07073791364680555] for 3 times.\n",
      "Iteration [14980]: Loss[0.07073621029912615] has not improved from the previous [0.07073479030579644] for 1 times.\n",
      "Iteration [14982]: Loss[0.07073409515644787] has not improved from the previous [0.07073404998293986] for 1 times.\n",
      "Iteration [14983]: Loss[0.07073417220662293] has not improved from the previous [0.07073409515644787] for 3 times.\n",
      "Iteration [14985]: Loss[0.07073355388640754] has not improved from the previous [0.07073241115399893] for 1 times.\n",
      "Iteration [14988]: Loss[0.07073249327476412] has not improved from the previous [0.07073144146228037] for 1 times.\n",
      "Iteration [14990]: Loss[0.07073048185463202] has not improved from the previous [0.07072951179323714] for 1 times.\n",
      "Iteration [14991]: Loss[0.07073128927101212] has not improved from the previous [0.07073048185463202] for 3 times.\n",
      "Iteration [14993]: Loss[0.07072965878967578] has not improved from the previous [0.0707275211183682] for 1 times.\n",
      "Iteration [14994]: Loss[0.07073986697970625] has not improved from the previous [0.07072965878967578] for 3 times.\n",
      "Iteration [14998]: Loss[0.0707275093322383] has not improved from the previous [0.07072706900149096] for 1 times.\n",
      "Iteration [15000]: Loss[0.07072663451305683] has not improved from the previous [0.07072598896309391] for 1 times.\n",
      "Iteration [15003]: Loss[0.07072688215650826] has not improved from the previous [0.07072438391652709] for 1 times.\n",
      "Iteration [15004]: Loss[0.07072785523446055] has not improved from the previous [0.07072688215650826] for 3 times.\n",
      "Iteration [15006]: Loss[0.07072656358667062] has not improved from the previous [0.07072258155356494] for 1 times.\n",
      "Iteration [15007]: Loss[0.07073729356680385] has not improved from the previous [0.07072656358667062] for 3 times.\n",
      "Iteration [15010]: Loss[0.07072234248770877] has not improved from the previous [0.07072213736621351] for 1 times.\n",
      "Iteration [15013]: Loss[0.07072232563350193] has not improved from the previous [0.07072028879322784] for 1 times.\n",
      "Iteration [15014]: Loss[0.0707238200337021] has not improved from the previous [0.07072232563350193] for 3 times.\n",
      "Iteration [15016]: Loss[0.07072074973234846] has not improved from the previous [0.07071859317692526] for 1 times.\n",
      "Iteration [15017]: Loss[0.07072257745024006] has not improved from the previous [0.07072074973234846] for 3 times.\n",
      "Iteration [15019]: Loss[0.07071889360395894] has not improved from the previous [0.0707174175064208] for 1 times.\n",
      "Iteration [15020]: Loss[0.070721419775582] has not improved from the previous [0.07071889360395894] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 15000 Loss 0.07072663451305683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [15022]: Loss[0.0707180872646274] has not improved from the previous [0.07071621147301718] for 1 times.\n",
      "Iteration [15023]: Loss[0.07073085126768563] has not improved from the previous [0.0707180872646274] for 3 times.\n",
      "Iteration [15026]: Loss[0.0707158699488379] has not improved from the previous [0.07071456668956466] for 1 times.\n",
      "Iteration [15029]: Loss[0.07071443681206378] has not improved from the previous [0.07071371628511032] for 1 times.\n",
      "Iteration [15030]: Loss[0.07071726741377792] has not improved from the previous [0.07071443681206378] for 3 times.\n",
      "Iteration [15032]: Loss[0.07071298867627503] has not improved from the previous [0.07071197310815354] for 1 times.\n",
      "Iteration [15033]: Loss[0.07071592507557264] has not improved from the previous [0.07071298867627503] for 3 times.\n",
      "Iteration [15035]: Loss[0.07071127704202992] has not improved from the previous [0.07071073646030125] for 1 times.\n",
      "Iteration [15036]: Loss[0.07071475963148967] has not improved from the previous [0.07071127704202992] for 3 times.\n",
      "Iteration [15038]: Loss[0.07071034846914674] has not improved from the previous [0.07070953145579545] for 1 times.\n",
      "Iteration [15039]: Loss[0.07072418858630286] has not improved from the previous [0.07071034846914674] for 3 times.\n",
      "Iteration [15042]: Loss[0.07070916493332664] has not improved from the previous [0.07070747547811344] for 1 times.\n",
      "Iteration [15045]: Loss[0.07070702021635657] has not improved from the previous [0.0707069964919201] for 1 times.\n",
      "Iteration [15047]: Loss[0.07070644367555497] has not improved from the previous [0.07070564952101994] for 1 times.\n",
      "Iteration [15050]: Loss[0.07070538462381477] has not improved from the previous [0.07070460958718391] for 1 times.\n",
      "Iteration [15052]: Loss[0.07070354729364384] has not improved from the previous [0.07070260794955063] for 1 times.\n",
      "Iteration [15053]: Loss[0.07070423337610697] has not improved from the previous [0.07070354729364384] for 3 times.\n",
      "Iteration [15055]: Loss[0.07070249960271861] has not improved from the previous [0.0707007182868716] for 1 times.\n",
      "Iteration [15056]: Loss[0.070702981108299] has not improved from the previous [0.07070249960271861] for 3 times.\n",
      "Iteration [15058]: Loss[0.07070606370018524] has not improved from the previous [0.07069905463856924] for 1 times.\n",
      "Iteration [15060]: Loss[0.07070161721377292] has not improved from the previous [0.07070039704030254] for 1 times.\n",
      "Iteration [15061]: Loss[0.07071512719264521] has not improved from the previous [0.07070161721377292] for 3 times.\n",
      "Iteration [15064]: Loss[0.07069935441773172] has not improved from the previous [0.07069800844826753] for 1 times.\n",
      "Iteration [15065]: Loss[0.07070210863262302] has not improved from the previous [0.07069935441773172] for 3 times.\n",
      "Iteration [15068]: Loss[0.0707010570537178] has not improved from the previous [0.07069704216399139] for 1 times.\n",
      "Iteration [15071]: Loss[0.07071058984006945] has not improved from the previous [0.07069539658024294] for 1 times.\n",
      "Iteration [15074]: Loss[0.07069560935712398] has not improved from the previous [0.07069402503317086] for 1 times.\n",
      "Iteration [15076]: Loss[0.07069337772844708] has not improved from the previous [0.07069278716082712] for 1 times.\n",
      "Iteration [15077]: Loss[0.07069363659047369] has not improved from the previous [0.07069337772844708] for 3 times.\n",
      "Iteration [15079]: Loss[0.07069246950668061] has not improved from the previous [0.07069162573563056] for 1 times.\n",
      "Iteration [15080]: Loss[0.0706956527335979] has not improved from the previous [0.07069246950668061] for 3 times.\n",
      "Iteration [15082]: Loss[0.07069183961768917] has not improved from the previous [0.07069030539281641] for 1 times.\n",
      "Iteration [15083]: Loss[0.07070504750660189] has not improved from the previous [0.07069183961768917] for 3 times.\n",
      "Iteration [15086]: Loss[0.07069136448264546] has not improved from the previous [0.07068801807300214] for 1 times.\n",
      "Iteration [15087]: Loss[0.07069212509050163] has not improved from the previous [0.07069136448264546] for 3 times.\n",
      "Iteration [15089]: Loss[0.07068901143327808] has not improved from the previous [0.07068708847123395] for 1 times.\n",
      "Iteration [15090]: Loss[0.07069114142621163] has not improved from the previous [0.07068901143327808] for 3 times.\n",
      "Iteration [15092]: Loss[0.07068717710646166] has not improved from the previous [0.07068594554117877] for 1 times.\n",
      "Iteration [15093]: Loss[0.07068998315393016] has not improved from the previous [0.07068717710646166] for 3 times.\n",
      "Iteration [15095]: Loss[0.07068551238642484] has not improved from the previous [0.0706847157246228] for 1 times.\n",
      "Iteration [15096]: Loss[0.07068876517865673] has not improved from the previous [0.07068551238642484] for 3 times.\n",
      "Iteration [15098]: Loss[0.07068431626938133] has not improved from the previous [0.0706834696327691] for 1 times.\n",
      "Iteration [15099]: Loss[0.07069814390885298] has not improved from the previous [0.07068431626938133] for 3 times.\n",
      "Iteration [15102]: Loss[0.07068424766063557] has not improved from the previous [0.07068120466675125] for 1 times.\n",
      "Iteration [15103]: Loss[0.07068533291790365] has not improved from the previous [0.07068424766063557] for 3 times.\n",
      "Iteration [15105]: Loss[0.07068187245297804] has not improved from the previous [0.07068029026419531] for 1 times.\n",
      "Iteration [15106]: Loss[0.07068430727759657] has not improved from the previous [0.07068187245297804] for 3 times.\n",
      "Iteration [15108]: Loss[0.07068008070345803] has not improved from the previous [0.07067913161753399] for 1 times.\n",
      "Iteration [15109]: Loss[0.07068312391044639] has not improved from the previous [0.07068008070345803] for 3 times.\n",
      "Iteration [15111]: Loss[0.07067846468388227] has not improved from the previous [0.07067789136379889] for 1 times.\n",
      "Iteration [15112]: Loss[0.07068193261809523] has not improved from the previous [0.07067846468388227] for 3 times.\n",
      "Iteration [15114]: Loss[0.07067690301942577] has not improved from the previous [0.07067666001846311] for 1 times.\n",
      "Iteration [15115]: Loss[0.07069132868928782] has not improved from the previous [0.07067690301942577] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 15100 Loss 0.0706859027714627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [15118]: Loss[0.07067718788347835] has not improved from the previous [0.07067440730191547] for 1 times.\n",
      "Iteration [15119]: Loss[0.07067852999463577] has not improved from the previous [0.07067718788347835] for 3 times.\n",
      "Iteration [15121]: Loss[0.0706748026804841] has not improved from the previous [0.07067350019570182] for 1 times.\n",
      "Iteration [15122]: Loss[0.07067751117866221] has not improved from the previous [0.0706748026804841] for 3 times.\n",
      "Iteration [15124]: Loss[0.07067301158303736] has not improved from the previous [0.07067234591684457] for 1 times.\n",
      "Iteration [15125]: Loss[0.07067635046141095] has not improved from the previous [0.07067301158303736] for 3 times.\n",
      "Iteration [15127]: Loss[0.070671428967978] has not improved from the previous [0.07067109695787001] for 1 times.\n",
      "Iteration [15128]: Loss[0.07067513883500152] has not improved from the previous [0.070671428967978] for 3 times.\n",
      "Iteration [15131]: Loss[0.07067383025616997] has not improved from the previous [0.07066983511767734] for 1 times.\n",
      "Iteration [15133]: Loss[0.07066925183143333] has not improved from the previous [0.07066856085523678] for 1 times.\n",
      "Iteration [15134]: Loss[0.07068321189294696] has not improved from the previous [0.07066925183143333] for 3 times.\n",
      "Iteration [15137]: Loss[0.07066817641839813] has not improved from the previous [0.07066640281451055] for 1 times.\n",
      "Iteration [15140]: Loss[0.07066618184445528] has not improved from the previous [0.07066595438066932] for 1 times.\n",
      "Iteration [15142]: Loss[0.07066543956326023] has not improved from the previous [0.07066442734801165] for 1 times.\n",
      "Iteration [15145]: Loss[0.07066436764710446] has not improved from the previous [0.0706633310099626] for 1 times.\n",
      "Iteration [15147]: Loss[0.0706622334243433] has not improved from the previous [0.07066202747885339] for 1 times.\n",
      "Iteration [15148]: Loss[0.07066316952559355] has not improved from the previous [0.0706622334243433] for 3 times.\n",
      "Iteration [15150]: Loss[0.0706611205603422] has not improved from the previous [0.07066028288338755] for 1 times.\n",
      "Iteration [15151]: Loss[0.07066189817186098] has not improved from the previous [0.0706611205603422] for 3 times.\n",
      "Iteration [15153]: Loss[0.07066046298105907] has not improved from the previous [0.07065863698989071] for 1 times.\n",
      "Iteration [15154]: Loss[0.07067051239655474] has not improved from the previous [0.07066046298105907] for 3 times.\n",
      "Iteration [15158]: Loss[0.07065866017666066] has not improved from the previous [0.07065760651547691] for 1 times.\n",
      "Iteration [15159]: Loss[0.07066160761855639] has not improved from the previous [0.07065866017666066] for 3 times.\n",
      "Iteration [15161]: Loss[0.07065787781172019] has not improved from the previous [0.07065619958792446] for 1 times.\n",
      "Iteration [15162]: Loss[0.07067099627695417] has not improved from the previous [0.07065787781172019] for 3 times.\n",
      "Iteration [15165]: Loss[0.07065777451850253] has not improved from the previous [0.07065386202346766] for 1 times.\n",
      "Iteration [15166]: Loss[0.07065799377388289] has not improved from the previous [0.07065777451850253] for 3 times.\n",
      "Iteration [15168]: Loss[0.07065546639425214] has not improved from the previous [0.07065292622486458] for 1 times.\n",
      "Iteration [15169]: Loss[0.07065702649528637] has not improved from the previous [0.07065546639425214] for 3 times.\n",
      "Iteration [15171]: Loss[0.07065366416064066] has not improved from the previous [0.07065179269187828] for 1 times.\n",
      "Iteration [15172]: Loss[0.07065585673617969] has not improved from the previous [0.07065366416064066] for 3 times.\n",
      "Iteration [15174]: Loss[0.07065202374822835] has not improved from the previous [0.07065058406976756] for 1 times.\n",
      "Iteration [15175]: Loss[0.07065463124441718] has not improved from the previous [0.07065202374822835] for 3 times.\n",
      "Iteration [15177]: Loss[0.07065055624168498] has not improved from the previous [0.07064934973706734] for 1 times.\n",
      "Iteration [15178]: Loss[0.0706640904064378] has not improved from the previous [0.07065055624168498] for 3 times.\n",
      "Iteration [15181]: Loss[0.07064902582015216] has not improved from the previous [0.07064776301605726] for 1 times.\n",
      "Iteration [15184]: Loss[0.07064802966376497] has not improved from the previous [0.070646803627136] for 1 times.\n",
      "Iteration [15185]: Loss[0.07065030930997156] has not improved from the previous [0.07064802966376497] for 3 times.\n",
      "Iteration [15187]: Loss[0.07064679191961828] has not improved from the previous [0.07064500211471805] for 1 times.\n",
      "Iteration [15188]: Loss[0.07064902754015966] has not improved from the previous [0.07064679191961828] for 3 times.\n",
      "Iteration [15190]: Loss[0.07064510976058028] has not improved from the previous [0.07064380361599114] for 1 times.\n",
      "Iteration [15191]: Loss[0.0706477784808322] has not improved from the previous [0.07064510976058028] for 3 times.\n",
      "Iteration [15193]: Loss[0.07064357378800329] has not improved from the previous [0.07064254680837684] for 1 times.\n",
      "Iteration [15194]: Loss[0.07064651492613995] has not improved from the previous [0.07064357378800329] for 3 times.\n",
      "Iteration [15196]: Loss[0.07064282989672986] has not improved from the previous [0.07064127313872902] for 1 times.\n",
      "Iteration [15197]: Loss[0.07065600348603111] has not improved from the previous [0.07064282989672986] for 3 times.\n",
      "Iteration [15200]: Loss[0.07064094753167688] has not improved from the previous [0.07064028903352271] for 1 times.\n",
      "Iteration [15203]: Loss[0.0706401316194093] has not improved from the previous [0.07063872022336429] for 1 times.\n",
      "Iteration [15205]: Loss[0.07063973877710068] has not improved from the previous [0.07063698219114369] for 1 times.\n",
      "Iteration [15206]: Loss[0.07065173146310713] has not improved from the previous [0.07063973877710068] for 3 times.\n",
      "Iteration [15209]: Loss[0.07063661644684872] has not improved from the previous [0.07063522656104206] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 15200 Loss 0.07064094753167688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [15210]: Loss[0.07063699539793782] has not improved from the previous [0.07063661644684872] for 3 times.\n",
      "Iteration [15212]: Loss[0.07063723785504351] has not improved from the previous [0.07063437946295126] for 1 times.\n",
      "Iteration [15213]: Loss[0.07063795671038837] has not improved from the previous [0.07063723785504351] for 3 times.\n",
      "Iteration [15215]: Loss[0.070635960818226] has not improved from the previous [0.07063260025379653] for 1 times.\n",
      "Iteration [15216]: Loss[0.07063664813288352] has not improved from the previous [0.070635960818226] for 3 times.\n",
      "Iteration [15218]: Loss[0.07063428708625967] has not improved from the previous [0.07063140128558977] for 1 times.\n",
      "Iteration [15219]: Loss[0.07063546107197256] has not improved from the previous [0.07063428708625967] for 3 times.\n",
      "Iteration [15221]: Loss[0.07063266296705316] has not improved from the previous [0.07063019544740197] for 1 times.\n",
      "Iteration [15222]: Loss[0.07063427149168562] has not improved from the previous [0.07063266296705316] for 3 times.\n",
      "Iteration [15224]: Loss[0.07063145431874879] has not improved from the previous [0.07062896036315043] for 1 times.\n",
      "Iteration [15225]: Loss[0.07064366618704729] has not improved from the previous [0.07063145431874879] for 3 times.\n",
      "Iteration [15228]: Loss[0.0706286216765446] has not improved from the previous [0.07062817515722222] for 1 times.\n",
      "Iteration [15231]: Loss[0.07062782127933666] has not improved from the previous [0.0706265712524932] for 1 times.\n",
      "Iteration [15234]: Loss[0.07062710486685102] has not improved from the previous [0.07062526404368209] for 1 times.\n",
      "Iteration [15235]: Loss[0.07062876546858335] has not improved from the previous [0.07062710486685102] for 3 times.\n",
      "Iteration [15237]: Loss[0.07062595046912191] has not improved from the previous [0.07062338887645904] for 1 times.\n",
      "Iteration [15238]: Loss[0.07062742302872298] has not improved from the previous [0.07062595046912191] for 3 times.\n",
      "Iteration [15240]: Loss[0.07062432242068666] has not improved from the previous [0.0706221775709045] for 1 times.\n",
      "Iteration [15241]: Loss[0.07062616962294656] has not improved from the previous [0.07062432242068666] for 3 times.\n",
      "Iteration [15243]: Loss[0.0706235275700453] has not improved from the previous [0.07062092570999448] for 1 times.\n",
      "Iteration [15244]: Loss[0.07063560864886206] has not improved from the previous [0.0706235275700453] for 3 times.\n",
      "Iteration [15249]: Loss[0.07061941141461633] has not improved from the previous [0.07061843032457563] for 1 times.\n",
      "Iteration [15250]: Loss[0.0706198082780819] has not improved from the previous [0.07061941141461633] for 3 times.\n",
      "Iteration [15252]: Loss[0.07061781675611836] has not improved from the previous [0.07061691804763968] for 1 times.\n",
      "Iteration [15253]: Loss[0.07061862703139929] has not improved from the previous [0.07061781675611836] for 3 times.\n",
      "Iteration [15255]: Loss[0.07061624276477343] has not improved from the previous [0.07061567742635781] for 1 times.\n",
      "Iteration [15256]: Loss[0.07061736012869471] has not improved from the previous [0.07061624276477343] for 3 times.\n",
      "Iteration [15258]: Loss[0.0706147244571659] has not improved from the previous [0.07061445403995315] for 1 times.\n",
      "Iteration [15259]: Loss[0.07061607959020857] has not improved from the previous [0.0706147244571659] for 3 times.\n",
      "Iteration [15261]: Loss[0.07062289100788265] has not improved from the previous [0.07061396202261046] for 1 times.\n",
      "Iteration [15265]: Loss[0.07061241511108034] has not improved from the previous [0.07061144655932508] for 1 times.\n",
      "Iteration [15266]: Loss[0.07061323128285997] has not improved from the previous [0.07061241511108034] for 3 times.\n",
      "Iteration [15268]: Loss[0.0706109634176251] has not improved from the previous [0.07061011199101383] for 1 times.\n",
      "Iteration [15269]: Loss[0.07061244331967359] has not improved from the previous [0.0706109634176251] for 3 times.\n",
      "Iteration [15270]: Loss[0.07061814124843753] has not improved from the previous [0.07061244331967359] for 5 times.\n",
      "Iteration [15274]: Loss[0.07061322263085641] has not improved from the previous [0.0706091832725487] for 1 times.\n",
      "Iteration [15276]: Loss[0.0706086468206971] has not improved from the previous [0.07060757092887736] for 1 times.\n",
      "Iteration [15277]: Loss[0.07061154419656639] has not improved from the previous [0.0706086468206971] for 3 times.\n",
      "Iteration [15279]: Loss[0.07060745924933941] has not improved from the previous [0.0706061147766668] for 1 times.\n",
      "Iteration [15280]: Loss[0.07061015792762557] has not improved from the previous [0.07060745924933941] for 3 times.\n",
      "Iteration [15282]: Loss[0.07060615096147782] has not improved from the previous [0.07060476086696303] for 1 times.\n",
      "Iteration [15283]: Loss[0.07060880309872472] has not improved from the previous [0.07060615096147782] for 3 times.\n",
      "Iteration [15285]: Loss[0.07060478364799869] has not improved from the previous [0.0706034478249763] for 1 times.\n",
      "Iteration [15286]: Loss[0.07060748388075307] has not improved from the previous [0.07060478364799869] for 3 times.\n",
      "Iteration [15288]: Loss[0.07060434977709647] has not improved from the previous [0.07060213534394333] for 1 times.\n",
      "Iteration [15289]: Loss[0.07061692601473098] has not improved from the previous [0.07060434977709647] for 3 times.\n",
      "Iteration [15292]: Loss[0.0706017820527585] has not improved from the previous [0.07060044597141686] for 1 times.\n",
      "Iteration [15295]: Loss[0.0706012713934993] has not improved from the previous [0.0705994569033595] for 1 times.\n",
      "Iteration [15296]: Loss[0.07060298735581298] has not improved from the previous [0.0706012713934993] for 3 times.\n",
      "Iteration [15298]: Loss[0.0706001387523858] has not improved from the previous [0.07059764722651819] for 1 times.\n",
      "Iteration [15299]: Loss[0.07060168736238355] has not improved from the previous [0.0706001387523858] for 3 times.\n",
      "Iteration [15301]: Loss[0.07059853963122513] has not improved from the previous [0.07059643749916185] for 1 times.\n",
      "Iteration [15302]: Loss[0.07060048679379646] has not improved from the previous [0.07059853963122513] for 3 times.\n",
      "Iteration [15304]: Loss[0.070596996423601] has not improved from the previous [0.07059521141496698] for 1 times.\n",
      "Iteration [15305]: Loss[0.07059920777781781] has not improved from the previous [0.070596996423601] for 3 times.\n",
      "Iteration [15307]: Loss[0.0705966544880811] has not improved from the previous [0.07059392298952781] for 1 times.\n",
      "Iteration [15308]: Loss[0.0706086449930784] has not improved from the previous [0.0705966544880811] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 15300 Loss 0.07059643749916185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [15313]: Loss[0.0705924582481469] has not improved from the previous [0.07059121455601855] for 1 times.\n",
      "Iteration [15314]: Loss[0.07059279852502534] has not improved from the previous [0.0705924582481469] for 3 times.\n",
      "Iteration [15316]: Loss[0.07059095123535623] has not improved from the previous [0.07058967053701685] for 1 times.\n",
      "Iteration [15317]: Loss[0.07059161862619542] has not improved from the previous [0.07059095123535623] for 3 times.\n",
      "Iteration [15319]: Loss[0.07058951009766591] has not improved from the previous [0.07058837302540286] for 1 times.\n",
      "Iteration [15320]: Loss[0.07059033086425479] has not improved from the previous [0.07058951009766591] for 3 times.\n",
      "Iteration [15322]: Loss[0.07058808955887681] has not improved from the previous [0.07058710346973288] for 1 times.\n",
      "Iteration [15323]: Loss[0.0705890309186658] has not improved from the previous [0.07058808955887681] for 3 times.\n",
      "Iteration [15325]: Loss[0.07059584526274269] has not improved from the previous [0.07058676653269495] for 1 times.\n",
      "Iteration [15329]: Loss[0.07058526836378029] has not improved from the previous [0.07058497635188625] for 1 times.\n",
      "Iteration [15330]: Loss[0.07058562852442284] has not improved from the previous [0.07058526836378029] for 3 times.\n",
      "Iteration [15332]: Loss[0.07058454148180539] has not improved from the previous [0.07058364454681247] for 1 times.\n",
      "Iteration [15335]: Loss[0.07058337340307767] has not improved from the previous [0.0705822564471649] for 1 times.\n",
      "Iteration [15336]: Loss[0.07058339022572314] has not improved from the previous [0.07058337340307767] for 3 times.\n",
      "Iteration [15337]: Loss[0.07059027627783465] has not improved from the previous [0.07058339022572314] for 5 times.\n",
      "Iteration [15341]: Loss[0.07058503187756364] has not improved from the previous [0.07058062131367818] for 1 times.\n",
      "Iteration [15343]: Loss[0.07058020618912693] has not improved from the previous [0.07057936362458399] for 1 times.\n",
      "Iteration [15344]: Loss[0.07058333502952617] has not improved from the previous [0.07058020618912693] for 3 times.\n",
      "Iteration [15346]: Loss[0.07057909575420117] has not improved from the previous [0.07057789097129386] for 1 times.\n",
      "Iteration [15347]: Loss[0.0705819015529801] has not improved from the previous [0.07057909575420117] for 3 times.\n",
      "Iteration [15349]: Loss[0.07057781175592256] has not improved from the previous [0.07057653909563914] for 1 times.\n",
      "Iteration [15350]: Loss[0.07058050976245671] has not improved from the previous [0.07057781175592256] for 3 times.\n",
      "Iteration [15352]: Loss[0.07057736419947137] has not improved from the previous [0.07057516041083761] for 1 times.\n",
      "Iteration [15353]: Loss[0.07058996200166637] has not improved from the previous [0.07057736419947137] for 3 times.\n",
      "Iteration [15356]: Loss[0.07057481129430436] has not improved from the previous [0.0705744448221433] for 1 times.\n",
      "Iteration [15358]: Loss[0.07057304024008591] has not improved from the previous [0.0705729648281495] for 1 times.\n",
      "Iteration [15359]: Loss[0.0705739728974684] has not improved from the previous [0.07057304024008591] for 3 times.\n",
      "Iteration [15361]: Loss[0.07057164971911203] has not improved from the previous [0.07057135012079567] for 1 times.\n",
      "Iteration [15362]: Loss[0.0705727650904695] has not improved from the previous [0.07057164971911203] for 3 times.\n",
      "Iteration [15364]: Loss[0.07057031201456604] has not improved from the previous [0.07056998920072985] for 1 times.\n",
      "Iteration [15365]: Loss[0.07057145709515669] has not improved from the previous [0.07057031201456604] for 3 times.\n",
      "Iteration [15367]: Loss[0.07056898505914162] has not improved from the previous [0.07056866117062127] for 1 times.\n",
      "Iteration [15368]: Loss[0.0705701418828818] has not improved from the previous [0.07056898505914162] for 3 times.\n",
      "Iteration [15370]: Loss[0.07057691989654967] has not improved from the previous [0.07056785488383115] for 1 times.\n",
      "Iteration [15374]: Loss[0.07056637151794361] has not improved from the previous [0.07056601828629239] for 1 times.\n",
      "Iteration [15375]: Loss[0.07056726111638015] has not improved from the previous [0.07056637151794361] for 3 times.\n",
      "Iteration [15377]: Loss[0.07056501238927411] has not improved from the previous [0.07056481488163895] for 1 times.\n",
      "Iteration [15378]: Loss[0.07056593749307517] has not improved from the previous [0.07056501238927411] for 3 times.\n",
      "Iteration [15379]: Loss[0.07057215961730393] has not improved from the previous [0.07056593749307517] for 5 times.\n",
      "Iteration [15382]: Loss[0.07056387942889261] has not improved from the previous [0.07056362229698543] for 1 times.\n",
      "Iteration [15383]: Loss[0.0705672546349239] has not improved from the previous [0.07056387942889261] for 3 times.\n",
      "Iteration [15385]: Loss[0.0705633797632922] has not improved from the previous [0.07056159286378418] for 1 times.\n",
      "Iteration [15386]: Loss[0.07056557470057179] has not improved from the previous [0.0705633797632922] for 3 times.\n",
      "Iteration [15388]: Loss[0.07056222686376008] has not improved from the previous [0.07056013928993975] for 1 times.\n",
      "Iteration [15389]: Loss[0.07056419252045207] has not improved from the previous [0.07056222686376008] for 3 times.\n",
      "Iteration [15391]: Loss[0.07056095328694012] has not improved from the previous [0.07055878622268297] for 1 times.\n",
      "Iteration [15392]: Loss[0.07056284257371138] has not improved from the previous [0.07056095328694012] for 3 times.\n",
      "Iteration [15394]: Loss[0.07055961908348894] has not improved from the previous [0.0705574745065872] for 1 times.\n",
      "Iteration [15395]: Loss[0.0705615268251321] has not improved from the previous [0.07055961908348894] for 3 times.\n",
      "Iteration [15397]: Loss[0.07055858753177006] has not improved from the previous [0.07055618010962876] for 1 times.\n",
      "Iteration [15398]: Loss[0.0705709707674155] has not improved from the previous [0.07055858753177006] for 3 times.\n",
      "Iteration [15403]: Loss[0.07055465690335734] has not improved from the previous [0.0705541183957735] for 1 times.\n",
      "Iteration [15404]: Loss[0.0705550085347374] has not improved from the previous [0.07055465690335734] for 3 times.\n",
      "Iteration [15406]: Loss[0.07055330005821463] has not improved from the previous [0.07055245817105278] for 1 times.\n",
      "Iteration [15407]: Loss[0.07055378190292338] has not improved from the previous [0.07055330005821463] for 3 times.\n",
      "Iteration [15409]: Loss[0.07055198536550696] has not improved from the previous [0.07055106980119613] for 1 times.\n",
      "Iteration [15410]: Loss[0.0705524947098709] has not improved from the previous [0.07055198536550696] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 15400 Loss 0.07055598601055584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [15412]: Loss[0.07055068353529714] has not improved from the previous [0.0705497136656283] for 1 times.\n",
      "Iteration [15413]: Loss[0.07055119765094786] has not improved from the previous [0.07055068353529714] for 3 times.\n",
      "Iteration [15415]: Loss[0.07054938923928761] has not improved from the previous [0.07054836288841208] for 1 times.\n",
      "Iteration [15416]: Loss[0.07054992052383825] has not improved from the previous [0.07054938923928761] for 3 times.\n",
      "Iteration [15418]: Loss[0.07055666516122229] has not improved from the previous [0.07054795662654201] for 1 times.\n",
      "Iteration [15423]: Loss[0.07054681508424333] has not improved from the previous [0.07054614968234359] for 1 times.\n",
      "Iteration [15424]: Loss[0.07055002052687913] has not improved from the previous [0.07054681508424333] for 3 times.\n",
      "Iteration [15426]: Loss[0.07054656452705352] has not improved from the previous [0.07054438239546519] for 1 times.\n",
      "Iteration [15427]: Loss[0.07055919450042128] has not improved from the previous [0.07054656452705352] for 3 times.\n",
      "Iteration [15430]: Loss[0.0705438124679481] has not improved from the previous [0.0705422685228349] for 1 times.\n",
      "Iteration [15431]: Loss[0.07054384815932314] has not improved from the previous [0.0705438124679481] for 3 times.\n",
      "Iteration [15433]: Loss[0.07054440743341502] has not improved from the previous [0.07054140206892853] for 1 times.\n",
      "Iteration [15434]: Loss[0.07054500824243126] has not improved from the previous [0.07054440743341502] for 3 times.\n",
      "Iteration [15436]: Loss[0.07054334873251589] has not improved from the previous [0.07053956540788735] for 1 times.\n",
      "Iteration [15437]: Loss[0.07054364529209715] has not improved from the previous [0.07054334873251589] for 3 times.\n",
      "Iteration [15439]: Loss[0.07054186308108981] has not improved from the previous [0.07053832579484665] for 1 times.\n",
      "Iteration [15440]: Loss[0.07054241462914476] has not improved from the previous [0.07054186308108981] for 3 times.\n",
      "Iteration [15442]: Loss[0.07054039925156687] has not improved from the previous [0.07053708872294316] for 1 times.\n",
      "Iteration [15443]: Loss[0.07054117157308488] has not improved from the previous [0.07054039925156687] for 3 times.\n",
      "Iteration [15445]: Loss[0.0705393755190112] has not improved from the previous [0.07053584062061817] for 1 times.\n",
      "Iteration [15446]: Loss[0.07055066495643407] has not improved from the previous [0.0705393755190112] for 3 times.\n",
      "Iteration [15451]: Loss[0.07053935579522469] has not improved from the previous [0.07053452120359108] for 1 times.\n",
      "Iteration [15453]: Loss[0.07053390338565171] has not improved from the previous [0.07053374738944575] for 1 times.\n",
      "Iteration [15454]: Loss[0.07053767573338] has not improved from the previous [0.07053390338565171] for 3 times.\n",
      "Iteration [15456]: Loss[0.07053295727428528] has not improved from the previous [0.07053221075268479] for 1 times.\n",
      "Iteration [15457]: Loss[0.07053619920728603] has not improved from the previous [0.07053295727428528] for 3 times.\n",
      "Iteration [15459]: Loss[0.07053187744269034] has not improved from the previous [0.07053077480219813] for 1 times.\n",
      "Iteration [15460]: Loss[0.07053476695187644] has not improved from the previous [0.07053187744269034] for 3 times.\n",
      "Iteration [15462]: Loss[0.07053153425970175] has not improved from the previous [0.07052939055354812] for 1 times.\n",
      "Iteration [15463]: Loss[0.07054412721812858] has not improved from the previous [0.07053153425970175] for 3 times.\n",
      "Iteration [15468]: Loss[0.07053277401760248] has not improved from the previous [0.07052652779641169] for 1 times.\n",
      "Iteration [15471]: Loss[0.07053104749240742] has not improved from the previous [0.07052603024874285] for 1 times.\n",
      "Iteration [15474]: Loss[0.07052949476694204] has not improved from the previous [0.07052519165289493] for 1 times.\n",
      "Iteration [15476]: Loss[0.07052436884329334] has not improved from the previous [0.07052405896296393] for 1 times.\n",
      "Iteration [15477]: Loss[0.07053885681547252] has not improved from the previous [0.07052436884329334] for 3 times.\n",
      "Iteration [15480]: Loss[0.07052362499431873] has not improved from the previous [0.07052302064468113] for 1 times.\n",
      "Iteration [15482]: Loss[0.07052182797507944] has not improved from the previous [0.07052090873955923] for 1 times.\n",
      "Iteration [15483]: Loss[0.0705227310371198] has not improved from the previous [0.07052182797507944] for 3 times.\n",
      "Iteration [15485]: Loss[0.07052062270852348] has not improved from the previous [0.07051918728981382] for 1 times.\n",
      "Iteration [15486]: Loss[0.07052148550731098] has not improved from the previous [0.07052062270852348] for 3 times.\n",
      "Iteration [15488]: Loss[0.07051945916559949] has not improved from the previous [0.07051772780355951] for 1 times.\n",
      "Iteration [15489]: Loss[0.07052014398721056] has not improved from the previous [0.07051945916559949] for 3 times.\n",
      "Iteration [15491]: Loss[0.07051829339157685] has not improved from the previous [0.07051630778563714] for 1 times.\n",
      "Iteration [15492]: Loss[0.0705188016592295] has not improved from the previous [0.07051829339157685] for 3 times.\n",
      "Iteration [15494]: Loss[0.07051712151795124] has not improved from the previous [0.07051489779749653] for 1 times.\n",
      "Iteration [15495]: Loss[0.07051746540800674] has not improved from the previous [0.07051712151795124] for 3 times.\n",
      "Iteration [15497]: Loss[0.07052418866890114] has not improved from the previous [0.07051423056298015] for 1 times.\n",
      "Iteration [15503]: Loss[0.07051759203706931] has not improved from the previous [0.07051358947439082] for 1 times.\n",
      "Iteration [15505]: Loss[0.07051309571758364] has not improved from the previous [0.0705119378945741] for 1 times.\n",
      "Iteration [15506]: Loss[0.07052674318444273] has not improved from the previous [0.07051309571758364] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 15500 Loss 0.07051444969378473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [15509]: Loss[0.07051135277866323] has not improved from the previous [0.07051040065286186] for 1 times.\n",
      "Iteration [15512]: Loss[0.07051044155166503] has not improved from the previous [0.07050934500026373] for 1 times.\n",
      "Iteration [15515]: Loss[0.07050914443823893] has not improved from the previous [0.07050825889878191] for 1 times.\n",
      "Iteration [15517]: Loss[0.0705071482329119] has not improved from the previous [0.07050675233032747] for 1 times.\n",
      "Iteration [15518]: Loss[0.07050785338039382] has not improved from the previous [0.0705071482329119] for 3 times.\n",
      "Iteration [15520]: Loss[0.07050602345334045] has not improved from the previous [0.07050524745269071] for 1 times.\n",
      "Iteration [15521]: Loss[0.07050650324841848] has not improved from the previous [0.07050602345334045] for 3 times.\n",
      "Iteration [15523]: Loss[0.07050489774857578] has not improved from the previous [0.07050378333698028] for 1 times.\n",
      "Iteration [15524]: Loss[0.0705052125329923] has not improved from the previous [0.07050489774857578] for 3 times.\n",
      "Iteration [15526]: Loss[0.07051194312859073] has not improved from the previous [0.070503001448556] for 1 times.\n",
      "Iteration [15532]: Loss[0.07050602849684265] has not improved from the previous [0.07050185155118835] for 1 times.\n",
      "Iteration [15534]: Loss[0.07050415957481587] has not improved from the previous [0.07049975772553704] for 1 times.\n",
      "Iteration [15535]: Loss[0.07051444019691643] has not improved from the previous [0.07050415957481587] for 3 times.\n",
      "Iteration [15538]: Loss[0.07049913405258415] has not improved from the previous [0.07049825972841031] for 1 times.\n",
      "Iteration [15541]: Loss[0.07049836642050185] has not improved from the previous [0.0704970808428998] for 1 times.\n",
      "Iteration [15544]: Loss[0.07049713273021353] has not improved from the previous [0.07049594725119533] for 1 times.\n",
      "Iteration [15547]: Loss[0.07049586480009236] has not improved from the previous [0.0704948474841501] for 1 times.\n",
      "Iteration [15550]: Loss[0.07049454840175574] has not improved from the previous [0.07049378098824025] for 1 times.\n",
      "Iteration [15552]: Loss[0.0704927415221006] has not improved from the previous [0.07049258630233639] for 1 times.\n",
      "Iteration [15553]: Loss[0.070493198417763] has not improved from the previous [0.0704927415221006] for 3 times.\n",
      "Iteration [15555]: Loss[0.07049994243254937] has not improved from the previous [0.07049173767849926] for 1 times.\n",
      "Iteration [15559]: Loss[0.07049740311482972] has not improved from the previous [0.07049081539083711] for 1 times.\n",
      "Iteration [15561]: Loss[0.07049071543647767] has not improved from the previous [0.07048944057840402] for 1 times.\n",
      "Iteration [15562]: Loss[0.07049301800985742] has not improved from the previous [0.07049071543647767] for 3 times.\n",
      "Iteration [15564]: Loss[0.07048948912380625] has not improved from the previous [0.0704875731787968] for 1 times.\n",
      "Iteration [15565]: Loss[0.07049155112115905] has not improved from the previous [0.07048948912380625] for 3 times.\n",
      "Iteration [15567]: Loss[0.07048809268451853] has not improved from the previous [0.07048624011421441] for 1 times.\n",
      "Iteration [15568]: Loss[0.07049029203137754] has not improved from the previous [0.07048809268451853] for 3 times.\n",
      "Iteration [15570]: Loss[0.07048728528481817] has not improved from the previous [0.07048496183190697] for 1 times.\n",
      "Iteration [15571]: Loss[0.07049962840295027] has not improved from the previous [0.07048728528481817] for 3 times.\n",
      "Iteration [15577]: Loss[0.07048458600683297] has not improved from the previous [0.07048342598275273] for 1 times.\n",
      "Iteration [15578]: Loss[0.07049041871280742] has not improved from the previous [0.07048458600683297] for 3 times.\n",
      "Iteration [15581]: Loss[0.07048189073739446] has not improved from the previous [0.07048169133893238] for 1 times.\n",
      "Iteration [15582]: Loss[0.07048537402787454] has not improved from the previous [0.07048189073739446] for 3 times.\n",
      "Iteration [15584]: Loss[0.0704814154947146] has not improved from the previous [0.07047973989645051] for 1 times.\n",
      "Iteration [15585]: Loss[0.07048374116539681] has not improved from the previous [0.0704814154947146] for 3 times.\n",
      "Iteration [15587]: Loss[0.07048037686106508] has not improved from the previous [0.0704782761873854] for 1 times.\n",
      "Iteration [15588]: Loss[0.07048232155806948] has not improved from the previous [0.07048037686106508] for 3 times.\n",
      "Iteration [15590]: Loss[0.0704792105507055] has not improved from the previous [0.07047688488035388] for 1 times.\n",
      "Iteration [15591]: Loss[0.0704808890948539] has not improved from the previous [0.0704792105507055] for 3 times.\n",
      "Iteration [15593]: Loss[0.0704780894924197] has not improved from the previous [0.07047548857495689] for 1 times.\n",
      "Iteration [15594]: Loss[0.07047955239655801] has not improved from the previous [0.0704780894924197] for 3 times.\n",
      "Iteration [15596]: Loss[0.07047685183789063] has not improved from the previous [0.07047415743884032] for 1 times.\n",
      "Iteration [15597]: Loss[0.07047818770547958] has not improved from the previous [0.07047685183789063] for 3 times.\n",
      "Iteration [15599]: Loss[0.07047571158818082] has not improved from the previous [0.07047278588572066] for 1 times.\n",
      "Iteration [15600]: Loss[0.07047685875337328] has not improved from the previous [0.07047571158818082] for 3 times.\n",
      "Iteration [15602]: Loss[0.07047390484726883] has not improved from the previous [0.07047148812168497] for 1 times.\n",
      "Iteration [15604]: Loss[0.07048143877613189] has not improved from the previous [0.07047081793164406] for 1 times.\n",
      "Iteration [15605]: Loss[0.07048517102675743] has not improved from the previous [0.07048143877613189] for 3 times.\n",
      "Iteration [15609]: Loss[0.0704716680434548] has not improved from the previous [0.07046969056605484] for 1 times.\n",
      "Iteration [15610]: Loss[0.07048438507183112] has not improved from the previous [0.0704716680434548] for 3 times.\n",
      "Iteration [15613]: Loss[0.0704693512081094] has not improved from the previous [0.07046713130118816] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 15600 Loss 0.07047685875337328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [15615]: Loss[0.07046811087843827] has not improved from the previous [0.0704677898669044] for 1 times.\n",
      "Iteration [15619]: Loss[0.07046671700881993] has not improved from the previous [0.07046582909408637] for 1 times.\n",
      "Iteration [15622]: Loss[0.07046541837332466] has not improved from the previous [0.07046449679518112] for 1 times.\n",
      "Iteration [15625]: Loss[0.07046408232866569] has not improved from the previous [0.07046341351324903] for 1 times.\n",
      "Iteration [15628]: Loss[0.07046273145802914] has not improved from the previous [0.0704624185985666] for 1 times.\n",
      "Iteration [15630]: Loss[0.07046934729084856] has not improved from the previous [0.07046104551659112] for 1 times.\n",
      "Iteration [15633]: Loss[0.07046239289249834] has not improved from the previous [0.0704615754957566] for 1 times.\n",
      "Iteration [15635]: Loss[0.07046189922432085] has not improved from the previous [0.07045910198326177] for 1 times.\n",
      "Iteration [15636]: Loss[0.07046282891196816] has not improved from the previous [0.07046189922432085] for 3 times.\n",
      "Iteration [15638]: Loss[0.07045826257155915] has not improved from the previous [0.07045758170944738] for 1 times.\n",
      "Iteration [15639]: Loss[0.07045886834173846] has not improved from the previous [0.07045826257155915] for 3 times.\n",
      "Iteration [15641]: Loss[0.07045705134171432] has not improved from the previous [0.07045626754021803] for 1 times.\n",
      "Iteration [15642]: Loss[0.07045748304620927] has not improved from the previous [0.07045705134171432] for 3 times.\n",
      "Iteration [15644]: Loss[0.07045944973096455] has not improved from the previous [0.07045493457420017] for 1 times.\n",
      "Iteration [15645]: Loss[0.07046961509311223] has not improved from the previous [0.07045944973096455] for 3 times.\n",
      "Iteration [15648]: Loss[0.07045561959117008] has not improved from the previous [0.07045505943772097] for 1 times.\n",
      "Iteration [15650]: Loss[0.07045818833385571] has not improved from the previous [0.07045439695897414] for 1 times.\n",
      "Iteration [15652]: Loss[0.07045434461559315] has not improved from the previous [0.07045227964628956] for 1 times.\n",
      "Iteration [15653]: Loss[0.0704562803746547] has not improved from the previous [0.07045434461559315] for 3 times.\n",
      "Iteration [15655]: Loss[0.07045322299759293] has not improved from the previous [0.07045073438010625] for 1 times.\n",
      "Iteration [15656]: Loss[0.07046547374992858] has not improved from the previous [0.07045322299759293] for 3 times.\n",
      "Iteration [15659]: Loss[0.07045126922574986] has not improved from the previous [0.07044993304941041] for 1 times.\n",
      "Iteration [15661]: Loss[0.07045492998154196] has not improved from the previous [0.070448785825036] for 1 times.\n",
      "Iteration [15663]: Loss[0.07044940487730267] has not improved from the previous [0.07044854857874845] for 1 times.\n",
      "Iteration [15664]: Loss[0.07045235090801027] has not improved from the previous [0.07044940487730267] for 3 times.\n",
      "Iteration [15666]: Loss[0.07044827183489717] has not improved from the previous [0.07044685090365155] for 1 times.\n",
      "Iteration [15667]: Loss[0.07045081801459527] has not improved from the previous [0.07044827183489717] for 3 times.\n",
      "Iteration [15669]: Loss[0.0704471089724951] has not improved from the previous [0.07044536536495234] for 1 times.\n",
      "Iteration [15670]: Loss[0.07044940447776932] has not improved from the previous [0.0704471089724951] for 3 times.\n",
      "Iteration [15672]: Loss[0.0704459290529223] has not improved from the previous [0.07044395336234646] for 1 times.\n",
      "Iteration [15673]: Loss[0.07044799563115964] has not improved from the previous [0.0704459290529223] for 3 times.\n",
      "Iteration [15675]: Loss[0.0704448058654225] has not improved from the previous [0.07044253342732941] for 1 times.\n",
      "Iteration [15676]: Loss[0.07044653706548938] has not improved from the previous [0.0704448058654225] for 3 times.\n",
      "Iteration [15678]: Loss[0.07044440106547208] has not improved from the previous [0.07044114582044934] for 1 times.\n",
      "Iteration [15680]: Loss[0.07045149252411269] has not improved from the previous [0.07044193233504499] for 1 times.\n",
      "Iteration [15684]: Loss[0.07044486886644478] has not improved from the previous [0.07043907502423861] for 1 times.\n",
      "Iteration [15686]: Loss[0.07043910628159293] has not improved from the previous [0.07043905217686429] for 1 times.\n",
      "Iteration [15687]: Loss[0.07044297168890325] has not improved from the previous [0.07043910628159293] for 3 times.\n",
      "Iteration [15689]: Loss[0.07043856919046539] has not improved from the previous [0.07043742053680446] for 1 times.\n",
      "Iteration [15690]: Loss[0.0704445248789165] has not improved from the previous [0.07043856919046539] for 3 times.\n",
      "Iteration [15692]: Loss[0.07043898477166714] has not improved from the previous [0.07043669862242657] for 1 times.\n",
      "Iteration [15693]: Loss[0.07045091818922138] has not improved from the previous [0.07043898477166714] for 3 times.\n",
      "Iteration [15698]: Loss[0.07043936051061121] has not improved from the previous [0.07043352867821366] for 1 times.\n",
      "Iteration [15701]: Loss[0.0704407653700208] has not improved from the previous [0.0704333741116871] for 1 times.\n",
      "Iteration [15703]: Loss[0.0704330066312119] has not improved from the previous [0.07043288635302089] for 1 times.\n",
      "Iteration [15705]: Loss[0.07043211822098469] has not improved from the previous [0.07043133773140817] for 1 times.\n",
      "Iteration [15708]: Loss[0.07043398318117149] has not improved from the previous [0.07042997618714054] for 1 times.\n",
      "Iteration [15709]: Loss[0.0704446158595542] has not improved from the previous [0.07043398318117149] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 15700 Loss 0.0704333741116871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [15712]: Loss[0.07043076370194042] has not improved from the previous [0.07042848368989305] for 1 times.\n",
      "Iteration [15714]: Loss[0.07042833381040417] has not improved from the previous [0.07042808946468547] for 1 times.\n",
      "Iteration [15718]: Loss[0.07042699737789192] has not improved from the previous [0.07042665677450329] for 1 times.\n",
      "Iteration [15720]: Loss[0.07042553961038778] has not improved from the previous [0.07042501010762763] for 1 times.\n",
      "Iteration [15721]: Loss[0.07042567374946673] has not improved from the previous [0.07042553961038778] for 3 times.\n",
      "Iteration [15723]: Loss[0.07042919145220239] has not improved from the previous [0.07042340084595725] for 1 times.\n",
      "Iteration [15725]: Loss[0.07042402577911018] has not improved from the previous [0.07042354968976858] for 1 times.\n",
      "Iteration [15726]: Loss[0.07042485194472428] has not improved from the previous [0.07042402577911018] for 3 times.\n",
      "Iteration [15727]: Loss[0.07043342266790621] has not improved from the previous [0.07042485194472428] for 5 times.\n",
      "Iteration [15729]: Loss[0.0704232330368713] has not improved from the previous [0.07042298188366296] for 1 times.\n",
      "Iteration [15731]: Loss[0.07042142410519213] has not improved from the previous [0.07042011675000673] for 1 times.\n",
      "Iteration [15732]: Loss[0.07042174925140889] has not improved from the previous [0.07042142410519213] for 3 times.\n",
      "Iteration [15734]: Loss[0.07042440449287957] has not improved from the previous [0.07041881574289496] for 1 times.\n",
      "Iteration [15737]: Loss[0.07042416330016052] has not improved from the previous [0.07041800023930478] for 1 times.\n",
      "Iteration [15739]: Loss[0.0704190972086467] has not improved from the previous [0.0704182034225778] for 1 times.\n",
      "Iteration [15740]: Loss[0.0704327427019901] has not improved from the previous [0.0704190972086467] for 3 times.\n",
      "Iteration [15743]: Loss[0.07041915719461185] has not improved from the previous [0.07041710150336686] for 1 times.\n",
      "Iteration [15745]: Loss[0.07041647283071013] has not improved from the previous [0.07041485929192057] for 1 times.\n",
      "Iteration [15746]: Loss[0.07041693868080902] has not improved from the previous [0.07041647283071013] for 3 times.\n",
      "Iteration [15748]: Loss[0.07041469084593364] has not improved from the previous [0.07041342790375678] for 1 times.\n",
      "Iteration [15749]: Loss[0.0704153631591824] has not improved from the previous [0.07041469084593364] for 3 times.\n",
      "Iteration [15751]: Loss[0.0704131110106659] has not improved from the previous [0.0704124457003393] for 1 times.\n",
      "Iteration [15752]: Loss[0.07041501574038718] has not improved from the previous [0.0704131110106659] for 3 times.\n",
      "Iteration [15753]: Loss[0.07042598798662456] has not improved from the previous [0.07041501574038718] for 5 times.\n",
      "Iteration [15756]: Loss[0.07041247159734852] has not improved from the previous [0.07041168446863393] for 1 times.\n",
      "Iteration [15757]: Loss[0.07041513900562282] has not improved from the previous [0.07041247159734852] for 3 times.\n",
      "Iteration [15759]: Loss[0.07041189774489225] has not improved from the previous [0.07040943406161648] for 1 times.\n",
      "Iteration [15760]: Loss[0.07041341972099292] has not improved from the previous [0.07041189774489225] for 3 times.\n",
      "Iteration [15762]: Loss[0.07041126087121789] has not improved from the previous [0.07040792154766747] for 1 times.\n",
      "Iteration [15763]: Loss[0.07041527628782744] has not improved from the previous [0.07041126087121789] for 3 times.\n",
      "Iteration [15765]: Loss[0.0704090065904546] has not improved from the previous [0.07040791232624743] for 1 times.\n",
      "Iteration [15767]: Loss[0.07041532583098956] has not improved from the previous [0.07040792246491084] for 1 times.\n",
      "Iteration [15771]: Loss[0.07041339347732388] has not improved from the previous [0.07040593332158257] for 1 times.\n",
      "Iteration [15773]: Loss[0.07040594246186617] has not improved from the previous [0.0704052253205882] for 1 times.\n",
      "Iteration [15774]: Loss[0.07040865439833253] has not improved from the previous [0.07040594246186617] for 3 times.\n",
      "Iteration [15776]: Loss[0.07040469229984488] has not improved from the previous [0.07040307553014792] for 1 times.\n",
      "Iteration [15777]: Loss[0.07040703988360612] has not improved from the previous [0.07040469229984488] for 3 times.\n",
      "Iteration [15779]: Loss[0.07040336416149803] has not improved from the previous [0.07040157343187448] for 1 times.\n",
      "Iteration [15781]: Loss[0.07040336439787138] has not improved from the previous [0.07040134214258169] for 1 times.\n",
      "Iteration [15782]: Loss[0.0704057252348276] has not improved from the previous [0.07040336439787138] for 3 times.\n",
      "Iteration [15784]: Loss[0.07040557948444434] has not improved from the previous [0.07039943038074555] for 1 times.\n",
      "Iteration [15785]: Loss[0.07041390926225305] has not improved from the previous [0.07040557948444434] for 3 times.\n",
      "Iteration [15788]: Loss[0.07039961722389047] has not improved from the previous [0.0703995711852389] for 1 times.\n",
      "Iteration [15790]: Loss[0.07040347943728659] has not improved from the previous [0.0703986469530632] for 1 times.\n",
      "Iteration [15792]: Loss[0.07039902440825249] has not improved from the previous [0.07039704816683952] for 1 times.\n",
      "Iteration [15793]: Loss[0.07040083246297144] has not improved from the previous [0.07039902440825249] for 3 times.\n",
      "Iteration [15795]: Loss[0.070397747728843] has not improved from the previous [0.07039525261664357] for 1 times.\n",
      "Iteration [15796]: Loss[0.070399245956513] has not improved from the previous [0.070397747728843] for 3 times.\n",
      "Iteration [15798]: Loss[0.07039686177715933] has not improved from the previous [0.07039373060647063] for 1 times.\n",
      "Iteration [15799]: Loss[0.07040115360396695] has not improved from the previous [0.07039686177715933] for 3 times.\n",
      "Iteration [15801]: Loss[0.07039484180484068] has not improved from the previous [0.0703933292150273] for 1 times.\n",
      "Iteration [15802]: Loss[0.07039619527282867] has not improved from the previous [0.07039484180484068] for 3 times.\n",
      "Iteration [15803]: Loss[0.0704011039322339] has not improved from the previous [0.07039619527282867] for 5 times.\n",
      "Iteration [15807]: Loss[0.07039915740736542] has not improved from the previous [0.0703922152731356] for 1 times.\n",
      "Iteration [15809]: Loss[0.070391818618006] has not improved from the previous [0.07039095652983533] for 1 times.\n",
      "Iteration [15810]: Loss[0.07039430815322524] has not improved from the previous [0.070391818618006] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 15800 Loss 0.0703933292150273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [15812]: Loss[0.07039069563481612] has not improved from the previous [0.07038870953729069] for 1 times.\n",
      "Iteration [15813]: Loss[0.07039260045642436] has not improved from the previous [0.07039069563481612] for 3 times.\n",
      "Iteration [15815]: Loss[0.07038972321272773] has not improved from the previous [0.07038714590549623] for 1 times.\n",
      "Iteration [15816]: Loss[0.07039458219073277] has not improved from the previous [0.07038972321272773] for 3 times.\n",
      "Iteration [15818]: Loss[0.07038829098779953] has not improved from the previous [0.0703878445089277] for 1 times.\n",
      "Iteration [15820]: Loss[0.07039450526545403] has not improved from the previous [0.07038752974057078] for 1 times.\n",
      "Iteration [15823]: Loss[0.07038779358980418] has not improved from the previous [0.07038592901942699] for 1 times.\n",
      "Iteration [15825]: Loss[0.07038577166700925] has not improved from the previous [0.07038471836888034] for 1 times.\n",
      "Iteration [15826]: Loss[0.07039893447110918] has not improved from the previous [0.07038577166700925] for 3 times.\n",
      "Iteration [15829]: Loss[0.07038440222015337] has not improved from the previous [0.07038267478398395] for 1 times.\n",
      "Iteration [15831]: Loss[0.0703881466001585] has not improved from the previous [0.07038191518159569] for 1 times.\n",
      "Iteration [15833]: Loss[0.07038245980555036] has not improved from the previous [0.07038160513395486] for 1 times.\n",
      "Iteration [15834]: Loss[0.0703853539383274] has not improved from the previous [0.07038245980555036] for 3 times.\n",
      "Iteration [15836]: Loss[0.07038129647854703] has not improved from the previous [0.0703797005214695] for 1 times.\n",
      "Iteration [15837]: Loss[0.07038366460529975] has not improved from the previous [0.07038129647854703] for 3 times.\n",
      "Iteration [15839]: Loss[0.07038110654539671] has not improved from the previous [0.07037809738621323] for 1 times.\n",
      "Iteration [15840]: Loss[0.07038552298142849] has not improved from the previous [0.07038110654539671] for 3 times.\n",
      "Iteration [15842]: Loss[0.07037996280097181] has not improved from the previous [0.0703776031622132] for 1 times.\n",
      "Iteration [15843]: Loss[0.07038099418885921] has not improved from the previous [0.07037996280097181] for 3 times.\n",
      "Iteration [15845]: Loss[0.07037857383594716] has not improved from the previous [0.07037541332253801] for 1 times.\n",
      "Iteration [15846]: Loss[0.07037935775311899] has not improved from the previous [0.07037857383594716] for 3 times.\n",
      "Iteration [15848]: Loss[0.07037757195209017] has not improved from the previous [0.07037408059616082] for 1 times.\n",
      "Iteration [15850]: Loss[0.0703835175115342] has not improved from the previous [0.07037382071384098] for 1 times.\n",
      "Iteration [15851]: Loss[0.07038781754458003] has not improved from the previous [0.0703835175115342] for 3 times.\n",
      "Iteration [15855]: Loss[0.07037415106780416] has not improved from the previous [0.07037260534815229] for 1 times.\n",
      "Iteration [15856]: Loss[0.07037732567763272] has not improved from the previous [0.07037415106780416] for 3 times.\n",
      "Iteration [15858]: Loss[0.07037518227725954] has not improved from the previous [0.0703708459162787] for 1 times.\n",
      "Iteration [15859]: Loss[0.07038522532840226] has not improved from the previous [0.07037518227725954] for 3 times.\n",
      "Iteration [15862]: Loss[0.07037181225246524] has not improved from the previous [0.07037078026451649] for 1 times.\n",
      "Iteration [15864]: Loss[0.07037413514386126] has not improved from the previous [0.07037047402821771] for 1 times.\n",
      "Iteration [15866]: Loss[0.0703701110744186] has not improved from the previous [0.07036813613498394] for 1 times.\n",
      "Iteration [15867]: Loss[0.0703719525840185] has not improved from the previous [0.0703701110744186] for 3 times.\n",
      "Iteration [15869]: Loss[0.0703693167823074] has not improved from the previous [0.07036634138851298] for 1 times.\n",
      "Iteration [15870]: Loss[0.07037390473929586] has not improved from the previous [0.0703693167823074] for 3 times.\n",
      "Iteration [15872]: Loss[0.07036884154712601] has not improved from the previous [0.07036591212387598] for 1 times.\n",
      "Iteration [15873]: Loss[0.07036923057980664] has not improved from the previous [0.07036884154712601] for 3 times.\n",
      "Iteration [15875]: Loss[0.07036736969628761] has not improved from the previous [0.07036365121493016] for 1 times.\n",
      "Iteration [15876]: Loss[0.07036757351475535] has not improved from the previous [0.07036736969628761] for 3 times.\n",
      "Iteration [15878]: Loss[0.0703660340741036] has not improved from the previous [0.07036241710044526] for 1 times.\n",
      "Iteration [15880]: Loss[0.07037204293387465] has not improved from the previous [0.07036204865506748] for 1 times.\n",
      "Iteration [15881]: Loss[0.0703760336097113] has not improved from the previous [0.07037204293387465] for 3 times.\n",
      "Iteration [15885]: Loss[0.07036295610692986] has not improved from the previous [0.07036124212833528] for 1 times.\n",
      "Iteration [15886]: Loss[0.0703655334078713] has not improved from the previous [0.07036295610692986] for 3 times.\n",
      "Iteration [15888]: Loss[0.07036359062319389] has not improved from the previous [0.07035902403696627] for 1 times.\n",
      "Iteration [15889]: Loss[0.07037336332897093] has not improved from the previous [0.07036359062319389] for 3 times.\n",
      "Iteration [15892]: Loss[0.07035999900264595] has not improved from the previous [0.07035991275796621] for 1 times.\n",
      "Iteration [15894]: Loss[0.07036232631392625] has not improved from the previous [0.07035923406464333] for 1 times.\n",
      "Iteration [15896]: Loss[0.07035888467037726] has not improved from the previous [0.0703563056527517] for 1 times.\n",
      "Iteration [15897]: Loss[0.07036015994048712] has not improved from the previous [0.07035888467037726] for 3 times.\n",
      "Iteration [15899]: Loss[0.0703587248448546] has not improved from the previous [0.07035455194770372] for 1 times.\n",
      "Iteration [15900]: Loss[0.07036211988742007] has not improved from the previous [0.0703587248448546] for 3 times.\n",
      "Iteration [15902]: Loss[0.07035754973525916] has not improved from the previous [0.07035411095765098] for 1 times.\n",
      "Iteration [15905]: Loss[0.0703539738858507] has not improved from the previous [0.07035215351152219] for 1 times.\n",
      "Iteration [15907]: Loss[0.07036651464300475] has not improved from the previous [0.07035358756057547] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 15900 Loss 0.07036211988742007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [15911]: Loss[0.07035526416682018] has not improved from the previous [0.07035196271697669] for 1 times.\n",
      "Iteration [15913]: Loss[0.07035273573705177] has not improved from the previous [0.07034946055962628] for 1 times.\n",
      "Iteration [15914]: Loss[0.07035693247469052] has not improved from the previous [0.07035273573705177] for 3 times.\n",
      "Iteration [15916]: Loss[0.07035048632509078] has not improved from the previous [0.07034980161546504] for 1 times.\n",
      "Iteration [15918]: Loss[0.07034853461614081] has not improved from the previous [0.07034781610271364] for 1 times.\n",
      "Iteration [15919]: Loss[0.07034885170599597] has not improved from the previous [0.07034853461614081] for 3 times.\n",
      "Iteration [15921]: Loss[0.07035589564474644] has not improved from the previous [0.07034622551549174] for 1 times.\n",
      "Iteration [15924]: Loss[0.07036126309523989] has not improved from the previous [0.07034703184156549] for 1 times.\n",
      "Iteration [15927]: Loss[0.07034768890883536] has not improved from the previous [0.0703458676418622] for 1 times.\n",
      "Iteration [15929]: Loss[0.07034482165176677] has not improved from the previous [0.07034395117593964] for 1 times.\n",
      "Iteration [15930]: Loss[0.07034511888006471] has not improved from the previous [0.07034482165176677] for 3 times.\n",
      "Iteration [15932]: Loss[0.0703427055372618] has not improved from the previous [0.0703424905521754] for 1 times.\n",
      "Iteration [15933]: Loss[0.07034387390641697] has not improved from the previous [0.0703427055372618] for 3 times.\n",
      "Iteration [15934]: Loss[0.07034591348804642] has not improved from the previous [0.07034387390641697] for 5 times.\n",
      "Iteration [15937]: Loss[0.07035663169179164] has not improved from the previous [0.0703421830903218] for 1 times.\n",
      "Iteration [15940]: Loss[0.07034246220635963] has not improved from the previous [0.07033966228423402] for 1 times.\n",
      "Iteration [15942]: Loss[0.07034463321996139] has not improved from the previous [0.07033966871540782] for 1 times.\n",
      "Iteration [15944]: Loss[0.07033941596109265] has not improved from the previous [0.07033851086013762] for 1 times.\n",
      "Iteration [15945]: Loss[0.07034229214616042] has not improved from the previous [0.07033941596109265] for 3 times.\n",
      "Iteration [15947]: Loss[0.07033897332258522] has not improved from the previous [0.07033667840038489] for 1 times.\n",
      "Iteration [15951]: Loss[0.07033974437390048] has not improved from the previous [0.0703369712836488] for 1 times.\n",
      "Iteration [15953]: Loss[0.07033678986412556] has not improved from the previous [0.07033383260833112] for 1 times.\n",
      "Iteration [15954]: Loss[0.07034154350712586] has not improved from the previous [0.07033678986412556] for 3 times.\n",
      "Iteration [15956]: Loss[0.070334958633819] has not improved from the previous [0.07033414642618546] for 1 times.\n",
      "Iteration [15958]: Loss[0.07034091870259207] has not improved from the previous [0.0703346263236254] for 1 times.\n",
      "Iteration [15961]: Loss[0.07033437291554935] has not improved from the previous [0.07033409602912903] for 1 times.\n",
      "Iteration [15963]: Loss[0.07033307878940029] has not improved from the previous [0.07033114642690225] for 1 times.\n",
      "Iteration [15964]: Loss[0.07033462022334175] has not improved from the previous [0.07033307878940029] for 3 times.\n",
      "Iteration [15966]: Loss[0.07033289190076243] has not improved from the previous [0.07032876677411587] for 1 times.\n",
      "Iteration [15967]: Loss[0.07033646206112841] has not improved from the previous [0.07033289190076243] for 3 times.\n",
      "Iteration [15969]: Loss[0.07033315159852153] has not improved from the previous [0.07032828711208083] for 1 times.\n",
      "Iteration [15970]: Loss[0.0703422551091752] has not improved from the previous [0.07033315159852153] for 3 times.\n",
      "Iteration [15972]: Loss[0.07032961189446031] has not improved from the previous [0.07032931745639762] for 1 times.\n",
      "Iteration [15975]: Loss[0.07033105034584786] has not improved from the previous [0.07032827967230434] for 1 times.\n",
      "Iteration [15977]: Loss[0.07032777221309894] has not improved from the previous [0.07032503538660405] for 1 times.\n",
      "Iteration [15978]: Loss[0.0703288257721079] has not improved from the previous [0.07032777221309894] for 3 times.\n",
      "Iteration [15980]: Loss[0.07032791873477515] has not improved from the previous [0.07032419520926989] for 1 times.\n",
      "Iteration [15982]: Loss[0.07032376881519964] has not improved from the previous [0.07032362655102745] for 1 times.\n",
      "Iteration [15983]: Loss[0.07032502893014125] has not improved from the previous [0.07032376881519964] for 3 times.\n",
      "Iteration [15984]: Loss[0.07032646018756611] has not improved from the previous [0.07032502893014125] for 5 times.\n",
      "Iteration [15986]: Loss[0.07032466746811193] has not improved from the previous [0.07032174779709285] for 1 times.\n",
      "Iteration [15988]: Loss[0.07032271933695516] has not improved from the previous [0.07032141879180034] for 1 times.\n",
      "Iteration [15991]: Loss[0.07033583654992212] has not improved from the previous [0.07032037183488025] for 1 times.\n",
      "Iteration [15993]: Loss[0.07032233238049065] has not improved from the previous [0.07032228340223563] for 1 times.\n",
      "Iteration [15996]: Loss[0.07032370327405232] has not improved from the previous [0.07031814653217415] for 1 times.\n",
      "Iteration [15998]: Loss[0.07031815258463248] has not improved from the previous [0.0703175852902407] for 1 times.\n",
      "Iteration [15999]: Loss[0.07032524055052057] has not improved from the previous [0.07031815258463248] for 3 times.\n",
      "Iteration [16001]: Loss[0.07031821630722479] has not improved from the previous [0.07031704072054154] for 1 times.\n",
      "Iteration [16002]: Loss[0.07032017453673381] has not improved from the previous [0.07031821630722479] for 3 times.\n",
      "Iteration [16004]: Loss[0.07031670714790424] has not improved from the previous [0.07031448771961114] for 1 times.\n",
      "Iteration [16005]: Loss[0.07031831359388174] has not improved from the previous [0.07031670714790424] for 3 times.\n",
      "Iteration [16007]: Loss[0.07031727456898725] has not improved from the previous [0.07031504274974326] for 1 times.\n",
      "Iteration [16009]: Loss[0.0703212074104721] has not improved from the previous [0.07031307613548728] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 16000 Loss 0.07031704072054154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [16011]: Loss[0.07031504716778839] has not improved from the previous [0.07031255499963647] for 1 times.\n",
      "Iteration [16012]: Loss[0.07031992011934693] has not improved from the previous [0.07031504716778839] for 3 times.\n",
      "Iteration [16014]: Loss[0.07031438669888691] has not improved from the previous [0.07031151618108812] for 1 times.\n",
      "Iteration [16015]: Loss[0.07031477244329842] has not improved from the previous [0.07031438669888691] for 3 times.\n",
      "Iteration [16017]: Loss[0.07031347934012015] has not improved from the previous [0.07030904048959756] for 1 times.\n",
      "Iteration [16018]: Loss[0.07033031964746483] has not improved from the previous [0.07031347934012015] for 3 times.\n",
      "Iteration [16022]: Loss[0.07031021709374859] has not improved from the previous [0.07030851458857798] for 1 times.\n",
      "Iteration [16023]: Loss[0.07031607725391593] has not improved from the previous [0.07031021709374859] for 3 times.\n",
      "Iteration [16025]: Loss[0.07030962993394256] has not improved from the previous [0.07030779346654036] for 1 times.\n",
      "Iteration [16026]: Loss[0.07031093237861352] has not improved from the previous [0.07030962993394256] for 3 times.\n",
      "Iteration [16028]: Loss[0.07030840980511179] has not improved from the previous [0.07030545619322666] for 1 times.\n",
      "Iteration [16030]: Loss[0.07031211703697786] has not improved from the previous [0.07030543178306044] for 1 times.\n",
      "Iteration [16031]: Loss[0.07031899267569493] has not improved from the previous [0.07031211703697786] for 3 times.\n",
      "Iteration [16034]: Loss[0.07030552564486948] has not improved from the previous [0.07030529759370517] for 1 times.\n",
      "Iteration [16036]: Loss[0.07030840729919437] has not improved from the previous [0.07030547246924616] for 1 times.\n",
      "Iteration [16038]: Loss[0.07030556539851464] has not improved from the previous [0.07030179667045344] for 1 times.\n",
      "Iteration [16041]: Loss[0.0703048605935801] has not improved from the previous [0.07030053369807168] for 1 times.\n",
      "Iteration [16042]: Loss[0.0703054025978224] has not improved from the previous [0.0703048605935801] for 3 times.\n",
      "Iteration [16044]: Loss[0.07030083396552171] has not improved from the previous [0.07030004801337449] for 1 times.\n",
      "Iteration [16047]: Loss[0.07030103857522338] has not improved from the previous [0.07029916577897417] for 1 times.\n",
      "Iteration [16049]: Loss[0.07029996221117518] has not improved from the previous [0.07029863607156002] for 1 times.\n",
      "Iteration [16052]: Loss[0.07030131741349081] has not improved from the previous [0.07029790620812006] for 1 times.\n",
      "Iteration [16054]: Loss[0.07029908847485736] has not improved from the previous [0.0702958958875267] for 1 times.\n",
      "Iteration [16056]: Loss[0.07030550125100744] has not improved from the previous [0.07029718868456898] for 1 times.\n",
      "Iteration [16059]: Loss[0.07029639391854536] has not improved from the previous [0.07029628981930751] for 1 times.\n",
      "Iteration [16061]: Loss[0.07029717717030097] has not improved from the previous [0.0702942092151556] for 1 times.\n",
      "Iteration [16062]: Loss[0.07030818270837152] has not improved from the previous [0.07029717717030097] for 3 times.\n",
      "Iteration [16065]: Loss[0.07029431946358534] has not improved from the previous [0.07029417150619496] for 1 times.\n",
      "Iteration [16067]: Loss[0.07029646827498219] has not improved from the previous [0.07029309134848143] for 1 times.\n",
      "Iteration [16069]: Loss[0.07029371923879436] has not improved from the previous [0.07029028815444915] for 1 times.\n",
      "Iteration [16070]: Loss[0.07029805368591682] has not improved from the previous [0.07029371923879436] for 3 times.\n",
      "Iteration [16072]: Loss[0.07029300601162966] has not improved from the previous [0.07028976236477534] for 1 times.\n",
      "Iteration [16075]: Loss[0.07029220090015159] has not improved from the previous [0.0702871411428335] for 1 times.\n",
      "Iteration [16076]: Loss[0.0702950490309878] has not improved from the previous [0.07029220090015159] for 3 times.\n",
      "Iteration [16078]: Loss[0.07028837644811056] has not improved from the previous [0.07028786305762591] for 1 times.\n",
      "Iteration [16080]: Loss[0.07029411472726191] has not improved from the previous [0.07028814468508605] for 1 times.\n",
      "Iteration [16085]: Loss[0.07028470629676359] has not improved from the previous [0.0702845619263073] for 1 times.\n",
      "Iteration [16086]: Loss[0.07028598705840808] has not improved from the previous [0.07028470629676359] for 3 times.\n",
      "Iteration [16088]: Loss[0.07028735431979835] has not improved from the previous [0.07028486229440443] for 1 times.\n",
      "Iteration [16091]: Loss[0.07028638248027151] has not improved from the previous [0.07028092874092977] for 1 times.\n",
      "Iteration [16094]: Loss[0.0702835857308403] has not improved from the previous [0.07028172692804575] for 1 times.\n",
      "Iteration [16096]: Loss[0.07029654306513815] has not improved from the previous [0.07028231017211155] for 1 times.\n",
      "Iteration [16099]: Loss[0.07028191068890519] has not improved from the previous [0.07028057529327023] for 1 times.\n",
      "Iteration [16101]: Loss[0.07028393911409159] has not improved from the previous [0.07027996131501027] for 1 times.\n",
      "Iteration [16103]: Loss[0.07028059227473235] has not improved from the previous [0.07027769335304301] for 1 times.\n",
      "Iteration [16104]: Loss[0.0702854847187002] has not improved from the previous [0.07028059227473235] for 3 times.\n",
      "Iteration [16106]: Loss[0.07027990539833062] has not improved from the previous [0.07027713622776116] for 1 times.\n",
      "Iteration [16107]: Loss[0.07028020991634946] has not improved from the previous [0.07027990539833062] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 16100 Loss 0.07027996131501027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [16109]: Loss[0.07027957069115363] has not improved from the previous [0.07027438872440785] for 1 times.\n",
      "Iteration [16110]: Loss[0.0702823082152591] has not improved from the previous [0.07027957069115363] for 3 times.\n",
      "Iteration [16112]: Loss[0.07027560092681082] has not improved from the previous [0.07027509170491421] for 1 times.\n",
      "Iteration [16113]: Loss[0.07027578761390481] has not improved from the previous [0.07027560092681082] for 3 times.\n",
      "Iteration [16114]: Loss[0.07028129517880312] has not improved from the previous [0.07027578761390481] for 5 times.\n",
      "Iteration [16119]: Loss[0.07027183900767704] has not improved from the previous [0.07027145591970867] for 1 times.\n",
      "Iteration [16120]: Loss[0.07027402685558287] has not improved from the previous [0.07027183900767704] for 3 times.\n",
      "Iteration [16122]: Loss[0.07027441447041738] has not improved from the previous [0.07027200616793149] for 1 times.\n",
      "Iteration [16125]: Loss[0.07027448263355737] has not improved from the previous [0.07026802533739102] for 1 times.\n",
      "Iteration [16126]: Loss[0.07028935174680682] has not improved from the previous [0.07027448263355737] for 3 times.\n",
      "Iteration [16128]: Loss[0.07027143018421143] has not improved from the previous [0.07027140405370338] for 1 times.\n",
      "Iteration [16130]: Loss[0.07027078576413133] has not improved from the previous [0.07026771798020343] for 1 times.\n",
      "Iteration [16131]: Loss[0.07027274160770194] has not improved from the previous [0.07027078576413133] for 3 times.\n",
      "Iteration [16133]: Loss[0.07026789610370127] has not improved from the previous [0.07026627914494489] for 1 times.\n",
      "Iteration [16135]: Loss[0.07027035453011389] has not improved from the previous [0.07026744941723836] for 1 times.\n",
      "Iteration [16138]: Loss[0.07027006778331508] has not improved from the previous [0.07026577567355616] for 1 times.\n",
      "Iteration [16140]: Loss[0.070266717872419] has not improved from the previous [0.07026379985643005] for 1 times.\n",
      "Iteration [16141]: Loss[0.07027156084378339] has not improved from the previous [0.070266717872419] for 3 times.\n",
      "Iteration [16143]: Loss[0.07026627217322384] has not improved from the previous [0.07026316672401423] for 1 times.\n",
      "Iteration [16144]: Loss[0.07027679344218143] has not improved from the previous [0.07026627217322384] for 3 times.\n",
      "Iteration [16146]: Loss[0.07026731097991985] has not improved from the previous [0.07026378447830071] for 1 times.\n",
      "Iteration [16148]: Loss[0.07026226091371099] has not improved from the previous [0.07026209128694806] for 1 times.\n",
      "Iteration [16149]: Loss[0.07026514444299806] has not improved from the previous [0.07026226091371099] for 3 times.\n",
      "Iteration [16151]: Loss[0.07026398765550146] has not improved from the previous [0.07026029025046121] for 1 times.\n",
      "Iteration [16153]: Loss[0.07026332946855189] has not improved from the previous [0.07025915088830302] for 1 times.\n",
      "Iteration [16154]: Loss[0.07027298505356361] has not improved from the previous [0.07026332946855189] for 3 times.\n",
      "Iteration [16156]: Loss[0.07026213790063114] has not improved from the previous [0.07026041623715223] for 1 times.\n",
      "Iteration [16158]: Loss[0.07025959572992846] has not improved from the previous [0.07025820803501018] for 1 times.\n",
      "Iteration [16159]: Loss[0.07026132881931009] has not improved from the previous [0.07025959572992846] for 3 times.\n",
      "Iteration [16161]: Loss[0.07025960506241917] has not improved from the previous [0.07025661788575972] for 1 times.\n",
      "Iteration [16162]: Loss[0.07026093448898237] has not improved from the previous [0.07025960506241917] for 3 times.\n",
      "Iteration [16164]: Loss[0.07025618214504281] has not improved from the previous [0.07025456280997094] for 1 times.\n",
      "Iteration [16165]: Loss[0.07025684678309474] has not improved from the previous [0.07025618214504281] for 3 times.\n",
      "Iteration [16166]: Loss[0.07025864818822616] has not improved from the previous [0.07025684678309474] for 5 times.\n",
      "Iteration [16169]: Loss[0.07025838432905808] has not improved from the previous [0.07025435357698685] for 1 times.\n",
      "Iteration [16171]: Loss[0.07025508297272563] has not improved from the previous [0.07025220732229182] for 1 times.\n",
      "Iteration [16172]: Loss[0.07025771216048937] has not improved from the previous [0.07025508297272563] for 3 times.\n",
      "Iteration [16174]: Loss[0.07025507791658109] has not improved from the previous [0.07025095910295386] for 1 times.\n",
      "Iteration [16175]: Loss[0.07026488264630973] has not improved from the previous [0.07025507791658109] for 3 times.\n",
      "Iteration [16177]: Loss[0.07025548587787314] has not improved from the previous [0.07025357889857749] for 1 times.\n",
      "Iteration [16179]: Loss[0.07025074735642095] has not improved from the previous [0.07025031965143101] for 1 times.\n",
      "Iteration [16180]: Loss[0.07025340110637701] has not improved from the previous [0.07025074735642095] for 3 times.\n",
      "Iteration [16182]: Loss[0.07025240513254731] has not improved from the previous [0.0702496638610706] for 1 times.\n",
      "Iteration [16184]: Loss[0.07025163568369568] has not improved from the previous [0.07024737177355213] for 1 times.\n",
      "Iteration [16185]: Loss[0.07026122821647332] has not improved from the previous [0.07025163568369568] for 3 times.\n",
      "Iteration [16187]: Loss[0.07025068068754793] has not improved from the previous [0.07024962105144045] for 1 times.\n",
      "Iteration [16189]: Loss[0.07024827072905758] has not improved from the previous [0.07024643430835341] for 1 times.\n",
      "Iteration [16190]: Loss[0.07024956370135377] has not improved from the previous [0.07024827072905758] for 3 times.\n",
      "Iteration [16192]: Loss[0.07024827653615016] has not improved from the previous [0.07024574197078386] for 1 times.\n",
      "Iteration [16193]: Loss[0.07024916182252021] has not improved from the previous [0.07024827653615016] for 3 times.\n",
      "Iteration [16195]: Loss[0.07024487994503789] has not improved from the previous [0.07024321687953357] for 1 times.\n",
      "Iteration [16197]: Loss[0.07024971155598579] has not improved from the previous [0.07024429601428808] for 1 times.\n",
      "Iteration [16199]: Loss[0.0702444540953559] has not improved from the previous [0.07024263874444447] for 1 times.\n",
      "Iteration [16200]: Loss[0.0702459472971715] has not improved from the previous [0.0702444540953559] for 3 times.\n",
      "Iteration [16202]: Loss[0.07024408389070785] has not improved from the previous [0.07024149266213227] for 1 times.\n",
      "Iteration [16203]: Loss[0.07024566140181696] has not improved from the previous [0.07024408389070785] for 3 times.\n",
      "Iteration [16205]: Loss[0.07024095115234417] has not improved from the previous [0.07023910815422582] for 1 times.\n",
      "Iteration [16207]: Loss[0.07024133083118085] has not improved from the previous [0.07024018923553928] for 1 times.\n",
      "Iteration [16209]: Loss[0.0702405372824924] has not improved from the previous [0.07024009219624265] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 16200 Loss 0.0702459472971715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [16210]: Loss[0.07025363299770812] has not improved from the previous [0.0702405372824924] for 3 times.\n",
      "Iteration [16215]: Loss[0.07024120153818797] has not improved from the previous [0.07023663064550854] for 1 times.\n",
      "Iteration [16217]: Loss[0.07023772271932917] has not improved from the previous [0.07023761851549264] for 1 times.\n",
      "Iteration [16221]: Loss[0.0702378856289413] has not improved from the previous [0.07023513548399617] for 1 times.\n",
      "Iteration [16223]: Loss[0.07023662619494775] has not improved from the previous [0.07023497599102826] for 1 times.\n",
      "Iteration [16225]: Loss[0.07023830057096873] has not improved from the previous [0.07023173304522279] for 1 times.\n",
      "Iteration [16227]: Loss[0.07023403233594254] has not improved from the previous [0.07023236746511363] for 1 times.\n",
      "Iteration [16229]: Loss[0.07024186537935131] has not improved from the previous [0.0702331347340587] for 1 times.\n",
      "Iteration [16231]: Loss[0.07023400178298296] has not improved from the previous [0.07023219404845206] for 1 times.\n",
      "Iteration [16234]: Loss[0.07023094318863086] has not improved from the previous [0.07023037163410727] for 1 times.\n",
      "Iteration [16235]: Loss[0.07024413806138782] has not improved from the previous [0.07023094318863086] for 3 times.\n",
      "Iteration [16240]: Loss[0.07023656388562123] has not improved from the previous [0.07022804337601206] for 1 times.\n",
      "Iteration [16242]: Loss[0.07022893116214544] has not improved from the previous [0.07022765256518361] for 1 times.\n",
      "Iteration [16243]: Loss[0.07023048433426785] has not improved from the previous [0.07022893116214544] for 3 times.\n",
      "Iteration [16245]: Loss[0.07022878815509016] has not improved from the previous [0.07022480636743116] for 1 times.\n",
      "Iteration [16248]: Loss[0.07022591763070352] has not improved from the previous [0.07022497472593882] for 1 times.\n",
      "Iteration [16249]: Loss[0.07022738911988223] has not improved from the previous [0.07022591763070352] for 3 times.\n",
      "Iteration [16251]: Loss[0.0702262335925408] has not improved from the previous [0.07022457426354531] for 1 times.\n",
      "Iteration [16253]: Loss[0.0702278356091886] has not improved from the previous [0.07022224773946668] for 1 times.\n",
      "Iteration [16255]: Loss[0.07022419170911165] has not improved from the previous [0.07022177571199723] for 1 times.\n",
      "Iteration [16259]: Loss[0.07023473039677974] has not improved from the previous [0.0702212339527282] for 1 times.\n",
      "Iteration [16261]: Loss[0.07022471720450955] has not improved from the previous [0.07022391024429012] for 1 times.\n",
      "Iteration [16263]: Loss[0.07021967099923816] has not improved from the previous [0.07021936746670569] for 1 times.\n",
      "Iteration [16264]: Loss[0.07022015318122742] has not improved from the previous [0.07021967099923816] for 3 times.\n",
      "Iteration [16265]: Loss[0.0702209228442506] has not improved from the previous [0.07022015318122742] for 5 times.\n",
      "Iteration [16266]: Loss[0.07022208036210462] has not improved from the previous [0.0702209228442506] for 7 times.\n",
      "Iteration [16268]: Loss[0.07022184986411502] has not improved from the previous [0.0702157703611182] for 1 times.\n",
      "Iteration [16269]: Loss[0.0702371760857653] has not improved from the previous [0.07022184986411502] for 3 times.\n",
      "Iteration [16273]: Loss[0.07021751187709663] has not improved from the previous [0.0702167269311728] for 1 times.\n",
      "Iteration [16276]: Loss[0.07021536510758487] has not improved from the previous [0.07021474146835327] for 1 times.\n",
      "Iteration [16277]: Loss[0.0702173054692692] has not improved from the previous [0.07021536510758487] for 3 times.\n",
      "Iteration [16281]: Loss[0.0702172872160404] has not improved from the previous [0.07021216663151647] for 1 times.\n",
      "Iteration [16283]: Loss[0.07021352284523244] has not improved from the previous [0.07021315344657193] for 1 times.\n",
      "Iteration [16287]: Loss[0.0702137286620027] has not improved from the previous [0.07021099285372873] for 1 times.\n",
      "Iteration [16291]: Loss[0.07022429941031234] has not improved from the previous [0.07020985337921763] for 1 times.\n",
      "Iteration [16296]: Loss[0.07021626846920062] has not improved from the previous [0.07020654817096196] for 1 times.\n",
      "Iteration [16299]: Loss[0.07021002359419475] has not improved from the previous [0.07020713675820962] for 1 times.\n",
      "Iteration [16301]: Loss[0.07020903553526768] has not improved from the previous [0.07020517428782379] for 1 times.\n",
      "Iteration [16303]: Loss[0.0702100260209176] has not improved from the previous [0.07020410868094969] for 1 times.\n",
      "Iteration [16304]: Loss[0.07021774859186211] has not improved from the previous [0.0702100260209176] for 3 times.\n",
      "Iteration [16306]: Loss[0.07020700310001442] has not improved from the previous [0.07020662367891617] for 1 times.\n",
      "Iteration [16308]: Loss[0.07020398151353682] has not improved from the previous [0.07020285901281187] for 1 times.\n",
      "Iteration [16309]: Loss[0.07021055922054098] has not improved from the previous [0.07020398151353682] for 3 times.\n",
      "Iteration [16311]: Loss[0.07020436714832097] has not improved from the previous [0.07020168310778259] for 1 times.\n",
      "Iteration [16312]: Loss[0.07020441619025801] has not improved from the previous [0.07020436714832097] for 3 times.\n",
      "Iteration [16314]: Loss[0.07020359473978457] has not improved from the previous [0.07020021780523382] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 16300 Loss 0.07020517428782379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [16316]: Loss[0.07020505070997837] has not improved from the previous [0.07019972661407782] for 1 times.\n",
      "Iteration [16318]: Loss[0.07020104792208394] has not improved from the previous [0.07019861463244839] for 1 times.\n",
      "Iteration [16320]: Loss[0.07020842376745039] has not improved from the previous [0.07019988733909303] for 1 times.\n",
      "Iteration [16322]: Loss[0.07020103874575603] has not improved from the previous [0.07019895761935918] for 1 times.\n",
      "Iteration [16325]: Loss[0.07019807271740819] has not improved from the previous [0.0701968739781444] for 1 times.\n",
      "Iteration [16326]: Loss[0.07020463148433885] has not improved from the previous [0.07019807271740819] for 3 times.\n",
      "Iteration [16328]: Loss[0.07019925698202083] has not improved from the previous [0.07019566986377894] for 1 times.\n",
      "Iteration [16329]: Loss[0.07020897777439516] has not improved from the previous [0.07019925698202083] for 3 times.\n",
      "Iteration [16331]: Loss[0.07019820170508102] has not improved from the previous [0.07019684753840623] for 1 times.\n",
      "Iteration [16333]: Loss[0.07019509473858436] has not improved from the previous [0.07019399789469317] for 1 times.\n",
      "Iteration [16334]: Loss[0.07019715501596702] has not improved from the previous [0.07019509473858436] for 3 times.\n",
      "Iteration [16336]: Loss[0.07019562051249087] has not improved from the previous [0.07019424420456068] for 1 times.\n",
      "Iteration [16337]: Loss[0.07019577337443876] has not improved from the previous [0.07019562051249087] for 3 times.\n",
      "Iteration [16339]: Loss[0.0701948248071841] has not improved from the previous [0.07019048796654942] for 1 times.\n",
      "Iteration [16341]: Loss[0.07019614274299331] has not improved from the previous [0.07019131353557218] for 1 times.\n",
      "Iteration [16343]: Loss[0.07019302402822493] has not improved from the previous [0.07018952211002327] for 1 times.\n",
      "Iteration [16344]: Loss[0.0701973483305889] has not improved from the previous [0.07019302402822493] for 3 times.\n",
      "Iteration [16346]: Loss[0.07019124339598942] has not improved from the previous [0.07018870703355297] for 1 times.\n",
      "Iteration [16347]: Loss[0.0701915191075104] has not improved from the previous [0.07019124339598942] for 3 times.\n",
      "Iteration [16349]: Loss[0.07019066943770166] has not improved from the previous [0.07018850184720267] for 1 times.\n",
      "Iteration [16351]: Loss[0.07020308513815925] has not improved from the previous [0.0701870345585643] for 1 times.\n",
      "Iteration [16356]: Loss[0.07019468025154064] has not improved from the previous [0.07018496268860289] for 1 times.\n",
      "Iteration [16358]: Loss[0.07018590212593913] has not improved from the previous [0.07018563469766818] for 1 times.\n",
      "Iteration [16359]: Loss[0.07018821096373096] has not improved from the previous [0.07018590212593913] for 3 times.\n",
      "Iteration [16361]: Loss[0.07018740899335765] has not improved from the previous [0.07018363386951483] for 1 times.\n",
      "Iteration [16363]: Loss[0.07018766181997901] has not improved from the previous [0.07018226141123837] for 1 times.\n",
      "Iteration [16364]: Loss[0.07019580345508117] has not improved from the previous [0.07018766181997901] for 3 times.\n",
      "Iteration [16368]: Loss[0.07018350787916662] has not improved from the previous [0.0701808990867453] for 1 times.\n",
      "Iteration [16369]: Loss[0.07018860235222056] has not improved from the previous [0.07018350787916662] for 3 times.\n",
      "Iteration [16371]: Loss[0.07018298345712931] has not improved from the previous [0.07017969282039824] for 1 times.\n",
      "Iteration [16374]: Loss[0.07018156569798784] has not improved from the previous [0.0701797610863448] for 1 times.\n",
      "Iteration [16376]: Loss[0.07018299774877773] has not improved from the previous [0.07017837722566897] for 1 times.\n",
      "Iteration [16378]: Loss[0.07017960447345621] has not improved from the previous [0.07017773102472986] for 1 times.\n",
      "Iteration [16381]: Loss[0.07017712281764073] has not improved from the previous [0.07017653495016844] for 1 times.\n",
      "Iteration [16382]: Loss[0.07018375916212569] has not improved from the previous [0.07017712281764073] for 3 times.\n",
      "Iteration [16385]: Loss[0.07017720319527174] has not improved from the previous [0.07017588789302211] for 1 times.\n",
      "Iteration [16386]: Loss[0.07019690442355177] has not improved from the previous [0.07017720319527174] for 3 times.\n",
      "Iteration [16389]: Loss[0.07017590161191571] has not improved from the previous [0.07017413628634714] for 1 times.\n",
      "Iteration [16394]: Loss[0.07017334499188425] has not improved from the previous [0.07017203910971462] for 1 times.\n",
      "Iteration [16395]: Loss[0.07017486843818431] has not improved from the previous [0.07017334499188425] for 3 times.\n",
      "Iteration [16396]: Loss[0.07018577598647209] has not improved from the previous [0.07017486843818431] for 5 times.\n",
      "Iteration [16398]: Loss[0.07017377033070593] has not improved from the previous [0.07017243655211347] for 1 times.\n",
      "Iteration [16400]: Loss[0.07017074538713279] has not improved from the previous [0.07017005556999524] for 1 times.\n",
      "Iteration [16401]: Loss[0.07017326364124213] has not improved from the previous [0.07017074538713279] for 3 times.\n",
      "Iteration [16403]: Loss[0.07017132128170449] has not improved from the previous [0.0701701862394386] for 1 times.\n",
      "Iteration [16404]: Loss[0.07017155750565293] has not improved from the previous [0.07017132128170449] for 3 times.\n",
      "Iteration [16406]: Loss[0.07017058594709352] has not improved from the previous [0.07016670074475884] for 1 times.\n",
      "Iteration [16408]: Loss[0.07017181835075544] has not improved from the previous [0.07016712890582133] for 1 times.\n",
      "Iteration [16410]: Loss[0.0701680801323016] has not improved from the previous [0.07016585766610482] for 1 times.\n",
      "Iteration [16411]: Loss[0.07017081248798984] has not improved from the previous [0.0701680801323016] for 3 times.\n",
      "Iteration [16413]: Loss[0.07016847140544273] has not improved from the previous [0.07016378080026653] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 16400 Loss 0.07017074538713279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [16414]: Loss[0.07018524360169581] has not improved from the previous [0.07016847140544273] for 3 times.\n",
      "Iteration [16418]: Loss[0.0701660897986081] has not improved from the previous [0.07016490800816863] for 1 times.\n",
      "Iteration [16420]: Loss[0.0701651107937121] has not improved from the previous [0.07016197392643445] for 1 times.\n",
      "Iteration [16421]: Loss[0.07016523386090986] has not improved from the previous [0.0701651107937121] for 3 times.\n",
      "Iteration [16423]: Loss[0.07016535247161598] has not improved from the previous [0.07016236581772668] for 1 times.\n",
      "Iteration [16426]: Loss[0.07016285981670689] has not improved from the previous [0.07015951319466496] for 1 times.\n",
      "Iteration [16428]: Loss[0.07017498816378455] has not improved from the previous [0.07016182660665814] for 1 times.\n",
      "Iteration [16432]: Loss[0.07015910002371448] has not improved from the previous [0.07015889936309114] for 1 times.\n",
      "Iteration [16433]: Loss[0.07016652078916379] has not improved from the previous [0.07015910002371448] for 3 times.\n",
      "Iteration [16435]: Loss[0.07015964924318001] has not improved from the previous [0.07015740340479963] for 1 times.\n",
      "Iteration [16436]: Loss[0.07015993385112099] has not improved from the previous [0.07015964924318001] for 3 times.\n",
      "Iteration [16438]: Loss[0.07015974850722324] has not improved from the previous [0.0701561409783979] for 1 times.\n",
      "Iteration [16440]: Loss[0.07015508985927889] has not improved from the previous [0.07015469944726531] for 1 times.\n",
      "Iteration [16441]: Loss[0.07015713092794026] has not improved from the previous [0.07015508985927889] for 3 times.\n",
      "Iteration [16443]: Loss[0.07015520777671848] has not improved from the previous [0.07015501433100747] for 1 times.\n",
      "Iteration [16446]: Loss[0.07016129205383698] has not improved from the previous [0.07015400968459334] for 1 times.\n",
      "Iteration [16448]: Loss[0.07015325312620649] has not improved from the previous [0.07015241428664859] for 1 times.\n",
      "Iteration [16449]: Loss[0.07015492694246102] has not improved from the previous [0.07015325312620649] for 3 times.\n",
      "Iteration [16450]: Loss[0.07016580296677223] has not improved from the previous [0.07015492694246102] for 5 times.\n",
      "Iteration [16453]: Loss[0.07015546674620553] has not improved from the previous [0.0701501728546653] for 1 times.\n",
      "Iteration [16455]: Loss[0.07015288001765811] has not improved from the previous [0.07014951355971813] for 1 times.\n",
      "Iteration [16456]: Loss[0.07016274323958192] has not improved from the previous [0.07015288001765811] for 3 times.\n",
      "Iteration [16458]: Loss[0.0701531960432205] has not improved from the previous [0.07015293104853966] for 1 times.\n",
      "Iteration [16460]: Loss[0.07015174925880119] has not improved from the previous [0.07014766570643263] for 1 times.\n",
      "Iteration [16461]: Loss[0.07015538101621384] has not improved from the previous [0.07015174925880119] for 3 times.\n",
      "Iteration [16463]: Loss[0.07014775808080993] has not improved from the previous [0.0701473731339606] for 1 times.\n",
      "Iteration [16464]: Loss[0.07014852808263684] has not improved from the previous [0.07014775808080993] for 3 times.\n",
      "Iteration [16465]: Loss[0.07015525834085258] has not improved from the previous [0.07014852808263684] for 5 times.\n",
      "Iteration [16467]: Loss[0.0701471572445445] has not improved from the previous [0.07014611783489243] for 1 times.\n",
      "Iteration [16468]: Loss[0.07014864838527982] has not improved from the previous [0.0701471572445445] for 3 times.\n",
      "Iteration [16470]: Loss[0.07014781311269866] has not improved from the previous [0.0701460361074098] for 1 times.\n",
      "Iteration [16472]: Loss[0.07014335189293032] has not improved from the previous [0.07014313305025908] for 1 times.\n",
      "Iteration [16473]: Loss[0.07014671773309739] has not improved from the previous [0.07014335189293032] for 3 times.\n",
      "Iteration [16475]: Loss[0.07014356956927584] has not improved from the previous [0.07014337618653115] for 1 times.\n",
      "Iteration [16477]: Loss[0.07014315906637214] has not improved from the previous [0.07014277713143233] for 1 times.\n",
      "Iteration [16478]: Loss[0.07014978176888641] has not improved from the previous [0.07014315906637214] for 3 times.\n",
      "Iteration [16480]: Loss[0.07014187474283556] has not improved from the previous [0.07014090640813772] for 1 times.\n",
      "Iteration [16482]: Loss[0.07014399547592605] has not improved from the previous [0.07014106733959814] for 1 times.\n",
      "Iteration [16483]: Loss[0.07015083815893618] has not improved from the previous [0.07014399547592605] for 3 times.\n",
      "Iteration [16485]: Loss[0.07014348955748595] has not improved from the previous [0.07013936620639721] for 1 times.\n",
      "Iteration [16487]: Loss[0.07013939005243981] has not improved from the previous [0.07013775635262433] for 1 times.\n",
      "Iteration [16488]: Loss[0.07014341895497733] has not improved from the previous [0.07013939005243981] for 3 times.\n",
      "Iteration [16491]: Loss[0.070141797090642] has not improved from the previous [0.07013691699267832] for 1 times.\n",
      "Iteration [16492]: Loss[0.07015824474867609] has not improved from the previous [0.070141797090642] for 3 times.\n",
      "Iteration [16495]: Loss[0.07013824433144505] has not improved from the previous [0.07013647792030073] for 1 times.\n",
      "Iteration [16497]: Loss[0.07014016323092717] has not improved from the previous [0.07013663615291102] for 1 times.\n",
      "Iteration [16499]: Loss[0.07013891818838837] has not improved from the previous [0.07013303972112488] for 1 times.\n",
      "Iteration [16500]: Loss[0.07014100484061088] has not improved from the previous [0.07013891818838837] for 3 times.\n",
      "Iteration [16502]: Loss[0.07013409426906181] has not improved from the previous [0.0701336272100198] for 1 times.\n",
      "Iteration [16504]: Loss[0.07013453835361169] has not improved from the previous [0.07013368979538272] for 1 times.\n",
      "Iteration [16507]: Loss[0.07014013299203976] has not improved from the previous [0.070131970760601] for 1 times.\n",
      "Iteration [16509]: Loss[0.07013353637384641] has not improved from the previous [0.07013067956142219] for 1 times.\n",
      "Iteration [16510]: Loss[0.07014363263728392] has not improved from the previous [0.07013353637384641] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 16500 Loss 0.07014100484061088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [16512]: Loss[0.07013531310294092] has not improved from the previous [0.07013372531425244] for 1 times.\n",
      "Iteration [16514]: Loss[0.0701322388837851] has not improved from the previous [0.0701285700932836] for 1 times.\n",
      "Iteration [16516]: Loss[0.07013125364272434] has not improved from the previous [0.07012944556524968] for 1 times.\n",
      "Iteration [16519]: Loss[0.07013000323728023] has not improved from the previous [0.07012732440409836] for 1 times.\n",
      "Iteration [16521]: Loss[0.07013113045050019] has not improved from the previous [0.07012726580122297] for 1 times.\n",
      "Iteration [16523]: Loss[0.07012832627807555] has not improved from the previous [0.07012684344391208] for 1 times.\n",
      "Iteration [16524]: Loss[0.07012995972598424] has not improved from the previous [0.07012832627807555] for 3 times.\n",
      "Iteration [16526]: Loss[0.0701316111033891] has not improved from the previous [0.07012282830170928] for 1 times.\n",
      "Iteration [16527]: Loss[0.0701443044262155] has not improved from the previous [0.0701316111033891] for 3 times.\n",
      "Iteration [16529]: Loss[0.0701273324939087] has not improved from the previous [0.07012589232647977] for 1 times.\n",
      "Iteration [16533]: Loss[0.07013186792035564] has not improved from the previous [0.0701226672484458] for 1 times.\n",
      "Iteration [16535]: Loss[0.0701231515841543] has not improved from the previous [0.07012251578405894] for 1 times.\n",
      "Iteration [16536]: Loss[0.07012479999632153] has not improved from the previous [0.0701231515841543] for 3 times.\n",
      "Iteration [16538]: Loss[0.07012375986659526] has not improved from the previous [0.07012174942097704] for 1 times.\n",
      "Iteration [16540]: Loss[0.07011912858884625] has not improved from the previous [0.07011905765103767] for 1 times.\n",
      "Iteration [16541]: Loss[0.07012300587850338] has not improved from the previous [0.07011912858884625] for 3 times.\n",
      "Iteration [16543]: Loss[0.0701193453499056] has not improved from the previous [0.07011919109004196] for 1 times.\n",
      "Iteration [16545]: Loss[0.07011944939342692] has not improved from the previous [0.07011868158527575] for 1 times.\n",
      "Iteration [16546]: Loss[0.07013950096100247] has not improved from the previous [0.07011944939342692] for 3 times.\n",
      "Iteration [16548]: Loss[0.0701204117274003] has not improved from the previous [0.0701199646853734] for 1 times.\n",
      "Iteration [16552]: Loss[0.07011869354179227] has not improved from the previous [0.07011578980193352] for 1 times.\n",
      "Iteration [16553]: Loss[0.07012026181127427] has not improved from the previous [0.07011869354179227] for 3 times.\n",
      "Iteration [16555]: Loss[0.0701203348106565] has not improved from the previous [0.07011299534121684] for 1 times.\n",
      "Iteration [16556]: Loss[0.07013444973084897] has not improved from the previous [0.0701203348106565] for 3 times.\n",
      "Iteration [16558]: Loss[0.07011692564672685] has not improved from the previous [0.07011571206456661] for 1 times.\n",
      "Iteration [16562]: Loss[0.07012185005864872] has not improved from the previous [0.07011251536034796] for 1 times.\n",
      "Iteration [16564]: Loss[0.07011310719605476] has not improved from the previous [0.07011245039556488] for 1 times.\n",
      "Iteration [16565]: Loss[0.07011472254109677] has not improved from the previous [0.07011310719605476] for 3 times.\n",
      "Iteration [16567]: Loss[0.07011353698221556] has not improved from the previous [0.07011178728448272] for 1 times.\n",
      "Iteration [16570]: Loss[0.0701131458382427] has not improved from the previous [0.07010891601281873] for 1 times.\n",
      "Iteration [16572]: Loss[0.07010922070521804] has not improved from the previous [0.07010904062243868] for 1 times.\n",
      "Iteration [16574]: Loss[0.07010970468186539] has not improved from the previous [0.07010847463719706] for 1 times.\n",
      "Iteration [16575]: Loss[0.07011546445563002] has not improved from the previous [0.07010970468186539] for 3 times.\n",
      "Iteration [16577]: Loss[0.07010886236045658] has not improved from the previous [0.07010659500528188] for 1 times.\n",
      "Iteration [16579]: Loss[0.07010939690344242] has not improved from the previous [0.07010684563183335] for 1 times.\n",
      "Iteration [16580]: Loss[0.07011631949092685] has not improved from the previous [0.07010939690344242] for 3 times.\n",
      "Iteration [16582]: Loss[0.07010894167152441] has not improved from the previous [0.07010681733322077] for 1 times.\n",
      "Iteration [16584]: Loss[0.07010629076403521] has not improved from the previous [0.07010362488162332] for 1 times.\n",
      "Iteration [16586]: Loss[0.0701093716055112] has not improved from the previous [0.07010555194797181] for 1 times.\n",
      "Iteration [16588]: Loss[0.07010926930285657] has not improved from the previous [0.07010194310360698] for 1 times.\n",
      "Iteration [16589]: Loss[0.07012339374141299] has not improved from the previous [0.07010926930285657] for 3 times.\n",
      "Iteration [16594]: Loss[0.07010454961260748] has not improved from the previous [0.07010293818285412] for 1 times.\n",
      "Iteration [16596]: Loss[0.07010348867895878] has not improved from the previous [0.07010112627076232] for 1 times.\n",
      "Iteration [16598]: Loss[0.07010448814447608] has not improved from the previous [0.07009910849699363] for 1 times.\n",
      "Iteration [16603]: Loss[0.0701009191732921] has not improved from the previous [0.070097481932624] for 1 times.\n",
      "Iteration [16604]: Loss[0.07010473137545231] has not improved from the previous [0.0701009191732921] for 3 times.\n",
      "Iteration [16606]: Loss[0.07009855103215887] has not improved from the previous [0.07009734736275418] for 1 times.\n",
      "Iteration [16609]: Loss[0.07010567368943339] has not improved from the previous [0.07009740677666838] for 1 times.\n",
      "Iteration [16611]: Loss[0.07009836798955625] has not improved from the previous [0.07009788492244651] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 16600 Loss 0.07010063493337226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [16613]: Loss[0.070096352609749] has not improved from the previous [0.07009413475884395] for 1 times.\n",
      "Iteration [16615]: Loss[0.07009877322658317] has not improved from the previous [0.07009585201235417] for 1 times.\n",
      "Iteration [16617]: Loss[0.0700969502838634] has not improved from the previous [0.07009162143977142] for 1 times.\n",
      "Iteration [16618]: Loss[0.07009715420474394] has not improved from the previous [0.0700969502838634] for 3 times.\n",
      "Iteration [16620]: Loss[0.07009444325493638] has not improved from the previous [0.07009069652666242] for 1 times.\n",
      "Iteration [16622]: Loss[0.07009688138131392] has not improved from the previous [0.0700921691455995] for 1 times.\n",
      "Iteration [16624]: Loss[0.07009778122159886] has not improved from the previous [0.07008936542271324] for 1 times.\n",
      "Iteration [16625]: Loss[0.07011077052487442] has not improved from the previous [0.07009778122159886] for 3 times.\n",
      "Iteration [16627]: Loss[0.07009254846978272] has not improved from the previous [0.07009182275813969] for 1 times.\n",
      "Iteration [16630]: Loss[0.07009182176531127] has not improved from the previous [0.07008995603569777] for 1 times.\n",
      "Iteration [16632]: Loss[0.07009069727119055] has not improved from the previous [0.07008939204063556] for 1 times.\n",
      "Iteration [16634]: Loss[0.07009178557397618] has not improved from the previous [0.07008621485895072] for 1 times.\n",
      "Iteration [16637]: Loss[0.07010044653761074] has not improved from the previous [0.07008779852649298] for 1 times.\n",
      "Iteration [16639]: Loss[0.07009031568712216] has not improved from the previous [0.07008907172275332] for 1 times.\n",
      "Iteration [16641]: Loss[0.07008731840538396] has not improved from the previous [0.07008459848477318] for 1 times.\n",
      "Iteration [16643]: Loss[0.07008644844785315] has not improved from the previous [0.0700854615136564] for 1 times.\n",
      "Iteration [16644]: Loss[0.07008682930545584] has not improved from the previous [0.07008644844785315] for 3 times.\n",
      "Iteration [16646]: Loss[0.07008648348977627] has not improved from the previous [0.07008387308388464] for 1 times.\n",
      "Iteration [16649]: Loss[0.07008454024090345] has not improved from the previous [0.07008199708532172] for 1 times.\n",
      "Iteration [16651]: Loss[0.07009608667148472] has not improved from the previous [0.07008303517163934] for 1 times.\n",
      "Iteration [16655]: Loss[0.07008319128388199] has not improved from the previous [0.07007979662226087] for 1 times.\n",
      "Iteration [16656]: Loss[0.07008738532388654] has not improved from the previous [0.07008319128388199] for 3 times.\n",
      "Iteration [16658]: Loss[0.07008128075869888] has not improved from the previous [0.07007819876844375] for 1 times.\n",
      "Iteration [16660]: Loss[0.07008027512660246] has not improved from the previous [0.0700784729808387] for 1 times.\n",
      "Iteration [16662]: Loss[0.07007875805272813] has not improved from the previous [0.07007847033187407] for 1 times.\n",
      "Iteration [16663]: Loss[0.07008553517224081] has not improved from the previous [0.07007875805272813] for 3 times.\n",
      "Iteration [16665]: Loss[0.07007863807193679] has not improved from the previous [0.07007590653323119] for 1 times.\n",
      "Iteration [16666]: Loss[0.07008367190458467] has not improved from the previous [0.07007863807193679] for 3 times.\n",
      "Iteration [16669]: Loss[0.07007670984587111] has not improved from the previous [0.07007588377391923] for 1 times.\n",
      "Iteration [16671]: Loss[0.07008440924441675] has not improved from the previous [0.07007605039000689] for 1 times.\n",
      "Iteration [16673]: Loss[0.07007711590232792] has not improved from the previous [0.07007607138290951] for 1 times.\n",
      "Iteration [16675]: Loss[0.07007491566362994] has not improved from the previous [0.0700728796299358] for 1 times.\n",
      "Iteration [16677]: Loss[0.0700772919190817] has not improved from the previous [0.07007441773278311] for 1 times.\n",
      "Iteration [16679]: Loss[0.0700755146491404] has not improved from the previous [0.07007043890329492] for 1 times.\n",
      "Iteration [16680]: Loss[0.07007558943355467] has not improved from the previous [0.0700755146491404] for 3 times.\n",
      "Iteration [16682]: Loss[0.07007347395482913] has not improved from the previous [0.07006938740898663] for 1 times.\n",
      "Iteration [16684]: Loss[0.07007521727067188] has not improved from the previous [0.07007067298974881] for 1 times.\n",
      "Iteration [16686]: Loss[0.07007221380970137] has not improved from the previous [0.07006849825490624] for 1 times.\n",
      "Iteration [16687]: Loss[0.07007325928479571] has not improved from the previous [0.07007221380970137] for 3 times.\n",
      "Iteration [16689]: Loss[0.07007130278630443] has not improved from the previous [0.07006661213895435] for 1 times.\n",
      "Iteration [16691]: Loss[0.07008444661891693] has not improved from the previous [0.07006850033208809] for 1 times.\n",
      "Iteration [16693]: Loss[0.07007011907915361] has not improved from the previous [0.07006978958588028] for 1 times.\n",
      "Iteration [16695]: Loss[0.07006787872273627] has not improved from the previous [0.07006656486971007] for 1 times.\n",
      "Iteration [16696]: Loss[0.07007422844429294] has not improved from the previous [0.07006787872273627] for 3 times.\n",
      "Iteration [16698]: Loss[0.07006696040204359] has not improved from the previous [0.07006475116445184] for 1 times.\n",
      "Iteration [16699]: Loss[0.0700725488660167] has not improved from the previous [0.07006696040204359] for 3 times.\n",
      "Iteration [16702]: Loss[0.07006513354622393] has not improved from the previous [0.07006467256600503] for 1 times.\n",
      "Iteration [16703]: Loss[0.07006523106854902] has not improved from the previous [0.07006513354622393] for 3 times.\n",
      "Iteration [16704]: Loss[0.07007314493291694] has not improved from the previous [0.07006523106854902] for 5 times.\n",
      "Iteration [16706]: Loss[0.0700658189592188] has not improved from the previous [0.07006468451661939] for 1 times.\n",
      "Iteration [16708]: Loss[0.07006387670609887] has not improved from the previous [0.07006132393398871] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 16700 Loss 0.07006503478846915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [16710]: Loss[0.07006595015619843] has not improved from the previous [0.07006281189913387] for 1 times.\n",
      "Iteration [16712]: Loss[0.07006395387939958] has not improved from the previous [0.07005943820879156] for 1 times.\n",
      "Iteration [16713]: Loss[0.07006416325205045] has not improved from the previous [0.07006395387939958] for 3 times.\n",
      "Iteration [16715]: Loss[0.07006249334525051] has not improved from the previous [0.07005792319363006] for 1 times.\n",
      "Iteration [16717]: Loss[0.07006379289980429] has not improved from the previous [0.07005910976783848] for 1 times.\n",
      "Iteration [16719]: Loss[0.07006066125973735] has not improved from the previous [0.07005753026474] for 1 times.\n",
      "Iteration [16720]: Loss[0.07006176803123348] has not improved from the previous [0.07006066125973735] for 3 times.\n",
      "Iteration [16722]: Loss[0.07006021933854746] has not improved from the previous [0.07005525990582917] for 1 times.\n",
      "Iteration [16723]: Loss[0.07006058159397506] has not improved from the previous [0.07006021933854746] for 3 times.\n",
      "Iteration [16724]: Loss[0.07007151171794972] has not improved from the previous [0.07006058159397506] for 5 times.\n",
      "Iteration [16728]: Loss[0.07005740693783329] has not improved from the previous [0.07005493187664881] for 1 times.\n",
      "Iteration [16729]: Loss[0.07006247996342874] has not improved from the previous [0.07005740693783329] for 3 times.\n",
      "Iteration [16731]: Loss[0.07005643936297852] has not improved from the previous [0.07005307017045503] for 1 times.\n",
      "Iteration [16732]: Loss[0.07006085928318245] has not improved from the previous [0.07005643936297852] for 3 times.\n",
      "Iteration [16734]: Loss[0.07005402626005176] has not improved from the previous [0.0700536484651992] for 1 times.\n",
      "Iteration [16737]: Loss[0.07006148092402337] has not improved from the previous [0.07005338133999274] for 1 times.\n",
      "Iteration [16741]: Loss[0.07005311662931896] has not improved from the previous [0.07005006786678272] for 1 times.\n",
      "Iteration [16743]: Loss[0.07005433565989834] has not improved from the previous [0.07005138438597648] for 1 times.\n",
      "Iteration [16745]: Loss[0.07005243703832018] has not improved from the previous [0.0700486547384572] for 1 times.\n",
      "Iteration [16746]: Loss[0.07005255478225364] has not improved from the previous [0.07005243703832018] for 3 times.\n",
      "Iteration [16748]: Loss[0.07005100637511658] has not improved from the previous [0.07004729895550693] for 1 times.\n",
      "Iteration [16750]: Loss[0.0700622494817089] has not improved from the previous [0.07004986956910404] for 1 times.\n",
      "Iteration [16754]: Loss[0.070048792679728] has not improved from the previous [0.07004614991058146] for 1 times.\n",
      "Iteration [16757]: Loss[0.07004753593973527] has not improved from the previous [0.07004485693603688] for 1 times.\n",
      "Iteration [16758]: Loss[0.07005203651362052] has not improved from the previous [0.07004753593973527] for 3 times.\n",
      "Iteration [16760]: Loss[0.07004611839554932] has not improved from the previous [0.07004466986092313] for 1 times.\n",
      "Iteration [16764]: Loss[0.07004530859374591] has not improved from the previous [0.07004255223583679] for 1 times.\n",
      "Iteration [16766]: Loss[0.07004636050689102] has not improved from the previous [0.07004269314031568] for 1 times.\n",
      "Iteration [16767]: Loss[0.07005456791241041] has not improved from the previous [0.07004636050689102] for 3 times.\n",
      "Iteration [16771]: Loss[0.0700433925413637] has not improved from the previous [0.07004048468363083] for 1 times.\n",
      "Iteration [16774]: Loss[0.07004340330118423] has not improved from the previous [0.07003832711920768] for 1 times.\n",
      "Iteration [16776]: Loss[0.0700390574911741] has not improved from the previous [0.0700385791051012] for 1 times.\n",
      "Iteration [16777]: Loss[0.07004297386085066] has not improved from the previous [0.0700390574911741] for 3 times.\n",
      "Iteration [16779]: Loss[0.07004098422649535] has not improved from the previous [0.07003936903090706] for 1 times.\n",
      "Iteration [16781]: Loss[0.07003949354393994] has not improved from the previous [0.07003753681878802] for 1 times.\n",
      "Iteration [16783]: Loss[0.07005084231103484] has not improved from the previous [0.07003712046265118] for 1 times.\n",
      "Iteration [16787]: Loss[0.070037473913324] has not improved from the previous [0.07003590340744818] for 1 times.\n",
      "Iteration [16789]: Loss[0.07003868134061475] has not improved from the previous [0.07003370746848576] for 1 times.\n",
      "Iteration [16790]: Loss[0.07005495677266556] has not improved from the previous [0.07003868134061475] for 3 times.\n",
      "Iteration [16795]: Loss[0.07003535849295538] has not improved from the previous [0.0700343753536624] for 1 times.\n",
      "Iteration [16799]: Loss[0.07003989876579823] has not improved from the previous [0.07003248942087602] for 1 times.\n",
      "Iteration [16801]: Loss[0.07003250902405479] has not improved from the previous [0.07003005562289276] for 1 times.\n",
      "Iteration [16802]: Loss[0.0700377771667337] has not improved from the previous [0.07003250902405479] for 3 times.\n",
      "Iteration [16804]: Loss[0.07003023897545557] has not improved from the previous [0.07002908420016478] for 1 times.\n",
      "Iteration [16806]: Loss[0.07003448318508051] has not improved from the previous [0.07002946955124692] for 1 times.\n",
      "Iteration [16808]: Loss[0.0700320126000225] has not improved from the previous [0.07002660461258584] for 1 times.\n",
      "Iteration [16809]: Loss[0.07004786666469376] has not improved from the previous [0.0700320126000225] for 3 times.\n",
      "Iteration [16811]: Loss[0.07003117252006764] has not improved from the previous [0.07002866243242815] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 16800 Loss 0.07003005562289276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [16814]: Loss[0.07002722047956231] has not improved from the previous [0.07002672194303904] for 1 times.\n",
      "Iteration [16815]: Loss[0.07002919414408368] has not improved from the previous [0.07002722047956231] for 3 times.\n",
      "Iteration [16818]: Loss[0.07003356140666561] has not improved from the previous [0.0700246959253642] for 1 times.\n",
      "Iteration [16820]: Loss[0.07002543118094512] has not improved from the previous [0.07002355074436122] for 1 times.\n",
      "Iteration [16821]: Loss[0.07003114428578223] has not improved from the previous [0.07002543118094512] for 3 times.\n",
      "Iteration [16823]: Loss[0.07002446306944554] has not improved from the previous [0.07002359044116974] for 1 times.\n",
      "Iteration [16826]: Loss[0.07002676911710112] has not improved from the previous [0.0700220897928675] for 1 times.\n",
      "Iteration [16827]: Loss[0.07004324924009757] has not improved from the previous [0.07002676911710112] for 3 times.\n",
      "Iteration [16830]: Loss[0.0700224044049461] has not improved from the previous [0.0700219528551101] for 1 times.\n",
      "Iteration [16832]: Loss[0.070023255411874] has not improved from the previous [0.07002094648627073] for 1 times.\n",
      "Iteration [16836]: Loss[0.07002320977233693] has not improved from the previous [0.07001965605250664] for 1 times.\n",
      "Iteration [16839]: Loss[0.07002593120134955] has not improved from the previous [0.07001833046102658] for 1 times.\n",
      "Iteration [16841]: Loss[0.07001914154734598] has not improved from the previous [0.07001596697534732] for 1 times.\n",
      "Iteration [16842]: Loss[0.07003711438427912] has not improved from the previous [0.07001914154734598] for 3 times.\n",
      "Iteration [16844]: Loss[0.07001972690394215] has not improved from the previous [0.07001827115718395] for 1 times.\n",
      "Iteration [16847]: Loss[0.07001578358383559] has not improved from the previous [0.0700157344224083] for 1 times.\n",
      "Iteration [16848]: Loss[0.07002321631342856] has not improved from the previous [0.07001578358383559] for 3 times.\n",
      "Iteration [16850]: Loss[0.07001549127852753] has not improved from the previous [0.07001350501597356] for 1 times.\n",
      "Iteration [16851]: Loss[0.07002114986130595] has not improved from the previous [0.07001549127852753] for 3 times.\n",
      "Iteration [16853]: Loss[0.07001470838338156] has not improved from the previous [0.0700132111465851] for 1 times.\n",
      "Iteration [16856]: Loss[0.07001626374501076] has not improved from the previous [0.07001209197752793] for 1 times.\n",
      "Iteration [16857]: Loss[0.07003319632243347] has not improved from the previous [0.07001626374501076] for 3 times.\n",
      "Iteration [16860]: Loss[0.07001231251268197] has not improved from the previous [0.07001200609878967] for 1 times.\n",
      "Iteration [16862]: Loss[0.07001355920418621] has not improved from the previous [0.07001076378581429] for 1 times.\n",
      "Iteration [16864]: Loss[0.07001147800131662] has not improved from the previous [0.07001038547274525] for 1 times.\n",
      "Iteration [16866]: Loss[0.07001079710428808] has not improved from the previous [0.07001071294236265] for 1 times.\n",
      "Iteration [16869]: Loss[0.07001631678642925] has not improved from the previous [0.07000718422602131] for 1 times.\n",
      "Iteration [16871]: Loss[0.07000879171468721] has not improved from the previous [0.07000609669679046] for 1 times.\n",
      "Iteration [16872]: Loss[0.07001373795407394] has not improved from the previous [0.07000879171468721] for 3 times.\n",
      "Iteration [16875]: Loss[0.07000680265920488] has not improved from the previous [0.07000648242909562] for 1 times.\n",
      "Iteration [16877]: Loss[0.0700135471879149] has not improved from the previous [0.0700053152687848] for 1 times.\n",
      "Iteration [16881]: Loss[0.07000502773181322] has not improved from the previous [0.07000409201029853] for 1 times.\n",
      "Iteration [16883]: Loss[0.07000546729554188] has not improved from the previous [0.0700048490948208] for 1 times.\n",
      "Iteration [16886]: Loss[0.07000395568421947] has not improved from the previous [0.07000272372693755] for 1 times.\n",
      "Iteration [16887]: Loss[0.07002355862820468] has not improved from the previous [0.07000395568421947] for 3 times.\n",
      "Iteration [16892]: Loss[0.07000399459805651] has not improved from the previous [0.07000032770543452] for 1 times.\n",
      "Iteration [16894]: Loss[0.07000114509040462] has not improved from the previous [0.07000026866612914] for 1 times.\n",
      "Iteration [16896]: Loss[0.07000136937224545] has not improved from the previous [0.07000111295676201] for 1 times.\n",
      "Iteration [16898]: Loss[0.06999934465739219] has not improved from the previous [0.06999817350824447] for 1 times.\n",
      "Iteration [16899]: Loss[0.07000554014677175] has not improved from the previous [0.06999934465739219] for 3 times.\n",
      "Iteration [16901]: Loss[0.06999953860883695] has not improved from the previous [0.06999568884950291] for 1 times.\n",
      "Iteration [16902]: Loss[0.07000329640598073] has not improved from the previous [0.06999953860883695] for 3 times.\n",
      "Iteration [16904]: Loss[0.0699972385496915] has not improved from the previous [0.06999642010460362] for 1 times.\n",
      "Iteration [16908]: Loss[0.06999841389616301] has not improved from the previous [0.0699935370160446] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 16900 Loss 0.06999568884950291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [16910]: Loss[0.07000172776466199] has not improved from the previous [0.06999224083178988] for 1 times.\n",
      "Iteration [16911]: Loss[0.07001344165246291] has not improved from the previous [0.07000172776466199] for 3 times.\n",
      "Iteration [16913]: Loss[0.06999644704498792] has not improved from the previous [0.06999365042304723] for 1 times.\n",
      "Iteration [16915]: Loss[0.06999372622229877] has not improved from the previous [0.06999254568916806] for 1 times.\n",
      "Iteration [16916]: Loss[0.06999482090937788] has not improved from the previous [0.06999372622229877] for 3 times.\n",
      "Iteration [16919]: Loss[0.0699935200786087] has not improved from the previous [0.06999048568840333] for 1 times.\n",
      "Iteration [16920]: Loss[0.06999779216778795] has not improved from the previous [0.0699935200786087] for 3 times.\n",
      "Iteration [16922]: Loss[0.06999268556668632] has not improved from the previous [0.06998808141429276] for 1 times.\n",
      "Iteration [16924]: Loss[0.06999418701357883] has not improved from the previous [0.06998867525578467] for 1 times.\n",
      "Iteration [16926]: Loss[0.06999058133252814] has not improved from the previous [0.06998854525322122] for 1 times.\n",
      "Iteration [16928]: Loss[0.06999517556170429] has not improved from the previous [0.06998853299128364] for 1 times.\n",
      "Iteration [16931]: Loss[0.06998869244611293] has not improved from the previous [0.06998789244834905] for 1 times.\n",
      "Iteration [16933]: Loss[0.06998837617582787] has not improved from the previous [0.06998681747053871] for 1 times.\n",
      "Iteration [16934]: Loss[0.0699885500857969] has not improved from the previous [0.06998837617582787] for 3 times.\n",
      "Iteration [16937]: Loss[0.06998963138373943] has not improved from the previous [0.06998461322932814] for 1 times.\n",
      "Iteration [16938]: Loss[0.07000553856113678] has not improved from the previous [0.06998963138373943] for 3 times.\n",
      "Iteration [16940]: Loss[0.06998682360945428] has not improved from the previous [0.06998550943881748] for 1 times.\n",
      "Iteration [16943]: Loss[0.06999089547774015] has not improved from the previous [0.06998410143323018] for 1 times.\n",
      "Iteration [16945]: Loss[0.06998361042465409] has not improved from the previous [0.0699819404270846] for 1 times.\n",
      "Iteration [16947]: Loss[0.0699867267419793] has not improved from the previous [0.06998231607658405] for 1 times.\n",
      "Iteration [16949]: Loss[0.06998381442447697] has not improved from the previous [0.06998019382753985] for 1 times.\n",
      "Iteration [16952]: Loss[0.06998408859586831] has not improved from the previous [0.06997835275801445] for 1 times.\n",
      "Iteration [16954]: Loss[0.06997970529581529] has not improved from the previous [0.06997871732821459] for 1 times.\n",
      "Iteration [16955]: Loss[0.06998308716059096] has not improved from the previous [0.06997970529581529] for 3 times.\n",
      "Iteration [16958]: Loss[0.06999326426945736] has not improved from the previous [0.06997776203599106] for 1 times.\n",
      "Iteration [16961]: Loss[0.06998029903280766] has not improved from the previous [0.06997846792390028] for 1 times.\n",
      "Iteration [16963]: Loss[0.0699800056041175] has not improved from the previous [0.06997440934082687] for 1 times.\n",
      "Iteration [16966]: Loss[0.06997757805456295] has not improved from the previous [0.0699745476414656] for 1 times.\n",
      "Iteration [16968]: Loss[0.06998827945337101] has not improved from the previous [0.06997598402060279] for 1 times.\n",
      "Iteration [16972]: Loss[0.06997577817324306] has not improved from the previous [0.0699740879484334] for 1 times.\n",
      "Iteration [16974]: Loss[0.06997161261304032] has not improved from the previous [0.06997147154401981] for 1 times.\n",
      "Iteration [16975]: Loss[0.06997583923825673] has not improved from the previous [0.06997161261304032] for 3 times.\n",
      "Iteration [16977]: Loss[0.06997648612671939] has not improved from the previous [0.06997027390793913] for 1 times.\n",
      "Iteration [16980]: Loss[0.069973419018367] has not improved from the previous [0.069971876541799] for 1 times.\n",
      "Iteration [16983]: Loss[0.06997126699036199] has not improved from the previous [0.0699697565511526] for 1 times.\n",
      "Iteration [16984]: Loss[0.0699770308038855] has not improved from the previous [0.06997126699036199] for 3 times.\n",
      "Iteration [16986]: Loss[0.06997184858412638] has not improved from the previous [0.06996706815213993] for 1 times.\n",
      "Iteration [16987]: Loss[0.06998812637056671] has not improved from the previous [0.06997184858412638] for 3 times.\n",
      "Iteration [16989]: Loss[0.06997303277885321] has not improved from the previous [0.06996956350830127] for 1 times.\n",
      "Iteration [16992]: Loss[0.06996844810173716] has not improved from the previous [0.06996819771998822] for 1 times.\n",
      "Iteration [16994]: Loss[0.06996906349129399] has not improved from the previous [0.06996586357551073] for 1 times.\n",
      "Iteration [16995]: Loss[0.06998688404194854] has not improved from the previous [0.06996906349129399] for 3 times.\n",
      "Iteration [16997]: Loss[0.06996781929281497] has not improved from the previous [0.06996699644924731] for 1 times.\n",
      "Iteration [17000]: Loss[0.06997243561260645] has not improved from the previous [0.06996503871953952] for 1 times.\n",
      "Iteration [17002]: Loss[0.06996525717840618] has not improved from the previous [0.06996314950571553] for 1 times.\n",
      "Iteration [17004]: Loss[0.06996365098028] has not improved from the previous [0.06996340329136465] for 1 times.\n",
      "Iteration [17006]: Loss[0.06996517055295487] has not improved from the previous [0.06996179066843211] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 17000 Loss 0.06997243561260645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [17009]: Loss[0.06996500528705005] has not improved from the previous [0.06995982937025538] for 1 times.\n",
      "Iteration [17011]: Loss[0.06996518138326496] has not improved from the previous [0.06995881658220611] for 1 times.\n",
      "Iteration [17014]: Loss[0.06996221147735918] has not improved from the previous [0.06996039112940075] for 1 times.\n",
      "Iteration [17017]: Loss[0.06996013662564542] has not improved from the previous [0.06995857764016426] for 1 times.\n",
      "Iteration [17018]: Loss[0.06997929692947585] has not improved from the previous [0.06996013662564542] for 3 times.\n",
      "Iteration [17020]: Loss[0.06996202761674662] has not improved from the previous [0.06995925833686556] for 1 times.\n",
      "Iteration [17024]: Loss[0.0699580584953556] has not improved from the previous [0.06995761276455108] for 1 times.\n",
      "Iteration [17025]: Loss[0.06995852315953564] has not improved from the previous [0.0699580584953556] for 3 times.\n",
      "Iteration [17028]: Loss[0.06995721023661235] has not improved from the previous [0.0699550993194273] for 1 times.\n",
      "Iteration [17029]: Loss[0.06997589911720653] has not improved from the previous [0.06995721023661235] for 3 times.\n",
      "Iteration [17031]: Loss[0.06995724007199355] has not improved from the previous [0.06995574143496484] for 1 times.\n",
      "Iteration [17034]: Loss[0.06996099752134595] has not improved from the previous [0.06995401097470681] for 1 times.\n",
      "Iteration [17036]: Loss[0.06995457141548372] has not improved from the previous [0.06995191960274783] for 1 times.\n",
      "Iteration [17040]: Loss[0.0699540494102429] has not improved from the previous [0.06995106661434423] for 1 times.\n",
      "Iteration [17042]: Loss[0.06995571721111256] has not improved from the previous [0.06994861737346457] for 1 times.\n",
      "Iteration [17043]: Loss[0.06996966120932128] has not improved from the previous [0.06995571721111256] for 3 times.\n",
      "Iteration [17045]: Loss[0.0699538126025615] has not improved from the previous [0.06994976496665753] for 1 times.\n",
      "Iteration [17047]: Loss[0.06995157365148626] has not improved from the previous [0.06994858612240862] for 1 times.\n",
      "Iteration [17048]: Loss[0.06995527635141141] has not improved from the previous [0.06995157365148626] for 3 times.\n",
      "Iteration [17050]: Loss[0.06994930166189939] has not improved from the previous [0.06994798510045017] for 1 times.\n",
      "Iteration [17052]: Loss[0.0699509827115563] has not improved from the previous [0.06994837714286138] for 1 times.\n",
      "Iteration [17054]: Loss[0.06994898416895431] has not improved from the previous [0.0699461806969326] for 1 times.\n",
      "Iteration [17056]: Loss[0.06995094804801424] has not improved from the previous [0.06994490844613209] for 1 times.\n",
      "Iteration [17059]: Loss[0.0699479932026688] has not improved from the previous [0.0699461227317221] for 1 times.\n",
      "Iteration [17062]: Loss[0.0699452439909163] has not improved from the previous [0.06994369290845925] for 1 times.\n",
      "Iteration [17065]: Loss[0.06994603933860537] has not improved from the previous [0.06994158615874502] for 1 times.\n",
      "Iteration [17066]: Loss[0.06994856362780501] has not improved from the previous [0.06994603933860537] for 3 times.\n",
      "Iteration [17068]: Loss[0.06994449173232353] has not improved from the previous [0.0699410414807532] for 1 times.\n",
      "Iteration [17070]: Loss[0.06995463486805452] has not improved from the previous [0.06994227109501522] for 1 times.\n",
      "Iteration [17074]: Loss[0.06994259585930312] has not improved from the previous [0.06994131521832818] for 1 times.\n",
      "Iteration [17076]: Loss[0.06994274254534433] has not improved from the previous [0.06993766477514989] for 1 times.\n",
      "Iteration [17079]: Loss[0.06994029506014765] has not improved from the previous [0.06993752994758194] for 1 times.\n",
      "Iteration [17081]: Loss[0.0699507704374472] has not improved from the previous [0.06993940287319993] for 1 times.\n",
      "Iteration [17085]: Loss[0.0699380322313421] has not improved from the previous [0.06993792439384966] for 1 times.\n",
      "Iteration [17087]: Loss[0.0699399200333402] has not improved from the previous [0.06993500933235582] for 1 times.\n",
      "Iteration [17090]: Loss[0.0699429879857637] has not improved from the previous [0.06993547819983172] for 1 times.\n",
      "Iteration [17092]: Loss[0.06993735194514861] has not improved from the previous [0.06993249421589026] for 1 times.\n",
      "Iteration [17093]: Loss[0.06993998587374382] has not improved from the previous [0.06993735194514861] for 3 times.\n",
      "Iteration [17095]: Loss[0.06993546840341151] has not improved from the previous [0.06993288552648885] for 1 times.\n",
      "Iteration [17098]: Loss[0.06993589639156382] has not improved from the previous [0.06993132640978784] for 1 times.\n",
      "Iteration [17101]: Loss[0.06993864178168625] has not improved from the previous [0.06993016242607683] for 1 times.\n",
      "Iteration [17103]: Loss[0.06993386787460205] has not improved from the previous [0.06993038561957055] for 1 times.\n",
      "Iteration [17104]: Loss[0.06993663831030318] has not improved from the previous [0.06993386787460205] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 17100 Loss 0.06993016242607683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [17106]: Loss[0.06993222349944732] has not improved from the previous [0.06992815257623522] for 1 times.\n",
      "Iteration [17108]: Loss[0.06993282647391219] has not improved from the previous [0.06992880725072854] for 1 times.\n",
      "Iteration [17110]: Loss[0.06992978032246584] has not improved from the previous [0.06992887189441205] for 1 times.\n",
      "Iteration [17111]: Loss[0.06993036098727551] has not improved from the previous [0.06992978032246584] for 3 times.\n",
      "Iteration [17114]: Loss[0.06992870712468403] has not improved from the previous [0.06992629360495062] for 1 times.\n",
      "Iteration [17116]: Loss[0.06993376094350456] has not improved from the previous [0.06992735367771442] for 1 times.\n",
      "Iteration [17119]: Loss[0.06992723143473793] has not improved from the previous [0.06992616685759613] for 1 times.\n",
      "Iteration [17121]: Loss[0.069926930033483] has not improved from the previous [0.06992490792815931] for 1 times.\n",
      "Iteration [17122]: Loss[0.06993148468583138] has not improved from the previous [0.069926930033483] for 3 times.\n",
      "Iteration [17124]: Loss[0.06992480441348811] has not improved from the previous [0.06992290085076658] for 1 times.\n",
      "Iteration [17126]: Loss[0.0699267521209147] has not improved from the previous [0.06992461642871438] for 1 times.\n",
      "Iteration [17128]: Loss[0.06992543888212542] has not improved from the previous [0.06992179142336627] for 1 times.\n",
      "Iteration [17132]: Loss[0.06992383172031098] has not improved from the previous [0.06992219231550496] for 1 times.\n",
      "Iteration [17133]: Loss[0.069942351635646] has not improved from the previous [0.06992383172031098] for 3 times.\n",
      "Iteration [17135]: Loss[0.0699236284066398] has not improved from the previous [0.06992123807124505] for 1 times.\n",
      "Iteration [17137]: Loss[0.06992071896780605] has not improved from the previous [0.06991978797570084] for 1 times.\n",
      "Iteration [17138]: Loss[0.06992626347800743] has not improved from the previous [0.06992071896780605] for 3 times.\n",
      "Iteration [17140]: Loss[0.06992015354728255] has not improved from the previous [0.06991876812257272] for 1 times.\n",
      "Iteration [17143]: Loss[0.06992142572885508] has not improved from the previous [0.06991668418275342] for 1 times.\n",
      "Iteration [17146]: Loss[0.06992104595617155] has not improved from the previous [0.0699151910030345] for 1 times.\n",
      "Iteration [17147]: Loss[0.06992975948833498] has not improved from the previous [0.06992104595617155] for 3 times.\n",
      "Iteration [17149]: Loss[0.06991864771931725] has not improved from the previous [0.06991724252626672] for 1 times.\n",
      "Iteration [17151]: Loss[0.06991674040935789] has not improved from the previous [0.06991481883792901] for 1 times.\n",
      "Iteration [17153]: Loss[0.06991825510650267] has not improved from the previous [0.06991503509961469] for 1 times.\n",
      "Iteration [17156]: Loss[0.06991425415553379] has not improved from the previous [0.06991385627423105] for 1 times.\n",
      "Iteration [17157]: Loss[0.069918136766351] has not improved from the previous [0.06991425415553379] for 3 times.\n",
      "Iteration [17159]: Loss[0.06991499303512774] has not improved from the previous [0.06991157514752934] for 1 times.\n",
      "Iteration [17162]: Loss[0.06991306627888819] has not improved from the previous [0.06991151318543341] for 1 times.\n",
      "Iteration [17164]: Loss[0.06993315126562698] has not improved from the previous [0.06991234853901825] for 1 times.\n",
      "Iteration [17166]: Loss[0.06991297976548802] has not improved from the previous [0.06991190391737719] for 1 times.\n",
      "Iteration [17169]: Loss[0.06991680206616817] has not improved from the previous [0.06990991451228605] for 1 times.\n",
      "Iteration [17171]: Loss[0.06991059887953506] has not improved from the previous [0.0699084944538885] for 1 times.\n",
      "Iteration [17174]: Loss[0.06991178421738949] has not improved from the previous [0.06990681006923567] for 1 times.\n",
      "Iteration [17177]: Loss[0.06990994406125986] has not improved from the previous [0.06990571771955464] for 1 times.\n",
      "Iteration [17180]: Loss[0.06990666303453827] has not improved from the previous [0.06990586831419626] for 1 times.\n",
      "Iteration [17181]: Loss[0.06990893163288563] has not improved from the previous [0.06990666303453827] for 3 times.\n",
      "Iteration [17182]: Loss[0.06992683747652463] has not improved from the previous [0.06990893163288563] for 5 times.\n",
      "Iteration [17184]: Loss[0.06990739754104565] has not improved from the previous [0.06990552413233352] for 1 times.\n",
      "Iteration [17186]: Loss[0.06990516940963346] has not improved from the previous [0.06990398546878852] for 1 times.\n",
      "Iteration [17187]: Loss[0.06991046201176498] has not improved from the previous [0.06990516940963346] for 3 times.\n",
      "Iteration [17189]: Loss[0.06990453253931374] has not improved from the previous [0.0699026490128196] for 1 times.\n",
      "Iteration [17192]: Loss[0.06990545524875912] has not improved from the previous [0.06990145770602324] for 1 times.\n",
      "Iteration [17195]: Loss[0.06990525738872291] has not improved from the previous [0.06989991008868217] for 1 times.\n",
      "Iteration [17197]: Loss[0.06990280666527726] has not improved from the previous [0.06990012072164563] for 1 times.\n",
      "Iteration [17198]: Loss[0.06992044371990173] has not improved from the previous [0.06990280666527726] for 3 times.\n",
      "Iteration [17200]: Loss[0.06990510258020612] has not improved from the previous [0.06989987506061118] for 1 times.\n",
      "Iteration [17202]: Loss[0.0699034837990433] has not improved from the previous [0.06989688394033648] for 1 times.\n",
      "Iteration [17204]: Loss[0.06990345288700549] has not improved from the previous [0.06989582167256309] for 1 times.\n",
      "Iteration [17205]: Loss[0.06991671145745632] has not improved from the previous [0.06990345288700549] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 17200 Loss 0.06990510258020612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [17207]: Loss[0.06990224016807096] has not improved from the previous [0.06989771620852009] for 1 times.\n",
      "Iteration [17209]: Loss[0.06989783365986413] has not improved from the previous [0.06989714236437075] for 1 times.\n",
      "Iteration [17212]: Loss[0.0698993587520897] has not improved from the previous [0.06989537275372064] for 1 times.\n",
      "Iteration [17215]: Loss[0.06989898279326558] has not improved from the previous [0.06989397602425447] for 1 times.\n",
      "Iteration [17217]: Loss[0.06989538697420969] has not improved from the previous [0.0698939697270612] for 1 times.\n",
      "Iteration [17218]: Loss[0.06990061057359831] has not improved from the previous [0.06989538697420969] for 3 times.\n",
      "Iteration [17220]: Loss[0.0698962276469433] has not improved from the previous [0.06989125782775245] for 1 times.\n",
      "Iteration [17222]: Loss[0.06989644354738445] has not improved from the previous [0.06989087132146052] for 1 times.\n",
      "Iteration [17226]: Loss[0.06989431996679314] has not improved from the previous [0.06989172100918348] for 1 times.\n",
      "Iteration [17228]: Loss[0.0698923586050919] has not improved from the previous [0.06989050075847829] for 1 times.\n",
      "Iteration [17230]: Loss[0.06988970066310662] has not improved from the previous [0.0698888114037407] for 1 times.\n",
      "Iteration [17231]: Loss[0.06989356972428469] has not improved from the previous [0.06988970066310662] for 3 times.\n",
      "Iteration [17233]: Loss[0.06989803098992242] has not improved from the previous [0.06988663381050593] for 1 times.\n",
      "Iteration [17235]: Loss[0.0698923397564235] has not improved from the previous [0.06988612524467737] for 1 times.\n",
      "Iteration [17236]: Loss[0.06989334880680553] has not improved from the previous [0.0698923397564235] for 3 times.\n",
      "Iteration [17238]: Loss[0.06988930719164996] has not improved from the previous [0.06988638913173169] for 1 times.\n",
      "Iteration [17240]: Loss[0.06989897544650729] has not improved from the previous [0.06988685610843096] for 1 times.\n",
      "Iteration [17244]: Loss[0.0698876184948986] has not improved from the previous [0.06988701601075911] for 1 times.\n",
      "Iteration [17246]: Loss[0.06988830648274517] has not improved from the previous [0.06988359365549153] for 1 times.\n",
      "Iteration [17250]: Loss[0.06988482869291779] has not improved from the previous [0.06988417816767567] for 1 times.\n",
      "Iteration [17251]: Loss[0.0698904414870452] has not improved from the previous [0.06988482869291779] for 3 times.\n",
      "Iteration [17253]: Loss[0.06988449733170499] has not improved from the previous [0.06988059397398895] for 1 times.\n",
      "Iteration [17255]: Loss[0.06988514877929827] has not improved from the previous [0.06988225165921991] for 1 times.\n",
      "Iteration [17257]: Loss[0.06988314886060207] has not improved from the previous [0.06988141659347924] for 1 times.\n",
      "Iteration [17259]: Loss[0.06988219073911733] has not improved from the previous [0.06988206591834922] for 1 times.\n",
      "Iteration [17261]: Loss[0.06988354701358226] has not improved from the previous [0.06988020196437784] for 1 times.\n",
      "Iteration [17262]: Loss[0.06990036923605118] has not improved from the previous [0.06988354701358226] for 3 times.\n",
      "Iteration [17264]: Loss[0.06988367300952084] has not improved from the previous [0.06987907802283357] for 1 times.\n",
      "Iteration [17266]: Loss[0.06988053580932392] has not improved from the previous [0.06987755531675459] for 1 times.\n",
      "Iteration [17267]: Loss[0.06988396306194933] has not improved from the previous [0.06988053580932392] for 3 times.\n",
      "Iteration [17269]: Loss[0.06987996307290609] has not improved from the previous [0.06987703662634194] for 1 times.\n",
      "Iteration [17272]: Loss[0.06987924928316447] has not improved from the previous [0.06987594444831173] for 1 times.\n",
      "Iteration [17273]: Loss[0.06989000416296386] has not improved from the previous [0.06987924928316447] for 3 times.\n",
      "Iteration [17275]: Loss[0.06987862472201241] has not improved from the previous [0.06987733055349563] for 1 times.\n",
      "Iteration [17277]: Loss[0.0698774592945485] has not improved from the previous [0.06987416698957916] for 1 times.\n",
      "Iteration [17279]: Loss[0.06987912181848988] has not improved from the previous [0.06987292661664002] for 1 times.\n",
      "Iteration [17281]: Loss[0.06987578729897267] has not improved from the previous [0.06987334389444184] for 1 times.\n",
      "Iteration [17282]: Loss[0.0698799277977553] has not improved from the previous [0.06987578729897267] for 3 times.\n",
      "Iteration [17284]: Loss[0.06987586421019774] has not improved from the previous [0.06987054723362843] for 1 times.\n",
      "Iteration [17286]: Loss[0.0698749035961022] has not improved from the previous [0.06987116874610462] for 1 times.\n",
      "Iteration [17288]: Loss[0.0698728969873215] has not improved from the previous [0.06987249577772521] for 1 times.\n",
      "Iteration [17290]: Loss[0.06987478901198196] has not improved from the previous [0.06986988812343661] for 1 times.\n",
      "Iteration [17292]: Loss[0.06987394586021954] has not improved from the previous [0.06986922479923205] for 1 times.\n",
      "Iteration [17293]: Loss[0.06988955574264548] has not improved from the previous [0.06987394586021954] for 3 times.\n",
      "Iteration [17295]: Loss[0.06987426574182128] has not improved from the previous [0.06987004012477377] for 1 times.\n",
      "Iteration [17297]: Loss[0.06987182498042668] has not improved from the previous [0.06986714121942991] for 1 times.\n",
      "Iteration [17299]: Loss[0.06987110090618537] has not improved from the previous [0.06986516395014734] for 1 times.\n",
      "Iteration [17302]: Loss[0.06986760732123033] has not improved from the previous [0.06986686595459556] for 1 times.\n",
      "Iteration [17303]: Loss[0.06986999363298227] has not improved from the previous [0.06986760732123033] for 3 times.\n",
      "Iteration [17304]: Loss[0.06987736820268065] has not improved from the previous [0.06986999363298227] for 5 times.\n",
      "Iteration [17310]: Loss[0.06987213133764923] has not improved from the previous [0.06986536288044329] for 1 times.\n",
      "Iteration [17312]: Loss[0.06986777864865673] has not improved from the previous [0.06986118181172755] for 1 times.\n",
      "Iteration [17314]: Loss[0.06986258256570257] has not improved from the previous [0.06986084111680835] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 17300 Loss 0.06987018699193037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [17315]: Loss[0.06986493307935869] has not improved from the previous [0.06986258256570257] for 3 times.\n",
      "Iteration [17317]: Loss[0.06987031794628018] has not improved from the previous [0.06986119313939274] for 1 times.\n",
      "Iteration [17319]: Loss[0.06986395869059096] has not improved from the previous [0.0698603214474828] for 1 times.\n",
      "Iteration [17321]: Loss[0.06986117673330756] has not improved from the previous [0.06986068973113554] for 1 times.\n",
      "Iteration [17322]: Loss[0.06987681132822979] has not improved from the previous [0.06986117673330756] for 3 times.\n",
      "Iteration [17324]: Loss[0.06986236607949388] has not improved from the previous [0.06986193719634126] for 1 times.\n",
      "Iteration [17327]: Loss[0.06986119070050094] has not improved from the previous [0.06985894722880442] for 1 times.\n",
      "Iteration [17331]: Loss[0.06985878586009654] has not improved from the previous [0.06985874811017813] for 1 times.\n",
      "Iteration [17332]: Loss[0.06986492314802112] has not improved from the previous [0.06985878586009654] for 3 times.\n",
      "Iteration [17334]: Loss[0.06985987534621296] has not improved from the previous [0.06985421137497232] for 1 times.\n",
      "Iteration [17336]: Loss[0.06985940699838325] has not improved from the previous [0.06985544244070035] for 1 times.\n",
      "Iteration [17340]: Loss[0.06986887799195367] has not improved from the previous [0.0698559697012635] for 1 times.\n",
      "Iteration [17345]: Loss[0.0698545058218863] has not improved from the previous [0.06985288678536099] for 1 times.\n",
      "Iteration [17346]: Loss[0.06985453024096645] has not improved from the previous [0.0698545058218863] for 3 times.\n",
      "Iteration [17347]: Loss[0.06987430715823859] has not improved from the previous [0.06985453024096645] for 5 times.\n",
      "Iteration [17349]: Loss[0.0698556025120487] has not improved from the previous [0.06985263649410461] for 1 times.\n",
      "Iteration [17351]: Loss[0.06985478426240403] has not improved from the previous [0.06985093925590452] for 1 times.\n",
      "Iteration [17352]: Loss[0.06985730955165914] has not improved from the previous [0.06985478426240403] for 3 times.\n",
      "Iteration [17354]: Loss[0.06985279738767823] has not improved from the previous [0.06984939257943575] for 1 times.\n",
      "Iteration [17356]: Loss[0.0698538263557353] has not improved from the previous [0.06985009291735456] for 1 times.\n",
      "Iteration [17358]: Loss[0.06984991591571477] has not improved from the previous [0.06984972553654788] for 1 times.\n",
      "Iteration [17360]: Loss[0.0698527971848439] has not improved from the previous [0.06984918567866902] for 1 times.\n",
      "Iteration [17362]: Loss[0.06985006868347145] has not improved from the previous [0.0698479488120796] for 1 times.\n",
      "Iteration [17364]: Loss[0.06985175207159833] has not improved from the previous [0.06984616485164855] for 1 times.\n",
      "Iteration [17366]: Loss[0.06984871887927423] has not improved from the previous [0.06984648138821525] for 1 times.\n",
      "Iteration [17367]: Loss[0.06986665257445922] has not improved from the previous [0.06984871887927423] for 3 times.\n",
      "Iteration [17369]: Loss[0.06985083747665281] has not improved from the previous [0.06984671616241699] for 1 times.\n",
      "Iteration [17371]: Loss[0.06984922473396872] has not improved from the previous [0.06984401682043795] for 1 times.\n",
      "Iteration [17373]: Loss[0.0698490470486652] has not improved from the previous [0.06984173868619384] for 1 times.\n",
      "Iteration [17376]: Loss[0.06984433936279785] has not improved from the previous [0.06984420274181995] for 1 times.\n",
      "Iteration [17378]: Loss[0.06985042012911272] has not improved from the previous [0.06984430290953429] for 1 times.\n",
      "Iteration [17380]: Loss[0.06984455948123368] has not improved from the previous [0.06984070306915596] for 1 times.\n",
      "Iteration [17382]: Loss[0.06984460547207137] has not improved from the previous [0.06984178242773335] for 1 times.\n",
      "Iteration [17384]: Loss[0.0698425422688337] has not improved from the previous [0.06984178295064558] for 1 times.\n",
      "Iteration [17386]: Loss[0.06985399401087085] has not improved from the previous [0.06984056846704456] for 1 times.\n",
      "Iteration [17391]: Loss[0.06984594315065455] has not improved from the previous [0.0698400438204535] for 1 times.\n",
      "Iteration [17393]: Loss[0.06984104139586675] has not improved from the previous [0.06983720230753088] for 1 times.\n",
      "Iteration [17395]: Loss[0.06984660818758345] has not improved from the previous [0.06983593611399416] for 1 times.\n",
      "Iteration [17397]: Loss[0.06983999987764665] has not improved from the previous [0.0698352103328369] for 1 times.\n",
      "Iteration [17398]: Loss[0.06985571160116565] has not improved from the previous [0.06983999987764665] for 3 times.\n",
      "Iteration [17400]: Loss[0.06984033438635509] has not improved from the previous [0.06983745066200069] for 1 times.\n",
      "Iteration [17402]: Loss[0.06983779463741174] has not improved from the previous [0.06983617405927811] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 17400 Loss 0.06984033438635509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [17404]: Loss[0.06983686337150169] has not improved from the previous [0.0698340511588194] for 1 times.\n",
      "Iteration [17406]: Loss[0.06983631903691193] has not improved from the previous [0.06983534789337667] for 1 times.\n",
      "Iteration [17409]: Loss[0.06983458771723629] has not improved from the previous [0.0698342546618349] for 1 times.\n",
      "Iteration [17410]: Loss[0.06984038267542156] has not improved from the previous [0.06983458771723629] for 3 times.\n",
      "Iteration [17412]: Loss[0.06983595846310356] has not improved from the previous [0.06982977512826113] for 1 times.\n",
      "Iteration [17414]: Loss[0.06983147389044818] has not improved from the previous [0.06982930750510691] for 1 times.\n",
      "Iteration [17415]: Loss[0.06983293223014415] has not improved from the previous [0.06983147389044818] for 3 times.\n",
      "Iteration [17417]: Loss[0.06985218635922777] has not improved from the previous [0.06983104636015278] for 1 times.\n",
      "Iteration [17419]: Loss[0.0698355202374568] has not improved from the previous [0.0698302337607316] for 1 times.\n",
      "Iteration [17421]: Loss[0.06983297064983487] has not improved from the previous [0.06982861002296907] for 1 times.\n",
      "Iteration [17423]: Loss[0.06982938442062475] has not improved from the previous [0.06982842846981448] for 1 times.\n",
      "Iteration [17424]: Loss[0.06983026932681247] has not improved from the previous [0.06982938442062475] for 3 times.\n",
      "Iteration [17425]: Loss[0.06983151009926086] has not improved from the previous [0.06983026932681247] for 5 times.\n",
      "Iteration [17427]: Loss[0.06983026418420292] has not improved from the previous [0.06982757901781035] for 1 times.\n",
      "Iteration [17429]: Loss[0.06982918898026684] has not improved from the previous [0.06982804162973123] for 1 times.\n",
      "Iteration [17431]: Loss[0.06983030512224776] has not improved from the previous [0.06982603606973037] for 1 times.\n",
      "Iteration [17432]: Loss[0.06984603905241796] has not improved from the previous [0.06983030512224776] for 3 times.\n",
      "Iteration [17434]: Loss[0.0698303897224462] has not improved from the previous [0.06982574768258297] for 1 times.\n",
      "Iteration [17436]: Loss[0.06982763619959327] has not improved from the previous [0.06982348524413787] for 1 times.\n",
      "Iteration [17438]: Loss[0.06983317571550081] has not improved from the previous [0.06982329873315503] for 1 times.\n",
      "Iteration [17440]: Loss[0.06982737641583037] has not improved from the previous [0.0698216688586128] for 1 times.\n",
      "Iteration [17441]: Loss[0.06982873558747031] has not improved from the previous [0.06982737641583037] for 3 times.\n",
      "Iteration [17443]: Loss[0.0698245584503828] has not improved from the previous [0.06982242094167711] for 1 times.\n",
      "Iteration [17445]: Loss[0.06983019654589247] has not improved from the previous [0.06982122190058043] for 1 times.\n",
      "Iteration [17447]: Loss[0.06982371013081762] has not improved from the previous [0.06982056363656423] for 1 times.\n",
      "Iteration [17449]: Loss[0.06982135062435607] has not improved from the previous [0.06982081689482712] for 1 times.\n",
      "Iteration [17450]: Loss[0.06983633967586556] has not improved from the previous [0.06982135062435607] for 3 times.\n",
      "Iteration [17452]: Loss[0.06982269421311263] has not improved from the previous [0.06982178070536449] for 1 times.\n",
      "Iteration [17455]: Loss[0.06982051049070157] has not improved from the previous [0.06981946821623977] for 1 times.\n",
      "Iteration [17457]: Loss[0.06982052443048992] has not improved from the previous [0.0698198796424537] for 1 times.\n",
      "Iteration [17459]: Loss[0.06981990021454004] has not improved from the previous [0.06981770609921806] for 1 times.\n",
      "Iteration [17460]: Loss[0.06982380195856] has not improved from the previous [0.06981990021454004] for 3 times.\n",
      "Iteration [17462]: Loss[0.06981899521546797] has not improved from the previous [0.06981522608014673] for 1 times.\n",
      "Iteration [17464]: Loss[0.06983837547042383] has not improved from the previous [0.06981726385017835] for 1 times.\n",
      "Iteration [17466]: Loss[0.06981946809940172] has not improved from the previous [0.06981631230676744] for 1 times.\n",
      "Iteration [17468]: Loss[0.06981696651030928] has not improved from the previous [0.06981451616455535] for 1 times.\n",
      "Iteration [17469]: Loss[0.0698206729498889] has not improved from the previous [0.06981696651030928] for 3 times.\n",
      "Iteration [17471]: Loss[0.06981605497473571] has not improved from the previous [0.06981400900297857] for 1 times.\n",
      "Iteration [17473]: Loss[0.06981455760526] has not improved from the previous [0.06981251131727143] for 1 times.\n",
      "Iteration [17474]: Loss[0.06981670093162233] has not improved from the previous [0.06981455760526] for 3 times.\n",
      "Iteration [17475]: Loss[0.06982382632362918] has not improved from the previous [0.06981670093162233] for 5 times.\n",
      "Iteration [17481]: Loss[0.06981481030570744] has not improved from the previous [0.06981136767406672] for 1 times.\n",
      "Iteration [17483]: Loss[0.06981235466734294] has not improved from the previous [0.06981041218835317] for 1 times.\n",
      "Iteration [17485]: Loss[0.06981409139862536] has not improved from the previous [0.06980837834385539] for 1 times.\n",
      "Iteration [17487]: Loss[0.06981106000239085] has not improved from the previous [0.06980902903257166] for 1 times.\n",
      "Iteration [17488]: Loss[0.06981529088257898] has not improved from the previous [0.06981106000239085] for 3 times.\n",
      "Iteration [17490]: Loss[0.069810529350738] has not improved from the previous [0.06980702389911755] for 1 times.\n",
      "Iteration [17492]: Loss[0.06981601134791171] has not improved from the previous [0.06980651853879592] for 1 times.\n",
      "Iteration [17494]: Loss[0.06981008927847958] has not improved from the previous [0.0698047604245408] for 1 times.\n",
      "Iteration [17496]: Loss[0.06980837702984842] has not improved from the previous [0.06980513600444394] for 1 times.\n",
      "Iteration [17497]: Loss[0.06982186461702185] has not improved from the previous [0.06980837702984842] for 3 times.\n",
      "Iteration [17499]: Loss[0.06980877583322047] has not improved from the previous [0.06980628111041576] for 1 times.\n",
      "Iteration [17501]: Loss[0.06980555345238394] has not improved from the previous [0.06980537375159022] for 1 times.\n",
      "Iteration [17502]: Loss[0.06980573830233945] has not improved from the previous [0.06980555345238394] for 3 times.\n",
      "Iteration [17503]: Loss[0.0698058841917878] has not improved from the previous [0.06980573830233945] for 5 times.\n",
      "Iteration [17506]: Loss[0.06980471802594448] has not improved from the previous [0.0698027898253576] for 1 times.\n",
      "Iteration [17507]: Loss[0.06980590481999525] has not improved from the previous [0.06980471802594448] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 17500 Loss 0.06980537375159022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [17509]: Loss[0.06980333304994676] has not improved from the previous [0.06980229350830494] for 1 times.\n",
      "Iteration [17510]: Loss[0.069804302747194] has not improved from the previous [0.06980333304994676] for 3 times.\n",
      "Iteration [17511]: Loss[0.06982306035089907] has not improved from the previous [0.069804302747194] for 5 times.\n",
      "Iteration [17513]: Loss[0.0698053772353509] has not improved from the previous [0.06980153421425578] for 1 times.\n",
      "Iteration [17515]: Loss[0.069803598979831] has not improved from the previous [0.06979927639091334] for 1 times.\n",
      "Iteration [17517]: Loss[0.06980495771475596] has not improved from the previous [0.06979747652249305] for 1 times.\n",
      "Iteration [17519]: Loss[0.06980061365336068] has not improved from the previous [0.06979928110409657] for 1 times.\n",
      "Iteration [17520]: Loss[0.06980557925582893] has not improved from the previous [0.06980061365336068] for 3 times.\n",
      "Iteration [17522]: Loss[0.06980082867463828] has not improved from the previous [0.06979737735394125] for 1 times.\n",
      "Iteration [17524]: Loss[0.06980626164120714] has not improved from the previous [0.06979613827561564] for 1 times.\n",
      "Iteration [17526]: Loss[0.06980047045164672] has not improved from the previous [0.06979512578295365] for 1 times.\n",
      "Iteration [17528]: Loss[0.06979750009609675] has not improved from the previous [0.06979547640260475] for 1 times.\n",
      "Iteration [17529]: Loss[0.06981198582600961] has not improved from the previous [0.06979750009609675] for 3 times.\n",
      "Iteration [17531]: Loss[0.06979924401699941] has not improved from the previous [0.06979656717913264] for 1 times.\n",
      "Iteration [17533]: Loss[0.06979602291303826] has not improved from the previous [0.06979566218110617] for 1 times.\n",
      "Iteration [17535]: Loss[0.06979637618809174] has not improved from the previous [0.06979515001454391] for 1 times.\n",
      "Iteration [17538]: Loss[0.06979459229808445] has not improved from the previous [0.06979329310846565] for 1 times.\n",
      "Iteration [17540]: Loss[0.0697963145923761] has not improved from the previous [0.06979222706349412] for 1 times.\n",
      "Iteration [17542]: Loss[0.06979505333002527] has not improved from the previous [0.06979257521040377] for 1 times.\n",
      "Iteration [17543]: Loss[0.06981251754066824] has not improved from the previous [0.06979505333002527] for 3 times.\n",
      "Iteration [17545]: Loss[0.06979508329469249] has not improved from the previous [0.06979283858505841] for 1 times.\n",
      "Iteration [17547]: Loss[0.06979350337453118] has not improved from the previous [0.06979038444726] for 1 times.\n",
      "Iteration [17549]: Loss[0.06979373258403106] has not improved from the previous [0.06978824165072743] for 1 times.\n",
      "Iteration [17553]: Loss[0.06979124662246435] has not improved from the previous [0.06978894123536201] for 1 times.\n",
      "Iteration [17554]: Loss[0.06980864843224611] has not improved from the previous [0.06979124662246435] for 3 times.\n",
      "Iteration [17556]: Loss[0.06979285310729086] has not improved from the previous [0.06978785562601597] for 1 times.\n",
      "Iteration [17558]: Loss[0.0697898464715911] has not improved from the previous [0.06978610301783177] for 1 times.\n",
      "Iteration [17560]: Loss[0.06979522772932398] has not improved from the previous [0.06978636818330297] for 1 times.\n",
      "Iteration [17562]: Loss[0.06979067248565267] has not improved from the previous [0.06978355853014297] for 1 times.\n",
      "Iteration [17565]: Loss[0.06978627845804461] has not improved from the previous [0.06978587305259568] for 1 times.\n",
      "Iteration [17567]: Loss[0.06979181740332648] has not improved from the previous [0.06978478784023164] for 1 times.\n",
      "Iteration [17569]: Loss[0.06978680874121597] has not improved from the previous [0.06978282478377473] for 1 times.\n",
      "Iteration [17571]: Loss[0.06978351402574366] has not improved from the previous [0.0697831189099144] for 1 times.\n",
      "Iteration [17572]: Loss[0.06978538080778213] has not improved from the previous [0.06978351402574366] for 3 times.\n",
      "Iteration [17576]: Loss[0.06978817377585202] has not improved from the previous [0.06978115073212522] for 1 times.\n",
      "Iteration [17578]: Loss[0.06978253348740426] has not improved from the previous [0.0697817938588948] for 1 times.\n",
      "Iteration [17579]: Loss[0.0697836874796216] has not improved from the previous [0.06978253348740426] for 3 times.\n",
      "Iteration [17581]: Loss[0.06978130377531584] has not improved from the previous [0.06978125356144532] for 1 times.\n",
      "Iteration [17583]: Loss[0.069783126384774] has not improved from the previous [0.0697794444993631] for 1 times.\n",
      "Iteration [17585]: Loss[0.06978052446791597] has not improved from the previous [0.06977897947034176] for 1 times.\n",
      "Iteration [17587]: Loss[0.0697822093412466] has not improved from the previous [0.06977728536209041] for 1 times.\n",
      "Iteration [17589]: Loss[0.06978036462224835] has not improved from the previous [0.06977803378735974] for 1 times.\n",
      "Iteration [17590]: Loss[0.06979781588280982] has not improved from the previous [0.06978036462224835] for 3 times.\n",
      "Iteration [17592]: Loss[0.06978090074160405] has not improved from the previous [0.06977854060001118] for 1 times.\n",
      "Iteration [17594]: Loss[0.06977931501108248] has not improved from the previous [0.06977609537024365] for 1 times.\n",
      "Iteration [17596]: Loss[0.06977919963516696] has not improved from the previous [0.06977399683702128] for 1 times.\n",
      "Iteration [17598]: Loss[0.06977695078663555] has not improved from the previous [0.06977637610586758] for 1 times.\n",
      "Iteration [17600]: Loss[0.06977727809743527] has not improved from the previous [0.0697740499200628] for 1 times.\n",
      "Iteration [17601]: Loss[0.06977997626045983] has not improved from the previous [0.06977727809743527] for 3 times.\n",
      "Iteration [17603]: Loss[0.06977500026802563] has not improved from the previous [0.0697725398546555] for 1 times.\n",
      "Iteration [17605]: Loss[0.06979421194971218] has not improved from the previous [0.06977493329275557] for 1 times.\n",
      "Iteration [17607]: Loss[0.06977740715215158] has not improved from the previous [0.06977192993764464] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 17600 Loss 0.06977727809743527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [17609]: Loss[0.06977535310996161] has not improved from the previous [0.06977020452070752] for 1 times.\n",
      "Iteration [17611]: Loss[0.0697728670961163] has not improved from the previous [0.06977001605937036] for 1 times.\n",
      "Iteration [17612]: Loss[0.0697733237555981] has not improved from the previous [0.0697728670961163] for 3 times.\n",
      "Iteration [17615]: Loss[0.06977361135925195] has not improved from the previous [0.06976991638952239] for 1 times.\n",
      "Iteration [17616]: Loss[0.06978939888174161] has not improved from the previous [0.06977361135925195] for 3 times.\n",
      "Iteration [17618]: Loss[0.06977336957689657] has not improved from the previous [0.06976935518131806] for 1 times.\n",
      "Iteration [17620]: Loss[0.06976999108116504] has not improved from the previous [0.06976847479953693] for 1 times.\n",
      "Iteration [17622]: Loss[0.06977033348123968] has not improved from the previous [0.0697686161379172] for 1 times.\n",
      "Iteration [17625]: Loss[0.06976781307071968] has not improved from the previous [0.0697672531116275] for 1 times.\n",
      "Iteration [17626]: Loss[0.06976926926217518] has not improved from the previous [0.06976781307071968] for 3 times.\n",
      "Iteration [17629]: Loss[0.0697670895225575] has not improved from the previous [0.06976659915618974] for 1 times.\n",
      "Iteration [17630]: Loss[0.06977230349840281] has not improved from the previous [0.0697670895225575] for 3 times.\n",
      "Iteration [17632]: Loss[0.06976720080786407] has not improved from the previous [0.0697627689015717] for 1 times.\n",
      "Iteration [17634]: Loss[0.06978632841076082] has not improved from the previous [0.06976421644801506] for 1 times.\n",
      "Iteration [17636]: Loss[0.06976785240109368] has not improved from the previous [0.06976395282966469] for 1 times.\n",
      "Iteration [17638]: Loss[0.06976577040415932] has not improved from the previous [0.06976197694261921] for 1 times.\n",
      "Iteration [17639]: Loss[0.06976798171019745] has not improved from the previous [0.06976577040415932] for 3 times.\n",
      "Iteration [17641]: Loss[0.06976321482494555] has not improved from the previous [0.0697630495689011] for 1 times.\n",
      "Iteration [17644]: Loss[0.06976359444888482] has not improved from the previous [0.06976158878934187] for 1 times.\n",
      "Iteration [17645]: Loss[0.06978117112559525] has not improved from the previous [0.06976359444888482] for 3 times.\n",
      "Iteration [17647]: Loss[0.06976569799541524] has not improved from the previous [0.06975947560585709] for 1 times.\n",
      "Iteration [17649]: Loss[0.06976223897828224] has not improved from the previous [0.06975871046749464] for 1 times.\n",
      "Iteration [17651]: Loss[0.06976248844235537] has not improved from the previous [0.06975891145994158] for 1 times.\n",
      "Iteration [17653]: Loss[0.06975934981967273] has not improved from the previous [0.06975923990573409] for 1 times.\n",
      "Iteration [17655]: Loss[0.06976089888294536] has not improved from the previous [0.06975829343654769] for 1 times.\n",
      "Iteration [17657]: Loss[0.06975867371395171] has not improved from the previous [0.06975801749442907] for 1 times.\n",
      "Iteration [17659]: Loss[0.06976031015417072] has not improved from the previous [0.06975648888897634] for 1 times.\n",
      "Iteration [17661]: Loss[0.06975771818429057] has not improved from the previous [0.06975610680214939] for 1 times.\n",
      "Iteration [17663]: Loss[0.06975931996038041] has not improved from the previous [0.06975403833071603] for 1 times.\n",
      "Iteration [17665]: Loss[0.06975655682060426] has not improved from the previous [0.0697545647168318] for 1 times.\n",
      "Iteration [17667]: Loss[0.06975856367114307] has not improved from the previous [0.069752410220717] for 1 times.\n",
      "Iteration [17668]: Loss[0.06976757145611508] has not improved from the previous [0.06975856367114307] for 3 times.\n",
      "Iteration [17670]: Loss[0.06975586139354459] has not improved from the previous [0.06975484431183299] for 1 times.\n",
      "Iteration [17672]: Loss[0.06975360503959456] has not improved from the previous [0.06975275002703302] for 1 times.\n",
      "Iteration [17673]: Loss[0.06975401383189266] has not improved from the previous [0.06975360503959456] for 3 times.\n",
      "Iteration [17674]: Loss[0.06975903638672803] has not improved from the previous [0.06975401383189266] for 5 times.\n",
      "Iteration [17676]: Loss[0.06975375799952943] has not improved from the previous [0.06974940988752507] for 1 times.\n",
      "Iteration [17678]: Loss[0.06975898701729481] has not improved from the previous [0.06975110362030397] for 1 times.\n",
      "Iteration [17680]: Loss[0.06975642330280049] has not improved from the previous [0.06974707622759313] for 1 times.\n",
      "Iteration [17681]: Loss[0.06976742755506832] has not improved from the previous [0.06975642330280049] for 3 times.\n",
      "Iteration [17686]: Loss[0.0697498130985605] has not improved from the previous [0.06974888852521761] for 1 times.\n",
      "Iteration [17687]: Loss[0.06975459233425775] has not improved from the previous [0.0697498130985605] for 3 times.\n",
      "Iteration [17689]: Loss[0.06974954666596656] has not improved from the previous [0.0697471775986556] for 1 times.\n",
      "Iteration [17691]: Loss[0.06975479433263876] has not improved from the previous [0.06974642802625967] for 1 times.\n",
      "Iteration [17693]: Loss[0.069750027453116] has not improved from the previous [0.06974450385202259] for 1 times.\n",
      "Iteration [17695]: Loss[0.06974675028041934] has not improved from the previous [0.06974480126935777] for 1 times.\n",
      "Iteration [17697]: Loss[0.06974731160754082] has not improved from the previous [0.06974640129127885] for 1 times.\n",
      "Iteration [17700]: Loss[0.06974574773655168] has not improved from the previous [0.06974447881559497] for 1 times.\n",
      "Iteration [17701]: Loss[0.06975824568785305] has not improved from the previous [0.06974574773655168] for 3 times.\n",
      "Iteration [17707]: Loss[0.06974528222052576] has not improved from the previous [0.06974255224327756] for 1 times.\n",
      "Iteration [17709]: Loss[0.0697427164792858] has not improved from the previous [0.06974211614706026] for 1 times.\n",
      "Iteration [17711]: Loss[0.06974437466181166] has not improved from the previous [0.06974018863091885] for 1 times.\n",
      "Iteration [17713]: Loss[0.06974165456786388] has not improved from the previous [0.0697405045685147] for 1 times.\n",
      "Iteration [17715]: Loss[0.06974324856733222] has not improved from the previous [0.0697384584375854] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 17700 Loss 0.06974574773655168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [17717]: Loss[0.06974044647822303] has not improved from the previous [0.06973907948483334] for 1 times.\n",
      "Iteration [17719]: Loss[0.0697448093086774] has not improved from the previous [0.06973830686349404] for 1 times.\n",
      "Iteration [17721]: Loss[0.06973850385250875] has not improved from the previous [0.06973827337424629] for 1 times.\n",
      "Iteration [17722]: Loss[0.06974066714628031] has not improved from the previous [0.06973850385250875] for 3 times.\n",
      "Iteration [17725]: Loss[0.06973877343737299] has not improved from the previous [0.06973744458850174] for 1 times.\n",
      "Iteration [17726]: Loss[0.0697430888999482] has not improved from the previous [0.06973877343737299] for 3 times.\n",
      "Iteration [17728]: Loss[0.06973842450142059] has not improved from the previous [0.06973406317979004] for 1 times.\n",
      "Iteration [17730]: Loss[0.06973959799329217] has not improved from the previous [0.06973392281562127] for 1 times.\n",
      "Iteration [17732]: Loss[0.06973665459648982] has not improved from the previous [0.0697335820007661] for 1 times.\n",
      "Iteration [17734]: Loss[0.06974881626696484] has not improved from the previous [0.06973351686420522] for 1 times.\n",
      "Iteration [17738]: Loss[0.06973424436167704] has not improved from the previous [0.06973351965258724] for 1 times.\n",
      "Iteration [17740]: Loss[0.06973600331704351] has not improved from the previous [0.06973160546048389] for 1 times.\n",
      "Iteration [17741]: Loss[0.06973747894753729] has not improved from the previous [0.06973600331704351] for 3 times.\n",
      "Iteration [17743]: Loss[0.06973236815964305] has not improved from the previous [0.0697302159337246] for 1 times.\n",
      "Iteration [17744]: Loss[0.06973380021310568] has not improved from the previous [0.06973236815964305] for 3 times.\n",
      "Iteration [17745]: Loss[0.06975153724851023] has not improved from the previous [0.06973380021310568] for 5 times.\n",
      "Iteration [17747]: Loss[0.06973427630509016] has not improved from the previous [0.06973027751986073] for 1 times.\n",
      "Iteration [17749]: Loss[0.0697323708635931] has not improved from the previous [0.06972843100314935] for 1 times.\n",
      "Iteration [17751]: Loss[0.06973363999492217] has not improved from the previous [0.06972748463263473] for 1 times.\n",
      "Iteration [17753]: Loss[0.06973075507938761] has not improved from the previous [0.06972771948813018] for 1 times.\n",
      "Iteration [17755]: Loss[0.06973218288634496] has not improved from the previous [0.0697258954082857] for 1 times.\n",
      "Iteration [17757]: Loss[0.06972928547726913] has not improved from the previous [0.06972670232308432] for 1 times.\n",
      "Iteration [17759]: Loss[0.06973012092144473] has not improved from the previous [0.06972473095473603] for 1 times.\n",
      "Iteration [17763]: Loss[0.0697308756921901] has not improved from the previous [0.06972532087105923] for 1 times.\n",
      "Iteration [17764]: Loss[0.06974480044643125] has not improved from the previous [0.0697308756921901] for 3 times.\n",
      "Iteration [17766]: Loss[0.06972819077123947] has not improved from the previous [0.06972490619511994] for 1 times.\n",
      "Iteration [17768]: Loss[0.06972529386108485] has not improved from the previous [0.06972326543199651] for 1 times.\n",
      "Iteration [17770]: Loss[0.06973047007163817] has not improved from the previous [0.0697245217199678] for 1 times.\n",
      "Iteration [17772]: Loss[0.06972500672385963] has not improved from the previous [0.06972073729178381] for 1 times.\n",
      "Iteration [17774]: Loss[0.06973009100919918] has not improved from the previous [0.06972214419703138] for 1 times.\n",
      "Iteration [17776]: Loss[0.06972453694897697] has not improved from the previous [0.06971856365692068] for 1 times.\n",
      "Iteration [17778]: Loss[0.06974335718305025] has not improved from the previous [0.0697216363852154] for 1 times.\n",
      "Iteration [17780]: Loss[0.06972458891284915] has not improved from the previous [0.06972056429503869] for 1 times.\n",
      "Iteration [17782]: Loss[0.06972271352258416] has not improved from the previous [0.06971841616781452] for 1 times.\n",
      "Iteration [17784]: Loss[0.06972416794899523] has not improved from the previous [0.06971730081102705] for 1 times.\n",
      "Iteration [17788]: Loss[0.06972027424117307] has not improved from the previous [0.0697183580313271] for 1 times.\n",
      "Iteration [17789]: Loss[0.06972367635025559] has not improved from the previous [0.06972027424117307] for 3 times.\n",
      "Iteration [17791]: Loss[0.06971828389880509] has not improved from the previous [0.06971584947157396] for 1 times.\n",
      "Iteration [17792]: Loss[0.06971901088578789] has not improved from the previous [0.06971828389880509] for 3 times.\n",
      "Iteration [17793]: Loss[0.0697371921764602] has not improved from the previous [0.06971901088578789] for 5 times.\n",
      "Iteration [17795]: Loss[0.0697208415116693] has not improved from the previous [0.06971530607102423] for 1 times.\n",
      "Iteration [17797]: Loss[0.0697173156523547] has not improved from the previous [0.0697144011788429] for 1 times.\n",
      "Iteration [17799]: Loss[0.06972237589765902] has not improved from the previous [0.06971488184633191] for 1 times.\n",
      "Iteration [17801]: Loss[0.06971681187640719] has not improved from the previous [0.06971219502168724] for 1 times.\n",
      "Iteration [17803]: Loss[0.06972180631289995] has not improved from the previous [0.06971277368226386] for 1 times.\n",
      "Iteration [17805]: Loss[0.06971618603324403] has not improved from the previous [0.06971024351116807] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 17800 Loss 0.06971219502168724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [17807]: Loss[0.06973488686295663] has not improved from the previous [0.06971097679530432] for 1 times.\n",
      "Iteration [17809]: Loss[0.06971640174960121] has not improved from the previous [0.06971207741489195] for 1 times.\n",
      "Iteration [17811]: Loss[0.0697145929489766] has not improved from the previous [0.06970989313511533] for 1 times.\n",
      "Iteration [17813]: Loss[0.06971515302810205] has not improved from the previous [0.06970874667379505] for 1 times.\n",
      "Iteration [17817]: Loss[0.06971134122973274] has not improved from the previous [0.06970975422228677] for 1 times.\n",
      "Iteration [17818]: Loss[0.06971498705541125] has not improved from the previous [0.06971134122973274] for 3 times.\n",
      "Iteration [17820]: Loss[0.06970960234334996] has not improved from the previous [0.06970787771514182] for 1 times.\n",
      "Iteration [17822]: Loss[0.06971466283295613] has not improved from the previous [0.0697088135811349] for 1 times.\n",
      "Iteration [17824]: Loss[0.06970912352871392] has not improved from the previous [0.0697057925332912] for 1 times.\n",
      "Iteration [17826]: Loss[0.06972790464844596] has not improved from the previous [0.06970813570874908] for 1 times.\n",
      "Iteration [17828]: Loss[0.0697112317438126] has not improved from the previous [0.06970571091244075] for 1 times.\n",
      "Iteration [17830]: Loss[0.06970785309408327] has not improved from the previous [0.06970468761687387] for 1 times.\n",
      "Iteration [17832]: Loss[0.06971283731550193] has not improved from the previous [0.06970422275377289] for 1 times.\n",
      "Iteration [17834]: Loss[0.06970721583019235] has not improved from the previous [0.06970264796021289] for 1 times.\n",
      "Iteration [17836]: Loss[0.06971214355105779] has not improved from the previous [0.06970227055371427] for 1 times.\n",
      "Iteration [17838]: Loss[0.06970648650332435] has not improved from the previous [0.06970082717810938] for 1 times.\n",
      "Iteration [17840]: Loss[0.0697141881673649] has not improved from the previous [0.06970059192462519] for 1 times.\n",
      "Iteration [17842]: Loss[0.06970441314007116] has not improved from the previous [0.0697000239784133] for 1 times.\n",
      "Iteration [17844]: Loss[0.06970910774255756] has not improved from the previous [0.06970209551114272] for 1 times.\n",
      "Iteration [17846]: Loss[0.06970345244128573] has not improved from the previous [0.06969845267615676] for 1 times.\n",
      "Iteration [17848]: Loss[0.06970840948076457] has not improved from the previous [0.06970023753014178] for 1 times.\n",
      "Iteration [17850]: Loss[0.06970327129760952] has not improved from the previous [0.06969684773177617] for 1 times.\n",
      "Iteration [17852]: Loss[0.06970406930521603] has not improved from the previous [0.06969663364709212] for 1 times.\n",
      "Iteration [17854]: Loss[0.06970090989468247] has not improved from the previous [0.06969719246951112] for 1 times.\n",
      "Iteration [17856]: Loss[0.06970214143742168] has not improved from the previous [0.06969560770569809] for 1 times.\n",
      "Iteration [17858]: Loss[0.06969912340108711] has not improved from the previous [0.06969669951801509] for 1 times.\n",
      "Iteration [17860]: Loss[0.06970329634039858] has not improved from the previous [0.0696956236479736] for 1 times.\n",
      "Iteration [17862]: Loss[0.06969687988276455] has not improved from the previous [0.06969633036595252] for 1 times.\n",
      "Iteration [17864]: Loss[0.06969814603417507] has not improved from the previous [0.06969625813125557] for 1 times.\n",
      "Iteration [17868]: Loss[0.06969685210677941] has not improved from the previous [0.06969504749610426] for 1 times.\n",
      "Iteration [17872]: Loss[0.06970849354545168] has not improved from the previous [0.06969372766915576] for 1 times.\n",
      "Iteration [17875]: Loss[0.06969376763111233] has not improved from the previous [0.06969354513355237] for 1 times.\n",
      "Iteration [17877]: Loss[0.06969306723971892] has not improved from the previous [0.06969073068912233] for 1 times.\n",
      "Iteration [17879]: Loss[0.06969385518714195] has not improved from the previous [0.06969205656960879] for 1 times.\n",
      "Iteration [17881]: Loss[0.06969101255681209] has not improved from the previous [0.06969065431243993] for 1 times.\n",
      "Iteration [17882]: Loss[0.06969113895647099] has not improved from the previous [0.06969101255681209] for 3 times.\n",
      "Iteration [17883]: Loss[0.06969247446424812] has not improved from the previous [0.06969113895647099] for 5 times.\n",
      "Iteration [17885]: Loss[0.0696906018040134] has not improved from the previous [0.06968986133353648] for 1 times.\n",
      "Iteration [17886]: Loss[0.06969079663420369] has not improved from the previous [0.0696906018040134] for 3 times.\n",
      "Iteration [17889]: Loss[0.0696886064950749] has not improved from the previous [0.06968828573373588] for 1 times.\n",
      "Iteration [17890]: Loss[0.0696894383273087] has not improved from the previous [0.0696886064950749] for 3 times.\n",
      "Iteration [17893]: Loss[0.069688111559649] has not improved from the previous [0.06968689448599337] for 1 times.\n",
      "Iteration [17894]: Loss[0.06968982953264086] has not improved from the previous [0.069688111559649] for 3 times.\n",
      "Iteration [17897]: Loss[0.06968751396867083] has not improved from the previous [0.06968592162294338] for 1 times.\n",
      "Iteration [17901]: Loss[0.06968616131579387] has not improved from the previous [0.06968465583385171] for 1 times.\n",
      "Iteration [17905]: Loss[0.06968501403412936] has not improved from the previous [0.06968332492643287] for 1 times.\n",
      "Iteration [17906]: Loss[0.06969581200329258] has not improved from the previous [0.06968501403412936] for 3 times.\n",
      "Iteration [17912]: Loss[0.06968240248555772] has not improved from the previous [0.06968153804780741] for 1 times.\n",
      "Iteration [17913]: Loss[0.06968323453749797] has not improved from the previous [0.06968240248555772] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 17900 Loss 0.06968465583385171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [17916]: Loss[0.06968065176785684] has not improved from the previous [0.06968054706879383] for 1 times.\n",
      "Iteration [17917]: Loss[0.06968219466992052] has not improved from the previous [0.06968065176785684] for 3 times.\n",
      "Iteration [17920]: Loss[0.06967932573404091] has not improved from the previous [0.06967877584355371] for 1 times.\n",
      "Iteration [17921]: Loss[0.06968178628013194] has not improved from the previous [0.06967932573404091] for 3 times.\n",
      "Iteration [17924]: Loss[0.06968562285909985] has not improved from the previous [0.06967740925276732] for 1 times.\n",
      "Iteration [17925]: Loss[0.06969642548221157] has not improved from the previous [0.06968562285909985] for 3 times.\n",
      "Iteration [17927]: Loss[0.0696789873630463] has not improved from the previous [0.06967764957932461] for 1 times.\n",
      "Iteration [17930]: Loss[0.0696789480705396] has not improved from the previous [0.06967567738050251] for 1 times.\n",
      "Iteration [17931]: Loss[0.06968105437803442] has not improved from the previous [0.0696789480705396] for 3 times.\n",
      "Iteration [17933]: Loss[0.06967561752543917] has not improved from the previous [0.06967524877578955] for 1 times.\n",
      "Iteration [17934]: Loss[0.06967698500522647] has not improved from the previous [0.06967561752543917] for 3 times.\n",
      "Iteration [17935]: Loss[0.06969436874009696] has not improved from the previous [0.06967698500522647] for 5 times.\n",
      "Iteration [17937]: Loss[0.06967692229582804] has not improved from the previous [0.06967496579543332] for 1 times.\n",
      "Iteration [17940]: Loss[0.06967467855438063] has not improved from the previous [0.06967356655174949] for 1 times.\n",
      "Iteration [17941]: Loss[0.06967885992564209] has not improved from the previous [0.06967467855438063] for 3 times.\n",
      "Iteration [17943]: Loss[0.06967341773033621] has not improved from the previous [0.06967255782950739] for 1 times.\n",
      "Iteration [17945]: Loss[0.06967836703017229] has not improved from the previous [0.0696722163689439] for 1 times.\n",
      "Iteration [17947]: Loss[0.06967272760875554] has not improved from the previous [0.0696707781667262] for 1 times.\n",
      "Iteration [17949]: Loss[0.06967051022491637] has not improved from the previous [0.06967046792770544] for 1 times.\n",
      "Iteration [17950]: Loss[0.06967551718294507] has not improved from the previous [0.06967051022491637] for 3 times.\n",
      "Iteration [17951]: Loss[0.06968969288471698] has not improved from the previous [0.06967551718294507] for 5 times.\n",
      "Iteration [17953]: Loss[0.06967240436151013] has not improved from the previous [0.06966926241711269] for 1 times.\n",
      "Iteration [17956]: Loss[0.0696706870015555] has not improved from the previous [0.06966902966227537] for 1 times.\n",
      "Iteration [17957]: Loss[0.06967431171257876] has not improved from the previous [0.0696706870015555] for 3 times.\n",
      "Iteration [17959]: Loss[0.06966882050954744] has not improved from the previous [0.06966704803633084] for 1 times.\n",
      "Iteration [17961]: Loss[0.06967374341161522] has not improved from the previous [0.06966827404030891] for 1 times.\n",
      "Iteration [17963]: Loss[0.06966805633492601] has not improved from the previous [0.06966538544829803] for 1 times.\n",
      "Iteration [17965]: Loss[0.06968667904368923] has not improved from the previous [0.06966651499693884] for 1 times.\n",
      "Iteration [17967]: Loss[0.06966953624398109] has not improved from the previous [0.0696659927121966] for 1 times.\n",
      "Iteration [17969]: Loss[0.0696663977225735] has not improved from the previous [0.0696648112578245] for 1 times.\n",
      "Iteration [17972]: Loss[0.06966704997689645] has not improved from the previous [0.06966415645895986] for 1 times.\n",
      "Iteration [17973]: Loss[0.0696696182358738] has not improved from the previous [0.06966704997689645] for 3 times.\n",
      "Iteration [17975]: Loss[0.0696641133094902] has not improved from the previous [0.06966169785723143] for 1 times.\n",
      "Iteration [17976]: Loss[0.06966443396055917] has not improved from the previous [0.0696641133094902] for 3 times.\n",
      "Iteration [17977]: Loss[0.06966902194463237] has not improved from the previous [0.06966443396055917] for 5 times.\n",
      "Iteration [17979]: Loss[0.06966328904622059] has not improved from the previous [0.06966013011896054] for 1 times.\n",
      "Iteration [17980]: Loss[0.06966374222003235] has not improved from the previous [0.06966328904622059] for 3 times.\n",
      "Iteration [17981]: Loss[0.0696819398816982] has not improved from the previous [0.06966374222003235] for 5 times.\n",
      "Iteration [17983]: Loss[0.06966426678681063] has not improved from the previous [0.06966126978217566] for 1 times.\n",
      "Iteration [17985]: Loss[0.06966154694415976] has not improved from the previous [0.06965968459294757] for 1 times.\n",
      "Iteration [17987]: Loss[0.0696664100743455] has not improved from the previous [0.06966058563052611] for 1 times.\n",
      "Iteration [17989]: Loss[0.06966066542851165] has not improved from the previous [0.06965802581127171] for 1 times.\n",
      "Iteration [17991]: Loss[0.06966549766800102] has not improved from the previous [0.06965884778620035] for 1 times.\n",
      "Iteration [17993]: Loss[0.06965973214465085] has not improved from the previous [0.06965652174323893] for 1 times.\n",
      "Iteration [17995]: Loss[0.06966451720816087] has not improved from the previous [0.06965715521933587] for 1 times.\n",
      "Iteration [17997]: Loss[0.0696587096038391] has not improved from the previous [0.06965512504365758] for 1 times.\n",
      "Iteration [17999]: Loss[0.06967725862343767] has not improved from the previous [0.06965630386291478] for 1 times.\n",
      "Iteration [18001]: Loss[0.06965919021319507] has not improved from the previous [0.06965673732583098] for 1 times.\n",
      "Iteration [18003]: Loss[0.06965723465984165] has not improved from the previous [0.06965518074685258] for 1 times.\n",
      "Iteration [18005]: Loss[0.06965596218174876] has not improved from the previous [0.06965367626892864] for 1 times.\n",
      "Iteration [18007]: Loss[0.06965587118553544] has not improved from the previous [0.0696551570100186] for 1 times.\n",
      "Iteration [18009]: Loss[0.0696542935295364] has not improved from the previous [0.0696530305309006] for 1 times.\n",
      "Iteration [18011]: Loss[0.06965571345200768] has not improved from the previous [0.06965217310013967] for 1 times.\n",
      "Iteration [18013]: Loss[0.06965286554784869] has not improved from the previous [0.06965231204592132] for 1 times.\n",
      "Iteration [18015]: Loss[0.06965434906265719] has not improved from the previous [0.06965068249480957] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 18000 Loss 0.06965673732583098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [18017]: Loss[0.06965151735533745] has not improved from the previous [0.06965143271793708] for 1 times.\n",
      "Iteration [18019]: Loss[0.06965584722095966] has not improved from the previous [0.06965044882288908] for 1 times.\n",
      "Iteration [18022]: Loss[0.0696507756539187] has not improved from the previous [0.06964954972777754] for 1 times.\n",
      "Iteration [18023]: Loss[0.06965090795145847] has not improved from the previous [0.0696507756539187] for 3 times.\n",
      "Iteration [18026]: Loss[0.06964934010082102] has not improved from the previous [0.06964812764911434] for 1 times.\n",
      "Iteration [18027]: Loss[0.06964975610270978] has not improved from the previous [0.06964934010082102] for 3 times.\n",
      "Iteration [18030]: Loss[0.06964792379277573] has not improved from the previous [0.06964701109306967] for 1 times.\n",
      "Iteration [18031]: Loss[0.06965982011399688] has not improved from the previous [0.06964792379277573] for 3 times.\n",
      "Iteration [18034]: Loss[0.06964900391470148] has not improved from the previous [0.06964596881588114] for 1 times.\n",
      "Iteration [18036]: Loss[0.0696465364511611] has not improved from the previous [0.06964444656373456] for 1 times.\n",
      "Iteration [18038]: Loss[0.06964753013352529] has not improved from the previous [0.06964586174461579] for 1 times.\n",
      "Iteration [18040]: Loss[0.06964492950204168] has not improved from the previous [0.06964426254001016] for 1 times.\n",
      "Iteration [18042]: Loss[0.06964616343428692] has not improved from the previous [0.06964436068046032] for 1 times.\n",
      "Iteration [18044]: Loss[0.06964360870727222] has not improved from the previous [0.06964333970313141] for 1 times.\n",
      "Iteration [18046]: Loss[0.06964484981618896] has not improved from the previous [0.06964287532126312] for 1 times.\n",
      "Iteration [18048]: Loss[0.06964233282044698] has not improved from the previous [0.06964226085833188] for 1 times.\n",
      "Iteration [18050]: Loss[0.06965323797247158] has not improved from the previous [0.06964220547000434] for 1 times.\n",
      "Iteration [18054]: Loss[0.06964191776475755] has not improved from the previous [0.06964069706258538] for 1 times.\n",
      "Iteration [18056]: Loss[0.06964063248432038] has not improved from the previous [0.06963909127433643] for 1 times.\n",
      "Iteration [18057]: Loss[0.06964075560377228] has not improved from the previous [0.06964063248432038] for 3 times.\n",
      "Iteration [18060]: Loss[0.06963883983510105] has not improved from the previous [0.06963809411964006] for 1 times.\n",
      "Iteration [18061]: Loss[0.06963970882754651] has not improved from the previous [0.06963883983510105] for 3 times.\n",
      "Iteration [18064]: Loss[0.06963790924601411] has not improved from the previous [0.0696370043746075] for 1 times.\n",
      "Iteration [18065]: Loss[0.06964143610216025] has not improved from the previous [0.06963790924601411] for 3 times.\n",
      "Iteration [18068]: Loss[0.06963859535543966] has not improved from the previous [0.06963522806648768] for 1 times.\n",
      "Iteration [18070]: Loss[0.0696374324329302] has not improved from the previous [0.06963665915947626] for 1 times.\n",
      "Iteration [18072]: Loss[0.0696370265203854] has not improved from the previous [0.06963391255871067] for 1 times.\n",
      "Iteration [18074]: Loss[0.06963607719170667] has not improved from the previous [0.06963560438873485] for 1 times.\n",
      "Iteration [18076]: Loss[0.06963530101739299] has not improved from the previous [0.06963294845979125] for 1 times.\n",
      "Iteration [18078]: Loss[0.06963479555758968] has not improved from the previous [0.06963463430256688] for 1 times.\n",
      "Iteration [18080]: Loss[0.0696347717723358] has not improved from the previous [0.06963194875740097] for 1 times.\n",
      "Iteration [18081]: Loss[0.06964558330521116] has not improved from the previous [0.0696347717723358] for 3 times.\n",
      "Iteration [18084]: Loss[0.06963527815130081] has not improved from the previous [0.06963090964617968] for 1 times.\n",
      "Iteration [18086]: Loss[0.06963278394785564] has not improved from the previous [0.06962941393506628] for 1 times.\n",
      "Iteration [18088]: Loss[0.06963378693326104] has not improved from the previous [0.06963041081917366] for 1 times.\n",
      "Iteration [18090]: Loss[0.06963121115284696] has not improved from the previous [0.06962919579455658] for 1 times.\n",
      "Iteration [18092]: Loss[0.06963237648558844] has not improved from the previous [0.06962894386741007] for 1 times.\n",
      "Iteration [18094]: Loss[0.06962979547669297] has not improved from the previous [0.06962835865142142] for 1 times.\n",
      "Iteration [18096]: Loss[0.0696397996901519] has not improved from the previous [0.06962779017142036] for 1 times.\n",
      "Iteration [18098]: Loss[0.06963152979486242] has not improved from the previous [0.06962952819596704] for 1 times.\n",
      "Iteration [18100]: Loss[0.0696292338213242] has not improved from the previous [0.06962615265235417] for 1 times.\n",
      "Iteration [18102]: Loss[0.06962823553686093] has not improved from the previous [0.06962524447156225] for 1 times.\n",
      "Iteration [18104]: Loss[0.06962794177217216] has not improved from the previous [0.06962697087948132] for 1 times.\n",
      "Iteration [18106]: Loss[0.06962614406575335] has not improved from the previous [0.06962439847038966] for 1 times.\n",
      "Iteration [18108]: Loss[0.0696266913401895] has not improved from the previous [0.06962604624395212] for 1 times.\n",
      "Iteration [18110]: Loss[0.06962464781239226] has not improved from the previous [0.06962336544528845] for 1 times.\n",
      "Iteration [18111]: Loss[0.06962578508509228] has not improved from the previous [0.06962464781239226] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 18100 Loss 0.0696292338213242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [18112]: Loss[0.06963466740424454] has not improved from the previous [0.06962578508509228] for 5 times.\n",
      "Iteration [18114]: Loss[0.0696265568822994] has not improved from the previous [0.06962474145459893] for 1 times.\n",
      "Iteration [18116]: Loss[0.06962433462909502] has not improved from the previous [0.06962137790586514] for 1 times.\n",
      "Iteration [18118]: Loss[0.06962546581172695] has not improved from the previous [0.0696209660732521] for 1 times.\n",
      "Iteration [18120]: Loss[0.06962290576116562] has not improved from the previous [0.06962087457084636] for 1 times.\n",
      "Iteration [18122]: Loss[0.06962245011157575] has not improved from the previous [0.0696196041195557] for 1 times.\n",
      "Iteration [18124]: Loss[0.06962175327058116] has not improved from the previous [0.06962125987575525] for 1 times.\n",
      "Iteration [18126]: Loss[0.06962118423859809] has not improved from the previous [0.06961860173625273] for 1 times.\n",
      "Iteration [18127]: Loss[0.06963153314003814] has not improved from the previous [0.06962118423859809] for 3 times.\n",
      "Iteration [18130]: Loss[0.06962232502571712] has not improved from the previous [0.06961752769874537] for 1 times.\n",
      "Iteration [18132]: Loss[0.06961882100928662] has not improved from the previous [0.06961607760167725] for 1 times.\n",
      "Iteration [18133]: Loss[0.06962005661578886] has not improved from the previous [0.06961882100928662] for 3 times.\n",
      "Iteration [18134]: Loss[0.06963696868593687] has not improved from the previous [0.06962005661578886] for 5 times.\n",
      "Iteration [18136]: Loss[0.0696189241801959] has not improved from the previous [0.06961710308161316] for 1 times.\n",
      "Iteration [18139]: Loss[0.06961913727946559] has not improved from the previous [0.06961527945553113] for 1 times.\n",
      "Iteration [18140]: Loss[0.06962039971923985] has not improved from the previous [0.06961913727946559] for 3 times.\n",
      "Iteration [18143]: Loss[0.06961700837118993] has not improved from the previous [0.06961470694663463] for 1 times.\n",
      "Iteration [18144]: Loss[0.06961945576872786] has not improved from the previous [0.06961700837118993] for 3 times.\n",
      "Iteration [18147]: Loss[0.06961540797507891] has not improved from the previous [0.06961362160231427] for 1 times.\n",
      "Iteration [18148]: Loss[0.0696183216588284] has not improved from the previous [0.06961540797507891] for 3 times.\n",
      "Iteration [18151]: Loss[0.06961389308963875] has not improved from the previous [0.0696124574415969] for 1 times.\n",
      "Iteration [18152]: Loss[0.06961713599891893] has not improved from the previous [0.06961389308963875] for 3 times.\n",
      "Iteration [18155]: Loss[0.069612820807191] has not improved from the previous [0.0696112716464196] for 1 times.\n",
      "Iteration [18156]: Loss[0.06962975088748963] has not improved from the previous [0.069612820807191] for 3 times.\n",
      "Iteration [18159]: Loss[0.06961303397970305] has not improved from the previous [0.06961203952065928] for 1 times.\n",
      "Iteration [18161]: Loss[0.06961243720534338] has not improved from the previous [0.06960865055444375] for 1 times.\n",
      "Iteration [18163]: Loss[0.06961091256401301] has not improved from the previous [0.06960780985906768] for 1 times.\n",
      "Iteration [18165]: Loss[0.06960992980476799] has not improved from the previous [0.06960981948709462] for 1 times.\n",
      "Iteration [18167]: Loss[0.06960812737145147] has not improved from the previous [0.06960795646372389] for 1 times.\n",
      "Iteration [18169]: Loss[0.0696198288705947] has not improved from the previous [0.0696079714034245] for 1 times.\n",
      "Iteration [18172]: Loss[0.06960955353011575] has not improved from the previous [0.06960644502064886] for 1 times.\n",
      "Iteration [18174]: Loss[0.06960732695545853] has not improved from the previous [0.06960475631271269] for 1 times.\n",
      "Iteration [18176]: Loss[0.06960847635275681] has not improved from the previous [0.06960577780253004] for 1 times.\n",
      "Iteration [18178]: Loss[0.0696059997189552] has not improved from the previous [0.069604347182654] for 1 times.\n",
      "Iteration [18180]: Loss[0.0696073118792238] has not improved from the previous [0.06960402809924869] for 1 times.\n",
      "Iteration [18182]: Loss[0.06960482033588541] has not improved from the previous [0.06960333008656847] for 1 times.\n",
      "Iteration [18184]: Loss[0.06960609795696152] has not improved from the previous [0.06960240838273711] for 1 times.\n",
      "Iteration [18186]: Loss[0.0696036131714532] has not improved from the previous [0.06960223209645933] for 1 times.\n",
      "Iteration [18188]: Loss[0.06960493340382774] has not improved from the previous [0.06960089808860435] for 1 times.\n",
      "Iteration [18190]: Loss[0.06960244480210967] has not improved from the previous [0.06960116584998591] for 1 times.\n",
      "Iteration [18192]: Loss[0.06960546902672321] has not improved from the previous [0.06960036707888684] for 1 times.\n",
      "Iteration [18194]: Loss[0.06960164656462746] has not improved from the previous [0.0696001244160134] for 1 times.\n",
      "Iteration [18196]: Loss[0.06960598354462244] has not improved from the previous [0.06959999958628309] for 1 times.\n",
      "Iteration [18199]: Loss[0.06960380779608384] has not improved from the previous [0.06959852238198547] for 1 times.\n",
      "Iteration [18200]: Loss[0.06961685208802602] has not improved from the previous [0.06960380779608384] for 3 times.\n",
      "Iteration [18203]: Loss[0.06960000424779283] has not improved from the previous [0.06959883448565386] for 1 times.\n",
      "Iteration [18205]: Loss[0.06960135246007909] has not improved from the previous [0.06959518317201537] for 1 times.\n",
      "Iteration [18209]: Loss[0.06959929610992935] has not improved from the previous [0.06959456950182988] for 1 times.\n",
      "Iteration [18210]: Loss[0.06959930807159727] has not improved from the previous [0.06959929610992935] for 3 times.\n",
      "Iteration [18213]: Loss[0.06959768640047431] has not improved from the previous [0.06959347616330872] for 1 times.\n",
      "Iteration [18214]: Loss[0.06959816676738666] has not improved from the previous [0.06959768640047431] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 18200 Loss 0.06961685208802602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [18217]: Loss[0.06959640117993215] has not improved from the previous [0.06959230855816798] for 1 times.\n",
      "Iteration [18218]: Loss[0.06961079956180338] has not improved from the previous [0.06959640117993215] for 3 times.\n",
      "Iteration [18221]: Loss[0.06959572913688233] has not improved from the previous [0.06959376464641476] for 1 times.\n",
      "Iteration [18223]: Loss[0.06959345144430293] has not improved from the previous [0.06959218536979211] for 1 times.\n",
      "Iteration [18225]: Loss[0.06959270737567673] has not improved from the previous [0.06959096926071212] for 1 times.\n",
      "Iteration [18226]: Loss[0.06959271875089551] has not improved from the previous [0.06959270737567673] for 3 times.\n",
      "Iteration [18230]: Loss[0.06959057029415143] has not improved from the previous [0.06959009285506423] for 1 times.\n",
      "Iteration [18231]: Loss[0.06959143409004657] has not improved from the previous [0.06959057029415143] for 3 times.\n",
      "Iteration [18234]: Loss[0.06958945227550807] has not improved from the previous [0.06958858440081028] for 1 times.\n",
      "Iteration [18235]: Loss[0.06959976395813168] has not improved from the previous [0.06958945227550807] for 3 times.\n",
      "Iteration [18238]: Loss[0.06959135581680027] has not improved from the previous [0.0695872338035564] for 1 times.\n",
      "Iteration [18241]: Loss[0.06959028511512137] has not improved from the previous [0.06958681295122378] for 1 times.\n",
      "Iteration [18242]: Loss[0.0695910131215252] has not improved from the previous [0.06959028511512137] for 3 times.\n",
      "Iteration [18245]: Loss[0.06959007587658989] has not improved from the previous [0.06958483536619461] for 1 times.\n",
      "Iteration [18246]: Loss[0.06960320187928981] has not improved from the previous [0.06959007587658989] for 3 times.\n",
      "Iteration [18249]: Loss[0.06958796277237046] has not improved from the previous [0.06958526270953119] for 1 times.\n",
      "Iteration [18251]: Loss[0.06958708388973688] has not improved from the previous [0.0695821535147202] for 1 times.\n",
      "Iteration [18253]: Loss[0.06958749788963979] has not improved from the previous [0.06958173669847094] for 1 times.\n",
      "Iteration [18255]: Loss[0.0695840464066759] has not improved from the previous [0.06958315503765196] for 1 times.\n",
      "Iteration [18257]: Loss[0.06958438159605648] has not improved from the previous [0.0695817609277703] for 1 times.\n",
      "Iteration [18261]: Loss[0.0695817375006291] has not improved from the previous [0.06958143856222802] for 1 times.\n",
      "Iteration [18262]: Loss[0.06958300008776847] has not improved from the previous [0.0695817375006291] for 3 times.\n",
      "Iteration [18266]: Loss[0.06958219724443672] has not improved from the previous [0.06957967646975186] for 1 times.\n",
      "Iteration [18268]: Loss[0.06957985982320591] has not improved from the previous [0.06957943335569036] for 1 times.\n",
      "Iteration [18270]: Loss[0.0695830019612544] has not improved from the previous [0.06957886454323896] for 1 times.\n",
      "Iteration [18272]: Loss[0.06957924600640032] has not improved from the previous [0.06957823522353353] for 1 times.\n",
      "Iteration [18274]: Loss[0.0695827427900459] has not improved from the previous [0.0695791914145673] for 1 times.\n",
      "Iteration [18277]: Loss[0.06958318125341792] has not improved from the previous [0.06957523036363472] for 1 times.\n",
      "Iteration [18278]: Loss[0.06959351513902856] has not improved from the previous [0.06958318125341792] for 3 times.\n",
      "Iteration [18281]: Loss[0.06957921165220411] has not improved from the previous [0.06957544401559697] for 1 times.\n",
      "Iteration [18283]: Loss[0.06957808142903117] has not improved from the previous [0.06957260397811331] for 1 times.\n",
      "Iteration [18285]: Loss[0.06958230414695868] has not improved from the previous [0.06957352168990127] for 1 times.\n",
      "Iteration [18287]: Loss[0.0695756474560407] has not improved from the previous [0.06957260797530034] for 1 times.\n",
      "Iteration [18289]: Loss[0.0695802150724532] has not improved from the previous [0.06957316794755704] for 1 times.\n",
      "Iteration [18291]: Loss[0.06957399869489582] has not improved from the previous [0.06957203930339995] for 1 times.\n",
      "Iteration [18293]: Loss[0.06957849683621138] has not improved from the previous [0.06957215571001911] for 1 times.\n",
      "Iteration [18295]: Loss[0.0695716242426931] has not improved from the previous [0.06957144328501019] for 1 times.\n",
      "Iteration [18296]: Loss[0.06957454440045023] has not improved from the previous [0.0695716242426931] for 3 times.\n",
      "Iteration [18297]: Loss[0.06958984563772687] has not improved from the previous [0.06957454440045023] for 5 times.\n",
      "Iteration [18301]: Loss[0.0695717725926001] has not improved from the previous [0.06957146005643844] for 1 times.\n",
      "Iteration [18303]: Loss[0.06957015358530713] has not improved from the previous [0.06956921560882047] for 1 times.\n",
      "Iteration [18305]: Loss[0.06957167396198485] has not improved from the previous [0.06956954972743672] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 18300 Loss 0.06957146005643844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [18307]: Loss[0.0695695283925737] has not improved from the previous [0.06956798578234209] for 1 times.\n",
      "Iteration [18309]: Loss[0.06957101773185916] has not improved from the previous [0.06956714321207336] for 1 times.\n",
      "Iteration [18311]: Loss[0.06956867915725959] has not improved from the previous [0.06956654418467252] for 1 times.\n",
      "Iteration [18313]: Loss[0.06957009881941857] has not improved from the previous [0.06956522368867897] for 1 times.\n",
      "Iteration [18315]: Loss[0.06956768023126127] has not improved from the previous [0.06956532066900793] for 1 times.\n",
      "Iteration [18317]: Loss[0.06957074434574036] has not improved from the previous [0.06956469225881709] for 1 times.\n",
      "Iteration [18319]: Loss[0.06956493590701489] has not improved from the previous [0.06956450797127306] for 1 times.\n",
      "Iteration [18320]: Loss[0.06956858818760173] has not improved from the previous [0.06956493590701489] for 3 times.\n",
      "Iteration [18324]: Loss[0.06956975759595775] has not improved from the previous [0.06956173431327885] for 1 times.\n",
      "Iteration [18325]: Loss[0.0695801182851908] has not improved from the previous [0.06956975759595775] for 3 times.\n",
      "Iteration [18328]: Loss[0.06956518385182227] has not improved from the previous [0.0695629571026604] for 1 times.\n",
      "Iteration [18330]: Loss[0.06956280039551445] has not improved from the previous [0.06956153628185892] for 1 times.\n",
      "Iteration [18332]: Loss[0.0695637544657631] has not improved from the previous [0.06956163234240653] for 1 times.\n",
      "Iteration [18336]: Loss[0.06956191333911532] has not improved from the previous [0.0695605286325071] for 1 times.\n",
      "Iteration [18339]: Loss[0.06955968595019839] has not improved from the previous [0.0695589667556251] for 1 times.\n",
      "Iteration [18341]: Loss[0.06956460420225484] has not improved from the previous [0.06955967195983474] for 1 times.\n",
      "Iteration [18344]: Loss[0.06956544806257424] has not improved from the previous [0.06955703813592044] for 1 times.\n",
      "Iteration [18345]: Loss[0.06957512033709702] has not improved from the previous [0.06956544806257424] for 3 times.\n",
      "Iteration [18348]: Loss[0.06956037832117754] has not improved from the previous [0.06955680490215557] for 1 times.\n",
      "Iteration [18350]: Loss[0.0695593529440936] has not improved from the previous [0.06955387096329346] for 1 times.\n",
      "Iteration [18352]: Loss[0.06956334814053373] has not improved from the previous [0.06955441465192652] for 1 times.\n",
      "Iteration [18354]: Loss[0.06955678173771075] has not improved from the previous [0.06955399747324122] for 1 times.\n",
      "Iteration [18356]: Loss[0.06956105327653826] has not improved from the previous [0.06955423807242528] for 1 times.\n",
      "Iteration [18358]: Loss[0.06955402500395974] has not improved from the previous [0.06955376061092536] for 1 times.\n",
      "Iteration [18359]: Loss[0.06955716121145364] has not improved from the previous [0.06955402500395974] for 3 times.\n",
      "Iteration [18360]: Loss[0.06957210799209927] has not improved from the previous [0.06955716121145364] for 5 times.\n",
      "Iteration [18364]: Loss[0.0695543074321972] has not improved from the previous [0.06955375389907562] for 1 times.\n",
      "Iteration [18366]: Loss[0.06955274180607521] has not improved from the previous [0.0695514815519767] for 1 times.\n",
      "Iteration [18368]: Loss[0.0695542988464394] has not improved from the previous [0.06955156779289233] for 1 times.\n",
      "Iteration [18370]: Loss[0.06955216840280139] has not improved from the previous [0.06955015451290067] for 1 times.\n",
      "Iteration [18372]: Loss[0.06955366219176003] has not improved from the previous [0.0695490607172811] for 1 times.\n",
      "Iteration [18374]: Loss[0.06955136516717446] has not improved from the previous [0.06954873698132194] for 1 times.\n",
      "Iteration [18376]: Loss[0.06954830170257453] has not improved from the previous [0.06954759506698649] for 1 times.\n",
      "Iteration [18377]: Loss[0.06955964785280319] has not improved from the previous [0.06954830170257453] for 3 times.\n",
      "Iteration [18378]: Loss[0.06956626885279216] has not improved from the previous [0.06955964785280319] for 5 times.\n",
      "Iteration [18381]: Loss[0.0695500770704632] has not improved from the previous [0.06954773297584244] for 1 times.\n",
      "Iteration [18383]: Loss[0.06955022544662963] has not improved from the previous [0.06954436293643385] for 1 times.\n",
      "Iteration [18385]: Loss[0.06954833235747734] has not improved from the previous [0.06954717084159225] for 1 times.\n",
      "Iteration [18387]: Loss[0.06954681232134147] has not improved from the previous [0.06954552629428346] for 1 times.\n",
      "Iteration [18388]: Loss[0.06954743486998179] has not improved from the previous [0.06954681232134147] for 3 times.\n",
      "Iteration [18392]: Loss[0.06955577283408895] has not improved from the previous [0.06954433761164977] for 1 times.\n",
      "Iteration [18394]: Loss[0.06954782851772488] has not improved from the previous [0.06954620226585796] for 1 times.\n",
      "Iteration [18397]: Loss[0.06954584125652288] has not improved from the previous [0.06954269925315242] for 1 times.\n",
      "Iteration [18398]: Loss[0.06954651320868588] has not improved from the previous [0.06954584125652288] for 3 times.\n",
      "Iteration [18401]: Loss[0.06954529805091139] has not improved from the previous [0.0695401893123635] for 1 times.\n",
      "Iteration [18403]: Loss[0.06954475385190696] has not improved from the previous [0.06953798501569813] for 1 times.\n",
      "Iteration [18405]: Loss[0.06956328213775487] has not improved from the previous [0.06954058207412395] for 1 times.\n",
      "Iteration [18407]: Loss[0.06954342775650861] has not improved from the previous [0.06954021863171349] for 1 times.\n",
      "Iteration [18410]: Loss[0.06954328671751538] has not improved from the previous [0.06953890794206823] for 1 times.\n",
      "Iteration [18414]: Loss[0.0695399715964296] has not improved from the previous [0.06953757330249295] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 18400 Loss 0.0695401893123635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [18416]: Loss[0.06954035418865112] has not improved from the previous [0.06953807990860267] for 1 times.\n",
      "Iteration [18419]: Loss[0.06953797967631783] has not improved from the previous [0.0695368850298259] for 1 times.\n",
      "Iteration [18420]: Loss[0.06954843337221916] has not improved from the previous [0.06953797967631783] for 3 times.\n",
      "Iteration [18423]: Loss[0.06953806800113604] has not improved from the previous [0.06953544586965237] for 1 times.\n",
      "Iteration [18426]: Loss[0.06954002818538459] has not improved from the previous [0.06953361425717156] for 1 times.\n",
      "Iteration [18430]: Loss[0.06953867536570223] has not improved from the previous [0.06953198378749932] for 1 times.\n",
      "Iteration [18434]: Loss[0.06953544319080697] has not improved from the previous [0.06953087958103288] for 1 times.\n",
      "Iteration [18436]: Loss[0.0695449711067343] has not improved from the previous [0.06953191567925937] for 1 times.\n",
      "Iteration [18439]: Loss[0.06953343249280806] has not improved from the previous [0.06953239527374284] for 1 times.\n",
      "Iteration [18442]: Loss[0.06953220922453132] has not improved from the previous [0.06953034516114288] for 1 times.\n",
      "Iteration [18444]: Loss[0.06953381989944904] has not improved from the previous [0.06953050568672528] for 1 times.\n",
      "Iteration [18446]: Loss[0.06953151345140078] has not improved from the previous [0.0695284250747806] for 1 times.\n",
      "Iteration [18448]: Loss[0.06953299649056413] has not improved from the previous [0.0695284543219082] for 1 times.\n",
      "Iteration [18450]: Loss[0.06953066978956988] has not improved from the previous [0.0695269528515208] for 1 times.\n",
      "Iteration [18452]: Loss[0.0695351093065999] has not improved from the previous [0.06952689748608815] for 1 times.\n",
      "Iteration [18454]: Loss[0.06952713679877566] has not improved from the previous [0.0695266262020418] for 1 times.\n",
      "Iteration [18455]: Loss[0.06953411364672205] has not improved from the previous [0.06952713679877566] for 3 times.\n",
      "Iteration [18456]: Loss[0.06954495478736017] has not improved from the previous [0.06953411364672205] for 5 times.\n",
      "Iteration [18459]: Loss[0.06952862244151706] has not improved from the previous [0.06952685866239547] for 1 times.\n",
      "Iteration [18461]: Loss[0.06952607582663212] has not improved from the previous [0.06952559952298112] for 1 times.\n",
      "Iteration [18463]: Loss[0.0695269186981587] has not improved from the previous [0.06952507359420805] for 1 times.\n",
      "Iteration [18466]: Loss[0.06952454942969596] has not improved from the previous [0.06952374310497259] for 1 times.\n",
      "Iteration [18467]: Loss[0.06952491049974537] has not improved from the previous [0.06952454942969596] for 3 times.\n",
      "Iteration [18468]: Loss[0.06954392872378681] has not improved from the previous [0.06952491049974537] for 5 times.\n",
      "Iteration [18471]: Loss[0.06952584478611969] has not improved from the previous [0.06952334963745488] for 1 times.\n",
      "Iteration [18473]: Loss[0.06952569876915399] has not improved from the previous [0.06951938569592873] for 1 times.\n",
      "Iteration [18475]: Loss[0.06952918982635231] has not improved from the previous [0.06952071557469645] for 1 times.\n",
      "Iteration [18477]: Loss[0.0695215966750147] has not improved from the previous [0.06952018171451432] for 1 times.\n",
      "Iteration [18478]: Loss[0.06952336402108136] has not improved from the previous [0.0695215966750147] for 3 times.\n",
      "Iteration [18479]: Loss[0.06953933446656946] has not improved from the previous [0.06952336402108136] for 5 times.\n",
      "Iteration [18483]: Loss[0.06952151808622377] has not improved from the previous [0.06952082367407156] for 1 times.\n",
      "Iteration [18485]: Loss[0.06952004257331353] has not improved from the previous [0.06951840199613285] for 1 times.\n",
      "Iteration [18487]: Loss[0.0695216584061516] has not improved from the previous [0.0695189847515203] for 1 times.\n",
      "Iteration [18489]: Loss[0.06951959119254911] has not improved from the previous [0.06951696251861886] for 1 times.\n",
      "Iteration [18491]: Loss[0.06952381265290421] has not improved from the previous [0.06951707083553604] for 1 times.\n",
      "Iteration [18494]: Loss[0.0695210689990341] has not improved from the previous [0.06951585842674239] for 1 times.\n",
      "Iteration [18495]: Loss[0.06953364206964674] has not improved from the previous [0.0695210689990341] for 3 times.\n",
      "Iteration [18498]: Loss[0.06951832726424388] has not improved from the previous [0.06951609430277382] for 1 times.\n",
      "Iteration [18500]: Loss[0.0695157458471767] has not improved from the previous [0.06951484505727346] for 1 times.\n",
      "Iteration [18502]: Loss[0.06951655902346006] has not improved from the previous [0.06951413732464226] for 1 times.\n",
      "Iteration [18505]: Loss[0.06951388110933157] has not improved from the previous [0.06951335981298139] for 1 times.\n",
      "Iteration [18506]: Loss[0.0695143424920726] has not improved from the previous [0.06951388110933157] for 3 times.\n",
      "Iteration [18507]: Loss[0.0695180625929951] has not improved from the previous [0.0695143424920726] for 5 times.\n",
      "Iteration [18510]: Loss[0.06951799753636244] has not improved from the previous [0.06951027209168048] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 18500 Loss 0.0695157458471767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [18511]: Loss[0.06952808890172812] has not improved from the previous [0.06951799753636244] for 3 times.\n",
      "Iteration [18514]: Loss[0.06951454474596475] has not improved from the previous [0.06951095081256664] for 1 times.\n",
      "Iteration [18516]: Loss[0.06951054012092703] has not improved from the previous [0.06951037810377958] for 1 times.\n",
      "Iteration [18517]: Loss[0.06951257445780004] has not improved from the previous [0.06951054012092703] for 3 times.\n",
      "Iteration [18518]: Loss[0.06951444112937423] has not improved from the previous [0.06951257445780004] for 5 times.\n",
      "Iteration [18521]: Loss[0.06951074163513944] has not improved from the previous [0.06950849224477353] for 1 times.\n",
      "Iteration [18523]: Loss[0.06951098929110239] has not improved from the previous [0.06950808900173315] for 1 times.\n",
      "Iteration [18526]: Loss[0.06950806990284075] has not improved from the previous [0.06950742006971576] for 1 times.\n",
      "Iteration [18527]: Loss[0.06950821708631123] has not improved from the previous [0.06950806990284075] for 3 times.\n",
      "Iteration [18528]: Loss[0.06951841595796006] has not improved from the previous [0.06950821708631123] for 5 times.\n",
      "Iteration [18530]: Loss[0.0695106901358402] has not improved from the previous [0.06950777837602501] for 1 times.\n",
      "Iteration [18533]: Loss[0.06950993528697115] has not improved from the previous [0.06950471498825739] for 1 times.\n",
      "Iteration [18537]: Loss[0.06950802041458264] has not improved from the previous [0.06950219068198478] for 1 times.\n",
      "Iteration [18539]: Loss[0.06951153336993493] has not improved from the previous [0.06950286244889807] for 1 times.\n",
      "Iteration [18541]: Loss[0.06950375715364139] has not improved from the previous [0.0695029344932664] for 1 times.\n",
      "Iteration [18542]: Loss[0.0695064999521128] has not improved from the previous [0.06950375715364139] for 3 times.\n",
      "Iteration [18543]: Loss[0.06952140085937997] has not improved from the previous [0.0695064999521128] for 5 times.\n",
      "Iteration [18545]: Loss[0.06950508826996354] has not improved from the previous [0.06950503647966745] for 1 times.\n",
      "Iteration [18547]: Loss[0.06950419312773333] has not improved from the previous [0.06950307204767218] for 1 times.\n",
      "Iteration [18549]: Loss[0.06950273307719744] has not improved from the previous [0.0695006642598607] for 1 times.\n",
      "Iteration [18551]: Loss[0.0695075020869071] has not improved from the previous [0.06950118587304634] for 1 times.\n",
      "Iteration [18554]: Loss[0.06950449581052746] has not improved from the previous [0.06949955609653749] for 1 times.\n",
      "Iteration [18555]: Loss[0.06951714275003192] has not improved from the previous [0.06950449581052746] for 3 times.\n",
      "Iteration [18558]: Loss[0.06950183659138681] has not improved from the previous [0.06949974346521563] for 1 times.\n",
      "Iteration [18560]: Loss[0.06949920506238627] has not improved from the previous [0.06949854367987894] for 1 times.\n",
      "Iteration [18562]: Loss[0.06950412956672157] has not improved from the previous [0.06949810478051523] for 1 times.\n",
      "Iteration [18566]: Loss[0.06949761445676916] has not improved from the previous [0.0694971647089005] for 1 times.\n",
      "Iteration [18568]: Loss[0.06949946479027551] has not improved from the previous [0.06949717934682675] for 1 times.\n",
      "Iteration [18570]: Loss[0.06949739206917324] has not improved from the previous [0.06949461986848889] for 1 times.\n",
      "Iteration [18572]: Loss[0.0695024113387232] has not improved from the previous [0.06949453943967586] for 1 times.\n",
      "Iteration [18574]: Loss[0.06949413748573906] has not improved from the previous [0.06949383489684116] for 1 times.\n",
      "Iteration [18575]: Loss[0.0695011110806783] has not improved from the previous [0.06949413748573906] for 3 times.\n",
      "Iteration [18576]: Loss[0.06951177446956847] has not improved from the previous [0.0695011110806783] for 5 times.\n",
      "Iteration [18579]: Loss[0.06949552866063398] has not improved from the previous [0.06949421687718278] for 1 times.\n",
      "Iteration [18583]: Loss[0.0694937281481436] has not improved from the previous [0.06949287811042222] for 1 times.\n",
      "Iteration [18587]: Loss[0.06949259580200137] has not improved from the previous [0.06949042715641325] for 1 times.\n",
      "Iteration [18589]: Loss[0.06950299146135124] has not improved from the previous [0.06949141304569699] for 1 times.\n",
      "Iteration [18591]: Loss[0.06949458918564916] has not improved from the previous [0.06949040523751167] for 1 times.\n",
      "Iteration [18593]: Loss[0.06949043848078933] has not improved from the previous [0.06948802400715118] for 1 times.\n",
      "Iteration [18594]: Loss[0.06949192810984368] has not improved from the previous [0.06949043848078933] for 3 times.\n",
      "Iteration [18595]: Loss[0.06949348683117507] has not improved from the previous [0.06949192810984368] for 5 times.\n",
      "Iteration [18598]: Loss[0.06949013184699399] has not improved from the previous [0.06948674851258542] for 1 times.\n",
      "Iteration [18600]: Loss[0.06949013120673499] has not improved from the previous [0.0694876383184194] for 1 times.\n",
      "Iteration [18603]: Loss[0.06948756279244026] has not improved from the previous [0.06948640607515061] for 1 times.\n",
      "Iteration [18605]: Loss[0.06948814726954147] has not improved from the previous [0.06948735115175944] for 1 times.\n",
      "Iteration [18608]: Loss[0.06949477793936905] has not improved from the previous [0.06948456095739972] for 1 times.\n",
      "Iteration [18609]: Loss[0.0695019479562887] has not improved from the previous [0.06949477793936905] for 3 times.\n",
      "Iteration [18612]: Loss[0.06948737716570545] has not improved from the previous [0.06948391246741258] for 1 times.\n",
      "Iteration [18614]: Loss[0.06948365268914064] has not improved from the previous [0.06948302211453355] for 1 times.\n",
      "Iteration [18615]: Loss[0.0694870795486767] has not improved from the previous [0.06948365268914064] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 18600 Loss 0.06949013120673499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [18616]: Loss[0.06950122683617739] has not improved from the previous [0.0694870795486767] for 5 times.\n",
      "Iteration [18618]: Loss[0.06948530852725862] has not improved from the previous [0.06948527660064188] for 1 times.\n",
      "Iteration [18620]: Loss[0.06948452891607672] has not improved from the previous [0.06948275473664285] for 1 times.\n",
      "Iteration [18622]: Loss[0.06948370291910363] has not improved from the previous [0.06948032416544732] for 1 times.\n",
      "Iteration [18624]: Loss[0.06948424726967493] has not improved from the previous [0.06948151567501211] for 1 times.\n",
      "Iteration [18627]: Loss[0.06948511578583834] has not improved from the previous [0.0694797565766215] for 1 times.\n",
      "Iteration [18631]: Loss[0.06948205389731714] has not improved from the previous [0.0694764377805396] for 1 times.\n",
      "Iteration [18633]: Loss[0.06948047021279521] has not improved from the previous [0.06947938037725146] for 1 times.\n",
      "Iteration [18635]: Loss[0.06947809026718765] has not improved from the previous [0.06947788198927801] for 1 times.\n",
      "Iteration [18636]: Loss[0.06947993540239755] has not improved from the previous [0.06947809026718765] for 3 times.\n",
      "Iteration [18638]: Loss[0.06947806908699969] has not improved from the previous [0.06947752756690767] for 1 times.\n",
      "Iteration [18641]: Loss[0.0694805919889378] has not improved from the previous [0.06947553737231339] for 1 times.\n",
      "Iteration [18642]: Loss[0.0694891676130542] has not improved from the previous [0.0694805919889378] for 3 times.\n",
      "Iteration [18645]: Loss[0.06947746987097171] has not improved from the previous [0.06947404669436197] for 1 times.\n",
      "Iteration [18648]: Loss[0.06948221232608233] has not improved from the previous [0.06947250762525871] for 1 times.\n",
      "Iteration [18649]: Loss[0.06949050249950094] has not improved from the previous [0.06948221232608233] for 3 times.\n",
      "Iteration [18656]: Loss[0.06947493303329431] has not improved from the previous [0.06947215624839184] for 1 times.\n",
      "Iteration [18658]: Loss[0.06947395399928781] has not improved from the previous [0.06947343597873329] for 1 times.\n",
      "Iteration [18659]: Loss[0.06947502187350825] has not improved from the previous [0.06947395399928781] for 3 times.\n",
      "Iteration [18662]: Loss[0.0694744390025449] has not improved from the previous [0.06946831450035029] for 1 times.\n",
      "Iteration [18664]: Loss[0.06946957668256398] has not improved from the previous [0.06946923670111657] for 1 times.\n",
      "Iteration [18665]: Loss[0.06947786405592737] has not improved from the previous [0.06946957668256398] for 3 times.\n",
      "Iteration [18666]: Loss[0.06948720441478302] has not improved from the previous [0.06947786405592737] for 5 times.\n",
      "Iteration [18672]: Loss[0.06947142552856327] has not improved from the previous [0.06946802898817343] for 1 times.\n",
      "Iteration [18674]: Loss[0.06946805323343697] has not improved from the previous [0.0694670027967208] for 1 times.\n",
      "Iteration [18675]: Loss[0.06947018013048395] has not improved from the previous [0.06946805323343697] for 3 times.\n",
      "Iteration [18676]: Loss[0.06947086434100244] has not improved from the previous [0.06947018013048395] for 5 times.\n",
      "Iteration [18679]: Loss[0.06947058660744601] has not improved from the previous [0.06946380925499337] for 1 times.\n",
      "Iteration [18681]: Loss[0.06946912142829258] has not improved from the previous [0.0694676785530405] for 1 times.\n",
      "Iteration [18683]: Loss[0.06946749574855242] has not improved from the previous [0.06946346945780463] for 1 times.\n",
      "Iteration [18685]: Loss[0.06947348557202089] has not improved from the previous [0.06946442864255337] for 1 times.\n",
      "Iteration [18689]: Loss[0.06946658095546562] has not improved from the previous [0.06946407008612315] for 1 times.\n",
      "Iteration [18691]: Loss[0.06946461225160683] has not improved from the previous [0.06946261694256302] for 1 times.\n",
      "Iteration [18693]: Loss[0.06946869814589178] has not improved from the previous [0.06946233651253571] for 1 times.\n",
      "Iteration [18696]: Loss[0.06946709171092713] has not improved from the previous [0.06946031472661983] for 1 times.\n",
      "Iteration [18697]: Loss[0.06947771772959566] has not improved from the previous [0.06946709171092713] for 3 times.\n",
      "Iteration [18700]: Loss[0.06946295516869022] has not improved from the previous [0.06946179563835116] for 1 times.\n",
      "Iteration [18702]: Loss[0.06946061030777227] has not improved from the previous [0.0694606088973855] for 1 times.\n",
      "Iteration [18704]: Loss[0.06946045551399072] has not improved from the previous [0.06946005161272165] for 1 times.\n",
      "Iteration [18706]: Loss[0.0694623795516211] has not improved from the previous [0.06945981103014648] for 1 times.\n",
      "Iteration [18708]: Loss[0.06946040055469345] has not improved from the previous [0.06945716239182008] for 1 times.\n",
      "Iteration [18710]: Loss[0.06946437404293994] has not improved from the previous [0.06945797495176333] for 1 times.\n",
      "Iteration [18713]: Loss[0.06946399909380345] has not improved from the previous [0.06945592359490024] for 1 times.\n",
      "Iteration [18714]: Loss[0.06947339505307558] has not improved from the previous [0.06946399909380345] for 3 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 18700 Loss 0.06946295516869022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [18717]: Loss[0.06945789802680548] has not improved from the previous [0.06945738013154926] for 1 times.\n",
      "Iteration [18721]: Loss[0.06945642928212221] has not improved from the previous [0.06945502708902748] for 1 times.\n",
      "Iteration [18723]: Loss[0.06945713635947501] has not improved from the previous [0.06945554033804935] for 1 times.\n",
      "Iteration [18725]: Loss[0.06945664842012038] has not improved from the previous [0.06945125410075603] for 1 times.\n",
      "Iteration [18727]: Loss[0.0694599143626009] has not improved from the previous [0.0694535378392214] for 1 times.\n",
      "Iteration [18730]: Loss[0.06946071783577916] has not improved from the previous [0.06945139528219721] for 1 times.\n",
      "Iteration [18731]: Loss[0.06946888212284898] has not improved from the previous [0.06946071783577916] for 3 times.\n",
      "Iteration [18738]: Loss[0.06945265085908765] has not improved from the previous [0.06944998217462213] for 1 times.\n",
      "Iteration [18740]: Loss[0.06945237990885432] has not improved from the previous [0.06945086651489808] for 1 times.\n",
      "Iteration [18742]: Loss[0.06945246607603739] has not improved from the previous [0.06944627497048667] for 1 times.\n",
      "Iteration [18744]: Loss[0.06945309012226089] has not improved from the previous [0.06944757983593156] for 1 times.\n",
      "Iteration [18746]: Loss[0.06944735698323443] has not improved from the previous [0.06944726283053462] for 1 times.\n",
      "Iteration [18747]: Loss[0.06945725124332415] has not improved from the previous [0.06944735698323443] for 3 times.\n",
      "Iteration [18748]: Loss[0.06946459077466693] has not improved from the previous [0.06945725124332415] for 5 times.\n",
      "Iteration [18751]: Loss[0.06944831935070796] has not improved from the previous [0.06944808087759868] for 1 times.\n",
      "Iteration [18754]: Loss[0.06945213696556903] has not improved from the previous [0.06944619569503059] for 1 times.\n",
      "Iteration [18757]: Loss[0.06945010154499327] has not improved from the previous [0.0694436611832241] for 1 times.\n",
      "Iteration [18758]: Loss[0.06946090893006363] has not improved from the previous [0.06945010154499327] for 3 times.\n",
      "Iteration [18761]: Loss[0.06944711604433772] has not improved from the previous [0.0694452483681842] for 1 times.\n",
      "Iteration [18764]: Loss[0.06944468638001493] has not improved from the previous [0.0694441808550132] for 1 times.\n",
      "Iteration [18766]: Loss[0.06944398196466997] has not improved from the previous [0.06944332701120186] for 1 times.\n",
      "Iteration [18767]: Loss[0.0694454967236411] has not improved from the previous [0.06944398196466997] for 3 times.\n",
      "Iteration [18769]: Loss[0.06944376777260519] has not improved from the previous [0.0694414891004592] for 1 times.\n",
      "Iteration [18771]: Loss[0.06944714425999936] has not improved from the previous [0.06944212403829712] for 1 times.\n",
      "Iteration [18774]: Loss[0.06944709183349997] has not improved from the previous [0.06943870550707978] for 1 times.\n",
      "Iteration [18775]: Loss[0.06945605523610678] has not improved from the previous [0.06944709183349997] for 3 times.\n",
      "Iteration [18778]: Loss[0.06944182090167529] has not improved from the previous [0.06944105251435591] for 1 times.\n",
      "Iteration [18783]: Loss[0.06943971882768081] has not improved from the previous [0.06943921684874575] for 1 times.\n",
      "Iteration [18784]: Loss[0.0694406190235379] has not improved from the previous [0.06943971882768081] for 3 times.\n",
      "Iteration [18786]: Loss[0.06943749207602364] has not improved from the previous [0.0694367002255613] for 1 times.\n",
      "Iteration [18787]: Loss[0.06944390055518547] has not improved from the previous [0.06943749207602364] for 3 times.\n",
      "Iteration [18788]: Loss[0.06945437821485301] has not improved from the previous [0.06944390055518547] for 5 times.\n",
      "Iteration [18791]: Loss[0.06943882909302328] has not improved from the previous [0.06943689933586922] for 1 times.\n",
      "Iteration [18793]: Loss[0.06943595459130507] has not improved from the previous [0.0694359032544439] for 1 times.\n",
      "Iteration [18794]: Loss[0.0694370564830714] has not improved from the previous [0.06943595459130507] for 3 times.\n",
      "Iteration [18795]: Loss[0.06943993667950059] has not improved from the previous [0.0694370564830714] for 5 times.\n",
      "Iteration [18799]: Loss[0.0694355404693937] has not improved from the previous [0.06943417320362615] for 1 times.\n",
      "Iteration [18801]: Loss[0.06943628657005425] has not improved from the previous [0.06943448031459455] for 1 times.\n",
      "Iteration [18804]: Loss[0.06944103714110547] has not improved from the previous [0.06943220131639342] for 1 times.\n",
      "Iteration [18805]: Loss[0.06944919442779908] has not improved from the previous [0.06944103714110547] for 3 times.\n",
      "Iteration [18808]: Loss[0.0694338656315251] has not improved from the previous [0.06943241713817402] for 1 times.\n",
      "Iteration [18810]: Loss[0.0694315770436767] has not improved from the previous [0.06943140152296819] for 1 times.\n",
      "Iteration [18811]: Loss[0.06943660042689573] has not improved from the previous [0.0694315770436767] for 3 times.\n",
      "Iteration [18814]: Loss[0.06943356020802831] has not improved from the previous [0.06942863122813808] for 1 times.\n",
      "Iteration [18816]: Loss[0.06942977090747438] has not improved from the previous [0.06942778904983544] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 18800 Loss 0.06943448031459455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [18817]: Loss[0.06943796865958896] has not improved from the previous [0.06942977090747438] for 3 times.\n",
      "Iteration [18818]: Loss[0.06944671412365547] has not improved from the previous [0.06943796865958896] for 5 times.\n",
      "Iteration [18821]: Loss[0.06943004044397594] has not improved from the previous [0.06942916202800052] for 1 times.\n",
      "Iteration [18823]: Loss[0.06942848874188706] has not improved from the previous [0.0694281347818219] for 1 times.\n",
      "Iteration [18824]: Loss[0.06943381958251785] has not improved from the previous [0.06942848874188706] for 3 times.\n",
      "Iteration [18827]: Loss[0.0694318563341609] has not improved from the previous [0.0694251530628208] for 1 times.\n",
      "Iteration [18829]: Loss[0.06942945756346246] has not improved from the previous [0.06942849331284168] for 1 times.\n",
      "Iteration [18831]: Loss[0.06942739012130521] has not improved from the previous [0.06942356630745851] for 1 times.\n",
      "Iteration [18832]: Loss[0.06942785571241296] has not improved from the previous [0.06942739012130521] for 3 times.\n",
      "Iteration [18833]: Loss[0.06944370459303491] has not improved from the previous [0.06942785571241296] for 5 times.\n",
      "Iteration [18837]: Loss[0.0694256403890085] has not improved from the previous [0.06942553334124557] for 1 times.\n",
      "Iteration [18840]: Loss[0.06942904103607024] has not improved from the previous [0.06942278195096126] for 1 times.\n",
      "Iteration [18844]: Loss[0.06942532199249093] has not improved from the previous [0.06941951437174144] for 1 times.\n",
      "Iteration [18846]: Loss[0.06942344368998091] has not improved from the previous [0.06942262178168057] for 1 times.\n",
      "Iteration [18850]: Loss[0.06942355985178153] has not improved from the previous [0.06942121273920124] for 1 times.\n",
      "Iteration [18851]: Loss[0.06943232665205357] has not improved from the previous [0.06942355985178153] for 3 times.\n",
      "Iteration [18854]: Loss[0.06942064351436826] has not improved from the previous [0.06941934668222648] for 1 times.\n",
      "Iteration [18855]: Loss[0.06942140756178801] has not improved from the previous [0.06942064351436826] for 3 times.\n",
      "Iteration [18856]: Loss[0.06942271050623454] has not improved from the previous [0.06942140756178801] for 5 times.\n",
      "Iteration [18859]: Loss[0.06942035595190299] has not improved from the previous [0.06941763740782536] for 1 times.\n",
      "Iteration [18861]: Loss[0.06941728679417211] has not improved from the previous [0.0694172368743354] for 1 times.\n",
      "Iteration [18862]: Loss[0.06941937536204736] has not improved from the previous [0.06941728679417211] for 3 times.\n",
      "Iteration [18863]: Loss[0.06941994336478052] has not improved from the previous [0.06941937536204736] for 5 times.\n",
      "Iteration [18865]: Loss[0.06941818087790867] has not improved from the previous [0.06941647233360627] for 1 times.\n",
      "Iteration [18867]: Loss[0.06943797342005754] has not improved from the previous [0.06941592828676421] for 1 times.\n",
      "Iteration [18870]: Loss[0.0694187177765467] has not improved from the previous [0.06941607453232758] for 1 times.\n",
      "Iteration [18872]: Loss[0.0694166852280381] has not improved from the previous [0.06941375689292441] for 1 times.\n",
      "Iteration [18874]: Loss[0.0694205510681018] has not improved from the previous [0.06941452179708815] for 1 times.\n",
      "Iteration [18876]: Loss[0.06941458801725213] has not improved from the previous [0.06941452967289831] for 1 times.\n",
      "Iteration [18878]: Loss[0.06941464698564685] has not improved from the previous [0.0694136092387327] for 1 times.\n",
      "Iteration [18880]: Loss[0.06941544804553208] has not improved from the previous [0.06941367814866826] for 1 times.\n",
      "Iteration [18882]: Loss[0.06941168486338459] has not improved from the previous [0.06941127562678635] for 1 times.\n",
      "Iteration [18883]: Loss[0.06942093069555356] has not improved from the previous [0.06941168486338459] for 3 times.\n",
      "Iteration [18884]: Loss[0.06942851393588223] has not improved from the previous [0.06942093069555356] for 5 times.\n",
      "Iteration [18887]: Loss[0.06941273576260994] has not improved from the previous [0.06941180111613603] for 1 times.\n",
      "Iteration [18889]: Loss[0.06941085901382797] has not improved from the previous [0.06941080810160338] for 1 times.\n",
      "Iteration [18891]: Loss[0.06941233734661613] has not improved from the previous [0.06940961708143492] for 1 times.\n",
      "Iteration [18893]: Loss[0.06941246214501834] has not improved from the previous [0.06940978864569335] for 1 times.\n",
      "Iteration [18895]: Loss[0.06940859925180348] has not improved from the previous [0.06940723934785668] for 1 times.\n",
      "Iteration [18896]: Loss[0.06941776248012507] has not improved from the previous [0.06940859925180348] for 3 times.\n",
      "Iteration [18897]: Loss[0.0694253527114091] has not improved from the previous [0.06941776248012507] for 5 times.\n",
      "Iteration [18900]: Loss[0.06940892050145926] has not improved from the previous [0.06940850232553097] for 1 times.\n",
      "Iteration [18902]: Loss[0.06940805677577813] has not improved from the previous [0.06940749491987547] for 1 times.\n",
      "Iteration [18903]: Loss[0.06941217705505387] has not improved from the previous [0.06940805677577813] for 3 times.\n",
      "Iteration [18906]: Loss[0.06940965422906477] has not improved from the previous [0.06940368465826696] for 1 times.\n",
      "Iteration [18908]: Loss[0.06941273325822304] has not improved from the previous [0.06940437531081567] for 1 times.\n",
      "Iteration [18911]: Loss[0.06940999649798921] has not improved from the previous [0.06940371408035265] for 1 times.\n",
      "Iteration [18912]: Loss[0.06942073177830377] has not improved from the previous [0.06940999649798921] for 3 times.\n",
      "Iteration [18916]: Loss[0.06940490910663331] has not improved from the previous [0.0694044246827619] for 1 times.\n",
      "Iteration [18919]: Loss[0.06940479962014014] has not improved from the previous [0.06940166665830058] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 18900 Loss 0.06940892050145926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [18921]: Loss[0.06940956122611093] has not improved from the previous [0.06940200827410757] for 1 times.\n",
      "Iteration [18924]: Loss[0.06940753350180313] has not improved from the previous [0.06940047018904669] for 1 times.\n",
      "Iteration [18925]: Loss[0.0694174222313711] has not improved from the previous [0.06940753350180313] for 3 times.\n",
      "Iteration [18929]: Loss[0.06940118613576211] has not improved from the previous [0.0694010534935821] for 1 times.\n",
      "Iteration [18932]: Loss[0.06940203383307937] has not improved from the previous [0.06939821031637737] for 1 times.\n",
      "Iteration [18934]: Loss[0.0694024136751745] has not improved from the previous [0.06939853163051513] for 1 times.\n",
      "Iteration [18936]: Loss[0.06939846609442547] has not improved from the previous [0.0693963639153337] for 1 times.\n",
      "Iteration [18937]: Loss[0.06940338608889036] has not improved from the previous [0.06939846609442547] for 3 times.\n",
      "Iteration [18938]: Loss[0.06941480353283602] has not improved from the previous [0.06940338608889036] for 5 times.\n",
      "Iteration [18944]: Loss[0.06940134837988791] has not improved from the previous [0.06939725634317227] for 1 times.\n",
      "Iteration [18947]: Loss[0.06939956317099112] has not improved from the previous [0.06939264719838557] for 1 times.\n",
      "Iteration [18949]: Loss[0.06940186917440527] has not improved from the previous [0.06939367235458326] for 1 times.\n",
      "Iteration [18952]: Loss[0.06939622304931967] has not improved from the previous [0.06939288233221123] for 1 times.\n",
      "Iteration [18954]: Loss[0.06939567661677104] has not improved from the previous [0.06939355258286288] for 1 times.\n",
      "Iteration [18958]: Loss[0.06940145296526383] has not improved from the previous [0.06939162574245365] for 1 times.\n",
      "Iteration [18959]: Loss[0.06940822116127532] has not improved from the previous [0.06940145296526383] for 3 times.\n",
      "Iteration [18962]: Loss[0.06939364845380787] has not improved from the previous [0.06939160510917493] for 1 times.\n",
      "Iteration [18966]: Loss[0.06939216623976734] has not improved from the previous [0.06939088949496919] for 1 times.\n",
      "Iteration [18968]: Loss[0.06939201515333493] has not improved from the previous [0.06939084011949562] for 1 times.\n",
      "Iteration [18971]: Loss[0.06939840856093735] has not improved from the previous [0.06938775146590646] for 1 times.\n",
      "Iteration [18972]: Loss[0.06940444896462013] has not improved from the previous [0.06939840856093735] for 3 times.\n",
      "Iteration [18975]: Loss[0.06938959294202349] has not improved from the previous [0.06938854665774163] for 1 times.\n",
      "Iteration [18979]: Loss[0.0693903749461023] has not improved from the previous [0.06938762681032917] for 1 times.\n",
      "Iteration [18981]: Loss[0.06938855395808803] has not improved from the previous [0.0693855787086413] for 1 times.\n",
      "Iteration [18983]: Loss[0.06939141756583948] has not improved from the previous [0.06938642438729108] for 1 times.\n",
      "Iteration [18986]: Loss[0.06938813592544157] has not improved from the previous [0.06938280176448605] for 1 times.\n",
      "Iteration [18988]: Loss[0.06940533690185308] has not improved from the previous [0.06938461133554424] for 1 times.\n",
      "Iteration [18992]: Loss[0.06938666738479476] has not improved from the previous [0.06938358358204556] for 1 times.\n",
      "Iteration [18994]: Loss[0.06938252418214552] has not improved from the previous [0.06938249057137892] for 1 times.\n",
      "Iteration [18995]: Loss[0.06938825994476161] has not improved from the previous [0.06938252418214552] for 3 times.\n",
      "Iteration [18999]: Loss[0.06938364591058675] has not improved from the previous [0.06937993790733721] for 1 times.\n",
      "Iteration [19003]: Loss[0.06938156241082333] has not improved from the previous [0.06938149224764927] for 1 times.\n",
      "Iteration [19005]: Loss[0.06938376075730476] has not improved from the previous [0.0693789940692251] for 1 times.\n",
      "Iteration [19006]: Loss[0.0693912798084821] has not improved from the previous [0.06938376075730476] for 3 times.\n",
      "Iteration [19010]: Loss[0.06938163588204657] has not improved from the previous [0.06937899659712747] for 1 times.\n",
      "Iteration [19012]: Loss[0.06938072806201656] has not improved from the previous [0.06937674778319158] for 1 times.\n",
      "Iteration [19015]: Loss[0.0693782785532027] has not improved from the previous [0.06937769208342602] for 1 times.\n",
      "Iteration [19016]: Loss[0.06937911057471172] has not improved from the previous [0.0693782785532027] for 3 times.\n",
      "Iteration [19017]: Loss[0.06938080054670898] has not improved from the previous [0.06937911057471172] for 5 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 19000 Loss 0.06938294863009464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [19020]: Loss[0.06937773427407574] has not improved from the previous [0.0693740246357508] for 1 times.\n",
      "Iteration [19021]: Loss[0.06938181864545107] has not improved from the previous [0.06937773427407574] for 3 times.\n",
      "Iteration [19022]: Loss[0.06939358894365287] has not improved from the previous [0.06938181864545107] for 5 times.\n",
      "Iteration [19029]: Loss[0.06937644487106498] has not improved from the previous [0.06937293603837433] for 1 times.\n",
      "Iteration [19031]: Loss[0.06937745367791151] has not improved from the previous [0.06937307469902772] for 1 times.\n",
      "Iteration [19034]: Loss[0.06938102493482023] has not improved from the previous [0.06937201184700437] for 1 times.\n",
      "Iteration [19035]: Loss[0.06938860244734507] has not improved from the previous [0.06938102493482023] for 3 times.\n",
      "Iteration [19040]: Loss[0.06937233975591096] has not improved from the previous [0.06937228661254216] for 1 times.\n",
      "Iteration [19042]: Loss[0.06937382532575659] has not improved from the previous [0.06936931134409684] for 1 times.\n",
      "Iteration [19044]: Loss[0.06937389702441092] has not improved from the previous [0.06936949196378019] for 1 times.\n",
      "Iteration [19047]: Loss[0.06937831696350448] has not improved from the previous [0.06936793702328657] for 1 times.\n",
      "Iteration [19048]: Loss[0.06938458314655997] has not improved from the previous [0.06937831696350448] for 3 times.\n",
      "Iteration [19052]: Loss[0.06936889619271465] has not improved from the previous [0.06936878262924799] for 1 times.\n",
      "Iteration [19053]: Loss[0.06936901047957783] has not improved from the previous [0.06936889619271465] for 3 times.\n",
      "Iteration [19055]: Loss[0.06937149121110842] has not improved from the previous [0.06936701115920946] for 1 times.\n",
      "Iteration [19057]: Loss[0.06936762933343016] has not improved from the previous [0.06936549428349256] for 1 times.\n",
      "Iteration [19058]: Loss[0.06937010173676245] has not improved from the previous [0.06936762933343016] for 3 times.\n",
      "Iteration [19062]: Loss[0.06936703163388383] has not improved from the previous [0.06936421568356561] for 1 times.\n",
      "Iteration [19064]: Loss[0.06937045841594126] has not improved from the previous [0.06936529035278961] for 1 times.\n",
      "Iteration [19068]: Loss[0.06936709319569175] has not improved from the previous [0.06936350497700737] for 1 times.\n",
      "Iteration [19069]: Loss[0.06937403392506623] has not improved from the previous [0.06936709319569175] for 3 times.\n",
      "Iteration [19073]: Loss[0.06936540954627579] has not improved from the previous [0.06936235931675001] for 1 times.\n",
      "Iteration [19075]: Loss[0.06936278296777437] has not improved from the previous [0.06936162808453294] for 1 times.\n",
      "Iteration [19076]: Loss[0.06936418945776379] has not improved from the previous [0.06936278296777437] for 3 times.\n",
      "Iteration [19079]: Loss[0.06936380022020944] has not improved from the previous [0.06936044757145049] for 1 times.\n",
      "Iteration [19081]: Loss[0.06936773967112315] has not improved from the previous [0.06935922416133501] for 1 times.\n",
      "Iteration [19084]: Loss[0.06936716641788361] has not improved from the previous [0.06935820040701506] for 1 times.\n",
      "Iteration [19085]: Loss[0.06937491815341872] has not improved from the previous [0.06936716641788361] for 3 times.\n",
      "Iteration [19090]: Loss[0.06936129384794543] has not improved from the previous [0.0693586095669652] for 1 times.\n",
      "Iteration [19092]: Loss[0.06935802715122918] has not improved from the previous [0.06935634179508816] for 1 times.\n",
      "Iteration [19093]: Loss[0.06936475541259753] has not improved from the previous [0.06935802715122918] for 3 times.\n",
      "Iteration [19094]: Loss[0.06937424541873746] has not improved from the previous [0.06936475541259753] for 5 times.\n",
      "Iteration [19101]: Loss[0.06935853170974049] has not improved from the previous [0.06935433399202932] for 1 times.\n",
      "Iteration [19103]: Loss[0.06935892949488494] has not improved from the previous [0.06935437832673834] for 1 times.\n",
      "Iteration [19106]: Loss[0.06936197812908869] has not improved from the previous [0.06935286607998259] for 1 times.\n",
      "Iteration [19107]: Loss[0.06936936327324551] has not improved from the previous [0.06936197812908869] for 3 times.\n",
      "Iteration [19112]: Loss[0.06935518848250816] has not improved from the previous [0.06935292761719791] for 1 times.\n",
      "Iteration [19114]: Loss[0.0693599703819504] has not improved from the previous [0.06935134685735579] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 19100 Loss 0.06935433399202932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [19117]: Loss[0.0693560533057203] has not improved from the previous [0.06935034610598856] for 1 times.\n",
      "Iteration [19119]: Loss[0.06935396890221925] has not improved from the previous [0.0693530999092789] for 1 times.\n",
      "Iteration [19121]: Loss[0.06935081657726007] has not improved from the previous [0.06935002268434663] for 1 times.\n",
      "Iteration [19123]: Loss[0.06935469935357444] has not improved from the previous [0.06935059848426144] for 1 times.\n",
      "Iteration [19125]: Loss[0.0693517213833836] has not improved from the previous [0.06934981708356876] for 1 times.\n",
      "Iteration [19127]: Loss[0.06934835344181105] has not improved from the previous [0.0693474424160228] for 1 times.\n",
      "Iteration [19128]: Loss[0.06935894430145707] has not improved from the previous [0.06934835344181105] for 3 times.\n",
      "Iteration [19129]: Loss[0.06936467073415853] has not improved from the previous [0.06935894430145707] for 5 times.\n",
      "Iteration [19134]: Loss[0.06934853232632182] has not improved from the previous [0.06934781705591582] for 1 times.\n",
      "Iteration [19135]: Loss[0.06935057086196367] has not improved from the previous [0.06934853232632182] for 3 times.\n",
      "Iteration [19138]: Loss[0.06934732352638263] has not improved from the previous [0.06934420121112693] for 1 times.\n",
      "Iteration [19139]: Loss[0.0693489861940164] has not improved from the previous [0.06934732352638263] for 3 times.\n",
      "Iteration [19140]: Loss[0.06936292664165815] has not improved from the previous [0.0693489861940164] for 5 times.\n",
      "Iteration [19144]: Loss[0.06934621654142194] has not improved from the previous [0.06934527340348712] for 1 times.\n",
      "Iteration [19147]: Loss[0.06934639524349122] has not improved from the previous [0.06934227252108856] for 1 times.\n",
      "Iteration [19149]: Loss[0.06934951887366086] has not improved from the previous [0.06934385709661328] for 1 times.\n",
      "Iteration [19152]: Loss[0.06934455254171512] has not improved from the previous [0.06934083756844385] for 1 times.\n",
      "Iteration [19155]: Loss[0.06934257230167039] has not improved from the previous [0.06934206712970718] for 1 times.\n",
      "Iteration [19157]: Loss[0.0693454996300213] has not improved from the previous [0.06934191581515034] for 1 times.\n",
      "Iteration [19160]: Loss[0.06934203410806572] has not improved from the previous [0.06933805638512665] for 1 times.\n",
      "Iteration [19161]: Loss[0.06934563927606203] has not improved from the previous [0.06934203410806572] for 3 times.\n",
      "Iteration [19162]: Loss[0.06935759733038503] has not improved from the previous [0.06934563927606203] for 5 times.\n",
      "Iteration [19166]: Loss[0.06934062474582495] has not improved from the previous [0.06933935078596083] for 1 times.\n",
      "Iteration [19169]: Loss[0.06934084158006376] has not improved from the previous [0.06933631417223789] for 1 times.\n",
      "Iteration [19171]: Loss[0.06934382460501837] has not improved from the previous [0.06933790264293385] for 1 times.\n",
      "Iteration [19174]: Loss[0.06933856887384876] has not improved from the previous [0.06933527068596484] for 1 times.\n",
      "Iteration [19175]: Loss[0.06934731793253868] has not improved from the previous [0.06933856887384876] for 3 times.\n",
      "Iteration [19177]: Loss[0.06933970021479524] has not improved from the previous [0.069337977579232] for 1 times.\n",
      "Iteration [19180]: Loss[0.06933942559479232] has not improved from the previous [0.06933311324170977] for 1 times.\n",
      "Iteration [19183]: Loss[0.06934302239554238] has not improved from the previous [0.06933333715447144] for 1 times.\n",
      "Iteration [19184]: Loss[0.06934976080307267] has not improved from the previous [0.06934302239554238] for 3 times.\n",
      "Iteration [19189]: Loss[0.06933589692628538] has not improved from the previous [0.06933282021345541] for 1 times.\n",
      "Iteration [19191]: Loss[0.06934006779994423] has not improved from the previous [0.06933187606032647] for 1 times.\n",
      "Iteration [19194]: Loss[0.06933394014754031] has not improved from the previous [0.06933055859126268] for 1 times.\n",
      "Iteration [19196]: Loss[0.0693329628030528] has not improved from the previous [0.06933203900678682] for 1 times.\n",
      "Iteration [19200]: Loss[0.06933334600898318] has not improved from the previous [0.06932850255272664] for 1 times.\n",
      "Iteration [19202]: Loss[0.06933368369354356] has not improved from the previous [0.06932875228475871] for 1 times.\n",
      "Iteration [19204]: Loss[0.06932810816385167] has not improved from the previous [0.06932795754163165] for 1 times.\n",
      "Iteration [19205]: Loss[0.06933811232697418] has not improved from the previous [0.06932810816385167] for 3 times.\n",
      "Iteration [19206]: Loss[0.06934412919998945] has not improved from the previous [0.06933811232697418] for 5 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 19200 Loss 0.06933334600898318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [19211]: Loss[0.06933044251881153] has not improved from the previous [0.06932672908156122] for 1 times.\n",
      "Iteration [19213]: Loss[0.06933393469144755] has not improved from the previous [0.06932604004689442] for 1 times.\n",
      "Iteration [19216]: Loss[0.06932802874300814] has not improved from the previous [0.06932498900897432] for 1 times.\n",
      "Iteration [19218]: Loss[0.06933630030835175] has not improved from the previous [0.069326833875098] for 1 times.\n",
      "Iteration [19222]: Loss[0.06932918680452671] has not improved from the previous [0.0693227454400356] for 1 times.\n",
      "Iteration [19224]: Loss[0.06932361169246741] has not improved from the previous [0.06932316565855622] for 1 times.\n",
      "Iteration [19225]: Loss[0.06932882941396482] has not improved from the previous [0.06932361169246741] for 3 times.\n",
      "Iteration [19230]: Loss[0.06932810729201054] has not improved from the previous [0.06932299473411696] for 1 times.\n",
      "Iteration [19233]: Loss[0.06932509485700643] has not improved from the previous [0.0693190782695772] for 1 times.\n",
      "Iteration [19235]: Loss[0.06934074704878693] has not improved from the previous [0.0693229690340239] for 1 times.\n",
      "Iteration [19237]: Loss[0.06932443424019649] has not improved from the previous [0.06932347007345248] for 1 times.\n",
      "Iteration [19239]: Loss[0.06932350011860754] has not improved from the previous [0.06931974385720042] for 1 times.\n",
      "Iteration [19242]: Loss[0.06932637108932015] has not improved from the previous [0.0693170236708593] for 1 times.\n",
      "Iteration [19243]: Loss[0.0693333526554675] has not improved from the previous [0.06932637108932015] for 3 times.\n",
      "Iteration [19246]: Loss[0.06932074709437396] has not improved from the previous [0.06932015990737436] for 1 times.\n",
      "Iteration [19248]: Loss[0.06932049381114784] has not improved from the previous [0.06931863345869166] for 1 times.\n",
      "Iteration [19251]: Loss[0.06932456032831334] has not improved from the previous [0.0693160545407642] for 1 times.\n",
      "Iteration [19253]: Loss[0.06931987037871423] has not improved from the previous [0.06931832511111782] for 1 times.\n",
      "Iteration [19255]: Loss[0.06931820795604474] has not improved from the previous [0.06931449129543345] for 1 times.\n",
      "Iteration [19259]: Loss[0.0693174220559161] has not improved from the previous [0.06931495729321838] for 1 times.\n",
      "Iteration [19261]: Loss[0.06932060996251266] has not improved from the previous [0.06931507274726738] for 1 times.\n",
      "Iteration [19264]: Loss[0.06931617461866166] has not improved from the previous [0.06931194321700321] for 1 times.\n",
      "Iteration [19265]: Loss[0.0693165286443564] has not improved from the previous [0.06931617461866166] for 3 times.\n",
      "Iteration [19266]: Loss[0.06933304769297675] has not improved from the previous [0.0693165286443564] for 5 times.\n",
      "Iteration [19268]: Loss[0.06931614813039622] has not improved from the previous [0.06931604555232351] for 1 times.\n",
      "Iteration [19270]: Loss[0.06931398017285212] has not improved from the previous [0.0693114364663042] for 1 times.\n",
      "Iteration [19271]: Loss[0.06931459800686031] has not improved from the previous [0.06931398017285212] for 3 times.\n",
      "Iteration [19272]: Loss[0.06931506528751903] has not improved from the previous [0.06931459800686031] for 5 times.\n",
      "Iteration [19275]: Loss[0.06931232550327766] has not improved from the previous [0.06931072304641224] for 1 times.\n",
      "Iteration [19279]: Loss[0.06931358540312389] has not improved from the previous [0.06931090113910279] for 1 times.\n",
      "Iteration [19281]: Loss[0.0693095331731503] has not improved from the previous [0.06930856261095036] for 1 times.\n",
      "Iteration [19282]: Loss[0.06931801146903754] has not improved from the previous [0.0693095331731503] for 3 times.\n",
      "Iteration [19283]: Loss[0.06932504516166318] has not improved from the previous [0.06931801146903754] for 5 times.\n",
      "Iteration [19288]: Loss[0.06931045347681442] has not improved from the previous [0.06930845574441932] for 1 times.\n",
      "Iteration [19290]: Loss[0.0693065816441046] has not improved from the previous [0.06930654801101577] for 1 times.\n",
      "Iteration [19291]: Loss[0.06931680132963046] has not improved from the previous [0.0693065816441046] for 3 times.\n",
      "Iteration [19292]: Loss[0.0693225681541504] has not improved from the previous [0.06931680132963046] for 5 times.\n",
      "Iteration [19297]: Loss[0.06930815805217436] has not improved from the previous [0.06930600715270228] for 1 times.\n",
      "Iteration [19299]: Loss[0.06931214749824699] has not improved from the previous [0.0693050002802903] for 1 times.\n",
      "Iteration [19302]: Loss[0.0693066889476979] has not improved from the previous [0.06930304178417077] for 1 times.\n",
      "Iteration [19304]: Loss[0.06930557270584435] has not improved from the previous [0.06930439648642443] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 19300 Loss 0.06930576175241313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [19308]: Loss[0.06930570671267028] has not improved from the previous [0.06930110058094081] for 1 times.\n",
      "Iteration [19310]: Loss[0.06930442398686096] has not improved from the previous [0.06930113522076452] for 1 times.\n",
      "Iteration [19311]: Loss[0.06930451930280153] has not improved from the previous [0.06930442398686096] for 3 times.\n",
      "Iteration [19312]: Loss[0.06931894053481792] has not improved from the previous [0.06930451930280153] for 5 times.\n",
      "Iteration [19317]: Loss[0.06930432376033886] has not improved from the previous [0.0693008523678927] for 1 times.\n",
      "Iteration [19319]: Loss[0.0692997490839317] has not improved from the previous [0.06929961846618926] for 1 times.\n",
      "Iteration [19320]: Loss[0.0693059551875468] has not improved from the previous [0.0692997490839317] for 3 times.\n",
      "Iteration [19321]: Loss[0.06931555919915462] has not improved from the previous [0.0693059551875468] for 5 times.\n",
      "Iteration [19326]: Loss[0.06930167011794224] has not improved from the previous [0.06929872340782209] for 1 times.\n",
      "Iteration [19328]: Loss[0.06929743979583793] has not improved from the previous [0.06929714541051245] for 1 times.\n",
      "Iteration [19329]: Loss[0.06930392278499847] has not improved from the previous [0.06929743979583793] for 3 times.\n",
      "Iteration [19331]: Loss[0.06929978569875472] has not improved from the previous [0.06929941859365922] for 1 times.\n",
      "Iteration [19333]: Loss[0.06929764728810239] has not improved from the previous [0.06929559854854002] for 1 times.\n",
      "Iteration [19334]: Loss[0.06929862412381806] has not improved from the previous [0.06929764728810239] for 3 times.\n",
      "Iteration [19337]: Loss[0.06929848165080953] has not improved from the previous [0.06929475757317022] for 1 times.\n",
      "Iteration [19339]: Loss[0.06930109126166678] has not improved from the previous [0.0692949787849311] for 1 times.\n",
      "Iteration [19342]: Loss[0.06929593167345734] has not improved from the previous [0.06929298966873487] for 1 times.\n",
      "Iteration [19343]: Loss[0.06929698228301848] has not improved from the previous [0.06929593167345734] for 3 times.\n",
      "Iteration [19344]: Loss[0.06931332166079769] has not improved from the previous [0.06929698228301848] for 5 times.\n",
      "Iteration [19348]: Loss[0.06929404573033417] has not improved from the previous [0.06929212433728293] for 1 times.\n",
      "Iteration [19349]: Loss[0.06929481504525159] has not improved from the previous [0.06929404573033417] for 3 times.\n",
      "Iteration [19350]: Loss[0.0692950475452949] has not improved from the previous [0.06929481504525159] for 5 times.\n",
      "Iteration [19351]: Loss[0.06929555870497045] has not improved from the previous [0.0692950475452949] for 7 times.\n",
      "Iteration [19353]: Loss[0.0692921521011118] has not improved from the previous [0.06929164447670272] for 1 times.\n",
      "Iteration [19357]: Loss[0.06930343289212405] has not improved from the previous [0.0692903012505892] for 1 times.\n",
      "Iteration [19360]: Loss[0.0692935060639305] has not improved from the previous [0.06929105965836226] for 1 times.\n",
      "Iteration [19362]: Loss[0.06929047463714426] has not improved from the previous [0.06928841457214363] for 1 times.\n",
      "Iteration [19364]: Loss[0.0692937078046199] has not improved from the previous [0.06929042871439148] for 1 times.\n",
      "Iteration [19366]: Loss[0.06929092452636099] has not improved from the previous [0.06928884690849464] for 1 times.\n",
      "Iteration [19369]: Loss[0.06929778566298041] has not improved from the previous [0.06928604327190316] for 1 times.\n",
      "Iteration [19370]: Loss[0.06930200912102624] has not improved from the previous [0.06929778566298041] for 3 times.\n",
      "Iteration [19373]: Loss[0.0692883312149971] has not improved from the previous [0.06928821820977653] for 1 times.\n",
      "Iteration [19375]: Loss[0.06928873821904193] has not improved from the previous [0.06928613632962083] for 1 times.\n",
      "Iteration [19377]: Loss[0.06929147610030661] has not improved from the previous [0.06928571182666161] for 1 times.\n",
      "Iteration [19380]: Loss[0.06928682388756657] has not improved from the previous [0.06928363337760254] for 1 times.\n",
      "Iteration [19382]: Loss[0.06928929842339274] has not improved from the previous [0.06928498062524761] for 1 times.\n",
      "Iteration [19386]: Loss[0.06928561557055865] has not improved from the previous [0.0692822868699106] for 1 times.\n",
      "Iteration [19387]: Loss[0.06929154683241068] has not improved from the previous [0.06928561557055865] for 3 times.\n",
      "Iteration [19391]: Loss[0.06928371150142117] has not improved from the previous [0.0692825893440793] for 1 times.\n",
      "Iteration [19395]: Loss[0.0692852016996034] has not improved from the previous [0.06928045037926783] for 1 times.\n",
      "Iteration [19398]: Loss[0.06928559595115635] has not improved from the previous [0.06927986228603158] for 1 times.\n",
      "Iteration [19399]: Loss[0.06929517158653244] has not improved from the previous [0.06928559595115635] for 3 times.\n",
      "Iteration [19402]: Loss[0.06928184095985485] has not improved from the previous [0.06928158716103666] for 1 times.\n",
      "Iteration [19404]: Loss[0.06928203892912206] has not improved from the previous [0.06927919343557658] for 1 times.\n",
      "Iteration [19407]: Loss[0.06928466811810732] has not improved from the previous [0.06927656446992683] for 1 times.\n",
      "Iteration [19409]: Loss[0.06928046039935441] has not improved from the previous [0.06927861346684554] for 1 times.\n",
      "Iteration [19411]: Loss[0.06927784895783744] has not improved from the previous [0.0692772827832629] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 19400 Loss 0.06928492769785431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [19413]: Loss[0.06928048509927844] has not improved from the previous [0.06927667904960796] for 1 times.\n",
      "Iteration [19415]: Loss[0.06927611185460185] has not improved from the previous [0.06927547197370391] for 1 times.\n",
      "Iteration [19416]: Loss[0.06928245522052041] has not improved from the previous [0.06927611185460185] for 3 times.\n",
      "Iteration [19417]: Loss[0.06929126774383139] has not improved from the previous [0.06928245522052041] for 5 times.\n",
      "Iteration [19422]: Loss[0.069277492673916] has not improved from the previous [0.06927471876068542] for 1 times.\n",
      "Iteration [19425]: Loss[0.06928150033994003] has not improved from the previous [0.06927230127739692] for 1 times.\n",
      "Iteration [19426]: Loss[0.06928800849495287] has not improved from the previous [0.06928150033994003] for 3 times.\n",
      "Iteration [19429]: Loss[0.069274781773092] has not improved from the previous [0.06927463690072538] for 1 times.\n",
      "Iteration [19431]: Loss[0.06927517244569069] has not improved from the previous [0.06927215310110703] for 1 times.\n",
      "Iteration [19434]: Loss[0.06927944030851467] has not improved from the previous [0.06926963377166413] for 1 times.\n",
      "Iteration [19436]: Loss[0.06927339005314204] has not improved from the previous [0.06927183582728991] for 1 times.\n",
      "Iteration [19438]: Loss[0.0692710645936393] has not improved from the previous [0.06927034878060771] for 1 times.\n",
      "Iteration [19439]: Loss[0.0692711595835936] has not improved from the previous [0.0692710645936393] for 3 times.\n",
      "Iteration [19440]: Loss[0.06927369957074808] has not improved from the previous [0.0692711595835936] for 5 times.\n",
      "Iteration [19442]: Loss[0.06926912748910943] has not improved from the previous [0.06926856978565521] for 1 times.\n",
      "Iteration [19443]: Loss[0.06927673259722252] has not improved from the previous [0.06926912748910943] for 3 times.\n",
      "Iteration [19444]: Loss[0.06928427445696607] has not improved from the previous [0.06927673259722252] for 5 times.\n",
      "Iteration [19449]: Loss[0.06927069832599786] has not improved from the previous [0.0692676477357625] for 1 times.\n",
      "Iteration [19452]: Loss[0.06927572849185504] has not improved from the previous [0.06926521746911633] for 1 times.\n",
      "Iteration [19453]: Loss[0.06928099494130749] has not improved from the previous [0.06927572849185504] for 3 times.\n",
      "Iteration [19456]: Loss[0.06926803605483083] has not improved from the previous [0.06926784800669765] for 1 times.\n",
      "Iteration [19458]: Loss[0.06926840454258112] has not improved from the previous [0.06926513341634032] for 1 times.\n",
      "Iteration [19460]: Loss[0.06927025845802286] has not improved from the previous [0.06926641977972364] for 1 times.\n",
      "Iteration [19463]: Loss[0.06926582316064495] has not improved from the previous [0.06926327325145036] for 1 times.\n",
      "Iteration [19465]: Loss[0.06926860683361727] has not improved from the previous [0.06926575649056554] for 1 times.\n",
      "Iteration [19467]: Loss[0.06926573878551637] has not improved from the previous [0.06926394469546623] for 1 times.\n",
      "Iteration [19470]: Loss[0.06927256539122933] has not improved from the previous [0.06926090381387368] for 1 times.\n",
      "Iteration [19471]: Loss[0.06927659772086255] has not improved from the previous [0.06927256539122933] for 3 times.\n",
      "Iteration [19474]: Loss[0.06926354282142583] has not improved from the previous [0.06926317350514799] for 1 times.\n",
      "Iteration [19476]: Loss[0.06926376523323748] has not improved from the previous [0.06926072774149702] for 1 times.\n",
      "Iteration [19478]: Loss[0.06926565416602175] has not improved from the previous [0.06926196040573943] for 1 times.\n",
      "Iteration [19481]: Loss[0.0692613127115369] has not improved from the previous [0.06925869373466312] for 1 times.\n",
      "Iteration [19483]: Loss[0.0692640551972628] has not improved from the previous [0.0692613074932736] for 1 times.\n",
      "Iteration [19485]: Loss[0.06926121215044527] has not improved from the previous [0.06925942028773177] for 1 times.\n",
      "Iteration [19488]: Loss[0.0692693192343786] has not improved from the previous [0.06925617634801254] for 1 times.\n",
      "Iteration [19489]: Loss[0.0692719012477702] has not improved from the previous [0.0692693192343786] for 3 times.\n",
      "Iteration [19492]: Loss[0.06925922174637165] has not improved from the previous [0.06925863781608936] for 1 times.\n",
      "Iteration [19494]: Loss[0.06925923842174257] has not improved from the previous [0.06925616286878282] for 1 times.\n",
      "Iteration [19496]: Loss[0.06926088364543705] has not improved from the previous [0.06925754344434332] for 1 times.\n",
      "Iteration [19499]: Loss[0.06925672158933216] has not improved from the previous [0.06925418747301772] for 1 times.\n",
      "Iteration [19500]: Loss[0.06925691802380685] has not improved from the previous [0.06925672158933216] for 3 times.\n",
      "Iteration [19501]: Loss[0.06925954710259868] has not improved from the previous [0.06925691802380685] for 5 times.\n",
      "Iteration [19503]: Loss[0.06925668878808483] has not improved from the previous [0.06925485473458612] for 1 times.\n",
      "Iteration [19506]: Loss[0.06926600271436364] has not improved from the previous [0.069251351331447] for 1 times.\n",
      "Iteration [19507]: Loss[0.06926710196630252] has not improved from the previous [0.06926600271436364] for 3 times.\n",
      "Iteration [19510]: Loss[0.06925495027847064] has not improved from the previous [0.06925414506106896] for 1 times.\n",
      "Iteration [19512]: Loss[0.06925475791148744] has not improved from the previous [0.06925158051903348] for 1 times.\n",
      "Iteration [19514]: Loss[0.06925603870276556] has not improved from the previous [0.06925316386223437] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 19500 Loss 0.06925691802380685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [19517]: Loss[0.06925218251034843] has not improved from the previous [0.06924967271675851] for 1 times.\n",
      "Iteration [19518]: Loss[0.0692525512437553] has not improved from the previous [0.06925218251034843] for 3 times.\n",
      "Iteration [19519]: Loss[0.0692550107553743] has not improved from the previous [0.0692525512437553] for 5 times.\n",
      "Iteration [19521]: Loss[0.06925214635585832] has not improved from the previous [0.06925029187118968] for 1 times.\n",
      "Iteration [19524]: Loss[0.06926262065090084] has not improved from the previous [0.06924651620820155] for 1 times.\n",
      "Iteration [19528]: Loss[0.06925075367547305] has not improved from the previous [0.06924959327597263] for 1 times.\n",
      "Iteration [19530]: Loss[0.06925019123584641] has not improved from the previous [0.06924704497551319] for 1 times.\n",
      "Iteration [19532]: Loss[0.0692511420298267] has not improved from the previous [0.06924882908352004] for 1 times.\n",
      "Iteration [19535]: Loss[0.06924762725020277] has not improved from the previous [0.06924513454172826] for 1 times.\n",
      "Iteration [19536]: Loss[0.06924881139484872] has not improved from the previous [0.06924762725020277] for 3 times.\n",
      "Iteration [19537]: Loss[0.06926308248224713] has not improved from the previous [0.06924881139484872] for 5 times.\n",
      "Iteration [19539]: Loss[0.06924986873333464] has not improved from the previous [0.06924973150101794] for 1 times.\n",
      "Iteration [19542]: Loss[0.06924941388704312] has not improved from the previous [0.06924290070339112] for 1 times.\n",
      "Iteration [19544]: Loss[0.06924694039933911] has not improved from the previous [0.06924625216184284] for 1 times.\n",
      "Iteration [19548]: Loss[0.06924461448269441] has not improved from the previous [0.06924292750423555] for 1 times.\n",
      "Iteration [19549]: Loss[0.0692454831523031] has not improved from the previous [0.06924461448269441] for 3 times.\n",
      "Iteration [19550]: Loss[0.06924600178278104] has not improved from the previous [0.0692454831523031] for 5 times.\n",
      "Iteration [19553]: Loss[0.06924184114239491] has not improved from the previous [0.0692408500011404] for 1 times.\n",
      "Iteration [19554]: Loss[0.06924957124091886] has not improved from the previous [0.06924184114239491] for 3 times.\n",
      "Iteration [19555]: Loss[0.06925690425029392] has not improved from the previous [0.06924957124091886] for 5 times.\n",
      "Iteration [19560]: Loss[0.06924471109938765] has not improved from the previous [0.06923860290423879] for 1 times.\n",
      "Iteration [19563]: Loss[0.06924344162349258] has not improved from the previous [0.0692376735578436] for 1 times.\n",
      "Iteration [19567]: Loss[0.06924157800902121] has not improved from the previous [0.06923818175887402] for 1 times.\n",
      "Iteration [19569]: Loss[0.06925946628831435] has not improved from the previous [0.06924007494887578] for 1 times.\n",
      "Iteration [19573]: Loss[0.06923865056274474] has not improved from the previous [0.0692378684522198] for 1 times.\n",
      "Iteration [19574]: Loss[0.06923877827420347] has not improved from the previous [0.06923865056274474] for 3 times.\n",
      "Iteration [19576]: Loss[0.06923784704107214] has not improved from the previous [0.06923407346853727] for 1 times.\n",
      "Iteration [19577]: Loss[0.06924480785090313] has not improved from the previous [0.06923784704107214] for 3 times.\n",
      "Iteration [19578]: Loss[0.0692524873140962] has not improved from the previous [0.06924480785090313] for 5 times.\n",
      "Iteration [19583]: Loss[0.06923843931969449] has not improved from the previous [0.0692337726716314] for 1 times.\n",
      "Iteration [19585]: Loss[0.06924010261267483] has not improved from the previous [0.06923524055887223] for 1 times.\n",
      "Iteration [19588]: Loss[0.06923412721747595] has not improved from the previous [0.06923334418490122] for 1 times.\n",
      "Iteration [19589]: Loss[0.0692349055285448] has not improved from the previous [0.06923412721747595] for 3 times.\n",
      "Iteration [19590]: Loss[0.06923872148389953] has not improved from the previous [0.0692349055285448] for 5 times.\n",
      "Iteration [19592]: Loss[0.06923580900350894] has not improved from the previous [0.06923217398739365] for 1 times.\n",
      "Iteration [19595]: Loss[0.0692436026061333] has not improved from the previous [0.06922978217532375] for 1 times.\n",
      "Iteration [19596]: Loss[0.06924531523935677] has not improved from the previous [0.0692436026061333] for 3 times.\n",
      "Iteration [19599]: Loss[0.06923316908226788] has not improved from the previous [0.06923307081762335] for 1 times.\n",
      "Iteration [19601]: Loss[0.06923365939741004] has not improved from the previous [0.0692289274752012] for 1 times.\n",
      "Iteration [19603]: Loss[0.06923394829241775] has not improved from the previous [0.06923167796514931] for 1 times.\n",
      "Iteration [19606]: Loss[0.06922951894645167] has not improved from the previous [0.06922855240716153] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 19600 Loss 0.0692289274752012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [19607]: Loss[0.06923115179143391] has not improved from the previous [0.06922951894645167] for 3 times.\n",
      "Iteration [19608]: Loss[0.069233891590668] has not improved from the previous [0.06923115179143391] for 5 times.\n",
      "Iteration [19610]: Loss[0.0692309873274287] has not improved from the previous [0.06922768739368598] for 1 times.\n",
      "Iteration [19613]: Loss[0.06923408419498706] has not improved from the previous [0.06922430318198422] for 1 times.\n",
      "Iteration [19614]: Loss[0.06923670773071935] has not improved from the previous [0.06923408419498706] for 3 times.\n",
      "Iteration [19617]: Loss[0.0692309427809315] has not improved from the previous [0.06922667862852307] for 1 times.\n",
      "Iteration [19620]: Loss[0.06922885532745725] has not improved from the previous [0.06922396133998827] for 1 times.\n",
      "Iteration [19624]: Loss[0.06922779177985021] has not improved from the previous [0.06922479493989331] for 1 times.\n",
      "Iteration [19626]: Loss[0.06923057356781992] has not improved from the previous [0.06922400931560381] for 1 times.\n",
      "Iteration [19629]: Loss[0.06922456479166493] has not improved from the previous [0.06922283488008181] for 1 times.\n",
      "Iteration [19630]: Loss[0.06922634624753934] has not improved from the previous [0.06922456479166493] for 3 times.\n",
      "Iteration [19631]: Loss[0.06924148337726667] has not improved from the previous [0.06922634624753934] for 5 times.\n",
      "Iteration [19633]: Loss[0.06922709852967004] has not improved from the previous [0.06922681319609539] for 1 times.\n",
      "Iteration [19636]: Loss[0.06922475117132205] has not improved from the previous [0.06922160156639678] for 1 times.\n",
      "Iteration [19638]: Loss[0.06922354458952316] has not improved from the previous [0.06922114809099095] for 1 times.\n",
      "Iteration [19639]: Loss[0.0692252928401228] has not improved from the previous [0.06922354458952316] for 3 times.\n",
      "Iteration [19642]: Loss[0.06922060420370385] has not improved from the previous [0.0692192142716288] for 1 times.\n",
      "Iteration [19643]: Loss[0.0692272750505203] has not improved from the previous [0.06922060420370385] for 3 times.\n",
      "Iteration [19644]: Loss[0.0692352925909109] has not improved from the previous [0.0692272750505203] for 5 times.\n",
      "Iteration [19649]: Loss[0.06922104811852296] has not improved from the previous [0.06921739107819631] for 1 times.\n",
      "Iteration [19651]: Loss[0.0692211483054344] has not improved from the previous [0.06922076549188354] for 1 times.\n",
      "Iteration [19652]: Loss[0.06922143926343519] has not improved from the previous [0.0692211483054344] for 3 times.\n",
      "Iteration [19654]: Loss[0.06921830263620724] has not improved from the previous [0.06921799776729706] for 1 times.\n",
      "Iteration [19656]: Loss[0.0692193918592051] has not improved from the previous [0.06921691710151263] for 1 times.\n",
      "Iteration [19658]: Loss[0.06923682540009873] has not improved from the previous [0.06921840013891646] for 1 times.\n",
      "Iteration [19663]: Loss[0.06921861182963869] has not improved from the previous [0.06921569772337158] for 1 times.\n",
      "Iteration [19666]: Loss[0.06922431661348771] has not improved from the previous [0.06921352212032517] for 1 times.\n",
      "Iteration [19667]: Loss[0.06922871706404331] has not improved from the previous [0.06922431661348771] for 3 times.\n",
      "Iteration [19672]: Loss[0.06921671240786098] has not improved from the previous [0.06921184688710345] for 1 times.\n",
      "Iteration [19674]: Loss[0.06921688601083488] has not improved from the previous [0.06921442077428033] for 1 times.\n",
      "Iteration [19677]: Loss[0.06921252356363695] has not improved from the previous [0.06921163196192961] for 1 times.\n",
      "Iteration [19679]: Loss[0.06921447086329607] has not improved from the previous [0.06921176953542837] for 1 times.\n",
      "Iteration [19681]: Loss[0.06921493300776486] has not improved from the previous [0.06921032204240721] for 1 times.\n",
      "Iteration [19684]: Loss[0.06921398964386818] has not improved from the previous [0.06920794625078848] for 1 times.\n",
      "Iteration [19688]: Loss[0.06921176202559855] has not improved from the previous [0.0692085437422142] for 1 times.\n",
      "Iteration [19689]: Loss[0.06921200742213324] has not improved from the previous [0.06921176202559855] for 3 times.\n",
      "Iteration [19690]: Loss[0.06922820580721276] has not improved from the previous [0.06921200742213324] for 5 times.\n",
      "Iteration [19695]: Loss[0.06921063754091758] has not improved from the previous [0.06920758397200719] for 1 times.\n",
      "Iteration [19697]: Loss[0.0692125202134908] has not improved from the previous [0.06920734138825158] for 1 times.\n",
      "Iteration [19700]: Loss[0.06920721033796551] has not improved from the previous [0.06920583326506272] for 1 times.\n",
      "Iteration [19701]: Loss[0.06920729016890922] has not improved from the previous [0.06920721033796551] for 3 times.\n",
      "Iteration [19702]: Loss[0.06922340342926532] has not improved from the previous [0.06920729016890922] for 5 times.\n",
      "Iteration [19704]: Loss[0.06921077750702731] has not improved from the previous [0.06920979647397675] for 1 times.\n",
      "Iteration [19707]: Loss[0.06920742734584535] has not improved from the previous [0.06920459785171541] for 1 times.\n",
      "Iteration [19709]: Loss[0.06920525683938793] has not improved from the previous [0.06920480884807349] for 1 times.\n",
      "Iteration [19711]: Loss[0.06920800738637929] has not improved from the previous [0.06920378548855831] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 19700 Loss 0.06920721033796551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [19714]: Loss[0.06920908675185655] has not improved from the previous [0.06920202758020111] for 1 times.\n",
      "Iteration [19715]: Loss[0.06921671227362194] has not improved from the previous [0.06920908675185655] for 3 times.\n",
      "Iteration [19718]: Loss[0.06920557445182735] has not improved from the previous [0.06920474558368231] for 1 times.\n",
      "Iteration [19720]: Loss[0.06920272917171483] has not improved from the previous [0.06920139583347602] for 1 times.\n",
      "Iteration [19721]: Loss[0.06920426712207108] has not improved from the previous [0.06920272917171483] for 3 times.\n",
      "Iteration [19723]: Loss[0.0692033101755046] has not improved from the previous [0.06919907107371143] for 1 times.\n",
      "Iteration [19727]: Loss[0.06920288666435548] has not improved from the previous [0.06919899836785719] for 1 times.\n",
      "Iteration [19729]: Loss[0.06920352402457891] has not improved from the previous [0.06920063700045864] for 1 times.\n",
      "Iteration [19732]: Loss[0.06919925180494757] has not improved from the previous [0.06919781196306159] for 1 times.\n",
      "Iteration [19734]: Loss[0.06920234178162502] has not improved from the previous [0.06919858560478147] for 1 times.\n",
      "Iteration [19735]: Loss[0.0692081840739897] has not improved from the previous [0.06920234178162502] for 3 times.\n",
      "Iteration [19739]: Loss[0.06919782281753477] has not improved from the previous [0.06919780228811985] for 1 times.\n",
      "Iteration [19740]: Loss[0.0691979052706532] has not improved from the previous [0.06919782281753477] for 3 times.\n",
      "Iteration [19741]: Loss[0.06919795998049601] has not improved from the previous [0.0691979052706532] for 5 times.\n",
      "Iteration [19743]: Loss[0.06920287701908744] has not improved from the previous [0.06919593905306462] for 1 times.\n",
      "Iteration [19746]: Loss[0.06919686903772866] has not improved from the previous [0.06919391171280401] for 1 times.\n",
      "Iteration [19747]: Loss[0.069198630468968] has not improved from the previous [0.06919686903772866] for 3 times.\n",
      "Iteration [19748]: Loss[0.06921257435508736] has not improved from the previous [0.069198630468968] for 5 times.\n",
      "Iteration [19750]: Loss[0.06919929081765996] has not improved from the previous [0.06919803974648642] for 1 times.\n",
      "Iteration [19753]: Loss[0.06919640561970299] has not improved from the previous [0.0691930868510245] for 1 times.\n",
      "Iteration [19755]: Loss[0.06919382896618102] has not improved from the previous [0.06919350640692523] for 1 times.\n",
      "Iteration [19757]: Loss[0.06919659863434266] has not improved from the previous [0.06919381169057781] for 1 times.\n",
      "Iteration [19760]: Loss[0.0692002171493646] has not improved from the previous [0.06919054509803137] for 1 times.\n",
      "Iteration [19761]: Loss[0.06920519577050495] has not improved from the previous [0.0692002171493646] for 3 times.\n",
      "Iteration [19764]: Loss[0.06919451810973916] has not improved from the previous [0.06919340082272446] for 1 times.\n",
      "Iteration [19766]: Loss[0.06919104442819941] has not improved from the previous [0.06919037387880562] for 1 times.\n",
      "Iteration [19767]: Loss[0.06919423680454206] has not improved from the previous [0.06919104442819941] for 3 times.\n",
      "Iteration [19769]: Loss[0.0691922478310231] has not improved from the previous [0.06919220496995371] for 1 times.\n",
      "Iteration [19771]: Loss[0.06919134620796849] has not improved from the previous [0.06919058817165832] for 1 times.\n",
      "Iteration [19773]: Loss[0.06918889977780661] has not improved from the previous [0.06918803352934914] for 1 times.\n",
      "Iteration [19774]: Loss[0.0691980326069535] has not improved from the previous [0.06918889977780661] for 3 times.\n",
      "Iteration [19775]: Loss[0.06920330408126475] has not improved from the previous [0.0691980326069535] for 5 times.\n",
      "Iteration [19778]: Loss[0.06919030951053524] has not improved from the previous [0.06918950327268113] for 1 times.\n",
      "Iteration [19780]: Loss[0.06919036264767718] has not improved from the previous [0.06918650040331986] for 1 times.\n",
      "Iteration [19782]: Loss[0.06919041376662449] has not improved from the previous [0.06918799609956965] for 1 times.\n",
      "Iteration [19785]: Loss[0.06918730684694699] has not improved from the previous [0.06918549233899944] for 1 times.\n",
      "Iteration [19787]: Loss[0.06918736108744639] has not improved from the previous [0.06918589648335272] for 1 times.\n",
      "Iteration [19789]: Loss[0.06918609812793808] has not improved from the previous [0.0691849146440628] for 1 times.\n",
      "Iteration [19790]: Loss[0.06918764803684838] has not improved from the previous [0.06918609812793808] for 3 times.\n",
      "Iteration [19792]: Loss[0.06918418753546474] has not improved from the previous [0.06918266379390109] for 1 times.\n",
      "Iteration [19793]: Loss[0.06918727243720978] has not improved from the previous [0.06918418753546474] for 3 times.\n",
      "Iteration [19794]: Loss[0.06919602906989318] has not improved from the previous [0.06918727243720978] for 5 times.\n",
      "Iteration [19797]: Loss[0.06918690825783655] has not improved from the previous [0.06918445935411753] for 1 times.\n",
      "Iteration [19799]: Loss[0.06918340388278339] has not improved from the previous [0.06918191178138636] for 1 times.\n",
      "Iteration [19801]: Loss[0.06918460460639689] has not improved from the previous [0.06918313710945787] for 1 times.\n",
      "Iteration [19803]: Loss[0.06918256500740778] has not improved from the previous [0.06918152786240343] for 1 times.\n",
      "Iteration [19804]: Loss[0.06918480072526967] has not improved from the previous [0.06918256500740778] for 3 times.\n",
      "Iteration [19806]: Loss[0.0691832346949494] has not improved from the previous [0.06918322787247412] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 19800 Loss 0.06918313710945787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [19808]: Loss[0.06918232654945682] has not improved from the previous [0.06918177557345025] for 1 times.\n",
      "Iteration [19810]: Loss[0.06917955634947655] has not improved from the previous [0.06917924948697934] for 1 times.\n",
      "Iteration [19811]: Loss[0.06919183171731043] has not improved from the previous [0.06917955634947655] for 3 times.\n",
      "Iteration [19812]: Loss[0.06919405569568822] has not improved from the previous [0.06919183171731043] for 5 times.\n",
      "Iteration [19815]: Loss[0.06918158438553992] has not improved from the previous [0.06918053651397153] for 1 times.\n",
      "Iteration [19817]: Loss[0.069179424823772] has not improved from the previous [0.06917756803010831] for 1 times.\n",
      "Iteration [19818]: Loss[0.0691823758272316] has not improved from the previous [0.069179424823772] for 3 times.\n",
      "Iteration [19819]: Loss[0.06919323406917553] has not improved from the previous [0.0691823758272316] for 5 times.\n",
      "Iteration [19822]: Loss[0.0691804078019567] has not improved from the previous [0.06917994501772044] for 1 times.\n",
      "Iteration [19824]: Loss[0.06917837660582102] has not improved from the previous [0.06917637677634197] for 1 times.\n",
      "Iteration [19825]: Loss[0.0691793401981756] has not improved from the previous [0.06917837660582102] for 3 times.\n",
      "Iteration [19827]: Loss[0.06917806789081658] has not improved from the previous [0.06917446482988371] for 1 times.\n",
      "Iteration [19831]: Loss[0.06917829194476136] has not improved from the previous [0.06917396236853289] for 1 times.\n",
      "Iteration [19833]: Loss[0.06917829256584779] has not improved from the previous [0.06917573541670645] for 1 times.\n",
      "Iteration [19836]: Loss[0.06917490495509268] has not improved from the previous [0.06917322809515478] for 1 times.\n",
      "Iteration [19838]: Loss[0.069176441384037] has not improved from the previous [0.06917338891456201] for 1 times.\n",
      "Iteration [19839]: Loss[0.06918257436697349] has not improved from the previous [0.069176441384037] for 3 times.\n",
      "Iteration [19843]: Loss[0.06917342311588685] has not improved from the previous [0.0691732027253672] for 1 times.\n",
      "Iteration [19845]: Loss[0.06917424959212735] has not improved from the previous [0.06917200211494064] for 1 times.\n",
      "Iteration [19847]: Loss[0.06917593097914805] has not improved from the previous [0.06917254512544023] for 1 times.\n",
      "Iteration [19850]: Loss[0.0691710995817197] has not improved from the previous [0.06916961543973334] for 1 times.\n",
      "Iteration [19852]: Loss[0.06917381069549348] has not improved from the previous [0.06917048862944922] for 1 times.\n",
      "Iteration [19853]: Loss[0.06917980814960956] has not improved from the previous [0.06917381069549348] for 3 times.\n",
      "Iteration [19859]: Loss[0.06917090146478329] has not improved from the previous [0.06916875580552176] for 1 times.\n",
      "Iteration [19861]: Loss[0.06917288841237083] has not improved from the previous [0.06916953094646132] for 1 times.\n",
      "Iteration [19864]: Loss[0.06916765485185916] has not improved from the previous [0.06916629775194383] for 1 times.\n",
      "Iteration [19865]: Loss[0.06917011678967797] has not improved from the previous [0.06916765485185916] for 3 times.\n",
      "Iteration [19866]: Loss[0.06918319268165474] has not improved from the previous [0.06917011678967797] for 5 times.\n",
      "Iteration [19868]: Loss[0.0691688712379665] has not improved from the previous [0.06916763344171693] for 1 times.\n",
      "Iteration [19869]: Loss[0.06916964140676848] has not improved from the previous [0.0691688712379665] for 3 times.\n",
      "Iteration [19871]: Loss[0.06916682810655338] has not improved from the previous [0.06916518965352753] for 1 times.\n",
      "Iteration [19873]: Loss[0.06916708912379467] has not improved from the previous [0.06916561560093851] for 1 times.\n",
      "Iteration [19875]: Loss[0.06916603535865608] has not improved from the previous [0.06916446248831143] for 1 times.\n",
      "Iteration [19876]: Loss[0.06916725889309418] has not improved from the previous [0.06916603535865608] for 3 times.\n",
      "Iteration [19878]: Loss[0.06916509976210922] has not improved from the previous [0.06916248111067513] for 1 times.\n",
      "Iteration [19879]: Loss[0.06916567007507245] has not improved from the previous [0.06916509976210922] for 3 times.\n",
      "Iteration [19882]: Loss[0.06916373854988116] has not improved from the previous [0.06916173039119157] for 1 times.\n",
      "Iteration [19883]: Loss[0.06917010781106168] has not improved from the previous [0.06916373854988116] for 3 times.\n",
      "Iteration [19884]: Loss[0.06917746227744695] has not improved from the previous [0.06917010781106168] for 5 times.\n",
      "Iteration [19887]: Loss[0.06916483955426586] has not improved from the previous [0.06916440848056495] for 1 times.\n",
      "Iteration [19889]: Loss[0.06916221509046838] has not improved from the previous [0.06916090045004562] for 1 times.\n",
      "Iteration [19890]: Loss[0.06916423566191797] has not improved from the previous [0.06916221509046838] for 3 times.\n",
      "Iteration [19894]: Loss[0.0691624385666892] has not improved from the previous [0.06916144957527476] for 1 times.\n",
      "Iteration [19896]: Loss[0.06915929204301381] has not improved from the previous [0.06915904055063109] for 1 times.\n",
      "Iteration [19897]: Loss[0.06916923221181484] has not improved from the previous [0.06915929204301381] for 3 times.\n",
      "Iteration [19898]: Loss[0.06917351481046312] has not improved from the previous [0.06916923221181484] for 5 times.\n",
      "Iteration [19901]: Loss[0.06916156694121815] has not improved from the previous [0.06916052964443015] for 1 times.\n",
      "Iteration [19903]: Loss[0.0691587305846936] has not improved from the previous [0.06915761053289762] for 1 times.\n",
      "Iteration [19904]: Loss[0.06916157741899481] has not improved from the previous [0.0691587305846936] for 3 times.\n",
      "Iteration [19908]: Loss[0.06915893437188543] has not improved from the previous [0.06915813394711118] for 1 times.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 19900 Loss 0.06916052964443015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration [19910]: Loss[0.06915578115846194] has not improved from the previous [0.06915574957553261] for 1 times.\n",
      "Iteration [19911]: Loss[0.06916643881248524] has not improved from the previous [0.06915578115846194] for 3 times.\n",
      "Iteration [19912]: Loss[0.06916999050839599] has not improved from the previous [0.06916643881248524] for 5 times.\n",
      "Iteration [19915]: Loss[0.06915829157318316] has not improved from the previous [0.06915708179147537] for 1 times.\n",
      "Iteration [19917]: Loss[0.06915519451476186] has not improved from the previous [0.0691543382517622] for 1 times.\n",
      "Iteration [19918]: Loss[0.06915882743123591] has not improved from the previous [0.06915519451476186] for 3 times.\n",
      "Iteration [19922]: Loss[0.06915551986269093] has not improved from the previous [0.0691548726725502] for 1 times.\n",
      "Iteration [19925]: Loss[0.0691635376918979] has not improved from the previous [0.06915222195326368] for 1 times.\n",
      "Iteration [19926]: Loss[0.06916649547292476] has not improved from the previous [0.0691635376918979] for 3 times.\n",
      "Iteration [19929]: Loss[0.06915504504920483] has not improved from the previous [0.06915370886877265] for 1 times.\n",
      "Iteration [19931]: Loss[0.0691516523560018] has not improved from the previous [0.06915109699601599] for 1 times.\n",
      "Iteration [19932]: Loss[0.06915599746504632] has not improved from the previous [0.0691516523560018] for 3 times.\n",
      "Iteration [19936]: Loss[0.06915215117789272] has not improved from the previous [0.06915164280763743] for 1 times.\n",
      "Iteration [19939]: Loss[0.06916048938422942] has not improved from the previous [0.0691486716364587] for 1 times.\n",
      "Iteration [19940]: Loss[0.06916294513349387] has not improved from the previous [0.06916048938422942] for 3 times.\n",
      "Iteration [19943]: Loss[0.06915183313762277] has not improved from the previous [0.06915033903216221] for 1 times.\n",
      "Iteration [19945]: Loss[0.06914809409244597] has not improved from the previous [0.06914788918922966] for 1 times.\n",
      "Iteration [19946]: Loss[0.06915314296572883] has not improved from the previous [0.06914809409244597] for 3 times.\n",
      "Iteration [19950]: Loss[0.06914878455203163] has not improved from the previous [0.06914844395007963] for 1 times.\n",
      "Iteration [19953]: Loss[0.06915738745051939] has not improved from the previous [0.06914511068652425] for 1 times.\n",
      "Iteration [19954]: Loss[0.06915938387044625] has not improved from the previous [0.06915738745051939] for 3 times.\n",
      "Iteration [19957]: Loss[0.06914864880515002] has not improved from the previous [0.0691469720890347] for 1 times.\n",
      "Iteration [19960]: Loss[0.06915026229469427] has not improved from the previous [0.06914452847625625] for 1 times.\n",
      "Iteration [19964]: Loss[0.0691454208976609] has not improved from the previous [0.06914527049277734] for 1 times.\n",
      "Iteration [19967]: Loss[0.06915423638418533] has not improved from the previous [0.06914154483423182] for 1 times.\n",
      "Iteration [19968]: Loss[0.06915581686327107] has not improved from the previous [0.06915423638418533] for 3 times.\n",
      "Iteration [19971]: Loss[0.06914494307962905] has not improved from the previous [0.06914371961709974] for 1 times.\n",
      "Iteration [19974]: Loss[0.06914814971193406] has not improved from the previous [0.06914020012551672] for 1 times.\n",
      "Iteration [19976]: Loss[0.06914290965513335] has not improved from the previous [0.06914225566912098] for 1 times.\n",
      "Iteration [19981]: Loss[0.06914480898185761] has not improved from the previous [0.06913793854903305] for 1 times.\n",
      "Iteration [19983]: Loss[0.06914160212371429] has not improved from the previous [0.06913814958383675] for 1 times.\n",
      "Iteration [19985]: Loss[0.06914345248898877] has not improved from the previous [0.06914036706252044] for 1 times.\n",
      "Iteration [19987]: Loss[0.06913855885854515] has not improved from the previous [0.06913815589722277] for 1 times.\n",
      "Iteration [19988]: Loss[0.06914356091989508] has not improved from the previous [0.06913855885854515] for 3 times.\n",
      "Iteration [19989]: Loss[0.06915192629498561] has not improved from the previous [0.06914356091989508] for 5 times.\n",
      "Iteration [19992]: Loss[0.06914065911796073] has not improved from the previous [0.06913953944689975] for 1 times.\n",
      "Iteration [19995]: Loss[0.06914079346604148] has not improved from the previous [0.0691357474519802] for 1 times.\n",
      "Iteration [19998]: Loss[0.0691370593722048] has not improved from the previous [0.0691355155878832] for 1 times.\n",
      "Iteration [19999]: Loss[0.06913816218381807] has not improved from the previous [0.0691370593722048] for 3 times.\n"
     ]
    }
   ],
   "source": [
    "MAX_TEST_TIMES = 20000\n",
    "D = 2\n",
    "M1 = 20\n",
    "W1 = weights.he(M1, D+1)\n",
    "M2: int = M                 # Number of categories to classify\n",
    "W2 = weights.he(M2, M1+1)\n",
    "optimizer = SGD(lr=0.6)\n",
    "\n",
    "X, T = transform_X_T(X, T)\n",
    "W1, W2, objective, prediction, history = train_two_layer_classifier(\n",
    "    N=N,\n",
    "    D=D,\n",
    "    X=X,\n",
    "    T=T,\n",
    "    M1=M1,\n",
    "    W1=W1,\n",
    "    M2=M2,\n",
    "    W2=W2,\n",
    "    log_loss_function=softmax_cross_entropy_log_loss,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=MAX_TEST_TIMES,\n",
    "    test_numerical_gradient=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oonisim/home/repository/git/oonisim/python_programs/nlp/src/drawing/functions.py:67: UserWarning: linewidths is ignored by contourf\n",
      "  axes.contourf(grid[0], grid[1], predictions, cmap=plt.cm.gist_rainbow, alpha=0.4, linewidths=3.0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,6)) \n",
    "x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "Y = COLOR_LABELS[T]\n",
    "\n",
    "x_grid, y_grid, predictions = prediction_grid_2d(x_min, x_max, y_min, y_max, prediction)\n",
    "plot_categorical_predictions(ax, [x_grid, y_grid], X, Y, predictions)\n",
    "\n",
    "ax.set_xlabel('x label')\n",
    "ax.set_ylabel('y label')\n",
    "ax.axis('equal')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = range(len(history))\n",
    "Y = history\n",
    "xlabel = 'iterations (log scale)'\n",
    "ylabel = 'loss'\n",
    "title = \"training error\"\n",
    "fig, ax = plot(X, Y, title=title, xlabel=xlabel, ylabel=ylabel,figsize=(10,5))\n",
    "ax.set_ylim(0.0, 1.5)\n",
    "ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
