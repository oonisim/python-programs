{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* [CS231n: Convolutional Neural Networks for Visual Recognition 2017](http://cs231n.stanford.edu/2017/syllabus)\n",
    "    - [cs231n 2017 assignment #1 kNN, SVM, SoftMax, two-layer network](https://cs231n.github.io/assignments2017/assignment1/)\n",
    "    - [Training a Softmax Linear Classifier](https://cs231n.github.io/neural-networks-case-study)\n",
    "* [ゼロから作る Deep Learning](https://github.com/oreilly-japan/deep-learning-from-scratch)\n",
    "* [Mathematics for Machine Learning](https://mml-book.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network \n",
    "Simple one layer neural network classifier. Mathjax formula not fully supported in github, hence the formulas get corrupted.\n",
    "\n",
    "<img src=\"image/nn_diagram.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/nn_functions.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Optional,\n",
    "    Union,\n",
    "    List,\n",
    "    Dict,\n",
    "    Tuple\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python path\n",
    "Python path setup to avoid the relative imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1    # Batch size\n",
    "D = 3    # Number of features in the input data\n",
    "M = 2    # Number of nodes in a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confiurations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X, T, V\n",
    "\n",
    "X is to have been standardized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W [ 0.8593437  -0.48408289  0.16490048]\n"
     ]
    }
   ],
   "source": [
    "from data.classifications import (\n",
    "    linear_separable\n",
    ")\n",
    "X, T, V = linear_separable(d=2, n=100)\n",
    "print(f\"W {V}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f69c46556d0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[T==0, 1], X[T==0, 2], c='red')\n",
    "plt.scatter(X[T==1, 1], X[T==1, 2], c='green')\n",
    "\n",
    "# Hyperplace (X-b)V = 0 -> x1V1 + x2V2 - bV2 = 0\n",
    "x = np.linspace(-3,3,100)\n",
    "y = -(V[1] / V[2]) * x - (V[0] / V[2])\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T\n",
    "Labels for data X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For One Hot Encoding labels\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ T_{_{OHE}} } &= ( \n",
    "    \\overset{ (M,) }{ T_{(n=0)} }, \\; \\dots \\;, \\overset{ (M,) }{ T_{(n=N-1)} } \n",
    ") \n",
    "\\\\\n",
    "\\overset{ (N,M) }{ T_{_{OHE}} } = ( \n",
    "    \\overset{ (M,) }{ T_{(n=0)} }, \\dots , \\overset{ (M,) }{ T_{(n=N-1)} } \n",
    ") \n",
    "\\\\\n",
    "\\overset{ (M,) }{ T_{ _{OHE} (n)} } &= ( \\overset{ () }{ t_{(n)(m=0)} }, \\; \\dots \\;, \\overset{ () }{ t_{(n)(m=M-1)} })\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "### For index labels\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,) }{ T_{_{IDX}} } &= (\\overset{ () }{ t_{(n=0)} }, \\; \\dots \\;, \\overset{ () }{ t_{(n=N-1)} }) \\qquad \\text {for index labels }\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W\n",
    "Weight parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import (\n",
    "    xavier,\n",
    "    he,\n",
    "    uniform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization Layer\n",
    "Apply normalization or use batch normaliation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matmul layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer.matmul import Matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ Y } \n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "{ Y_{(n=0)} } \\\\\n",
    "\\vdots \\\\\n",
    "{ Y_{(n)} } \\\\\n",
    "\\vdots \\\\\n",
    "{ Y_{(n=N-1)} }\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\overset{ (N,D) }{ X } \\; @ \\; \\overset{ (D,M) }{ W^T }\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (M,) }{ Y_{(n)} } &= (y_{(n)(m=0)}, \\; \\dots, \\; y_{(n)(m)},  \\; \\dots, \\; y_{(n)(m=M-1)})\n",
    "\\\\\n",
    "\\overset{ () }{ y_{(n)(m)} }\n",
    "&= \\overset{ (D,) }{ X_{(n)} } \\cdot \\overset{ (D,) }{ W_{(m)}^T }\n",
    "= \\sum\\limits ^{D}_{d=0}  \\overset{ () }{ x_{(n)(d)} } * \\overset{ () }{ w_{(m)(d)} }\n",
    "\\\\\n",
    "_{(0 \\le d \\le D, \\; 0 \\le n \\lt N, \\; 0 \\le m \\lt M)}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dX\n",
    "\n",
    "Impact on L by $dX$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,D) }{ \\frac {\\partial L }{ \\partial X } }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial Y} }\n",
    "@ \\overset { (M,D) }{ W } \n",
    "\\end{align*}\n",
    "$\n",
    "<img src=\"image/nn_back_propagation_dL_dX.png\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dW.T\n",
    "Impact on L by $dW^T$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{ \\partial W^T } }\n",
    "= \\overset { (D,N) }{ X^T } \n",
    "@ \n",
    "\\overset { (N,M) }{ \\frac {\\partial L}{\\partial Y} }\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "<img src=\"image/nn_back_propagation_dL_dWT.png\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ A } &= \n",
    "activation \\left( \n",
    "    \\overset{ (N,M) }{ Y }  = \n",
    "    \\begin{bmatrix}\n",
    "    { Y_{(n=0)} } \\\\\n",
    "    \\vdots \\\\\n",
    "    { Y_{(n)} } \\\\\n",
    "    \\vdots \\\\\n",
    "    { Y_{(n=N-1)} }\n",
    "    \\end{bmatrix}\n",
    "\\right)\n",
    "\\\\\n",
    "\\overset{ (M,) }{ A_{(n)} } \n",
    "&= activation \\left( \\overset{ (M,) }{ Y_{(n) }} \\right)  \\\\\n",
    "&= (a_{(n)(m=0)}, \\; \\dots, \\; a_{(n)(m)},  \\; \\dots, \\; a_{(n)(m=M-1)})\n",
    "\\\\\n",
    "\\overset{ () }{ a_{(n)(m)} } &= activation \\left( \\overset{ () }{ y_{(n)(m)} } \\right)\n",
    "\\quad _{(0 \\le n \\lt N, \\; 0 \\le m \\lt M)}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dY\n",
    "\n",
    "Impact on L by dY from the matmul layer.\n",
    "\n",
    "$\n",
    "\\begin {align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{ \\partial Y } }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial A} } \n",
    "* \n",
    "\\overset { (N,M) }{ \\frac {\\partial A}{\\partial Y} }\n",
    "\\end {align*}\n",
    "$\n",
    "\n",
    "### For sigmoid activation\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset {(N,M)}{\\frac { \\partial L }{ \\partial Y} }\n",
    "&= \\frac { \\partial A }{ \\partial Y} * A * (1 - A)\n",
    "\\\\\n",
    "\\frac { \\partial y_{(n)(m)} } { \\partial a_{(n)(m)} }\n",
    "&= a_{(n)(m)} * (1 - a_{(n)(m)} )  \\\\ \n",
    "y_{(n)(m)} = sigmoid(a_{(n)(m)} )&=  \\frac {1}{ 1 + exp(y_{(n)(m)})}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "### For ReLU activation\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac { \\partial a_{(n)(m)} }{ \\partial y_{(n)(m)} }\n",
    "&= 1 \\quad y_{(n)(m)}  \\gt 0 \\\\\n",
    "&= 0 \\quad y_{(n)(m)}  \\le 0 \\\\\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax layer\n",
    "$C_n$ is to prevent the overflow at $np.exp()$.\n",
    "\n",
    "<img src=\"image/softmax.png\" align=\"left\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exp(x) can take all x values and produces a positive, which is required for log(y) that needs y > 0, hence fit-for-purpose to build a probability function.\n",
    "\n",
    "<img src=\"image/exp.gif\" align=\"left\" width=250/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax and Cross Entropy Log Loss are combined as the gradient results in a simple form $P - T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def softmax(X: Union[np.ndarray, float]) -> Union[np.ndarray, float]:\n",
      "    \"\"\"Softmax P = exp(X) / sum(exp(X))\n",
      "    Args:\n",
      "        X: batch input data of shape (N,M).\n",
      "            N: Batch size\n",
      "            M: Number of nodes\n",
      "    Returns:\n",
      "        P: Probability of shape (N,M)\n",
      "    \"\"\"\n",
      "    name = inspect.stack()[0][3]\n",
      "    assert isinstance(X, float) or (isinstance(X, np.ndarray) and X.dtype == float), \\\n",
      "        \"X must be float or ndarray(dtype=float)\"\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # exp(x-c) to prevent the infinite exp(x) for a large value x, with c = max(x).\n",
      "    # keepdims=True to be able to broadcast.\n",
      "    # --------------------------------------------------------------------------------\n",
      "    C = np.max(X, axis=-1, keepdims=True)   # オーバーフロー対策\n",
      "    exp = np.exp(X - C)\n",
      "    P = exp / np.sum(exp, axis=-1, keepdims=True)\n",
      "    Logger.debug(\"%s: X %s exp %s P %s\", name, X, exp, P)\n",
      "\n",
      "    return P\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from layer import CrossEntropyLogLoss\n",
    "from common import softmax\n",
    "\n",
    "lines = inspect.getsource(softmax)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,1) }{ C } &= np.max\\left( \n",
    "    \\overset{ (N,M) }{ A }, \\; axis=-1,  \\; keepdim=True \n",
    "\\right) \\\\\n",
    "&=  \n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n=0  )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n    )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n=N-1)} } \\end{bmatrix}\n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\overset{ (N,M) }{ EXP } &= np.exp \\left( \\overset{ (N,M) }{ A } - \\overset{ (N,1) }{ C } \\right)\n",
    "= np.exp \\left(\n",
    "    \\begin{bmatrix}\n",
    "    { A_{(n=0)} } - { C_{(n=0)} }\\\\\n",
    "    \\vdots \\\\\n",
    "    { A_{(n)} }   - { C_{(n)} }\\\\\n",
    "    \\vdots \\\\\n",
    "    { A_{(n=N-1)} } - { C_{(n=N-1)} }\\\\\n",
    "    \\end{bmatrix}\n",
    "\\right) \n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "    e_{(n=0)(m=0)}   & \\dots      & e_{(n=0)(m=M-1)}   \\\\  \n",
    "    \\vdots           & e_{(n)(m)} & \\vdots             \\\\\n",
    "    e_{(n=N-1)(m=0)} & \\dots      & e_{(n=N-1)(m=M-1)} \n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,1) }{ S } &= \\overset{ (N,1) }{ sum(EXP) } = np.sum \\left( \n",
    "    \\overset{ (N,M) }{ EXP }, \\; axis=-1,  \\; keepdim=True \n",
    "\\right)\n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n=0  )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n    )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n=N-1)} } \\end{bmatrix}\n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\overset{ () }{ s_{(n)} } &= \\sum\\limits ^{M-1}_{m=0} np.exp(\\; a_{(n)(m)} - c_{(n)} \\; )\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,M) }{ P } &= \\overset{ (N,M) }{ EXP }  \\;\\; / \\;\\; \\overset{ (N,1) }{ sum(EXP) } \n",
    "\\\\\n",
    "\\overset{ (N,) }{ P_{(n)} } &= (p_{(n)(m=0)}, \\; \\dots, \\; p_{(n)(m)} , \\; \\dots, \\; p_{(n)(m=M-1)})\n",
    "\\\\\n",
    "{ p_{(n)(m)} } \n",
    "&= \\frac {np.exp \\left( \n",
    "    { a_{(n)(m) } } - { c_{(n)} }) \\right) \n",
    "}\n",
    "{  \n",
    "np.sum \\left( \n",
    "    np.exp \\left( \n",
    "        a_{(n)(m) } - c_{(n)}\n",
    "    \\right)\n",
    "\\right) \n",
    "}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dA\n",
    "\n",
    "Impact on L by dA from the activation layer.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{\\partial A} }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial P} }\n",
    "* \n",
    "\\overset { (N,M) }{ \\frac {\\partial P }{\\partial A} } \n",
    "= \n",
    "\\frac {1}{N} (P - T)\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "$\n",
    "Jacobian \\; : \\; f \\circ g \\rightarrow Jf \\circ Jg\n",
    "\\\\\n",
    "\\begin{align*}\n",
    "\\\\\n",
    "L &= f(\\; p_{(n)(m=0)} \\;) = f( \\; g(\\;  a_{(n)(m=0)} \\; ) \\; ) \\quad : p = g(a) = softmax(a)\n",
    "\\\\\n",
    "\\frac {\\partial L} { \\partial a_{(n)(m=0)} }\n",
    "&= Jf(p) \\circ Jg(a) \n",
    "=  \\frac {\\partial L} { \\partial p_{(n)(m=0)} } * \\frac {\\partial  p_{(n)(m=0)}} { \\partial a_{(n)(m=0)} }\n",
    "\\\\\n",
    "&= \\frac {1}{N} \\left(\n",
    " p_{(n)(m=0)} -t_{(n)(m=0)}\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient from cross entropy log loss\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac { \\partial L }{ \\partial  p_{(n)(m=0)} }\n",
    "&= \\frac{-1}{N} t_{(n)(m=0)} * \\frac {s_{(n)}}{e_{(n)(m=0)}}\n",
    "\\\\\n",
    "\\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "&= \\frac{-1}{N} t_{(n)(m=1)} * \\frac {s_{(n)}}{e_{(n)(m=1)}}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Gradient $\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "p_{(n)(m=0)} &= \\frac {e_{(n)(m=0)} }{ s_{(n)} } \\\\\n",
    "p_{(n)(m=0)} &= f \\circ g_{(m=0)} = { s^{-1}_{(n)} } \\; * \\; { e_{(n)(m=0)} }\n",
    "\\rightarrow \\frac { \\partial  p_{(n)(m=0)} } { \\partial s^{-1}_{(n)} } = e_{(n)(m=0)}\n",
    "\\\\\n",
    "p_{(n)(m=1)} &= \\frac {e_{(n)(m=1)} }{ s_{(n)} } \\\\\n",
    "p_{(n)(m=1)} &= f \\circ g_{(m=1)} = { s^{-1}_{(n)} } \\; * \\; { e_{(n)(m=1)} }\n",
    "\\rightarrow \\frac { \\partial  p_{(n)(m=1)} } { \\partial s^{-1}_{(n)} } = e_{(n)(m=1)}\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "\\left[\n",
    "    \\frac { \\partial  p_{(n)(m=0)} } { \\partial s^{-1}_{(n)} } * \n",
    "    \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \n",
    "    +\n",
    "    \\frac { \\partial  p_{(n)(m=1)} } { \\partial s^{-1}_{(n)} } * \n",
    "    \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "\\right]\n",
    "\\\\\n",
    "&= \\sum\\limits^{M-1}_{m=0} \n",
    "    e_{(n)(m)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m)} } \n",
    "\\\\\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "    \\begin{bmatrix}\n",
    "    e_{(n)(m=0)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \\\\\n",
    "    + \\\\\n",
    "    e_{(n)(m=1)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "    \\end{bmatrix}\n",
    "\\\\\n",
    "&= -s_{(n)}(\\; t_{(n)(m=0)} + t_{(n)(m=1)} \\;) \\\\\n",
    "&= -s_{(n)}\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "\\left[\n",
    "    e_{(n)(m=0)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \n",
    "    + \n",
    "    e_{(n)(m=1)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "\\right]\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient $\\frac {\\partial L }{ \\partial { s_{(n)} } } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac {1} { s_{(n)} } &= s^{-1}_{(n)} \\rightarrow\n",
    "\\frac { \\partial { s^{-1}_{(n)} } } {\\partial s_{(n)}} = \\frac {-1}{s^{2}_{(n)}}\n",
    "\\\\\n",
    "\\frac {\\partial L }{ \\partial { s_{(n)} } } \n",
    "&=\n",
    "\\frac {-1}{s^{2}_{(n)}} * \n",
    "\\frac {\\partial L}{ \\partial s^{-1}_{(n)} } \\\\\n",
    "&= \\frac {1}{s_n}\n",
    "\\end{align*} \\\\\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient $\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } $\n",
    "$\n",
    "\\begin{align*}\n",
    "s_{(n)} &= \\sum\\limits ^{M-1}_{m=0} e_{(n)(m)} \\rightarrow \n",
    "\\frac { \\partial { s_{(n)} } } {\\partial e_{(n)(m=0)}} = 1\n",
    "\\\\\n",
    "p_{(n)(m=0)} &= \\frac {e_{(n)(m=0)} }{ s_{(n)} }\\rightarrow \n",
    "\\frac { \\partial { p_{(n)(m=0)} } } {\\partial e_{(n)(m=0)}} = \\frac {1}{s_{(n)}}\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } \n",
    "&= \\begin{bmatrix}  \n",
    "    \\frac { \\partial { s_{(n)} } } {\\partial e_{(n)(m=0)}} *  \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \\\\\n",
    "    + \\\\\n",
    "    \\frac { \\partial { p_{(n)(m=0)} } } {\\partial e_{(n)(m=0)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}  \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \\\\\n",
    "    + \\\\\n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } &= \n",
    "\\left[\n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "    + \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \n",
    "\\right]\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } &= \n",
    "\\begin{bmatrix}  \n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} }  \\\\\n",
    "    +  \\\\\n",
    "    \\frac {\\partial L }{ \\partial s_{(n)} } \n",
    "\\end{bmatrix} \\\\\n",
    "&= \\frac {-t_{(n)(m=0)}}{e_{(n)(m=0)} } + \\frac {1}{s_{n}}\n",
    "\\end{align*}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gardient $\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "e_{(n)(m)} &= exp(\\; a_{(n)(m)} \\; ) \\rightarrow \\frac { \\partial e_{(n)(m)} }{ \\partial a_{(n)(m)} } = e_{(n)(m)} \n",
    "\\\\\n",
    "e_{(n)(m=0)} &= exp(a_{(n)(m=0)}) \\rightarrow \\frac { \\partial e_{(n)(m=0)} }{ \\partial a_{(n)(m=0)} } = e_{(n)(m=0)} \n",
    "\\\\\n",
    "e_{(n)(m=1)} &= exp(a_{(n)(m=1)}) \\\\\n",
    "\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } \n",
    "&=   \\frac { \\partial e_{(n)(m=0)} }{ \\partial a_{(n)(m=0)} } * \n",
    "    \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \n",
    "= e_{(n)(m=0)} * \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } \n",
    "&= e_{(n)(m=0)} * \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \\\\\n",
    "&= -t_{(n)(m=0)} + \\frac { e_{(n)(m=0)} }{ s_{n} } \\\\\n",
    "&= p_{(n)(m=0)} -t_{(n)(m=0)} \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Log Loss\n",
    "\n",
    "A probability distribution $P(x)$ can be represented with its entropy $E(x) = \\sum\\limits_{x}  \\frac {p(x)}{log(p(x)} = - \\sum\\limits_{x} p(x) log(p(x))$. In the diagram, x: (0:dog, 1:cat, 2:fish, 3:bird) are labels and p(dog) is 0.5. When  a NN predicts an input x as a probability distribution $P(x)$, then the $E(x) = 1.75$. \n",
    "\n",
    "0. $p(dog)=\\frac {1}{2}$\n",
    "1. $p(cat)=\\frac {1}{4}$\n",
    "2. $p(fish)=\\frac {1}{8}$\n",
    "3. $p(bird)=\\frac {1}{8}$\n",
    "\n",
    "When the truth is that x is a dog, then the probability distribution of the truth $P(t)$ has the entropy $E(t) = 0$.\n",
    "\n",
    "0. $p(dog)=1$\n",
    "1. $p(cat)=0$\n",
    "2. $p(fish)=0$\n",
    "3. $p(bird)=0$\n",
    "\n",
    "The difference E(x) - E(t) = E(x) = 1.75 can be used as the distance or the error of the prediction from the truth. Need to understand further but  the actuall loss function is $E(x) = -tlog(p(x)) = -log(p(x))$ where p(x) is the probability from the softmax for the correct label.\n",
    "\n",
    "\n",
    "<img src=\"image/entropy.png\" align=\"left\" width=600/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.log() is ln based on the mathematical constant $e$ and its derivative $\\frac {\\partial log(x)}{\\partial x} = \\frac {1}{x}$.\n",
    "\n",
    "* [Logarithm](https://en.wikipedia.org/wiki/Logarithm)\n",
    "\n",
    "\n",
    "<img src=\"image/logarithm_plots.png\" align=\"left\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [ML Grossary - Loss Functions](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)\n",
    "\n",
    "<img src=\"image/cross_entropy_log_loss.png\" align=\"left\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cross_entropy_log_loss_input_combinations.xlsx](./common/cross_entropy_log_loss_input_combinations.xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cross_entropy_log_loss(\n",
      "        P: Union[np.ndarray, float],\n",
      "        T: Union[np.ndarray, int],\n",
      "        f: Callable = categorical_log_loss,\n",
      "        offset: float = OFFSET_LOG\n",
      ") -> np.ndarray:\n",
      "    \"\"\"Cross entropy log loss [ -t(n)(m) * log(p(n)(m)) ] for multi labels.\n",
      "    Assumption:\n",
      "        Label is integer 0 or 1 for an OHE label and any integer for an index label.\n",
      "\n",
      "    NOTE:\n",
      "        Handle only the label whose value is True. The reason not to use non-labels to\n",
      "        calculate the loss is TBD.\n",
      "\n",
      "    Args:\n",
      "        P: probabilities of shape (N,M) from soft-max layer where:\n",
      "            N is Batch size\n",
      "            M is Number of nodes\n",
      "        T: label either in OHE format of shape (N,M) or index format of shape (N,).\n",
      "           OHE: One Hot Encoding\n",
      "        f: Cross entropy log loss function\n",
      "        offset: small number to avoid np.inf by log(0) by log(0+offset)\n",
      "\n",
      "    Returns:\n",
      "        J: Loss value of shape (N,), a loss value per batch.\n",
      "    \"\"\"\n",
      "    name = inspect.stack()[0][3]\n",
      "    P, T = transform_X_T(P, T)\n",
      "\n",
      "    if P.ndim == 0:\n",
      "        assert False, \"P.ndim needs (N,M) after transform_X_T(P, T)\"\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # P is scalar, T is a scalar binary OHE label. Return -t * log(p).\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # assert T.ndim == 0, \"P.ndim==0 requires T.ndim==0 but %s\" % T.shape\n",
      "        # return f(P, T, offset)\n",
      "\n",
      "    if (1 < P.ndim == T.ndim) and (P.shape[1] == T.shape[1] == 1):\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # This condition X:(N,1), T(N,1) tells T is the 2D binary OHE labels.\n",
      "        # T is 2D binary OHE labels e.g. T[[0],[1],[0]], P[[0.9],[0.1],[0.3]].\n",
      "        # Return -T * log(P)\n",
      "        # --------------------------------------------------------------------------------\n",
      "        return np.squeeze(f(P=P, T=T, offset=offset), axis=-1)    # Shape from (N,M) to (N,)\n",
      "\n",
      "    # ================================================================================\n",
      "    # Calculate Cross entropy log loss -t * log(p).\n",
      "    # Select an element P[n][t] at each row n which corresponds to the true label t.\n",
      "    # Use the Numpy tuple indexing. e.g. P[n=0][t=2] and P[n=3][t=4].\n",
      "    # P[\n",
      "    #   (0, 3),\n",
      "    #   (2, 4)     # The tuple sizes must be the same at all axes\n",
      "    # ]\n",
      "    #\n",
      "    # Tuple indexing selects only one element per row.\n",
      "    # Beware the numpy behavior difference between P[(n),(m)] and P[[n],[m]].\n",
      "    # https://stackoverflow.com/questions/66269684\n",
      "    # P[1,1]  and P[(0)(0)] results in a scalar value, HOWEVER, P[[1],[1]] in array.\n",
      "    #\n",
      "    # P shape can be (1,1), (1, M), (N, 1), (N, M), hence P[rows, cols] are:\n",
      "    # P (1,1) -> P[rows, cols] results in a 1D of  (1,).\n",
      "    # P (1,M) -> P[rows, cols] results in a 1D of  (1,)\n",
      "    # P (N,1) -> P[rows, cols] results in a 1D of  (N,)\n",
      "    # P (N,M) -> P[rows, cols] results in a 1D of  (N,)\n",
      "    #\n",
      "    # J shape matches with the P[rows, cols] shape.\n",
      "    # ================================================================================\n",
      "    N = batch_size = P.shape[0]\n",
      "    rows = np.arange(N)     # (N,)\n",
      "    cols = T                # Same shape (N,) with rows\n",
      "    assert rows.shape == cols.shape, \\\n",
      "        f\"np P indices need the same shape but rows {rows.shape} cols {cols.shape}.\"\n",
      "\n",
      "    _P = P[rows, cols]\n",
      "    Logger.debug(\"%s: N is [%s]\", name, N)\n",
      "    Logger.debug(\"%s: P.shape %s\", name, P.shape)\n",
      "    Logger.debug(\"%s: P[rows, cols].shape %s\", name, _P.shape)\n",
      "    Logger.debug(\"%s: P[rows, cols] is %s\", name, _P)\n",
      "\n",
      "    J = f(P=_P, T=int(1), offset=offset)\n",
      "\n",
      "    assert not np.all(np.isnan(J)), \"log(x) caused nan for P \\n%s.\" % P\n",
      "    Logger.debug(\"%s: J is [%s]\", name, J)\n",
      "    Logger.debug(\"%s: J.shape %s\\n\", name, J.shape)\n",
      "\n",
      "    assert (J.ndim > 0) and (0 < N == J.shape[0]), \\\n",
      "        \"Loss J.shape is expected to be (%s,) but %s\" % (N, J.shape)\n",
      "    return J\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from common import (\n",
    "    cross_entropy_log_loss,\n",
    "    OFFSET_LOG\n",
    ")\n",
    "lines = inspect.getsource(cross_entropy_log_loss)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For label using One Hot Encoding (OHE)\n",
    "For instance, if multi labels are (0,1,2,3,4) and each label is OHE, then the label for 2 is (0,0,1,0,0).   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product of matrix rows\n",
    "\n",
    "There is no formal operation to calculate the dot products of the rows from two matrices, but to calculate the diagonal of the matlix multiplication that also calculate non-diagonals. To avoid calculating non-diagonals, use [einsum](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html).\n",
    "\n",
    "* [Name of matrix operation of ```[A[0] dot B[0], A[1] dot B[1] ]``` from 2x2 matrices A, B](https://math.stackexchange.com/questions/4010721/name-of-matrix-operation-of-a0-dot-b0-a1-dot-b1-from-2x2-matrices-a)\n",
    "\n",
    "<img src=\"image/dot_products_of_matrix_rows.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is \n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "b.T is \n",
      "[[ 0 -3]\n",
      " [-1 -4]\n",
      " [-2 -5]]\n",
      "\n",
      "c[\n",
      "    np.inner(a[0], b[0]),\n",
      "    np.inner(a[1], b[1]),    \n",
      "] is [-5, -50]\n",
      "\n",
      "\n",
      "np.einsum('ij,ji->i', a, b.T) is [ -5 -50]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(6).reshape(2,3)\n",
    "b = np.arange(0,-6,-1).reshape(2,3)\n",
    "c = [\n",
    "    np.inner(a[0], b[0]),\n",
    "    np.inner(a[1], b[1]),    \n",
    "]\n",
    "print(f\"a is \\n{a}\")\n",
    "print(f\"b.T is \\n{b.T}\\n\")\n",
    "fmt=f\"\"\"c[\n",
    "    np.inner(a[0], b[0]),\n",
    "    np.inner(a[1], b[1]),    \n",
    "] is {c}\\n\n",
    "\"\"\"\n",
    "print(fmt)\n",
    "\n",
    "# Use einsum\n",
    "e = np.einsum('ij,ji->i', a, b.T)\n",
    "fmt=\"np.einsum('ij,ji->i', a, b.T)\"\n",
    "print(f\"{fmt} is {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foward path (OHE)\n",
    "$\n",
    "\\text{ for one hot encoding labels }\n",
    "\\\\\n",
    "\\begin{align*}\n",
    "\\overset{ (N,) }{ J } &= - \\sum\\limits^{M-1}_{m=0} \n",
    "    \\left[ \\; \\;  \n",
    "        t_{(n)(m)} \\;  * \\;  np.log(p_{(n)(m)}) \\;\\;  \n",
    "    \\right]\n",
    "\\\\\n",
    "\\overset{ () }{ j_{(n)} } &= \\overset{ (M,) }{ T_{(n)} } \\cdot \\overset{ (M,) }{ P_{(n)} } \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient dL/dP\n",
    "\n",
    "Impact on L by the $dP$ from the softmax layer for one hot encoding labels.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac { \\partial L }{ \\partial P} }\n",
    "&= \\overset { (N,) }{ \\frac { \\partial L }{ \\partial J} } * \n",
    "\\overset { (N,M) }{ \n",
    "\\left(\n",
    " - \\frac { \\partial T } { \\partial P }\n",
    " \\right) \n",
    "} \n",
    "= - \\frac {1}{N }  \\frac { \\partial T } { \\partial P }\n",
    "\\\\\n",
    "\\frac {\\partial L }{\\partial p_{(n)(m=0)}} \n",
    "&= \\frac {\\partial L}{\\partial j_{(n)}} * \\frac {\\partial j_{(n)}} {\\partial p_{(n)(m=0)}} \n",
    "= \\frac {1}{N} \\frac { -t_{(n)(m=0)}}{ p_{(n)(m=0)} } \n",
    "=  \\frac {1}{N} \\left(\n",
    " -t_{(n)(m=0)} * \\frac { s_{(n)} }{ e_{(n)(m=0)} }\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For label using indexing \n",
    "For instance, if the multi labels are (0,1,2,3,4) then the index is 2 for the label 2. If the labels are (2,4,6,8,9), then the index is 3 for the label 8.  \n",
    "\n",
    "Use LP to select the probabilities from P for the corresponding labels. For instance, if the label is 2 (hence the index is 2) for X(n=0), and 4 for X(n=3), then the numpy tuple indexing selects ```P[n=0][m=2]``` and ```P[n=3][m=4] ```.\n",
    "\n",
    "```\n",
    "P[\n",
    "   (0, 3),\n",
    "   (2, 4)\n",
    "]\n",
    "```\n",
    "\n",
    "$\n",
    "\\text{ for index labels e.g. (5, 2, 0, 9, ...)}\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,) }{ J } = - np.sum(\\; np.log(LP), \\; axis = -1 \\;) \\\\\n",
    "LP = label\\_probability = P \\left[ \\\\\n",
    "\\quad ( \\; 0, \\; \\dots, \\;  {N-1}) , \\\\\n",
    "\\quad ( \\; t_{(n=0)} \\; , \\dots , \\; t_{(n=N-1)}) \\\\\n",
    "\\right]\n",
    "\\\\\n",
    "\\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward path\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ () }{ L } = \\frac {1}{N} \\sum\\limits^{N-1}_{n=0} \\overset{ () }{ j_{{(n)}} }\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gardient dL/dJ\n",
    "\n",
    "Impact on L by $dJ$ from the cross entropy log loss layer.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,) }{ \\frac {\\partial L}{\\partial J} }  &= \\frac {1}{N} \\overset{(N,)}{ones}\n",
    "\\\\\n",
    "\\frac {\\partial L}{\\partial j_{(n)} } &= \\frac {1}{N} \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ = np.ones(N) / N\n",
    "dJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [two_layer_net.ipynb defines the lambda with parameter W which is redundant #254](https://github.com/cs231n/cs231n.github.io/issues/254)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```numerical_jacobian(f, X)``` returns ```J``` of the same shape with ```X```. It takes each element in ```x``` in ```X```, and calculate ```(f(x+h) and f(x-h))/2h```. For ```cross_entropy_logg_loss()```, the expected numerical gradient is ```gn = (-np.log(p+h+e) + -np.log(p-h+e)) / (2*h)``` for each element ```p``` in ```P```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def numerical_jacobian(\n",
      "        f: Callable[[np.ndarray], np.ndarray],\n",
      "        X: Union[np.ndarray, float],\n",
      "        delta: float = OFFSET_DELTA\n",
      ") -> np.ndarray:\n",
      "    \"\"\"Calculate Jacobian matrix J numerically with (f(X+h) - f(X-h)) / 2h\n",
      "    Jacobian matrix element Jpq = df/dXpq, the impact on J by the\n",
      "    small difference to Xpq where p is row index and q is col index of J.\n",
      "\n",
      "    Note:\n",
      "        Beware limitations by the float storage size, e.g. loss of significance.\n",
      "        https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html\n",
      "        https://ece.uwaterloo.ca/~dwharder/NumericalAnalysis/Contents/\n",
      "        https://ece.uwaterloo.ca/~dwharder/NumericalAnalysis/02Numerics/Weaknesses/\n",
      "        https://www.cise.ufl.edu/~mssz/CompOrg/CDA-arith.html\n",
      "\n",
      "    Args:\n",
      "        f: Y=f(X) where Y is a scalar or shape() array.\n",
      "        X: input of shame (N, M), or (N,) or ()\n",
      "        delta: small delta value to calculate the f value for X+/-h\n",
      "    Returns:\n",
      "        J: Jacobian matrix that has the same shape of X.\n",
      "    \"\"\"\n",
      "    name = inspect.stack()[0][3]\n",
      "    X = np.array(X, dtype=float) if isinstance(X, (float, int)) else X\n",
      "    J = np.zeros_like(X, dtype=float)\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # (x+h) or (x-h) may cause an invalid value area for the function f.\n",
      "    # e.g log loss tries to offset x=0 by adding a small value k as log(0+k).\n",
      "    # However because k=1e-7 << h=1e-5, f(x-h) causes nan due to log(x < 0)\n",
      "    # as x needs to be > 0 for log.\n",
      "    #\n",
      "    # X and tmp must be float, or it will be int causing float calculation fail.\n",
      "    # e.g. f(1-h) = log(1-h) causes log(0) instead of log(1-h).\n",
      "    # --------------------------------------------------------------------------------\n",
      "    assert (X.dtype == float), \"X must be float type\"\n",
      "    assert delta > 0.0\n",
      "\n",
      "    it = np.nditer(X, flags=['multi_index'], op_flags=['readwrite'])\n",
      "    while not it.finished:\n",
      "        idx = it.multi_index\n",
      "        tmp: float = X[idx]\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # f(x+h)\n",
      "        # --------------------------------------------------------------------------------\n",
      "        X[idx] = tmp + delta\n",
      "        fx1: Union[np.ndarray, float] = f(X)  # f(x+h)\n",
      "        Logger.debug(\n",
      "            \"%s: idx[%s] x[%s] (x+h)[%s] fx1=[%s]\",\n",
      "            name, idx, tmp, tmp+delta, fx1\n",
      "        )\n",
      "\n",
      "        assert \\\n",
      "            ((isinstance(fx1, np.ndarray) and fx1.size == 1) or isinstance(fx1, float)), \\\n",
      "            \"The f function needs to return scalar or shape () but %s\" % fx1\n",
      "        assert np.isfinite(fx1), \\\n",
      "            \"f(x+h) caused nan for f %s for X %s\" % (f, (tmp + delta))\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # f(x-h)\n",
      "        # --------------------------------------------------------------------------------\n",
      "        X[idx] = tmp - delta\n",
      "        fx2: Union[np.ndarray, float] = f(X)\n",
      "        Logger.debug(\n",
      "            \"%s: idx[%s] x[%s] (x-h)[%s] fx2=[%s]\",\n",
      "            name, idx, tmp, tmp-delta, fx2\n",
      "        )\n",
      "        assert \\\n",
      "            ((isinstance(fx2, np.ndarray) and fx2.size == 1) or isinstance(fx2, float)), \\\n",
      "            \"The f function needs to return scalar or shape () but %s\" % fx2\n",
      "        assert np.isfinite(fx2), \\\n",
      "            \"f(x-h) caused nan for f %s for X %s\" % (f, (tmp - delta))\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # When f(x+k) and f(x-k) are relatively too close, subtract between them can ben\n",
      "        # too small, and the precision error 1/f(x) relative to f(x+k) and f(x-k) is much\n",
      "        # larger relative to that of f(x+k) or f(x-k), hence the result can be unstable.\n",
      "        # Prevent the subtract df(x) from being too small to f(x) by assuring df(x)/dx is\n",
      "        # greater than GN_DIFF_ACCEPTANCE_RATIO.\n",
      "        #\n",
      "        # e.g. For logistic log loss function f(x) with log(+1e-7) to avoid log(0)/inf.\n",
      "        # x[14.708627877981929] (x+h)[14.708627878981929] fx1=[14.708628288297405]\n",
      "        # x[14.708627877981929] (x-h)[14.708627876981929] fx2=[14.708628286670217]\n",
      "        # (fx1-fx2)=[1.6271872738116144e-09]\n",
      "        # (fx1-fx2) / fxn < 1e-10. The difference is relatively too small to f(x).\n",
      "        #\n",
      "        #\n",
      "        # If the gradient of f(x) at x is nearly zero, or saturation, then reconsider\n",
      "        # if using the numerical gradient is fit for the purpose. Prevent the gradient\n",
      "        # from being too close to zero by f(x+k)-f(x-k) > GN_DIFF_ACCEPTANCE_VALUE\n",
      "        # --------------------------------------------------------------------------------\n",
      "        Logger.debug(\"%s: (fx1-fx2)=[%s]\", name, (fx1-fx2))\n",
      "        difference = (fx1 - fx2)\n",
      "        assert \\\n",
      "            (np.abs(difference) > (fx1 * GN_DIFF_ACCEPTANCE_RATIO)) and \\\n",
      "            (np.abs(difference) > (fx2 * GN_DIFF_ACCEPTANCE_RATIO)), \\\n",
      "            \"Need (fx1-fx2) / fxn > %s to avoid float error but %s. GN is %s\" \\\n",
      "            % (GN_DIFF_ACCEPTANCE_RATIO, np.abs(difference), (fx1-fx2) / (2 * delta))\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Set the gradient element scalar value or shape()\n",
      "        # --------------------------------------------------------------------------------\n",
      "        g: Union[np.ndarray, float] = np.subtract(fx1, fx2) / (2 * delta)\n",
      "        assert np.isfinite(g)\n",
      "        assert \\\n",
      "            np.abs(g) > GRADIENT_SATURATION_THRESHOLD, \\\n",
      "            \"The gradient [%s] may have been saturated.\" % g\n",
      "\n",
      "        J[idx] = g\n",
      "\n",
      "        Logger.debug(\"%s: idx[%s] j=[%s]\", name, idx, g)\n",
      "\n",
      "        X[idx] = tmp\n",
      "        it.iternext()\n",
      "\n",
      "    return J\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from common import (\n",
    "    numerical_jacobian,\n",
    "    OFFSET_DELTA\n",
    ")\n",
    "lines = inspect.getsource(numerical_jacobian)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected numerical gradient=-1.1591416437806146\n",
      "Actual numerical gradient=[-1.15914178]\n",
      "Expected analytical gradient -T/P=-1.1591416792330183\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Example gradients for the cross entropy log loss -t*log(p).\n",
    "# --------------------------------------------------------------------------------\n",
    "p =0.86270721\n",
    "h = OFFSET_DELTA\n",
    "e = OFFSET_LOG\n",
    "\n",
    "expected_gn = (-np.log(p+h+e) + np.log(p-h+e)) / (2*h)\n",
    "actual_gn = (cross_entropy_log_loss(p+h, 1) - cross_entropy_log_loss(p-h, 1)) / (2*h)\n",
    "print(f\"Expected numerical gradient={expected_gn}\")\n",
    "print(f\"Actual numerical gradient={actual_gn}\")\n",
    "print(f\"Expected analytical gradient -T/P={-1 / (p+e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.96369904])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0.50924298 \n",
    "gn = (cross_entropy_log_loss(p+h, 1) - cross_entropy_log_loss(p-h, 1)) / (2*h)\n",
    "gn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Matmul binary classification (logisitc regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (10, 3) T.shape (10,) W [0.05460245 0.98746254 0.14810912]\n"
     ]
    }
   ],
   "source": [
    "MAX_TEST_TIMES = 100\n",
    "N = 10\n",
    "D = 2\n",
    "from data.classifications import (\n",
    "    linear_separable\n",
    ")\n",
    "X, T, V = linear_separable(d=D, n=N)\n",
    "print(f\"X.shape {X.shape} T.shape {T.shape} W {V}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[T==0, 1], X[T==0, 2], c='red')\n",
    "plt.scatter(X[T==1, 1], X[T==1, 2], c='green')\n",
    "\n",
    "# Hyperplace (X-b)V = 0 -> x1V1 + x2V2 - bV2 = 0\n",
    "x = np.linspace(-3,3,100)\n",
    "y = -(V[1] / V[2]) * x - (V[0] / V[2])\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show(ax, layer, colors=['b']):\n",
    "    normal = layer.W[0]\n",
    "    w0 = normal[0]\n",
    "    w1 = normal[1]\n",
    "    w2 = normal[2]\n",
    "    \n",
    "    ax.set_title(label=str(normal))\n",
    "    ax.scatter(X[T==0, 1], X[T==0, 2], c='red')\n",
    "    ax.scatter(X[T==1, 1], X[T==1, 2], c='green')\n",
    "    x = np.linspace(-3,3,100)\n",
    "    if ax.lines:\n",
    "        for line in ax.lines:\n",
    "            line.set_xdata(x)\n",
    "            y = -w1/w2 * x - w0 / w2\n",
    "            line.set_ydata(y)\n",
    "    else:\n",
    "        for color in colors:\n",
    "            y = -w1/w2 * x - w0 / w2\n",
    "            ax.plot(x, y, color)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    _x = np.linspace(-3,3,100)\n",
    "    _y = -w1/w2 * x - w0 / w2\n",
    "    plt.plot(_x, _y, label='linear')  # Plot some data on the axes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import copy\n",
    "from common import (\n",
    "    OFFSET_LOG,\n",
    "    OFFSET_DELTA,\n",
    "    numerical_jacobian,\n",
    "    weights,\n",
    "    random_string\n",
    ")\n",
    "from layer import (\n",
    "    Matmul\n",
    ")\n",
    "from optimizer import(\n",
    "    SGD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Need (fx1-fx2) / fxn > 1e-12 to avoid float error but 0.0. GN is 0.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-25b385fcb6f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# --------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mGN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_numerical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# [dL/dX, dL/dW]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/home/repository/git/oonisim/python_programs/nlp/src/layer/matmul.py\u001b[0m in \u001b[0;36mgradient_numerical\u001b[0;34m(self, h)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mdX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_jacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_jacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/home/repository/git/oonisim/python_programs/nlp/src/common/functions.py\u001b[0m in \u001b[0;36mnumerical_jacobian\u001b[0;34m(f, X, delta)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0mLogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: (fx1-fx2)=[%s]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfx1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mfx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0mdifference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfx1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m         \u001b[0;32massert\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfx1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mGN_DIFF_ACCEPTANCE_RATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfx2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mGN_DIFF_ACCEPTANCE_RATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Need (fx1-fx2) / fxn > 1e-12 to avoid float error but 0.0. GN is 0.0"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUf0lEQVR4nO3df5Bd5X3f8ffH+hHVgA2VZBxLYFSPDFE9xjVroBmc4LiNEXHLJENbMGNs6gmlDk7adBKY1HYmwU1DZtqhqbEVhaiyaxfGcZhEdsG408ZgQ0hZHBDIGKwIGzZyyiJjsBVjEHz7x72KlmX32burPbtXy/s1szN7znnuud99Rrqf+5wfz0lVIUnSdF622AVIkoabQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQi8JSS5K8sWO9r09yUc62O83k/yj+d6vNFsGhZaMJGcluSPJk0m+k+T2JG8BqKpPV9VPL3aNkyWpJPuTfD/JXyX5z0mWzXIfZycZ66pGafliFyDNhySvAD4P/GvgM8BK4K3ADxezrgGdWlW7k5wCfAl4CNiyuCVJhzii0FLxeoCqur6qnquqH1TVF6tqJ0CS9yb5ysHG/W/y70/yjSTfS3JVktcl+bMkTyX5TJKV/bZnJxlL8mtJHu8fErpoukKSvDPJPUm+2x/hvHGQP6Cqvg58GXjDFPv8kSTXJNnb/7mmv+4o4GbgNf1RyfeTvGY2HSfNxKDQUvEQ8FySTyTZnOS4AV5zDnAacCbwq8BW4CLgBHof1hdOaPtqYA2wDngPsDXJyZN3mOTNwDbgXwGrgd8DdiT5kZmKSbKJ3ijoL6bY/O/7db4JOBU4HfhgVe0HNgN7q+ro/s/eGf9yaRYMCi0JVfUUcBZQwO8D40l2JDm+8bKrq+qpqtoF3A98sar2VNWT9L6l/4NJ7T9UVT+sqluB/wn88yn2+fPA71XVn/dHNp+gd/jrzEYdX03yBPA54Drgv03R5iLgN6vqsaoaB34DeHdjn9K88RyFloyqegB4L0D/eP+ngGt44chgov834fcfTLH86gnLT/S/vR/0LWCqQzyvBd6T5AMT1q2cpu1Bb66q3Y3t9F//rQHeX5p3jii0JPWP929niuP9c3Rc/3zAQScCUx3ieRT4D1V17ISfl1fV9Yf5/nvphdBU7+8U0OqUQaElIckpSf5dkvX95RPojSTunMe3+Y0kK5O8FXgn8IdTtPl94LIkZ6TnqCQ/k+SYw3zv64EPJlmbZA3wYXojJuiNhFYneeVhvoc0JQ89aan4HnAG8MtJjgW+S+9y2V+Zp/3/NfAEvW/xfwNc1h+1vEBVjSb5eeCjwEZ6h7C+Atx2mO//EeAVwM7+8h/211FVX09yPbCnfw/GJk9oaz7FBxdJbUnOBj5VVesXuRRpUXjoSZLU1FlQJNmW5LEk90+zPUl+N8nuJDv7159LkoZMlyOK7fRuaJrOZnrHcDcClwIf77AWac6q6ksedtJLWWdBUVW3Ad9pNDkP+GT13Akcm+RHu6pHkjQ3i3nV0zp615wfNNZf9+3JDZNcSm/UwVFHHXXaKaecsiAFStJScffddz9eVWvn8trFDIpMsW7KS7Cqaiu9eXgYGRmp0dHRLuuSpCUnybdmbjW1xbzqaYze5GsHrWfqO10lSYtoMYNiB3Bx/+qnM4Enq+pFh50kSYurs0NP/TtFzwbW9J++9evACoCq2gLcBJwL7KZ3p+slXdUiSZq7zoKiqqabsfPg9gJ+oav3l6Sl6tlnn2VsbIynn376RdtWrVrF+vXrWbFixby9n3M9SdIRZmxsjGOOOYaTTjqJ5NB1QVXFvn37GBsbY8OGDfP2fk7hIUlHmKeffprVq1e/ICQAkrB69eopRxqHw6CQpCPQ5JCYaf3hMCgkSU0GhSSpyaCQpCPQdM8S6uIZQwaFJB1hVq1axb59+14UCgevelq1atW8vp+Xx0rSEWb9+vWMjY0xPj7+om0H76OYTwaFJB1hVqxYMa/3SczEQ0+SpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLU1GlQJDknyYNJdie5cortr0zyuST3JtmV5JIu65EkzV5nQZFkGXAtsBnYBFyYZNOkZr8AfK2qTgXOBv5TkpVd1SRJmr0uRxSnA7urak9VPQPcAJw3qU0BxyQJcDTwHeBAhzVJkmapy6BYBzw6YXmsv26ijwI/BuwF7gN+qaqen7yjJJcmGU0yOj4+3lW9kqQpdBkUmWJdTVp+B3AP8BrgTcBHk7ziRS+q2lpVI1U1snbt2vmuU5LU0GVQjAEnTFheT2/kMNElwI3Vsxt4GDilw5okSbPUZVDcBWxMsqF/gvoCYMekNo8AbwdIcjxwMrCnw5okSbO0vKsdV9WBJJcDtwDLgG1VtSvJZf3tW4CrgO1J7qN3qOqKqnq8q5okSbPXWVAAVNVNwE2T1m2Z8Pte4Ke7rEGSdHi8M1uS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmjoNiiTnJHkwye4kV07T5uwk9yTZleTWLuuRJM3e8q52nGQZcC3wj4Ex4K4kO6rqaxPaHAt8DDinqh5J8qqu6pEkzU2XI4rTgd1VtaeqngFuAM6b1OZdwI1V9QhAVT3WYT2SpDnoMijWAY9OWB7rr5vo9cBxSb6U5O4kF0+1oySXJhlNMjo+Pt5RuZKkqXQZFJliXU1aXg6cBvwM8A7gQ0le/6IXVW2tqpGqGlm7du38VypJmlZn5yjojSBOmLC8Htg7RZvHq2o/sD/JbcCpwEMd1iVJmoUuRxR3ARuTbEiyErgA2DGpzZ8Ab02yPMnLgTOABzqsSZI0S52NKKrqQJLLgVuAZcC2qtqV5LL+9i1V9UCSLwA7geeB66rq/q5qkiTNXqomnzYYbiMjIzU6OrrYZUjSESXJ3VU1MpfXeme2JKnJoJAkNU17jiLJz7VeWFU3zn85kqRh0zqZ/U8a2wowKCTpJWDaoKiqSxayEEnScJrxHEWS45P8QZKb+8ubkryv+9IkScNgkJPZ2+ndC/Ga/vJDwL/pqB5J0pAZJCjWVNVn6N0QR1UdAJ7rtCpJ0tAYJCj2J1lNf0K/JGcCT3ZalSRpaAwyhccv05uj6XVJbgfWAud3WpUkaWjMGBRV9dUkPwmcTG/q8Aer6tnOK5MkDYUZgyLJKuD9wFn0Dj99OcmWqnq66+IkSYtvkENPnwS+B/zX/vKFwH8H/llXRUmShscgQXFyVZ06YflPk9zbVUGSpOEyyFVPf9G/0gmAJGcAt3dXkiRpmLQmBbyP3jmJFcDFSR7pL78W+NrClCdJWmytQ0/vXLAqJElDqzUp4LcmLid5FbCq84okSUNlkEkB/2mSbwAPA7cC3wRu7rguSdKQGORk9lXAmcBDVbUBeDuezJakl4xBguLZqtoHvCzJy6rqT4E3dVuWJGlYDHIfxXeTHA3cBnw6yWPAgW7LkiQNi0FGFOcBPwD+LfAF4C9pPyZVkrSEDDIp4P4Ji5/osBZJ0hBq3XD3PfrPoJi8CaiqekVnVUmShkbrPopjFrIQSdJwGuQchSTpJcygkCQ1GRSSpKZBpvC4PMlxC1GMJGn4DDKieDVwV5LPJDknSbouSpI0PGYMiqr6ILAR+APgvcA3kvxWktd1XJskaQgMdI6iqgr46/7PAeA44LNJfqfD2iRJQ2DGO7OT/CLwHuBx4DrgV6rq2SQvA74B/Gq3JUqSFtMgkwKuAX5u8oOMqur5JD4FT5KWuEHOUXx4ckhM2PZA67X9k98PJtmd5MpGu7ckeS7J+TOXLElaSJ3dR5FkGXAtsBnYBFyYZNM07a4GbumqFknS3HV5w93pwO6q2lNVzwA30JuyfLIPAH8EPNZhLZKkOeoyKNYBj05YHuuv+1tJ1gE/C2xp7SjJpUlGk4yOj4/Pe6GSpOl1GRRT3Zg3edrya4Arquq51o6qamtVjVTVyNq1a+erPknSAAa56mmuxoATJiyvB/ZOajMC3NC/2XsNcG6SA1X1xx3WJUmahS6D4i5gY5INwF8BFwDvmtigqjYc/D3JduDzhoQkDZfOgqKqDiS5nN7VTMuAbVW1K8ll/e3N8xKSpOHQ5YiCqroJuGnSuikDoqre22UtkqS58XkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktTUaVAkOSfJg0l2J7lyiu0XJdnZ/7kjyald1iNJmr3OgiLJMuBaYDOwCbgwyaZJzR4GfrKq3ghcBWztqh5J0tx0OaI4HdhdVXuq6hngBuC8iQ2q6o6qeqK/eCewvsN6JElz0GVQrAMenbA81l83nfcBN0+1IcmlSUaTjI6Pj89jiZKkmXQZFJliXU3ZMHkbvaC4YqrtVbW1qkaqamTt2rXzWKIkaSbLO9z3GHDChOX1wN7JjZK8EbgO2FxV+zqsR5I0B12OKO4CNibZkGQlcAGwY2KDJCcCNwLvrqqHOqxFkjRHnY0oqupAksuBW4BlwLaq2pXksv72LcCHgdXAx5IAHKiqka5qkiTNXqqmPG0wtEZGRmp0dHSxy5CkI0qSu+f6Rdw7syVJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDV1GhRJzknyYJLdSa6cYnuS/G5/+84kb+6yHknS7HUWFEmWAdcCm4FNwIVJNk1qthnY2P+5FPh4V/VIkuamyxHF6cDuqtpTVc8ANwDnTWpzHvDJ6rkTODbJj3ZYkyRplpZ3uO91wKMTlseAMwZosw749sRGSS6lN+IA+GGS++e31CPWGuDxxS5iSNgXh9gXh9gXh5w81xd2GRSZYl3NoQ1VtRXYCpBktKpGDr+8I599cYh9cYh9cYh9cUiS0bm+tstDT2PACROW1wN759BGkrSIugyKu4CNSTYkWQlcAOyY1GYHcHH/6qczgSer6tuTdyRJWjydHXqqqgNJLgduAZYB26pqV5LL+tu3ADcB5wK7gb8BLhlg11s7KvlIZF8cYl8cYl8cYl8cMue+SNWLTglIkvS3vDNbktRkUEiSmoY2KJz+45AB+uKifh/sTHJHklMXo86FMFNfTGj3liTPJTl/IetbSIP0RZKzk9yTZFeSWxe6xoUywP+RVyb5XJJ7+30xyPnQI06SbUkem+5eszl/blbV0P3QO/n9l8DfA1YC9wKbJrU5F7iZ3r0YZwJ/vth1L2Jf/DhwXP/3zS/lvpjQ7v/Qu1ji/MWuexH/XRwLfA04sb/8qsWuexH74teAq/u/rwW+A6xc7No76IufAN4M3D/N9jl9bg7riMLpPw6ZsS+q6o6qeqK/eCe9+1GWokH+XQB8APgj4LGFLG6BDdIX7wJurKpHAKpqqfbHIH1RwDFJAhxNLygOLGyZ3auq2+j9bdOZ0+fmsAbFdFN7zLbNUjDbv/N99L4xLEUz9kWSdcDPAlsWsK7FMMi/i9cDxyX5UpK7k1y8YNUtrEH64qPAj9G7ofc+4Jeq6vmFKW+ozOlzs8spPA7HvE3/sQQM/HcmeRu9oDir04oWzyB9cQ1wRVU91/vyuGQN0hfLgdOAtwN/B/izJHdW1UNdF7fABumLdwD3AD8FvA74X0m+XFVPdVzbsJnT5+awBoXTfxwy0N+Z5I3AdcDmqtq3QLUttEH6YgS4oR8Sa4Bzkxyoqj9ekAoXzqD/Rx6vqv3A/iS3AacCSy0oBumLS4Dfrt6B+t1JHgZOAf7vwpQ4NOb0uTmsh56c/uOQGfsiyYnAjcC7l+C3xYlm7Iuq2lBVJ1XVScBngfcvwZCAwf6P/Anw1iTLk7yc3uzNDyxwnQthkL54hN7IiiTH05tJdc+CVjkc5vS5OZQjiupu+o8jzoB98WFgNfCx/jfpA7UEZ8wcsC9eEgbpi6p6IMkXgJ3A88B1VbXkpugf8N/FVcD2JPfRO/xyRVUtuenHk1wPnA2sSTIG/DqwAg7vc9MpPCRJTcN66EmSNCQMCklSk0EhSWoyKCRJTQaFJKnJoJBmKcn3Z9h+0nSzdzZes30pz3SrI5tBIUlqMiikvv4zLHYmWZXkqP5zC97QaH90kv+d5KtJ7ksyccbS5Uk+0d/fZ/t3RpPktCS39ifpu2WJznisJcYb7qQJknwEWEVvEr2xqvqPU7T5flUdnWQ58PKqeirJGnpTvG8EXgs8DJxVVbcn2UbvuRD/BbgVOK+qxpP8C+AdVfUvk2wHPl9Vn12Iv1OajaGcwkNaRL9Jb+6gp4FfnKFtgN9K8hP0pshYBxzf3/ZoVd3e//1T/X19AXgDvZlLoTfdxFKcn0xLjEEhvdDfpfdgmxX0Rhb7G20vove0tNOq6tkk3+y/Bl48dXPRC5ZdVfUP57ViqWOeo5BeaCvwIeDTwNUztH0l8Fg/JN5G75DTQScmORgIFwJfAR4E1h5cn2RFkr8/r9VLHTAopL7+E+AOVNX/AH4beEuSn2q85NPASJJReqOLr0/Y9gDwniQ76Y1SPt5/TOf5wNVJ7qX3IJ0fn/+/RJpfnsyWJDU5opAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU3/H7gE3/sHUejCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()  # Create a figure and an axes.\n",
    "ax.set_xlabel('x label')  # Add an x-label to the axes.\n",
    "ax.set_ylabel('y label')  # Add a y-label to the axes.\n",
    "ax.set_title(\"Simple Plot\")  # Add a title to the axes.\n",
    "ax.legend()  # Add a legend.\n",
    "\n",
    "M: int = 1\n",
    "W = weights.he(M, D+1)\n",
    "name = \"test_020_matmul\"\n",
    "sgd = SGD(lr=0.1)\n",
    "\n",
    "\n",
    "def objective(X: np.ndarray) -> Union[float, np.ndarray]:\n",
    "    \"\"\"Dummy objective function to calculate the loss L\"\"\"\n",
    "    return np.sum(cross_entropy_log_loss(softmax(X), T)) / N\n",
    "\n",
    "layer = Matmul(\n",
    "    name=name,\n",
    "    num_nodes=M,\n",
    "    W=W,\n",
    "    optimizer=sgd,\n",
    "    log_level=logging.DEBUG\n",
    ")\n",
    "layer.objective = objective\n",
    "#X = np.random.randn(N, D)\n",
    "\n",
    "losses: List[float] = []\n",
    "\n",
    "for i in range(MAX_TEST_TIMES):\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Layer forward path\n",
    "    # Calculate the layer output Y=f(X), and get the loss L = objective(Y)\n",
    "    # Test the numerical gradient dL/dX=layer.gradient_numerical().\n",
    "    # --------------------------------------------------------------------------------\n",
    "    Y = layer.function(X)\n",
    "    GN = layer.gradient_numerical()         # [dL/dX, dL/dW]\n",
    "    L = layer.objective(Y)\n",
    "    losses.append(L)\n",
    "    \n",
    "    if L > losses[-1]: print(f\"Loss {L} > previous loss {losses[-1]}\")\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Layer backward path\n",
    "    # Calculate the analytical gradient dL/dX=layer.gradient(dL/dY) with a dummy dL/dY.\n",
    "    # Confirm the numerical gradient (dL/dX, dL/dW) are closer to the analytical ones.\n",
    "    # --------------------------------------------------------------------------------\n",
    "    dY = np.ones_like(Y)\n",
    "    dX = layer.gradient(dY)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Gradient update.\n",
    "    # Run the gradient descent to update Wn+1 = Wn - lr * dL/dX.\n",
    "    # Confirm the new objective L(Yn+1) < L(Yn) with the Wn+1.\n",
    "    # Confirm W in the layer has been updated by the gradient descent.\n",
    "    # --------------------------------------------------------------------------------\n",
    "    backup = copy.deepcopy(W)\n",
    "\n",
    "    dS = layer.update()         # Analytical dL/dX, dL/dW\n",
    "    # assert np.all(np.abs(dS[0] - GN[0]) < OFFSET_FOR_DELTA) # dL/dX\n",
    "    # assert np.all(np.abs(dS[1] - GN[1]) < OFFSET_FOR_DELTA) # dL/dW\n",
    "\n",
    "    # Objective L with the updated W should be smaller than previous L\n",
    "    assert np.all(np.abs(objective(layer.function(X)) < L))\n",
    "    assert np.any(backup != layer.W), \"W has not been updated \"\n",
    "\n",
    "    if not i % 100:\n",
    "        show(ax, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std: [0. 0.]\n",
      "deviation: [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-dbe848d2dd4c>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  E = ((X - np.mean(X, axis=0)) / np.std(X, axis=0))\n"
     ]
    }
   ],
   "source": [
    "X = np.array(\n",
    "    [[ 47., 341.],\n",
    "     [ 47., 341.],\n",
    "     [ 47., 341.]]\n",
    ")\n",
    "print(f\"std: {np.std(X, axis=0)}\")\n",
    "print(f\"deviation: {X - np.mean(X, axis=0)}\")\n",
    "E = ((X - np.mean(X, axis=0)) / np.std(X, axis=0))\n",
    "print(E)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
