{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network \n",
    "Simple one layer neural network classifier. Mathjax formula not fully supported in github, hence the formulas get corrupted.\n",
    "\n",
    "<img src=\"image/nn_diagram.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/network.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1    # Batch size\n",
    "D = 3    # Number of features in the input data\n",
    "M = 2    # Number of nodes in a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confiurations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.classifications import (\n",
    "    linear_separable\n",
    ")\n",
    "X, T, V = linear_separable(d=2, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7feb1cf78820>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[T==0, 1], X[T==0, 2], c='red')\n",
    "plt.scatter(X[T==1, 1], X[T==1, 2], c='green')\n",
    "\n",
    "# Hyperplace (X-b)V = 0 -> x1V1 + x2V2 - bV2 = 0\n",
    "x = np.linspace(-3,3,100)\n",
    "y = -(V[1] / V[2]) * x - (V[0] / V[2])\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T\n",
    "Labels for data X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For One Hot Encoding labels\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ T_{_{OHE}} } &= ( \n",
    "    \\overset{ (M,) }{ T_{(n=0)} }, \\; \\dots \\;, \\overset{ (M,) }{ T_{(n=N-1)} } \n",
    ") \n",
    "\\\\\n",
    "\\overset{ (N,M) }{ T_{_{OHE}} } = ( \n",
    "    \\overset{ (M,) }{ T_{(n=0)} }, \\dots , \\overset{ (M,) }{ T_{(n=N-1)} } \n",
    ") \n",
    "\\\\\n",
    "\\overset{ (M,) }{ T_{ _{OHE} (n)} } &= ( \\overset{ () }{ t_{(n)(m=0)} }, \\; \\dots \\;, \\overset{ () }{ t_{(n)(m=M-1)} })\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "### For index labels\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,) }{ T_{_{IDX}} } &= (\\overset{ () }{ t_{(n=0)} }, \\; \\dots \\;, \\overset{ () }{ t_{(n=N-1)} }) \\qquad \\text {for index labels }\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W\n",
    "Weight parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.weights import (\n",
    "    xavier,\n",
    "    he,\n",
    "    uniform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matmul layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer.matmul import Matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ Y } \n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "{ Y_{(n=0)} } \\\\\n",
    "\\vdots \\\\\n",
    "{ Y_{(n)} } \\\\\n",
    "\\vdots \\\\\n",
    "{ Y_{(n=N-1)} }\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\overset{ (N,D) }{ X } \\; @ \\; \\overset{ (D,M) }{ W^T }\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (M,) }{ Y_{(n)} } &= (y_{(n)(m=0)}, \\; \\dots, \\; y_{(n)(m)},  \\; \\dots, \\; y_{(n)(m=M-1)})\n",
    "\\\\\n",
    "\\overset{ () }{ y_{(n)(m)} }\n",
    "&= \\overset{ (D,) }{ X_{(n)} } \\cdot \\overset{ (D,) }{ W_{(m)}^T }\n",
    "= \\sum\\limits ^{D}_{d=0}  \\overset{ () }{ x_{(n)(d)} } * \\overset{ () }{ w_{(m)(d)} }\n",
    "\\\\\n",
    "_{(0 \\le d \\le D, \\; 0 \\le n \\lt N, \\; 0 \\le m \\lt M)}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dX\n",
    "\n",
    "Impact on L by $dX$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,D) }{ \\frac {\\partial L }{ \\partial X } }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial Y} }\n",
    "@ \\overset { (M,D) }{ W } \n",
    "\\end{align*}\n",
    "$\n",
    "<img src=\"image/nn_back_propagation_dL_dX.png\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dW.T\n",
    "Impact on L by $dW^T$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{ \\partial W^T } }\n",
    "= \\overset { (D,N) }{ X^T } \n",
    "@ \n",
    "\\overset { (N,M) }{ \\frac {\\partial L}{\\partial Y} }\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "<img src=\"image/nn_back_propagation_dL_dWT.png\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ A } &= \n",
    "activation \\left( \n",
    "    \\overset{ (N,M) }{ Y }  = \n",
    "    \\begin{bmatrix}\n",
    "    { Y_{(n=0)} } \\\\\n",
    "    \\vdots \\\\\n",
    "    { Y_{(n)} } \\\\\n",
    "    \\vdots \\\\\n",
    "    { Y_{(n=N-1)} }\n",
    "    \\end{bmatrix}\n",
    "\\right)\n",
    "\\\\\n",
    "\\overset{ (M,) }{ A_{(n)} } \n",
    "&= activation \\left( \\overset{ (M,) }{ Y_{(n) }} \\right)  \\\\\n",
    "&= (a_{(n)(m=0)}, \\; \\dots, \\; a_{(n)(m)},  \\; \\dots, \\; a_{(n)(m=M-1)})\n",
    "\\\\\n",
    "\\overset{ () }{ a_{(n)(m)} } &= activation \\left( \\overset{ () }{ y_{(n)(m)} } \\right)\n",
    "\\quad _{(0 \\le n \\lt N, \\; 0 \\le m \\lt M)}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dY\n",
    "\n",
    "Impact on L by dY from the matmul layer.\n",
    "\n",
    "$\n",
    "\\begin {align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{ \\partial Y } }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial A} } \n",
    "* \n",
    "\\overset { (N,M) }{ \\frac {\\partial A}{\\partial Y} }\n",
    "\\end {align*}\n",
    "$\n",
    "\n",
    "### For sigmoid activation\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset {(N,M)}{\\frac { \\partial L }{ \\partial Y} }\n",
    "&= \\frac { \\partial A }{ \\partial Y} * A * (1 - A)\n",
    "\\\\\n",
    "\\frac { \\partial y_{(n)(m)} } { \\partial a_{(n)(m)} }\n",
    "&= a_{(n)(m)} * (1 - a_{(n)(m)} )  \\\\ \n",
    "y_{(n)(m)} = sigmoid(a_{(n)(m)} )&=  \\frac {1}{ 1 + exp(y_{(n)(m)})}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "### For ReLU activation\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac { \\partial a_{(n)(m)} }{ \\partial y_{(n)(m)} }\n",
    "&= 1 \\quad y_{(n)(m)}  \\gt 0 \\\\\n",
    "&= 0 \\quad y_{(n)(m)}  \\le 0 \\\\\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax layer\n",
    "$C_n$ is to prevent the overflow at $np.exp()$.\n",
    "\n",
    "<img src=\"image/softmax.png\" align=\"left\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,1) }{ C } &= np.max\\left( \n",
    "    \\overset{ (N,M) }{ A }, \\; axis=-1,  \\; keepdim=True \n",
    "\\right) \\\\\n",
    "&=  \n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n=0  )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n    )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n=N-1)} } \\end{bmatrix}\n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\overset{ (N,M) }{ EXP } &= np.exp \\left( \\overset{ (N,M) }{ A } - \\overset{ (N,1) }{ C } \\right)\n",
    "= np.exp \\left(\n",
    "    \\begin{bmatrix}\n",
    "    { A_{(n=0)} } - { C_{(n=0)} }\\\\\n",
    "    \\vdots \\\\\n",
    "    { A_{(n)} }   - { C_{(n)} }\\\\\n",
    "    \\vdots \\\\\n",
    "    { A_{(n=N-1)} } - { C_{(n=N-1)} }\\\\\n",
    "    \\end{bmatrix}\n",
    "\\right) \n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "    e_{(n=0)(m=0)}   & \\dots      & e_{(n=0)(m=M-1)}   \\\\  \n",
    "    \\vdots           & e_{(n)(m)} & \\vdots             \\\\\n",
    "    e_{(n=N-1)(m=0)} & \\dots      & e_{(n=N-1)(m=M-1)} \n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,1) }{ S } &= \\overset{ (N,1) }{ sum(EXP) } = np.sum \\left( \n",
    "    \\overset{ (N,M) }{ EXP }, \\; axis=-1,  \\; keepdim=True \n",
    "\\right)\n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n=0  )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n    )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n=N-1)} } \\end{bmatrix}\n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\overset{ () }{ s_{(n)} } &= \\sum\\limits ^{M-1}_{m=0} np.exp(\\; a_{(n)(m)} - c_{(n)} \\; )\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,M) }{ P } &= \\overset{ (N,M) }{ EXP }  \\;\\; / \\;\\; \\overset{ (N,1) }{ sum(EXP) } \n",
    "\\\\\n",
    "\\overset{ (N,) }{ P_{(n)} } &= (p_{(n)(m=0)}, \\; \\dots, \\; p_{(n)(m)} , \\; \\dots, \\; p_{(n)(m=M-1)})\n",
    "\\\\\n",
    "{ p_{(n)(m)} } \n",
    "&= \\frac {np.exp \\left( \n",
    "    { a_{(n)(m) } } - { c_{(n)} }) \\right) \n",
    "}\n",
    "{  \n",
    "np.sum \\left( \n",
    "    np.exp \\left( \n",
    "        a_{(n)(m) } - c_{(n)}\n",
    "    \\right)\n",
    "\\right) \n",
    "}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(A):\n",
    "    \"\"\"Softmax function from deep-learning-from-scratch\n",
    "    Args:\n",
    "        A: batch input of shape (N x M).\n",
    "            N: Batch size\n",
    "            M: Number of nodes\n",
    "    Returns:\n",
    "        Prediction probability matrix P of shape (N x M)\n",
    "    \"\"\"\n",
    "    C = np.max(A, axis=-1, keepdims=True)   # オーバーフロー対策\n",
    "    exp = np.exp(A - C)\n",
    "    P = exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dA\n",
    "\n",
    "Impact on L by dA from the activation layer.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{\\partial A} }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial P} }\n",
    "* \n",
    "\\overset { (N,M) }{ \\frac {\\partial P }{\\partial A} } \n",
    "= \n",
    "\\frac {1}{N} (P - T)\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "$\n",
    "Jacobian \\; : \\; f \\circ g \\rightarrow Jf \\circ Jg\n",
    "\\\\\n",
    "\\begin{align*}\n",
    "\\\\\n",
    "L &= f(\\; p_{(n)(m=0)} \\;) = f( \\; g(\\;  a_{(n)(m=0)} \\; ) \\; ) \\quad : p = g(a) = softmax(a)\n",
    "\\\\\n",
    "\\frac {\\partial L} { \\partial a_{(n)(m=0)} }\n",
    "&= Jf(p) \\circ Jg(a) \n",
    "=  \\frac {\\partial L} { \\partial p_{(n)(m=0)} } * \\frac {\\partial  p_{(n)(m=0)}} { \\partial a_{(n)(m=0)} }\n",
    "\\\\\n",
    "&= \\frac {1}{N} \\left(\n",
    " p_{(n)(m=0)} -t_{(n)(m=0)}\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient from cross entropy log loss\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac { \\partial L }{ \\partial  p_{(n)(m=0)} }\n",
    "&= \\frac{-1}{N} t_{(n)(m=0)} * \\frac {s_{(n)}}{e_{(n)(m=0)}}\n",
    "\\\\\n",
    "\\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "&= \\frac{-1}{N} t_{(n)(m=1)} * \\frac {s_{(n)}}{e_{(n)(m=1)}}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Gradient $\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "p_{(n)(m=0)} &= \\frac {e_{(n)(m=0)} }{ s_{(n)} } \\\\\n",
    "p_{(n)(m=0)} &= f \\circ g_{(m=0)} = { s^{-1}_{(n)} } \\; * \\; { e_{(n)(m=0)} }\n",
    "\\rightarrow \\frac { \\partial  p_{(n)(m=0)} } { \\partial s^{-1}_{(n)} } = e_{(n)(m=0)}\n",
    "\\\\\n",
    "p_{(n)(m=1)} &= \\frac {e_{(n)(m=1)} }{ s_{(n)} } \\\\\n",
    "p_{(n)(m=1)} &= f \\circ g_{(m=1)} = { s^{-1}_{(n)} } \\; * \\; { e_{(n)(m=1)} }\n",
    "\\rightarrow \\frac { \\partial  p_{(n)(m=1)} } { \\partial s^{-1}_{(n)} } = e_{(n)(m=1)}\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "\\left[\n",
    "    \\frac { \\partial  p_{(n)(m=0)} } { \\partial s^{-1}_{(n)} } * \n",
    "    \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \n",
    "    +\n",
    "    \\frac { \\partial  p_{(n)(m=1)} } { \\partial s^{-1}_{(n)} } * \n",
    "    \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "\\right]\n",
    "\\\\\n",
    "&= \\sum\\limits^{M-1}_{m=0} \n",
    "    e_{(n)(m)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m)} } \n",
    "\\\\\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "    \\begin{bmatrix}\n",
    "    e_{(n)(m=0)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \\\\\n",
    "    + \\\\\n",
    "    e_{(n)(m=1)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "    \\end{bmatrix}\n",
    "\\\\\n",
    "&= -s_{(n)}(\\; t_{(n)(m=0)} + t_{(n)(m=1)} \\;) \\\\\n",
    "&= -s_{(n)}\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "\\left[\n",
    "    e_{(n)(m=0)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \n",
    "    + \n",
    "    e_{(n)(m=1)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "\\right]\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient $\\frac {\\partial L }{ \\partial { s_{(n)} } } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac {1} { s_{(n)} } &= s^{-1}_{(n)} \\rightarrow\n",
    "\\frac { \\partial { s^{-1}_{(n)} } } {\\partial s_{(n)}} = \\frac {-1}{s^{2}_{(n)}}\n",
    "\\\\\n",
    "\\frac {\\partial L }{ \\partial { s_{(n)} } } \n",
    "&=\n",
    "\\frac {-1}{s^{2}_{(n)}} * \n",
    "\\frac {\\partial L}{ \\partial s^{-1}_{(n)} } \\\\\n",
    "&= \\frac {1}{s_n}\n",
    "\\end{align*} \\\\\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient $\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } $\n",
    "$\n",
    "\\begin{align*}\n",
    "s_{(n)} &= \\sum\\limits ^{M-1}_{m=0} e_{(n)(m)} \\rightarrow \n",
    "\\frac { \\partial { s_{(n)} } } {\\partial e_{(n)(m=0)}} = 1\n",
    "\\\\\n",
    "p_{(n)(m=0)} &= \\frac {e_{(n)(m=0)} }{ s_{(n)} }\\rightarrow \n",
    "\\frac { \\partial { p_{(n)(m=0)} } } {\\partial e_{(n)(m=0)}} = \\frac {1}{s_{(n)}}\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } \n",
    "&= \\begin{bmatrix}  \n",
    "    \\frac { \\partial { s_{(n)} } } {\\partial e_{(n)(m=0)}} *  \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \\\\\n",
    "    + \\\\\n",
    "    \\frac { \\partial { p_{(n)(m=0)} } } {\\partial e_{(n)(m=0)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}  \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \\\\\n",
    "    + \\\\\n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } &= \n",
    "\\left[\n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "    + \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \n",
    "\\right]\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } &= \n",
    "\\begin{bmatrix}  \n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} }  \\\\\n",
    "    +  \\\\\n",
    "    \\frac {\\partial L }{ \\partial s_{(n)} } \n",
    "\\end{bmatrix} \\\\\n",
    "&= \\frac {-t_{(n)(m=0)}}{e_{(n)(m=0)} } + \\frac {1}{s_{n}}\n",
    "\\end{align*}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gardient $\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "e_{(n)(m)} &= exp(\\; a_{(n)(m)} \\; ) \\rightarrow \\frac { \\partial e_{(n)(m)} }{ \\partial a_{(n)(m)} } = e_{(n)(m)} \n",
    "\\\\\n",
    "e_{(n)(m=0)} &= exp(a_{(n)(m=0)}) \\rightarrow \\frac { \\partial e_{(n)(m=0)} }{ \\partial a_{(n)(m=0)} } = e_{(n)(m=0)} \n",
    "\\\\\n",
    "e_{(n)(m=1)} &= exp(a_{(n)(m=1)}) \\\\\n",
    "\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } \n",
    "&=   \\frac { \\partial e_{(n)(m=0)} }{ \\partial a_{(n)(m=0)} } * \n",
    "    \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \n",
    "= e_{(n)(m=0)} * \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } \n",
    "&= e_{(n)(m=0)} * \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \\\\\n",
    "&= -t_{(n)(m=0)} + \\frac { e_{(n)(m=0)} }{ s_{n} } \\\\\n",
    "&= p_{(n)(m=0)} -t_{(n)(m=0)} \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Log Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer.loss import SoftmaxWithLogLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For label using One Hot Encoding (OHE)\n",
    "For instance, if multi labels are (0,1,2,3,4) and each label is OHE, then the label for 2 is (0,0,1,0,0).   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product of matrix rows\n",
    "\n",
    "There is no formal operation to calculate the dot products of the rows from two matrices, but to calculate the diagonal of the matlix multiplication that also calculate non-diagonals. To avoid calculating non-diagonals, use [einsum](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html).\n",
    "\n",
    "* [Name of matrix operation of ```[A[0] dot B[0], A[1] dot B[1] ]``` from 2x2 matrices A, B](https://math.stackexchange.com/questions/4010721/name-of-matrix-operation-of-a0-dot-b0-a1-dot-b1-from-2x2-matrices-a)\n",
    "\n",
    "<img src=\"image/dot_products_of_matrix_rows.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is \n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "b.T is \n",
      "[[ 0 -3]\n",
      " [-1 -4]\n",
      " [-2 -5]]\n",
      "\n",
      "c[\n",
      "    np.inner(a[0], b[0]),\n",
      "    np.inner(a[1], b[1]),    \n",
      "] is [-5, -50]\n",
      "\n",
      "\n",
      "np.einsum('ij,ji->i', a, b.T) is [ -5 -50]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(6).reshape(2,3)\n",
    "b = np.arange(0,-6,-1).reshape(2,3)\n",
    "c = [\n",
    "    np.inner(a[0], b[0]),\n",
    "    np.inner(a[1], b[1]),    \n",
    "]\n",
    "print(f\"a is \\n{a}\")\n",
    "print(f\"b.T is \\n{b.T}\\n\")\n",
    "fmt=f\"\"\"c[\n",
    "    np.inner(a[0], b[0]),\n",
    "    np.inner(a[1], b[1]),    \n",
    "] is {c}\\n\n",
    "\"\"\"\n",
    "print(fmt)\n",
    "\n",
    "# Use einsum\n",
    "e = np.einsum('ij,ji->i', a, b.T)\n",
    "fmt=\"np.einsum('ij,ji->i', a, b.T)\"\n",
    "print(f\"{fmt} is {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foward path (OHE)\n",
    "$\n",
    "\\text{ for one hot encoding labels }\n",
    "\\\\\n",
    "\\begin{align*}\n",
    "\\overset{ (N,) }{ J } &= - \\sum\\limits^{M-1}_{m=0} \n",
    "    \\left[ \\; \\;  \n",
    "        t_{(n)(m)} \\;  * \\;  np.log(p_{(n)(m)}) \\;\\;  \n",
    "    \\right]\n",
    "\\\\\n",
    "\\overset{ () }{ j_{(n)} } &= \\overset{ (M,) }{ T_{(n)} } \\cdot \\overset{ (M,) }{ P_{(n)} } \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient dL/dP\n",
    "\n",
    "Impact on L by the $dP$ from the softmax layer for one hot encoding labels.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac { \\partial L }{ \\partial P} }\n",
    "&= \\overset { (N,) }{ \\frac { \\partial L }{ \\partial J} } * \n",
    "\\overset { (N,M) }{ \n",
    "\\left(\n",
    " - \\frac { \\partial T } { \\partial P }\n",
    " \\right) \n",
    "} \n",
    "= - \\frac {1}{N }  \\frac { \\partial T } { \\partial P }\n",
    "\\\\\n",
    "\\frac {\\partial L }{\\partial p_{(n)(m=0)}} \n",
    "&= \\frac {\\partial L}{\\partial j_{(n)}} * \\frac {\\partial j_{(n)}} {\\partial p_{(n)(m=0)}} \n",
    "= \\frac {1}{N} \\frac { -t_{(n)(m=0)}}{ p_{(n)(m=0)} } \n",
    "=  \\frac {1}{N} \\left(\n",
    " -t_{(n)(m=0)} * \\frac { s_{(n)} }{ e_{(n)(m=0)} }\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For label using indexing \n",
    "For instance, if the multi labels are (0,1,2,3,4) then the index is 2 for the label 2. If the labels are (2,4,6,8,9), then the index is 3 for the label 8.  \n",
    "\n",
    "Use LP to select the probabilities from P for the corresponding labels. For instance, if the label is 2 (hence the index is 2) for X(n=0), and 4 for X(n=3), then the numpy tuple indexing selects ```P[n=0][m=2]``` and ```P[n=3][m=4] ```.\n",
    "\n",
    "```\n",
    "P[\n",
    "   (0, 3),\n",
    "   (2, 4)\n",
    "]\n",
    "```\n",
    "\n",
    "$\n",
    "\\text{ for index labels e.g. (5, 2, 0, 9, ...)}\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,) }{ J } = - np.sum(\\; np.log(LP), \\; axis = -1 \\;) \\\\\n",
    "LP = label\\_probability = P \\left[ \\\\\n",
    "\\quad ( \\; 0, \\; \\dots, \\;  {N-1}) , \\\\\n",
    "\\quad ( \\; t_{(n=0)} \\; , \\dots , \\; t_{(n=N-1)}) \\\\\n",
    "\\right]\n",
    "\\\\\n",
    "\\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward path\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ () }{ L } = \\frac {1}{N} \\sum\\limits^{N-1}_{n=0} \\overset{ () }{ j_{{(n)}} }\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gardient dL/dJ\n",
    "\n",
    "Impact on L by $dJ$ from the cross entropy log loss layer.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,) }{ \\frac {\\partial L}{\\partial J} }  &= \\frac {1}{N} \\overset{(N,)}{ones}\n",
    "\\\\\n",
    "\\frac {\\partial L}{\\partial j_{(n)} } &= \\frac {1}{N} \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ = np.ones(N) / N\n",
    "dJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [two_layer_net.ipynb defines the lambda with parameter W which is redundant #254](https://github.com/cs231n/cs231n.github.io/issues/254)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.classifications import (\n",
    "    linear_separable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 0]), array([-0.3166693 , -0.28063476, -0.90607101]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W, X, T = linear_separable(d=2,n=3)\n",
    "X,T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is \n",
      "[[ 1.         -2.35352438 -0.29712016 -0.74581152]\n",
      " [ 1.         -0.09376576  0.32909668  0.09359668]\n",
      " [ 1.          0.95917525 -0.31510786  0.00531054]]\n",
      "w is [ 0.13101703 -0.93108699  0.13531909  0.31241045]\n"
     ]
    }
   ],
   "source": [
    "n=3\n",
    "d=4\n",
    "w = np.random.randn(d)\n",
    "w /=np.linalg.norm(w)\n",
    "X = np.random.randn(n, d)\n",
    "X[\n",
    "    ::,\n",
    "    0\n",
    "] = 1\n",
    "\n",
    "print(f\"X is \\n{X}\")\n",
    "print(f\"w is {w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y is [ 2.04914762  0.29209476 -0.80303961]\n",
      "T is [1 1 0]\n"
     ]
    }
   ],
   "source": [
    "Y = np.einsum('ij,j', X, w)\n",
    "T = (Y > 0).astype(int)\n",
    "\n",
    "print(f\"Y is {Y}\")\n",
    "print(f\"T is {T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0491476220320415, 0.29209475696468723, -0.8030396106644051]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ np.inner(X[i], w) for i in range(X.shape[0]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(0)\n",
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
