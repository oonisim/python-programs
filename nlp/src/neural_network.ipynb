{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network \n",
    "Simple one layer neural network classifier. Mathjax formula not fully supported in github, hence the formulas get corrupted.\n",
    "\n",
    "<img src=\"image/nn_diagram.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/nn_functions.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python path\n",
    "Python path setup to avoid the relative imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1    # Batch size\n",
    "D = 3    # Number of features in the input data\n",
    "M = 2    # Number of nodes in a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confiurations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.classifications import (\n",
    "    linear_separable\n",
    ")\n",
    "X, T, V = linear_separable(d=2, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcb5f1db550>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[T==0, 1], X[T==0, 2], c='red')\n",
    "plt.scatter(X[T==1, 1], X[T==1, 2], c='green')\n",
    "\n",
    "# Hyperplace (X-b)V = 0 -> x1V1 + x2V2 - bV2 = 0\n",
    "x = np.linspace(-3,3,100)\n",
    "y = -(V[1] / V[2]) * x - (V[0] / V[2])\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T\n",
    "Labels for data X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For One Hot Encoding labels\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ T_{_{OHE}} } &= ( \n",
    "    \\overset{ (M,) }{ T_{(n=0)} }, \\; \\dots \\;, \\overset{ (M,) }{ T_{(n=N-1)} } \n",
    ") \n",
    "\\\\\n",
    "\\overset{ (N,M) }{ T_{_{OHE}} } = ( \n",
    "    \\overset{ (M,) }{ T_{(n=0)} }, \\dots , \\overset{ (M,) }{ T_{(n=N-1)} } \n",
    ") \n",
    "\\\\\n",
    "\\overset{ (M,) }{ T_{ _{OHE} (n)} } &= ( \\overset{ () }{ t_{(n)(m=0)} }, \\; \\dots \\;, \\overset{ () }{ t_{(n)(m=M-1)} })\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "### For index labels\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,) }{ T_{_{IDX}} } &= (\\overset{ () }{ t_{(n=0)} }, \\; \\dots \\;, \\overset{ () }{ t_{(n=N-1)} }) \\qquad \\text {for index labels }\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W\n",
    "Weight parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.weights import (\n",
    "    xavier,\n",
    "    he,\n",
    "    uniform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matmul layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer.matmul import Matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ Y } \n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "{ Y_{(n=0)} } \\\\\n",
    "\\vdots \\\\\n",
    "{ Y_{(n)} } \\\\\n",
    "\\vdots \\\\\n",
    "{ Y_{(n=N-1)} }\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\overset{ (N,D) }{ X } \\; @ \\; \\overset{ (D,M) }{ W^T }\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (M,) }{ Y_{(n)} } &= (y_{(n)(m=0)}, \\; \\dots, \\; y_{(n)(m)},  \\; \\dots, \\; y_{(n)(m=M-1)})\n",
    "\\\\\n",
    "\\overset{ () }{ y_{(n)(m)} }\n",
    "&= \\overset{ (D,) }{ X_{(n)} } \\cdot \\overset{ (D,) }{ W_{(m)}^T }\n",
    "= \\sum\\limits ^{D}_{d=0}  \\overset{ () }{ x_{(n)(d)} } * \\overset{ () }{ w_{(m)(d)} }\n",
    "\\\\\n",
    "_{(0 \\le d \\le D, \\; 0 \\le n \\lt N, \\; 0 \\le m \\lt M)}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dX\n",
    "\n",
    "Impact on L by $dX$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,D) }{ \\frac {\\partial L }{ \\partial X } }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial Y} }\n",
    "@ \\overset { (M,D) }{ W } \n",
    "\\end{align*}\n",
    "$\n",
    "<img src=\"image/nn_back_propagation_dL_dX.png\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dW.T\n",
    "Impact on L by $dW^T$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{ \\partial W^T } }\n",
    "= \\overset { (D,N) }{ X^T } \n",
    "@ \n",
    "\\overset { (N,M) }{ \\frac {\\partial L}{\\partial Y} }\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "<img src=\"image/nn_back_propagation_dL_dWT.png\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ A } &= \n",
    "activation \\left( \n",
    "    \\overset{ (N,M) }{ Y }  = \n",
    "    \\begin{bmatrix}\n",
    "    { Y_{(n=0)} } \\\\\n",
    "    \\vdots \\\\\n",
    "    { Y_{(n)} } \\\\\n",
    "    \\vdots \\\\\n",
    "    { Y_{(n=N-1)} }\n",
    "    \\end{bmatrix}\n",
    "\\right)\n",
    "\\\\\n",
    "\\overset{ (M,) }{ A_{(n)} } \n",
    "&= activation \\left( \\overset{ (M,) }{ Y_{(n) }} \\right)  \\\\\n",
    "&= (a_{(n)(m=0)}, \\; \\dots, \\; a_{(n)(m)},  \\; \\dots, \\; a_{(n)(m=M-1)})\n",
    "\\\\\n",
    "\\overset{ () }{ a_{(n)(m)} } &= activation \\left( \\overset{ () }{ y_{(n)(m)} } \\right)\n",
    "\\quad _{(0 \\le n \\lt N, \\; 0 \\le m \\lt M)}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dY\n",
    "\n",
    "Impact on L by dY from the matmul layer.\n",
    "\n",
    "$\n",
    "\\begin {align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{ \\partial Y } }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial A} } \n",
    "* \n",
    "\\overset { (N,M) }{ \\frac {\\partial A}{\\partial Y} }\n",
    "\\end {align*}\n",
    "$\n",
    "\n",
    "### For sigmoid activation\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset {(N,M)}{\\frac { \\partial L }{ \\partial Y} }\n",
    "&= \\frac { \\partial A }{ \\partial Y} * A * (1 - A)\n",
    "\\\\\n",
    "\\frac { \\partial y_{(n)(m)} } { \\partial a_{(n)(m)} }\n",
    "&= a_{(n)(m)} * (1 - a_{(n)(m)} )  \\\\ \n",
    "y_{(n)(m)} = sigmoid(a_{(n)(m)} )&=  \\frac {1}{ 1 + exp(y_{(n)(m)})}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "### For ReLU activation\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac { \\partial a_{(n)(m)} }{ \\partial y_{(n)(m)} }\n",
    "&= 1 \\quad y_{(n)(m)}  \\gt 0 \\\\\n",
    "&= 0 \\quad y_{(n)(m)}  \\le 0 \\\\\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax layer\n",
    "$C_n$ is to prevent the overflow at $np.exp()$.\n",
    "\n",
    "<img src=\"image/softmax.png\" align=\"left\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax and Cross Entropy Log Loss are combined as the gradient results in a simple form $P - T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def softmax(X: Union[np.ndarray, float]) -> Union[np.ndarray, float]:\n",
      "    \"\"\"Softmax P = exp(X) / sum(exp(X))\n",
      "    Args:\n",
      "        X: batch input data of shape (N,M).\n",
      "            N: Batch size\n",
      "            M: Number of nodes\n",
      "    Returns:\n",
      "        Probability P of shape (N,M)\n",
      "    \"\"\"\n",
      "    assert X.dtype == float\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # exp(x-c) to prevent the infinite exp(x) for a large value x, with c = max(x).\n",
      "    # keepdims=True to be able to broadcast.\n",
      "    # --------------------------------------------------------------------------------\n",
      "    C = np.max(X, axis=-1, keepdims=True)   # オーバーフロー対策\n",
      "    exp = np.exp(X - C)\n",
      "    return exp / np.sum(exp, axis=-1, keepdims=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from layer import SoftmaxWithLogLoss\n",
    "from common import softmax\n",
    "\n",
    "lines = inspect.getsource(softmax)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,1) }{ C } &= np.max\\left( \n",
    "    \\overset{ (N,M) }{ A }, \\; axis=-1,  \\; keepdim=True \n",
    "\\right) \\\\\n",
    "&=  \n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n=0  )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n    )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n=N-1)} } \\end{bmatrix}\n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\overset{ (N,M) }{ EXP } &= np.exp \\left( \\overset{ (N,M) }{ A } - \\overset{ (N,1) }{ C } \\right)\n",
    "= np.exp \\left(\n",
    "    \\begin{bmatrix}\n",
    "    { A_{(n=0)} } - { C_{(n=0)} }\\\\\n",
    "    \\vdots \\\\\n",
    "    { A_{(n)} }   - { C_{(n)} }\\\\\n",
    "    \\vdots \\\\\n",
    "    { A_{(n=N-1)} } - { C_{(n=N-1)} }\\\\\n",
    "    \\end{bmatrix}\n",
    "\\right) \n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "    e_{(n=0)(m=0)}   & \\dots      & e_{(n=0)(m=M-1)}   \\\\  \n",
    "    \\vdots           & e_{(n)(m)} & \\vdots             \\\\\n",
    "    e_{(n=N-1)(m=0)} & \\dots      & e_{(n=N-1)(m=M-1)} \n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,1) }{ S } &= \\overset{ (N,1) }{ sum(EXP) } = np.sum \\left( \n",
    "    \\overset{ (N,M) }{ EXP }, \\; axis=-1,  \\; keepdim=True \n",
    "\\right)\n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n=0  )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n    )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n=N-1)} } \\end{bmatrix}\n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\overset{ () }{ s_{(n)} } &= \\sum\\limits ^{M-1}_{m=0} np.exp(\\; a_{(n)(m)} - c_{(n)} \\; )\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,M) }{ P } &= \\overset{ (N,M) }{ EXP }  \\;\\; / \\;\\; \\overset{ (N,1) }{ sum(EXP) } \n",
    "\\\\\n",
    "\\overset{ (N,) }{ P_{(n)} } &= (p_{(n)(m=0)}, \\; \\dots, \\; p_{(n)(m)} , \\; \\dots, \\; p_{(n)(m=M-1)})\n",
    "\\\\\n",
    "{ p_{(n)(m)} } \n",
    "&= \\frac {np.exp \\left( \n",
    "    { a_{(n)(m) } } - { c_{(n)} }) \\right) \n",
    "}\n",
    "{  \n",
    "np.sum \\left( \n",
    "    np.exp \\left( \n",
    "        a_{(n)(m) } - c_{(n)}\n",
    "    \\right)\n",
    "\\right) \n",
    "}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dA\n",
    "\n",
    "Impact on L by dA from the activation layer.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{\\partial A} }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial P} }\n",
    "* \n",
    "\\overset { (N,M) }{ \\frac {\\partial P }{\\partial A} } \n",
    "= \n",
    "\\frac {1}{N} (P - T)\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "$\n",
    "Jacobian \\; : \\; f \\circ g \\rightarrow Jf \\circ Jg\n",
    "\\\\\n",
    "\\begin{align*}\n",
    "\\\\\n",
    "L &= f(\\; p_{(n)(m=0)} \\;) = f( \\; g(\\;  a_{(n)(m=0)} \\; ) \\; ) \\quad : p = g(a) = softmax(a)\n",
    "\\\\\n",
    "\\frac {\\partial L} { \\partial a_{(n)(m=0)} }\n",
    "&= Jf(p) \\circ Jg(a) \n",
    "=  \\frac {\\partial L} { \\partial p_{(n)(m=0)} } * \\frac {\\partial  p_{(n)(m=0)}} { \\partial a_{(n)(m=0)} }\n",
    "\\\\\n",
    "&= \\frac {1}{N} \\left(\n",
    " p_{(n)(m=0)} -t_{(n)(m=0)}\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient from cross entropy log loss\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac { \\partial L }{ \\partial  p_{(n)(m=0)} }\n",
    "&= \\frac{-1}{N} t_{(n)(m=0)} * \\frac {s_{(n)}}{e_{(n)(m=0)}}\n",
    "\\\\\n",
    "\\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "&= \\frac{-1}{N} t_{(n)(m=1)} * \\frac {s_{(n)}}{e_{(n)(m=1)}}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Gradient $\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "p_{(n)(m=0)} &= \\frac {e_{(n)(m=0)} }{ s_{(n)} } \\\\\n",
    "p_{(n)(m=0)} &= f \\circ g_{(m=0)} = { s^{-1}_{(n)} } \\; * \\; { e_{(n)(m=0)} }\n",
    "\\rightarrow \\frac { \\partial  p_{(n)(m=0)} } { \\partial s^{-1}_{(n)} } = e_{(n)(m=0)}\n",
    "\\\\\n",
    "p_{(n)(m=1)} &= \\frac {e_{(n)(m=1)} }{ s_{(n)} } \\\\\n",
    "p_{(n)(m=1)} &= f \\circ g_{(m=1)} = { s^{-1}_{(n)} } \\; * \\; { e_{(n)(m=1)} }\n",
    "\\rightarrow \\frac { \\partial  p_{(n)(m=1)} } { \\partial s^{-1}_{(n)} } = e_{(n)(m=1)}\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "\\left[\n",
    "    \\frac { \\partial  p_{(n)(m=0)} } { \\partial s^{-1}_{(n)} } * \n",
    "    \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \n",
    "    +\n",
    "    \\frac { \\partial  p_{(n)(m=1)} } { \\partial s^{-1}_{(n)} } * \n",
    "    \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "\\right]\n",
    "\\\\\n",
    "&= \\sum\\limits^{M-1}_{m=0} \n",
    "    e_{(n)(m)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m)} } \n",
    "\\\\\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "    \\begin{bmatrix}\n",
    "    e_{(n)(m=0)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \\\\\n",
    "    + \\\\\n",
    "    e_{(n)(m=1)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "    \\end{bmatrix}\n",
    "\\\\\n",
    "&= -s_{(n)}(\\; t_{(n)(m=0)} + t_{(n)(m=1)} \\;) \\\\\n",
    "&= -s_{(n)}\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "\\left[\n",
    "    e_{(n)(m=0)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \n",
    "    + \n",
    "    e_{(n)(m=1)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "\\right]\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient $\\frac {\\partial L }{ \\partial { s_{(n)} } } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac {1} { s_{(n)} } &= s^{-1}_{(n)} \\rightarrow\n",
    "\\frac { \\partial { s^{-1}_{(n)} } } {\\partial s_{(n)}} = \\frac {-1}{s^{2}_{(n)}}\n",
    "\\\\\n",
    "\\frac {\\partial L }{ \\partial { s_{(n)} } } \n",
    "&=\n",
    "\\frac {-1}{s^{2}_{(n)}} * \n",
    "\\frac {\\partial L}{ \\partial s^{-1}_{(n)} } \\\\\n",
    "&= \\frac {1}{s_n}\n",
    "\\end{align*} \\\\\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient $\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } $\n",
    "$\n",
    "\\begin{align*}\n",
    "s_{(n)} &= \\sum\\limits ^{M-1}_{m=0} e_{(n)(m)} \\rightarrow \n",
    "\\frac { \\partial { s_{(n)} } } {\\partial e_{(n)(m=0)}} = 1\n",
    "\\\\\n",
    "p_{(n)(m=0)} &= \\frac {e_{(n)(m=0)} }{ s_{(n)} }\\rightarrow \n",
    "\\frac { \\partial { p_{(n)(m=0)} } } {\\partial e_{(n)(m=0)}} = \\frac {1}{s_{(n)}}\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } \n",
    "&= \\begin{bmatrix}  \n",
    "    \\frac { \\partial { s_{(n)} } } {\\partial e_{(n)(m=0)}} *  \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \\\\\n",
    "    + \\\\\n",
    "    \\frac { \\partial { p_{(n)(m=0)} } } {\\partial e_{(n)(m=0)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}  \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \\\\\n",
    "    + \\\\\n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } &= \n",
    "\\left[\n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "    + \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \n",
    "\\right]\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } &= \n",
    "\\begin{bmatrix}  \n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} }  \\\\\n",
    "    +  \\\\\n",
    "    \\frac {\\partial L }{ \\partial s_{(n)} } \n",
    "\\end{bmatrix} \\\\\n",
    "&= \\frac {-t_{(n)(m=0)}}{e_{(n)(m=0)} } + \\frac {1}{s_{n}}\n",
    "\\end{align*}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gardient $\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "e_{(n)(m)} &= exp(\\; a_{(n)(m)} \\; ) \\rightarrow \\frac { \\partial e_{(n)(m)} }{ \\partial a_{(n)(m)} } = e_{(n)(m)} \n",
    "\\\\\n",
    "e_{(n)(m=0)} &= exp(a_{(n)(m=0)}) \\rightarrow \\frac { \\partial e_{(n)(m=0)} }{ \\partial a_{(n)(m=0)} } = e_{(n)(m=0)} \n",
    "\\\\\n",
    "e_{(n)(m=1)} &= exp(a_{(n)(m=1)}) \\\\\n",
    "\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } \n",
    "&=   \\frac { \\partial e_{(n)(m=0)} }{ \\partial a_{(n)(m=0)} } * \n",
    "    \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \n",
    "= e_{(n)(m=0)} * \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } \n",
    "&= e_{(n)(m=0)} * \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \\\\\n",
    "&= -t_{(n)(m=0)} + \\frac { e_{(n)(m=0)} }{ s_{n} } \\\\\n",
    "&= p_{(n)(m=0)} -t_{(n)(m=0)} \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Log Loss\n",
    "\n",
    "A probability distribution $P(x)$ can be represented with its entropy $E(x) = \\sum\\limits_{x}  \\frac {p(x)}{log(p(x)} = - \\sum\\limits_{x} p(x) log(p(x))$. In the diagram, x: (0:dog, 1:cat, 2:fish, 3:bird) are labels and p(dog) is 0.5. When  a NN predicts an input x as a probability distribution $P(x)$, then the $E(x) = 1.75$. \n",
    "\n",
    "0. $p(dog)=\\frac {1}{2}$\n",
    "1. $p(cat)=\\frac {1}{4}$\n",
    "2. $p(fish)=\\frac {1}{8}$\n",
    "3. $p(bird)=\\frac {1}{8}$\n",
    "\n",
    "When the truth is that x is a dog, then the probability distribution of the truth $P(t)$ has the entropy $E(t) = 0$.\n",
    "\n",
    "0. $p(dog)=1$\n",
    "1. $p(cat)=0$\n",
    "2. $p(fish)=0$\n",
    "3. $p(bird)=0$\n",
    "\n",
    "The difference E(x) - E(t) = E(x) = 1.75 can be used as the distance or the error of the prediction from the truth. Need to understand further but  the actuall loss function is $E(x) = -tlog(p(x)) = -log(p(x))$ where p(x) is the probability from the softmax for the correct label.\n",
    "\n",
    "\n",
    "<img src=\"image/entropy.png\" align=\"left\" width=600/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.log() is ln based on the mathematical constant $e$ and its derivative $\\frac {\\partial log(x)}{\\partial x} = \\frac {1}{x}$.\n",
    "\n",
    "* [Logarithm](https://en.wikipedia.org/wiki/Logarithm)\n",
    "\n",
    "\n",
    "<img src=\"image/logarithm_plots.png\" align=\"left\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [ML Grossary - Loss Functions](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)\n",
    "\n",
    "<img src=\"image/cross_entropy_log_loss.png\" align=\"left\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cross_entropy_log_loss_input_combinations.xlsx](./common/cross_entropy_log_loss_input_combinations.xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cross_entropy_log_loss(\n",
      "        P: Union[np.ndarray, float],\n",
      "        T: Union[np.ndarray, int],\n",
      "        offset: float = OFFSET_FOR_LOG\n",
      ") -> np.ndarray:\n",
      "    \"\"\"Cross entropy log loss [ -t(n)(m) * log(p(n)(m)) ] for multi labels.\n",
      "    Assumption:\n",
      "        Label is integer 0 or 1 for an OHE label and any integer for an index label.\n",
      "\n",
      "    NOTE:\n",
      "        Handle only the label whose value is True. The reason not to use non-labels to\n",
      "        calculate the loss is TBD.\n",
      "\n",
      "    Args:\n",
      "        P: probabilities of shape (N,M) from soft-max layer where:\n",
      "            N is Batch size\n",
      "            M is Number of nodes\n",
      "        T: label either in OHE format of shape (N,M) or index format of shape (N,).\n",
      "           OHE: One Hot Encoding\n",
      "        offset: small number to avoid np.inf by log(0) by log(0+offset)\n",
      "    Returns:\n",
      "        J: Loss value of shape (N,), a loss value per batch.\n",
      "    \"\"\"\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # P must be float, T must be integer.\n",
      "    # --------------------------------------------------------------------------------\n",
      "    assert (isinstance(P, np.ndarray) and P.dtype == float) or isinstance(P, float), \\\n",
      "        \"Type of P must be float\"\n",
      "    assert (isinstance(T, np.ndarray) and T.dtype == int) or isinstance(T, int), \\\n",
      "        \"Type of T must be integer\"\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # P is scalar, then return -t * log(p).\n",
      "    # --------------------------------------------------------------------------------\n",
      "    if isinstance(P, float) or P.ndim == 0:\n",
      "        assert isinstance(T, int) or T.ndim == 0\n",
      "        return -T * np.log(P+offset)\n",
      "\n",
      "    # ================================================================================\n",
      "    # Hereafter, P and T are np arrays.\n",
      "    # Convert T in OHE format into index format with T = np.argmax(T). If P is in 1D,\n",
      "    # convert it into 2D so as to get the log loss with numpy tuple- like indices\n",
      "    # P[\n",
      "    #   rangee(N),\n",
      "    #   T\n",
      "    # ]\n",
      "    # ================================================================================\n",
      "    assert isinstance(P, np.ndarray)\n",
      "    T = np.array(T, dtype=int) if isinstance(T, int) else T\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # P is 1D array, then T dim should be in (1,2)\n",
      "    # Convert T.ndim==0 scalar index label into single element 1D index label T.\n",
      "    # Convert T.ndim==1 OHE labels into a 1D index labels T.\n",
      "    # T.reshape(-1) because np.argmax(ndim=1) results in ().\n",
      "    # --------------------------------------------------------------------------------\n",
      "    if P.ndim == 1:\n",
      "        assert T.ndim in {1, 2}, \\\n",
      "            \"For P.ndim=1, T.ndim is 0 or 1 but %s\" % T.ndim\n",
      "        P = P.reshape(1, -1)\n",
      "        T = T.reshape(-1) if T.ndim == 0 else np.argmax(T).reshape(-1)\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # P is 2D array, then\n",
      "    # Convert the OHE labels into index labels when T.ndim==2.\n",
      "    # Otherwise T should be index labels when T.ndim==1.\n",
      "    # --------------------------------------------------------------------------------\n",
      "    if P.ndim == T.ndim == 2:\n",
      "        T = T.argmax(axis=-1)\n",
      "\n",
      "    assert P.ndim == 2 and T.ndim == 1, \\\n",
      "        \"P.ndim==2 and T.ndim==1 are expected but P %s T %s\" % (P.shape, T.shape)\n",
      "\n",
      "    # ================================================================================\n",
      "    # Calculate Cross entropy log loss -t * log(p).\n",
      "    # Select an element P[n][t] at each row n which corresponds to the true label t.\n",
      "    # Use the Numpy tuple indexing. e.g. P[n=0][t=2] and P[n=3][t=4].\n",
      "    # P[\n",
      "    #   (0, 3),\n",
      "    #   (2, 4)     # The tuple sizes must be the same at all axes\n",
      "    # ]\n",
      "    #\n",
      "    # Tuple indexing selects only one element per row.\n",
      "    # Beware the numpy behavior difference between P[(n),(m)] and P[[n],[m]].\n",
      "    # https://stackoverflow.com/questions/66269684\n",
      "    # P[1,1]  and P[(0)(0)] results in a scalar value, HOWEVER, P[[1],[1]] in array.\n",
      "    #\n",
      "    # P shape can be (1,1), (1, M), (N, 1), (N, M), hence P[rows, cols] are:\n",
      "    # P (1,1) -> P[rows, cols] results in a 1D of  (1,).\n",
      "    # P (1,M) -> P[rows, cols] results in a 1D of  (1,)\n",
      "    # P (N,1) -> P[rows, cols] results in a 1D of  (N,)\n",
      "    # P (N,M) -> P[rows, cols] results in a 1D of  (N,)\n",
      "    #\n",
      "    # J shape matches with the P[rows, cols] shape.\n",
      "    # ================================================================================\n",
      "    N = batch_size = P.shape[0]\n",
      "    rows = np.arange(N)     # (N,)\n",
      "    cols = T                # Same shape (N,) with rows\n",
      "    assert rows.shape == cols.shape, \\\n",
      "        f\"np P indices need the same shape but rows {rows.shape} cols {cols.shape}.\"\n",
      "\n",
      "    _P = P[rows, cols]\n",
      "    Logger.debug(\"cross_entropy_log_loss(): N is [%s]\", N)\n",
      "    Logger.debug(\"cross_entropy_log_loss(): P.shape %s\", P.shape)\n",
      "    Logger.debug(\"cross_entropy_log_loss(): P[rows, cols].shape %s\", _P.shape)\n",
      "    Logger.debug(\"cross_entropy_log_loss(): P[rows, cols] is %s\" % _P)\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # Log loss per batch. Log(0+e) prevents the infinitive value log(0).\n",
      "    # NOTE:\n",
      "    #   Numerical gradient calculate f(x+/-h) with a small h e.g. 1e-5.\n",
      "    #   When x=0 and h >> e, f(0-h)=log(e-h) is nan as x in log(x) cannot be < 0.\n",
      "    # --------------------------------------------------------------------------------\n",
      "    assert np.all((_P + offset) > 0), \\\n",
      "        \"x for log(x) needs to be > 0 but %s.\" % (_P + offset)\n",
      "\n",
      "    J = -np.log(_P + offset)\n",
      "\n",
      "    assert not np.all(np.isnan(J)), \"log(x) caused nan for P \\n%s.\" % P\n",
      "    Logger.debug(\"J is [%s]\", J)\n",
      "    Logger.debug(\"J.shape %s\\n\", J.shape)\n",
      "\n",
      "    assert 0 < N == J.shape[0], \\\n",
      "        \"Loss J.shape is expected to be (%s,) but %s\" % (N, J.shape)\n",
      "    return J\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from common import (\n",
    "    cross_entropy_log_loss,\n",
    "    OFFSET_FOR_LOG\n",
    ")\n",
    "lines = inspect.getsource(cross_entropy_log_loss)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For label using One Hot Encoding (OHE)\n",
    "For instance, if multi labels are (0,1,2,3,4) and each label is OHE, then the label for 2 is (0,0,1,0,0).   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product of matrix rows\n",
    "\n",
    "There is no formal operation to calculate the dot products of the rows from two matrices, but to calculate the diagonal of the matlix multiplication that also calculate non-diagonals. To avoid calculating non-diagonals, use [einsum](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html).\n",
    "\n",
    "* [Name of matrix operation of ```[A[0] dot B[0], A[1] dot B[1] ]``` from 2x2 matrices A, B](https://math.stackexchange.com/questions/4010721/name-of-matrix-operation-of-a0-dot-b0-a1-dot-b1-from-2x2-matrices-a)\n",
    "\n",
    "<img src=\"image/dot_products_of_matrix_rows.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is \n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "b.T is \n",
      "[[ 0 -3]\n",
      " [-1 -4]\n",
      " [-2 -5]]\n",
      "\n",
      "c[\n",
      "    np.inner(a[0], b[0]),\n",
      "    np.inner(a[1], b[1]),    \n",
      "] is [-5, -50]\n",
      "\n",
      "\n",
      "np.einsum('ij,ji->i', a, b.T) is [ -5 -50]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(6).reshape(2,3)\n",
    "b = np.arange(0,-6,-1).reshape(2,3)\n",
    "c = [\n",
    "    np.inner(a[0], b[0]),\n",
    "    np.inner(a[1], b[1]),    \n",
    "]\n",
    "print(f\"a is \\n{a}\")\n",
    "print(f\"b.T is \\n{b.T}\\n\")\n",
    "fmt=f\"\"\"c[\n",
    "    np.inner(a[0], b[0]),\n",
    "    np.inner(a[1], b[1]),    \n",
    "] is {c}\\n\n",
    "\"\"\"\n",
    "print(fmt)\n",
    "\n",
    "# Use einsum\n",
    "e = np.einsum('ij,ji->i', a, b.T)\n",
    "fmt=\"np.einsum('ij,ji->i', a, b.T)\"\n",
    "print(f\"{fmt} is {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foward path (OHE)\n",
    "$\n",
    "\\text{ for one hot encoding labels }\n",
    "\\\\\n",
    "\\begin{align*}\n",
    "\\overset{ (N,) }{ J } &= - \\sum\\limits^{M-1}_{m=0} \n",
    "    \\left[ \\; \\;  \n",
    "        t_{(n)(m)} \\;  * \\;  np.log(p_{(n)(m)}) \\;\\;  \n",
    "    \\right]\n",
    "\\\\\n",
    "\\overset{ () }{ j_{(n)} } &= \\overset{ (M,) }{ T_{(n)} } \\cdot \\overset{ (M,) }{ P_{(n)} } \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient dL/dP\n",
    "\n",
    "Impact on L by the $dP$ from the softmax layer for one hot encoding labels.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac { \\partial L }{ \\partial P} }\n",
    "&= \\overset { (N,) }{ \\frac { \\partial L }{ \\partial J} } * \n",
    "\\overset { (N,M) }{ \n",
    "\\left(\n",
    " - \\frac { \\partial T } { \\partial P }\n",
    " \\right) \n",
    "} \n",
    "= - \\frac {1}{N }  \\frac { \\partial T } { \\partial P }\n",
    "\\\\\n",
    "\\frac {\\partial L }{\\partial p_{(n)(m=0)}} \n",
    "&= \\frac {\\partial L}{\\partial j_{(n)}} * \\frac {\\partial j_{(n)}} {\\partial p_{(n)(m=0)}} \n",
    "= \\frac {1}{N} \\frac { -t_{(n)(m=0)}}{ p_{(n)(m=0)} } \n",
    "=  \\frac {1}{N} \\left(\n",
    " -t_{(n)(m=0)} * \\frac { s_{(n)} }{ e_{(n)(m=0)} }\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For label using indexing \n",
    "For instance, if the multi labels are (0,1,2,3,4) then the index is 2 for the label 2. If the labels are (2,4,6,8,9), then the index is 3 for the label 8.  \n",
    "\n",
    "Use LP to select the probabilities from P for the corresponding labels. For instance, if the label is 2 (hence the index is 2) for X(n=0), and 4 for X(n=3), then the numpy tuple indexing selects ```P[n=0][m=2]``` and ```P[n=3][m=4] ```.\n",
    "\n",
    "```\n",
    "P[\n",
    "   (0, 3),\n",
    "   (2, 4)\n",
    "]\n",
    "```\n",
    "\n",
    "$\n",
    "\\text{ for index labels e.g. (5, 2, 0, 9, ...)}\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,) }{ J } = - np.sum(\\; np.log(LP), \\; axis = -1 \\;) \\\\\n",
    "LP = label\\_probability = P \\left[ \\\\\n",
    "\\quad ( \\; 0, \\; \\dots, \\;  {N-1}) , \\\\\n",
    "\\quad ( \\; t_{(n=0)} \\; , \\dots , \\; t_{(n=N-1)}) \\\\\n",
    "\\right]\n",
    "\\\\\n",
    "\\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward path\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ () }{ L } = \\frac {1}{N} \\sum\\limits^{N-1}_{n=0} \\overset{ () }{ j_{{(n)}} }\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gardient dL/dJ\n",
    "\n",
    "Impact on L by $dJ$ from the cross entropy log loss layer.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,) }{ \\frac {\\partial L}{\\partial J} }  &= \\frac {1}{N} \\overset{(N,)}{ones}\n",
    "\\\\\n",
    "\\frac {\\partial L}{\\partial j_{(n)} } &= \\frac {1}{N} \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ = np.ones(N) / N\n",
    "dJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [two_layer_net.ipynb defines the lambda with parameter W which is redundant #254](https://github.com/cs231n/cs231n.github.io/issues/254)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```numerical_jacobian(f, X)``` returns ```J``` of the same shape with ```X```. It takes each element in ```x``` in ```X```, and calculate ```(f(x+h) and f(x-h))/2h```. For ```cross_entropy_logg_loss()```, the expected numerical gradient is ```gn = (-np.log(p+h+e) + -np.log(p-h+e)) / (2*h)``` for each element ```p``` in ```P```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def numerical_jacobian(\n",
      "        f: Callable[[np.ndarray], np.ndarray],\n",
      "        X: Union[np.ndarray, float],\n",
      "        delta: float = OFFSET_FOR_DELTA\n",
      ") -> np.ndarray:\n",
      "    \"\"\"Calculate Jacobian matrix J numerically with (f(X+h) - f(X-h)) / 2h\n",
      "    Jacobian matrix element Jpq = df/dXpq, the impact on J by the\n",
      "    small difference to Xpq where p is row index and q is col index of J.\n",
      "\n",
      "    Args:\n",
      "        f: Y=f(X) where Y is a scalar or shape() array.\n",
      "        X: input of shame (N, M), or (N,) or ()\n",
      "        delta: small delta value to calculate the f value for X+/-h\n",
      "    Returns:\n",
      "        J: Jacobian matrix that has the same shape of X.\n",
      "    \"\"\"\n",
      "    X = np.array(X, dtype=float) if isinstance(X, (float, int)) else X\n",
      "    J = np.zeros_like(X, dtype=float)\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # (x+h) or (x-h) may cause an invalid value area for the function f.\n",
      "    # e.g log loss tries to offset x=0 by adding a small value e as log(0+e).\n",
      "    # However because e=1e-7 << h=1e-5, f(x-h) causes nan due to log(x < 0)\n",
      "    # as x needs to be > 0 for log.\n",
      "    #\n",
      "    # X and tmp must be float, or it will be int causing float calculation fail.\n",
      "    # e.g. f(1-h) = log(1-h) causes log(0) instead of log(1-h).\n",
      "    # --------------------------------------------------------------------------------\n",
      "    assert (X.dtype == float), \"X must be float type\"\n",
      "    assert delta > 0.0\n",
      "\n",
      "    it = np.nditer(X, flags=['multi_index'], op_flags=['readwrite'])\n",
      "    while not it.finished:\n",
      "        idx = it.multi_index\n",
      "        tmp: float = X[idx]\n",
      "\n",
      "        X[idx] = tmp + delta\n",
      "        fx1: float = f(X)  # f(x+h)\n",
      "        assert not np.isnan(fx1), \\\n",
      "            \"numerical delta f(x+h) caused nan for f %s for X %s\" \\\n",
      "            % (f, (tmp + delta))\n",
      "\n",
      "        X[idx] = tmp - delta\n",
      "        fx2: float = f(X)  # f(x-h)\n",
      "        assert not np.isnan(fx2), \\\n",
      "            \"numerical delta f(x-h) caused nan for f %s for X %s\" \\\n",
      "            % (f, (tmp - delta))\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Set the gradient element scalar value or shape()\n",
      "        # --------------------------------------------------------------------------------\n",
      "        g = (fx1 - fx2) / (2 * delta)\n",
      "        assert g.size == 1, \"The f function needs to return scalar or shape ()\"\n",
      "        J[idx] = g\n",
      "\n",
      "        X[idx] = tmp\n",
      "        it.iternext()\n",
      "\n",
      "    return J\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from common import (\n",
    "    numerical_jacobian,\n",
    "    OFFSET_FOR_DELTA\n",
    ")\n",
    "lines = inspect.getsource(numerical_jacobian)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected numerical gradient=-1.1591403356603358\n",
      "Actual numerical gradient=-1.1591403356603358\n",
      "Expected analytical gradient -T/P=-1.1591403356251433\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Example gradients for the cross entropy log loss -t*log(p).\n",
    "# --------------------------------------------------------------------------------\n",
    "p =0.86270721\n",
    "h = OFFSET_FOR_DELTA\n",
    "e = OFFSET_FOR_LOG\n",
    "\n",
    "expected_gn = (-np.log(p+h+e) + np.log(p-h+e)) / (2*h)\n",
    "actual_gn = (cross_entropy_log_loss(p+h, 1) - cross_entropy_log_loss(p-h, 1)) / (2*h)\n",
    "print(f\"Expected numerical gradient={expected_gn}\")\n",
    "print(f\"Actual numerical gradient={actual_gn}\")\n",
    "print(f\"Expected analytical gradient -T/P={-1 / (p+e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
