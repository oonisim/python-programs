{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network \n",
    "Simple one layer neural network classifier. Mathjax formula not fully supported in github, hence the formulas get corrupted.\n",
    "\n",
    "<img src=\"image/nn_diagram.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/nn_functions.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python path\n",
    "Python path setup to avoid the relative imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1    # Batch size\n",
    "D = 3    # Number of features in the input data\n",
    "M = 2    # Number of nodes in a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confiurations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.classifications import (\n",
    "    linear_separable\n",
    ")\n",
    "X, T, V = linear_separable(d=2, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa03ebc4520>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[T==0, 1], X[T==0, 2], c='red')\n",
    "plt.scatter(X[T==1, 1], X[T==1, 2], c='green')\n",
    "\n",
    "# Hyperplace (X-b)V = 0 -> x1V1 + x2V2 - bV2 = 0\n",
    "x = np.linspace(-3,3,100)\n",
    "y = -(V[1] / V[2]) * x - (V[0] / V[2])\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T\n",
    "Labels for data X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For One Hot Encoding labels\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ T_{_{OHE}} } &= ( \n",
    "    \\overset{ (M,) }{ T_{(n=0)} }, \\; \\dots \\;, \\overset{ (M,) }{ T_{(n=N-1)} } \n",
    ") \n",
    "\\\\\n",
    "\\overset{ (N,M) }{ T_{_{OHE}} } = ( \n",
    "    \\overset{ (M,) }{ T_{(n=0)} }, \\dots , \\overset{ (M,) }{ T_{(n=N-1)} } \n",
    ") \n",
    "\\\\\n",
    "\\overset{ (M,) }{ T_{ _{OHE} (n)} } &= ( \\overset{ () }{ t_{(n)(m=0)} }, \\; \\dots \\;, \\overset{ () }{ t_{(n)(m=M-1)} })\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "### For index labels\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,) }{ T_{_{IDX}} } &= (\\overset{ () }{ t_{(n=0)} }, \\; \\dots \\;, \\overset{ () }{ t_{(n=N-1)} }) \\qquad \\text {for index labels }\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W\n",
    "Weight parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.weights import (\n",
    "    xavier,\n",
    "    he,\n",
    "    uniform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matmul layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer.matmul import Matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ Y } \n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "{ Y_{(n=0)} } \\\\\n",
    "\\vdots \\\\\n",
    "{ Y_{(n)} } \\\\\n",
    "\\vdots \\\\\n",
    "{ Y_{(n=N-1)} }\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\overset{ (N,D) }{ X } \\; @ \\; \\overset{ (D,M) }{ W^T }\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (M,) }{ Y_{(n)} } &= (y_{(n)(m=0)}, \\; \\dots, \\; y_{(n)(m)},  \\; \\dots, \\; y_{(n)(m=M-1)})\n",
    "\\\\\n",
    "\\overset{ () }{ y_{(n)(m)} }\n",
    "&= \\overset{ (D,) }{ X_{(n)} } \\cdot \\overset{ (D,) }{ W_{(m)}^T }\n",
    "= \\sum\\limits ^{D}_{d=0}  \\overset{ () }{ x_{(n)(d)} } * \\overset{ () }{ w_{(m)(d)} }\n",
    "\\\\\n",
    "_{(0 \\le d \\le D, \\; 0 \\le n \\lt N, \\; 0 \\le m \\lt M)}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dX\n",
    "\n",
    "Impact on L by $dX$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,D) }{ \\frac {\\partial L }{ \\partial X } }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial Y} }\n",
    "@ \\overset { (M,D) }{ W } \n",
    "\\end{align*}\n",
    "$\n",
    "<img src=\"image/nn_back_propagation_dL_dX.png\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dW.T\n",
    "Impact on L by $dW^T$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{ \\partial W^T } }\n",
    "= \\overset { (D,N) }{ X^T } \n",
    "@ \n",
    "\\overset { (N,M) }{ \\frac {\\partial L}{\\partial Y} }\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "<img src=\"image/nn_back_propagation_dL_dWT.png\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ A } &= \n",
    "activation \\left( \n",
    "    \\overset{ (N,M) }{ Y }  = \n",
    "    \\begin{bmatrix}\n",
    "    { Y_{(n=0)} } \\\\\n",
    "    \\vdots \\\\\n",
    "    { Y_{(n)} } \\\\\n",
    "    \\vdots \\\\\n",
    "    { Y_{(n=N-1)} }\n",
    "    \\end{bmatrix}\n",
    "\\right)\n",
    "\\\\\n",
    "\\overset{ (M,) }{ A_{(n)} } \n",
    "&= activation \\left( \\overset{ (M,) }{ Y_{(n) }} \\right)  \\\\\n",
    "&= (a_{(n)(m=0)}, \\; \\dots, \\; a_{(n)(m)},  \\; \\dots, \\; a_{(n)(m=M-1)})\n",
    "\\\\\n",
    "\\overset{ () }{ a_{(n)(m)} } &= activation \\left( \\overset{ () }{ y_{(n)(m)} } \\right)\n",
    "\\quad _{(0 \\le n \\lt N, \\; 0 \\le m \\lt M)}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dY\n",
    "\n",
    "Impact on L by dY from the matmul layer.\n",
    "\n",
    "$\n",
    "\\begin {align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{ \\partial Y } }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial A} } \n",
    "* \n",
    "\\overset { (N,M) }{ \\frac {\\partial A}{\\partial Y} }\n",
    "\\end {align*}\n",
    "$\n",
    "\n",
    "### For sigmoid activation\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset {(N,M)}{\\frac { \\partial L }{ \\partial Y} }\n",
    "&= \\frac { \\partial A }{ \\partial Y} * A * (1 - A)\n",
    "\\\\\n",
    "\\frac { \\partial y_{(n)(m)} } { \\partial a_{(n)(m)} }\n",
    "&= a_{(n)(m)} * (1 - a_{(n)(m)} )  \\\\ \n",
    "y_{(n)(m)} = sigmoid(a_{(n)(m)} )&=  \\frac {1}{ 1 + exp(y_{(n)(m)})}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "### For ReLU activation\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac { \\partial a_{(n)(m)} }{ \\partial y_{(n)(m)} }\n",
    "&= 1 \\quad y_{(n)(m)}  \\gt 0 \\\\\n",
    "&= 0 \\quad y_{(n)(m)}  \\le 0 \\\\\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax layer\n",
    "$C_n$ is to prevent the overflow at $np.exp()$.\n",
    "\n",
    "<img src=\"image/softmax.png\" align=\"left\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax and Cross Entropy Log Loss are combined as the gradient results in a simple form $P - T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def softmax(X: Union[np.ndarray, float]) -> Union[np.ndarray, float]:\n",
      "    \"\"\"Softmax P = exp(X) / sum(exp(X))\n",
      "    Args:\n",
      "        X: batch input data of shape (N,M).\n",
      "            N: Batch size\n",
      "            M: Number of nodes\n",
      "    Returns:\n",
      "        Probability P of shape (N,M)\n",
      "    \"\"\"\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # exp(x-c) to prevent the infinite exp(x) for a large value x, with c = max(x).\n",
      "    # keepdims=True to be able to broadcast.\n",
      "    # --------------------------------------------------------------------------------\n",
      "    C = np.max(X, axis=-1, keepdims=True)   # オーバーフロー対策\n",
      "    exp = np.exp(X - C)\n",
      "    return exp / np.sum(exp, axis=-1, keepdims=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from layer import SoftmaxWithLogLoss\n",
    "from common import softmax\n",
    "\n",
    "lines = inspect.getsource(softmax)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,1) }{ C } &= np.max\\left( \n",
    "    \\overset{ (N,M) }{ A }, \\; axis=-1,  \\; keepdim=True \n",
    "\\right) \\\\\n",
    "&=  \n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n=0  )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n    )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n=N-1)} } \\end{bmatrix}\n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\overset{ (N,M) }{ EXP } &= np.exp \\left( \\overset{ (N,M) }{ A } - \\overset{ (N,1) }{ C } \\right)\n",
    "= np.exp \\left(\n",
    "    \\begin{bmatrix}\n",
    "    { A_{(n=0)} } - { C_{(n=0)} }\\\\\n",
    "    \\vdots \\\\\n",
    "    { A_{(n)} }   - { C_{(n)} }\\\\\n",
    "    \\vdots \\\\\n",
    "    { A_{(n=N-1)} } - { C_{(n=N-1)} }\\\\\n",
    "    \\end{bmatrix}\n",
    "\\right) \n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "    e_{(n=0)(m=0)}   & \\dots      & e_{(n=0)(m=M-1)}   \\\\  \n",
    "    \\vdots           & e_{(n)(m)} & \\vdots             \\\\\n",
    "    e_{(n=N-1)(m=0)} & \\dots      & e_{(n=N-1)(m=M-1)} \n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,1) }{ S } &= \\overset{ (N,1) }{ sum(EXP) } = np.sum \\left( \n",
    "    \\overset{ (N,M) }{ EXP }, \\; axis=-1,  \\; keepdim=True \n",
    "\\right)\n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n=0  )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n    )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n=N-1)} } \\end{bmatrix}\n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\overset{ () }{ s_{(n)} } &= \\sum\\limits ^{M-1}_{m=0} np.exp(\\; a_{(n)(m)} - c_{(n)} \\; )\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,M) }{ P } &= \\overset{ (N,M) }{ EXP }  \\;\\; / \\;\\; \\overset{ (N,1) }{ sum(EXP) } \n",
    "\\\\\n",
    "\\overset{ (N,) }{ P_{(n)} } &= (p_{(n)(m=0)}, \\; \\dots, \\; p_{(n)(m)} , \\; \\dots, \\; p_{(n)(m=M-1)})\n",
    "\\\\\n",
    "{ p_{(n)(m)} } \n",
    "&= \\frac {np.exp \\left( \n",
    "    { a_{(n)(m) } } - { c_{(n)} }) \\right) \n",
    "}\n",
    "{  \n",
    "np.sum \\left( \n",
    "    np.exp \\left( \n",
    "        a_{(n)(m) } - c_{(n)}\n",
    "    \\right)\n",
    "\\right) \n",
    "}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dA\n",
    "\n",
    "Impact on L by dA from the activation layer.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{\\partial A} }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial P} }\n",
    "* \n",
    "\\overset { (N,M) }{ \\frac {\\partial P }{\\partial A} } \n",
    "= \n",
    "\\frac {1}{N} (P - T)\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "$\n",
    "Jacobian \\; : \\; f \\circ g \\rightarrow Jf \\circ Jg\n",
    "\\\\\n",
    "\\begin{align*}\n",
    "\\\\\n",
    "L &= f(\\; p_{(n)(m=0)} \\;) = f( \\; g(\\;  a_{(n)(m=0)} \\; ) \\; ) \\quad : p = g(a) = softmax(a)\n",
    "\\\\\n",
    "\\frac {\\partial L} { \\partial a_{(n)(m=0)} }\n",
    "&= Jf(p) \\circ Jg(a) \n",
    "=  \\frac {\\partial L} { \\partial p_{(n)(m=0)} } * \\frac {\\partial  p_{(n)(m=0)}} { \\partial a_{(n)(m=0)} }\n",
    "\\\\\n",
    "&= \\frac {1}{N} \\left(\n",
    " p_{(n)(m=0)} -t_{(n)(m=0)}\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient from cross entropy log loss\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac { \\partial L }{ \\partial  p_{(n)(m=0)} }\n",
    "&= \\frac{-1}{N} t_{(n)(m=0)} * \\frac {s_{(n)}}{e_{(n)(m=0)}}\n",
    "\\\\\n",
    "\\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "&= \\frac{-1}{N} t_{(n)(m=1)} * \\frac {s_{(n)}}{e_{(n)(m=1)}}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Gradient $\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "p_{(n)(m=0)} &= \\frac {e_{(n)(m=0)} }{ s_{(n)} } \\\\\n",
    "p_{(n)(m=0)} &= f \\circ g_{(m=0)} = { s^{-1}_{(n)} } \\; * \\; { e_{(n)(m=0)} }\n",
    "\\rightarrow \\frac { \\partial  p_{(n)(m=0)} } { \\partial s^{-1}_{(n)} } = e_{(n)(m=0)}\n",
    "\\\\\n",
    "p_{(n)(m=1)} &= \\frac {e_{(n)(m=1)} }{ s_{(n)} } \\\\\n",
    "p_{(n)(m=1)} &= f \\circ g_{(m=1)} = { s^{-1}_{(n)} } \\; * \\; { e_{(n)(m=1)} }\n",
    "\\rightarrow \\frac { \\partial  p_{(n)(m=1)} } { \\partial s^{-1}_{(n)} } = e_{(n)(m=1)}\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "\\left[\n",
    "    \\frac { \\partial  p_{(n)(m=0)} } { \\partial s^{-1}_{(n)} } * \n",
    "    \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \n",
    "    +\n",
    "    \\frac { \\partial  p_{(n)(m=1)} } { \\partial s^{-1}_{(n)} } * \n",
    "    \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "\\right]\n",
    "\\\\\n",
    "&= \\sum\\limits^{M-1}_{m=0} \n",
    "    e_{(n)(m)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m)} } \n",
    "\\\\\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "    \\begin{bmatrix}\n",
    "    e_{(n)(m=0)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \\\\\n",
    "    + \\\\\n",
    "    e_{(n)(m=1)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "    \\end{bmatrix}\n",
    "\\\\\n",
    "&= -s_{(n)}(\\; t_{(n)(m=0)} + t_{(n)(m=1)} \\;) \\\\\n",
    "&= -s_{(n)}\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "\\left[\n",
    "    e_{(n)(m=0)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \n",
    "    + \n",
    "    e_{(n)(m=1)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "\\right]\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient $\\frac {\\partial L }{ \\partial { s_{(n)} } } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac {1} { s_{(n)} } &= s^{-1}_{(n)} \\rightarrow\n",
    "\\frac { \\partial { s^{-1}_{(n)} } } {\\partial s_{(n)}} = \\frac {-1}{s^{2}_{(n)}}\n",
    "\\\\\n",
    "\\frac {\\partial L }{ \\partial { s_{(n)} } } \n",
    "&=\n",
    "\\frac {-1}{s^{2}_{(n)}} * \n",
    "\\frac {\\partial L}{ \\partial s^{-1}_{(n)} } \\\\\n",
    "&= \\frac {1}{s_n}\n",
    "\\end{align*} \\\\\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient $\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } $\n",
    "$\n",
    "\\begin{align*}\n",
    "s_{(n)} &= \\sum\\limits ^{M-1}_{m=0} e_{(n)(m)} \\rightarrow \n",
    "\\frac { \\partial { s_{(n)} } } {\\partial e_{(n)(m=0)}} = 1\n",
    "\\\\\n",
    "p_{(n)(m=0)} &= \\frac {e_{(n)(m=0)} }{ s_{(n)} }\\rightarrow \n",
    "\\frac { \\partial { p_{(n)(m=0)} } } {\\partial e_{(n)(m=0)}} = \\frac {1}{s_{(n)}}\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } \n",
    "&= \\begin{bmatrix}  \n",
    "    \\frac { \\partial { s_{(n)} } } {\\partial e_{(n)(m=0)}} *  \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \\\\\n",
    "    + \\\\\n",
    "    \\frac { \\partial { p_{(n)(m=0)} } } {\\partial e_{(n)(m=0)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}  \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \\\\\n",
    "    + \\\\\n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } &= \n",
    "\\left[\n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "    + \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \n",
    "\\right]\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } &= \n",
    "\\begin{bmatrix}  \n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} }  \\\\\n",
    "    +  \\\\\n",
    "    \\frac {\\partial L }{ \\partial s_{(n)} } \n",
    "\\end{bmatrix} \\\\\n",
    "&= \\frac {-t_{(n)(m=0)}}{e_{(n)(m=0)} } + \\frac {1}{s_{n}}\n",
    "\\end{align*}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gardient $\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "e_{(n)(m)} &= exp(\\; a_{(n)(m)} \\; ) \\rightarrow \\frac { \\partial e_{(n)(m)} }{ \\partial a_{(n)(m)} } = e_{(n)(m)} \n",
    "\\\\\n",
    "e_{(n)(m=0)} &= exp(a_{(n)(m=0)}) \\rightarrow \\frac { \\partial e_{(n)(m=0)} }{ \\partial a_{(n)(m=0)} } = e_{(n)(m=0)} \n",
    "\\\\\n",
    "e_{(n)(m=1)} &= exp(a_{(n)(m=1)}) \\\\\n",
    "\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } \n",
    "&=   \\frac { \\partial e_{(n)(m=0)} }{ \\partial a_{(n)(m=0)} } * \n",
    "    \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \n",
    "= e_{(n)(m=0)} * \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } \n",
    "&= e_{(n)(m=0)} * \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \\\\\n",
    "&= -t_{(n)(m=0)} + \\frac { e_{(n)(m=0)} }{ s_{n} } \\\\\n",
    "&= p_{(n)(m=0)} -t_{(n)(m=0)} \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Log Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cross_entropy_log_loss(\n",
      "        P: Union[np.ndarray, float],\n",
      "        T: Union[np.ndarray, int],\n",
      "        e: float = 1e-7\n",
      ") -> np.ndarray:\n",
      "    \"\"\"Cross entropy log loss [ -t(n)(m) * log(p(n)(m)) ] for multi labels.\n",
      "    Assumption:\n",
      "        Label is integer 0 or 1 for an OHE label and any integer for an index label.\n",
      "\n",
      "    NOTE:\n",
      "        Handle only the label whose value is True. The reason not to use non-labels to\n",
      "        calculate the loss is TBD.\n",
      "\n",
      "    Args:\n",
      "        P: probabilities of shape (N,M) from soft-max layer where:\n",
      "            N is Batch size\n",
      "            M is Number of nodes\n",
      "        T: label either in OHE format of shape (N,M) or index format of shape (N,).\n",
      "           OHE: One Hot Encoding\n",
      "        e: small number to avoid np.inf by log(0) by log(0+e)\n",
      "    Returns:\n",
      "        J: Loss value of shape (N,), a loss value per batch.\n",
      "    \"\"\"\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # Convert scalar and 1D array into (N,M) shape to to run P[rows, cols]\n",
      "    # --------------------------------------------------------------------------------\n",
      "    P = np.array(P) if isinstance(P, float) else P\n",
      "    T = np.array(T) if isinstance(T, (float, int)) else T\n",
      "    if P.ndim <= 1:\n",
      "        T = T.reshape(1, T.size)\n",
      "        P = P.reshape(1, P.size)\n",
      "\n",
      "    # Label is integer\n",
      "    T = T.astype(int)\n",
      "    assert T.shape[0] == P.shape[0], \\\n",
      "        f\"Batch size of T {T.shape[0]} and P {P.shape[0]} should be the same.\"\n",
      "\n",
      "    N = batch_size = P.shape[0]\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # Convert OHE format into index label format of shape (N,).\n",
      "    # T in OHE format has the shape (N,M) with P, hence same size N*M.\n",
      "    # --------------------------------------------------------------------------------\n",
      "    if T.size == P.size:\n",
      "        T = T.argmax(axis=1)\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # Calculate Cross entropy log loss -t * log(p).\n",
      "    # Select an element P[n][t] at each row n which corresponds to the true label t.\n",
      "    # Use the Numpy tuple indexing. e.g. P[n=0][t=2] and P[n=3][t=4].\n",
      "    # P[\n",
      "    #   (0, 3),\n",
      "    #   (2, 4)     # The tuple sizes must be the same at all axes\n",
      "    # ]\n",
      "    #\n",
      "    # Tuple indexing selects only one element per row.\n",
      "    # Beware the numpy behavior difference between P[(n),(m)] and P[[n],[m]].\n",
      "    # https://stackoverflow.com/questions/66269684\n",
      "    # P[1,1]  and P[(0)(0)] results in a scalar value, HOWEVER, P[[1],[1]] in array.\n",
      "    #\n",
      "    # P shape can be (1,1), (1, M), (N, 1), (N, M), hence P[rows, cols] are:\n",
      "    # P (1,1) -> P[rows, cols] results in a 1D of  (1,).\n",
      "    # P (1,M) -> P[rows, cols] results in a 1D of  (1,)\n",
      "    # P (N,1) -> P[rows, cols] results in a 1D of  (N,)\n",
      "    # P (N,M) -> P[rows, cols] results in a 1D of  (N,)\n",
      "    #\n",
      "    # J shape matches with the P[rows, cols] shape.\n",
      "    # --------------------------------------------------------------------------------\n",
      "    rows = np.arange(N)     # 1D tuple index for rows\n",
      "    cols = T                # 1D tuple index for columns\n",
      "    assert rows.ndim == cols.ndim == 1 and rows.size == cols.size, \\\n",
      "        f\"np tuple indices size need to be same but rows {rows.size} cols {cols.size}.\"\n",
      "\n",
      "    # Log loss per batch. Log( +e) prevents the infinitive value log(0).\n",
      "    # Do NOT sum as it results in a total loss for a batch.\n",
      "    J = -np.log(P[rows, cols] + e)\n",
      "\n",
      "    Logger.debug(\"P.shape %s\", P.shape)\n",
      "    Logger.debug(\"P[rows, cols].shape %s\", P[rows, cols].shape)\n",
      "    Logger.debug(\"N is [%s]\", N)\n",
      "    Logger.debug(\"J is [%s]\", J)\n",
      "    Logger.debug(\"J.shape %s\\n\", J.shape)\n",
      "\n",
      "    assert 0 < N == J.shape[0], f\"Loss J.shape is expected to be ({N},) but {J.shape}\"\n",
      "    return J\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from common import cross_entropy_log_loss\n",
    "lines = inspect.getsource(cross_entropy_log_loss)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For label using One Hot Encoding (OHE)\n",
    "For instance, if multi labels are (0,1,2,3,4) and each label is OHE, then the label for 2 is (0,0,1,0,0).   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product of matrix rows\n",
    "\n",
    "There is no formal operation to calculate the dot products of the rows from two matrices, but to calculate the diagonal of the matlix multiplication that also calculate non-diagonals. To avoid calculating non-diagonals, use [einsum](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html).\n",
    "\n",
    "* [Name of matrix operation of ```[A[0] dot B[0], A[1] dot B[1] ]``` from 2x2 matrices A, B](https://math.stackexchange.com/questions/4010721/name-of-matrix-operation-of-a0-dot-b0-a1-dot-b1-from-2x2-matrices-a)\n",
    "\n",
    "<img src=\"image/dot_products_of_matrix_rows.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is \n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "b.T is \n",
      "[[ 0 -3]\n",
      " [-1 -4]\n",
      " [-2 -5]]\n",
      "\n",
      "c[\n",
      "    np.inner(a[0], b[0]),\n",
      "    np.inner(a[1], b[1]),    \n",
      "] is [-5, -50]\n",
      "\n",
      "\n",
      "np.einsum('ij,ji->i', a, b.T) is [ -5 -50]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(6).reshape(2,3)\n",
    "b = np.arange(0,-6,-1).reshape(2,3)\n",
    "c = [\n",
    "    np.inner(a[0], b[0]),\n",
    "    np.inner(a[1], b[1]),    \n",
    "]\n",
    "print(f\"a is \\n{a}\")\n",
    "print(f\"b.T is \\n{b.T}\\n\")\n",
    "fmt=f\"\"\"c[\n",
    "    np.inner(a[0], b[0]),\n",
    "    np.inner(a[1], b[1]),    \n",
    "] is {c}\\n\n",
    "\"\"\"\n",
    "print(fmt)\n",
    "\n",
    "# Use einsum\n",
    "e = np.einsum('ij,ji->i', a, b.T)\n",
    "fmt=\"np.einsum('ij,ji->i', a, b.T)\"\n",
    "print(f\"{fmt} is {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foward path (OHE)\n",
    "$\n",
    "\\text{ for one hot encoding labels }\n",
    "\\\\\n",
    "\\begin{align*}\n",
    "\\overset{ (N,) }{ J } &= - \\sum\\limits^{M-1}_{m=0} \n",
    "    \\left[ \\; \\;  \n",
    "        t_{(n)(m)} \\;  * \\;  np.log(p_{(n)(m)}) \\;\\;  \n",
    "    \\right]\n",
    "\\\\\n",
    "\\overset{ () }{ j_{(n)} } &= \\overset{ (M,) }{ T_{(n)} } \\cdot \\overset{ (M,) }{ P_{(n)} } \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient dL/dP\n",
    "\n",
    "Impact on L by the $dP$ from the softmax layer for one hot encoding labels.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac { \\partial L }{ \\partial P} }\n",
    "&= \\overset { (N,) }{ \\frac { \\partial L }{ \\partial J} } * \n",
    "\\overset { (N,M) }{ \n",
    "\\left(\n",
    " - \\frac { \\partial T } { \\partial P }\n",
    " \\right) \n",
    "} \n",
    "= - \\frac {1}{N }  \\frac { \\partial T } { \\partial P }\n",
    "\\\\\n",
    "\\frac {\\partial L }{\\partial p_{(n)(m=0)}} \n",
    "&= \\frac {\\partial L}{\\partial j_{(n)}} * \\frac {\\partial j_{(n)}} {\\partial p_{(n)(m=0)}} \n",
    "= \\frac {1}{N} \\frac { -t_{(n)(m=0)}}{ p_{(n)(m=0)} } \n",
    "=  \\frac {1}{N} \\left(\n",
    " -t_{(n)(m=0)} * \\frac { s_{(n)} }{ e_{(n)(m=0)} }\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For label using indexing \n",
    "For instance, if the multi labels are (0,1,2,3,4) then the index is 2 for the label 2. If the labels are (2,4,6,8,9), then the index is 3 for the label 8.  \n",
    "\n",
    "Use LP to select the probabilities from P for the corresponding labels. For instance, if the label is 2 (hence the index is 2) for X(n=0), and 4 for X(n=3), then the numpy tuple indexing selects ```P[n=0][m=2]``` and ```P[n=3][m=4] ```.\n",
    "\n",
    "```\n",
    "P[\n",
    "   (0, 3),\n",
    "   (2, 4)\n",
    "]\n",
    "```\n",
    "\n",
    "$\n",
    "\\text{ for index labels e.g. (5, 2, 0, 9, ...)}\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,) }{ J } = - np.sum(\\; np.log(LP), \\; axis = -1 \\;) \\\\\n",
    "LP = label\\_probability = P \\left[ \\\\\n",
    "\\quad ( \\; 0, \\; \\dots, \\;  {N-1}) , \\\\\n",
    "\\quad ( \\; t_{(n=0)} \\; , \\dots , \\; t_{(n=N-1)}) \\\\\n",
    "\\right]\n",
    "\\\\\n",
    "\\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward path\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ () }{ L } = \\frac {1}{N} \\sum\\limits^{N-1}_{n=0} \\overset{ () }{ j_{{(n)}} }\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gardient dL/dJ\n",
    "\n",
    "Impact on L by $dJ$ from the cross entropy log loss layer.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,) }{ \\frac {\\partial L}{\\partial J} }  &= \\frac {1}{N} \\overset{(N,)}{ones}\n",
    "\\\\\n",
    "\\frac {\\partial L}{\\partial j_{(n)} } &= \\frac {1}{N} \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ = np.ones(N) / N\n",
    "dJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [two_layer_net.ipynb defines the lambda with parameter W which is redundant #254](https://github.com/cs231n/cs231n.github.io/issues/254)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def numerical_jacobian(\n",
      "        f: Callable[[np.ndarray], np.ndarray],\n",
      "        X: Union[np.ndarray, float],\n",
      "        h: float = 1e-5\n",
      ") -> np.ndarray:\n",
      "    \"\"\"Calculate Jacobian matrix J numerically with (f(X+h) - f(X-h)) / 2h\n",
      "    Jacobian matrix element Jpq = df/dXpq, the impact on J by the\n",
      "    small difference to Xpq where p is row index and q is col index of J.\n",
      "\n",
      "    Args:\n",
      "        f: Y=f(X) where Y is a scalar or shape() array.\n",
      "        X: input of shame (N, M), or (N,) or ()\n",
      "        h: small delta value to calculate the f value for X+/-h\n",
      "    Returns:\n",
      "        J: Jacobian matrix that has the same shape of X.\n",
      "    \"\"\"\n",
      "    assert h > 0.0\n",
      "    X = np.array(X) if isinstance(X, (float, int)) else X\n",
      "    J = np.zeros_like(X)\n",
      "\n",
      "    it = np.nditer(X, flags=['multi_index'], op_flags=['readwrite'])\n",
      "    while not it.finished:\n",
      "        idx = it.multi_index\n",
      "        tmp_val = X[idx]\n",
      "        X[idx] = tmp_val + h\n",
      "        fx1 = f(X)  # f(x+h)\n",
      "\n",
      "        X[idx] = tmp_val - h\n",
      "        fx2 = f(X)  # f(x-h)\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Set the gradient element scalar value or shape()\n",
      "        # --------------------------------------------------------------------------------\n",
      "        g = (fx1 - fx2) / (2 * h)\n",
      "        assert g.size == 1, \"The f function needs to return scalar or shape ()\"\n",
      "        J[idx] = g\n",
      "\n",
      "        X[idx] = tmp_val\n",
      "        it.iternext()\n",
      "\n",
      "    return J\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from common import numerical_jacobian\n",
    "lines = inspect.getsource(numerical_jacobian)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
