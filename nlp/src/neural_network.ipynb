{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* [CS231n: Convolutional Neural Networks for Visual Recognition 2017](http://cs231n.stanford.edu/2017/syllabus)\n",
    "    - [cs231n 2017 assignment #1 kNN, SVM, SoftMax, two-layer network](https://cs231n.github.io/assignments2017/assignment1/)\n",
    "    - [Training a Softmax Linear Classifier](https://cs231n.github.io/neural-networks-case-study)\n",
    "* [ゼロから作る Deep Learning](https://github.com/oreilly-japan/deep-learning-from-scratch)\n",
    "* [Mathematics for Machine Learning](https://mml-book.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network \n",
    "Simple one layer neural network classifier. Mathjax formula not fully supported in github, hence the formulas get corrupted.\n",
    "\n",
    "<img src=\"image/nn_diagram.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/nn_functions.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Optional,\n",
    "    Union,\n",
    "    List,\n",
    "    Dict,\n",
    "    Tuple\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python path\n",
    "Python path setup to avoid the relative imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Union,\n",
    "    List,\n",
    "    Callable\n",
    ")\n",
    "import inspect\n",
    "from functools import partial\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=80) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install line_profile memory_profiler\n",
    "%load_ext memory_profiler\n",
    "%load_ext line_profiler\n",
    "\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1    # Batch size\n",
    "D = 3    # Number of features in the input data\n",
    "M = 2    # Number of nodes in a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confiurations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X\n",
    "X is to have been standardized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T\n",
    "Labels for data X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For One Hot Encoding labels\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ T_{_{OHE}} } &= ( \n",
    "    \\overset{ (M,) }{ T_{(n=0)} }, \\; \\dots \\;, \\overset{ (M,) }{ T_{(n=N-1)} } \n",
    ") \n",
    "\\\\\n",
    "\\overset{ (N,M) }{ T_{_{OHE}} } = ( \n",
    "    \\overset{ (M,) }{ T_{(n=0)} }, \\dots , \\overset{ (M,) }{ T_{(n=N-1)} } \n",
    ") \n",
    "\\\\\n",
    "\\overset{ (M,) }{ T_{ _{OHE} (n)} } &= ( \\overset{ () }{ t_{(n)(m=0)} }, \\; \\dots \\;, \\overset{ () }{ t_{(n)(m=M-1)} })\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "### For index labels\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,) }{ T_{_{IDX}} } &= (\\overset{ () }{ t_{(n=0)} }, \\; \\dots \\;, \\overset{ () }{ t_{(n=N-1)} }) \\qquad \\text {for index labels }\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W\n",
    "Weight parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import (\n",
    "    xavier,\n",
    "    he,\n",
    "    uniform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization Layer\n",
    "Apply normalization or use batch normaliation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matmul layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer.matmul import Matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ Y } \n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "{ Y_{(n=0)} } \\\\\n",
    "\\vdots \\\\\n",
    "{ Y_{(n)} } \\\\\n",
    "\\vdots \\\\\n",
    "{ Y_{(n=N-1)} }\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\overset{ (N,D) }{ X } \\; @ \\; \\overset{ (D,M) }{ W^T }\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (M,) }{ Y_{(n)} } &= (y_{(n)(m=0)}, \\; \\dots, \\; y_{(n)(m)},  \\; \\dots, \\; y_{(n)(m=M-1)})\n",
    "\\\\\n",
    "\\overset{ () }{ y_{(n)(m)} }\n",
    "&= \\overset{ (D,) }{ X_{(n)} } \\cdot \\overset{ (D,) }{ W_{(m)}^T }\n",
    "= \\sum\\limits ^{D}_{d=0}  \\overset{ () }{ x_{(n)(d)} } * \\overset{ () }{ w_{(m)(d)} }\n",
    "\\\\\n",
    "_{(0 \\le d \\le D, \\; 0 \\le n \\lt N, \\; 0 \\le m \\lt M)}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dX\n",
    "\n",
    "Impact on L by $dX$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,D) }{ \\frac {\\partial L }{ \\partial X } }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial Y} }\n",
    "@ \\overset { (M,D) }{ W } \n",
    "\\end{align*}\n",
    "$\n",
    "<img src=\"image/nn_back_propagation_dL_dX.png\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dW.T\n",
    "Impact on L by $dW^T$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{ \\partial W^T } }\n",
    "= \\overset { (D,N) }{ X^T } \n",
    "@ \n",
    "\\overset { (N,M) }{ \\frac {\\partial L}{\\partial Y} }\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "<img src=\"image/nn_back_propagation_dL_dWT.png\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ A } &= \n",
    "activation \\left( \n",
    "    \\overset{ (N,M) }{ Y }  = \n",
    "    \\begin{bmatrix}\n",
    "    { Y_{(n=0)} } \\\\\n",
    "    \\vdots \\\\\n",
    "    { Y_{(n)} } \\\\\n",
    "    \\vdots \\\\\n",
    "    { Y_{(n=N-1)} }\n",
    "    \\end{bmatrix}\n",
    "\\right)\n",
    "\\\\\n",
    "\\overset{ (M,) }{ A_{(n)} } \n",
    "&= activation \\left( \\overset{ (M,) }{ Y_{(n) }} \\right)  \\\\\n",
    "&= (a_{(n)(m=0)}, \\; \\dots, \\; a_{(n)(m)},  \\; \\dots, \\; a_{(n)(m=M-1)})\n",
    "\\\\\n",
    "\\overset{ () }{ a_{(n)(m)} } &= activation \\left( \\overset{ () }{ y_{(n)(m)} } \\right)\n",
    "\\quad _{(0 \\le n \\lt N, \\; 0 \\le m \\lt M)}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dY\n",
    "\n",
    "Impact on L by dY from the matmul layer.\n",
    "\n",
    "$\n",
    "\\begin {align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{ \\partial Y } }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial A} } \n",
    "* \n",
    "\\overset { (N,M) }{ \\frac {\\partial A}{\\partial Y} }\n",
    "\\end {align*}\n",
    "$\n",
    "\n",
    "### For sigmoid activation\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset {(N,M)}{\\frac { \\partial L }{ \\partial Y} }\n",
    "&= \\frac { \\partial A }{ \\partial Y} * A * (1 - A)\n",
    "\\\\\n",
    "\\frac { \\partial y_{(n)(m)} } { \\partial a_{(n)(m)} }\n",
    "&= a_{(n)(m)} * (1 - a_{(n)(m)} )  \\\\ \n",
    "y_{(n)(m)} = sigmoid(a_{(n)(m)} )&=  \\frac {1}{ 1 + exp(y_{(n)(m)})}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "### For ReLU activation\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac { \\partial a_{(n)(m)} }{ \\partial y_{(n)(m)} }\n",
    "&= 1 \\quad y_{(n)(m)}  \\gt 0 \\\\\n",
    "&= 0 \\quad y_{(n)(m)}  \\le 0 \\\\\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax layer\n",
    "$C_n$ is to prevent the overflow at $np.exp()$.\n",
    "\n",
    "<img src=\"image/softmax.png\" align=\"left\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exp(x) can take all x values and produces a positive, which is required for log(y) that needs y > 0, hence fit-for-purpose to build a probability function.\n",
    "\n",
    "<img src=\"image/exp.gif\" align=\"left\" width=250/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax and Cross Entropy Log Loss are combined as the gradient results in a simple form $P - T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer import CrossEntropyLogLoss\n",
    "from common import softmax\n",
    "\n",
    "lines = inspect.getsource(softmax)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,1) }{ C } &= np.max\\left( \n",
    "    \\overset{ (N,M) }{ A }, \\; axis=-1,  \\; keepdim=True \n",
    "\\right) \\\\\n",
    "&=  \n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n=0  )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n    )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n=N-1)} } \\end{bmatrix}\n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\overset{ (N,M) }{ EXP } &= np.exp \\left( \\overset{ (N,M) }{ A } - \\overset{ (N,1) }{ C } \\right)\n",
    "= np.exp \\left(\n",
    "    \\begin{bmatrix}\n",
    "    { A_{(n=0)} } - { C_{(n=0)} }\\\\\n",
    "    \\vdots \\\\\n",
    "    { A_{(n)} }   - { C_{(n)} }\\\\\n",
    "    \\vdots \\\\\n",
    "    { A_{(n=N-1)} } - { C_{(n=N-1)} }\\\\\n",
    "    \\end{bmatrix}\n",
    "\\right) \n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "    e_{(n=0)(m=0)}   & \\dots      & e_{(n=0)(m=M-1)}   \\\\  \n",
    "    \\vdots           & e_{(n)(m)} & \\vdots             \\\\\n",
    "    e_{(n=N-1)(m=0)} & \\dots      & e_{(n=N-1)(m=M-1)} \n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,1) }{ S } &= \\overset{ (N,1) }{ sum(EXP) } = np.sum \\left( \n",
    "    \\overset{ (N,M) }{ EXP }, \\; axis=-1,  \\; keepdim=True \n",
    "\\right)\n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n=0  )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n    )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n=N-1)} } \\end{bmatrix}\n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\overset{ () }{ s_{(n)} } &= \\sum\\limits ^{M-1}_{m=0} np.exp(\\; a_{(n)(m)} - c_{(n)} \\; )\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,M) }{ P } &= \\overset{ (N,M) }{ EXP }  \\;\\; / \\;\\; \\overset{ (N,1) }{ sum(EXP) } \n",
    "\\\\\n",
    "\\overset{ (N,) }{ P_{(n)} } &= (p_{(n)(m=0)}, \\; \\dots, \\; p_{(n)(m)} , \\; \\dots, \\; p_{(n)(m=M-1)})\n",
    "\\\\\n",
    "{ p_{(n)(m)} } \n",
    "&= \\frac {np.exp \\left( \n",
    "    { a_{(n)(m) } } - { c_{(n)} }) \\right) \n",
    "}\n",
    "{  \n",
    "np.sum \\left( \n",
    "    np.exp \\left( \n",
    "        a_{(n)(m) } - c_{(n)}\n",
    "    \\right)\n",
    "\\right) \n",
    "}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dA\n",
    "\n",
    "Impact on L by dA from the activation layer.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{\\partial A} }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial P} }\n",
    "* \n",
    "\\overset { (N,M) }{ \\frac {\\partial P }{\\partial A} } \n",
    "= \n",
    "\\frac {1}{N} (P - T)\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "$\n",
    "Jacobian \\; : \\; f \\circ g \\rightarrow Jf \\circ Jg\n",
    "\\\\\n",
    "\\begin{align*}\n",
    "\\\\\n",
    "L &= f(\\; p_{(n)(m=0)} \\;) = f( \\; g(\\;  a_{(n)(m=0)} \\; ) \\; ) \\quad : p = g(a) = softmax(a)\n",
    "\\\\\n",
    "\\frac {\\partial L} { \\partial a_{(n)(m=0)} }\n",
    "&= Jf(p) \\circ Jg(a) \n",
    "=  \\frac {\\partial L} { \\partial p_{(n)(m=0)} } * \\frac {\\partial  p_{(n)(m=0)}} { \\partial a_{(n)(m=0)} }\n",
    "\\\\\n",
    "&= \\frac {1}{N} \\left(\n",
    " p_{(n)(m=0)} -t_{(n)(m=0)}\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient from cross entropy log loss\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac { \\partial L }{ \\partial  p_{(n)(m=0)} }\n",
    "&= \\frac{-1}{N} t_{(n)(m=0)} * \\frac {s_{(n)}}{e_{(n)(m=0)}}\n",
    "\\\\\n",
    "\\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "&= \\frac{-1}{N} t_{(n)(m=1)} * \\frac {s_{(n)}}{e_{(n)(m=1)}}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Gradient $\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "p_{(n)(m=0)} &= \\frac {e_{(n)(m=0)} }{ s_{(n)} } \\\\\n",
    "p_{(n)(m=0)} &= f \\circ g_{(m=0)} = { s^{-1}_{(n)} } \\; * \\; { e_{(n)(m=0)} }\n",
    "\\rightarrow \\frac { \\partial  p_{(n)(m=0)} } { \\partial s^{-1}_{(n)} } = e_{(n)(m=0)}\n",
    "\\\\\n",
    "p_{(n)(m=1)} &= \\frac {e_{(n)(m=1)} }{ s_{(n)} } \\\\\n",
    "p_{(n)(m=1)} &= f \\circ g_{(m=1)} = { s^{-1}_{(n)} } \\; * \\; { e_{(n)(m=1)} }\n",
    "\\rightarrow \\frac { \\partial  p_{(n)(m=1)} } { \\partial s^{-1}_{(n)} } = e_{(n)(m=1)}\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "\\left[\n",
    "    \\frac { \\partial  p_{(n)(m=0)} } { \\partial s^{-1}_{(n)} } * \n",
    "    \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \n",
    "    +\n",
    "    \\frac { \\partial  p_{(n)(m=1)} } { \\partial s^{-1}_{(n)} } * \n",
    "    \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "\\right]\n",
    "\\\\\n",
    "&= \\sum\\limits^{M-1}_{m=0} \n",
    "    e_{(n)(m)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m)} } \n",
    "\\\\\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "    \\begin{bmatrix}\n",
    "    e_{(n)(m=0)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \\\\\n",
    "    + \\\\\n",
    "    e_{(n)(m=1)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "    \\end{bmatrix}\n",
    "\\\\\n",
    "&= -s_{(n)}(\\; t_{(n)(m=0)} + t_{(n)(m=1)} \\;) \\\\\n",
    "&= -s_{(n)}\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "\\left[\n",
    "    e_{(n)(m=0)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \n",
    "    + \n",
    "    e_{(n)(m=1)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "\\right]\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient $\\frac {\\partial L }{ \\partial { s_{(n)} } } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac {1} { s_{(n)} } &= s^{-1}_{(n)} \\rightarrow\n",
    "\\frac { \\partial { s^{-1}_{(n)} } } {\\partial s_{(n)}} = \\frac {-1}{s^{2}_{(n)}}\n",
    "\\\\\n",
    "\\frac {\\partial L }{ \\partial { s_{(n)} } } \n",
    "&=\n",
    "\\frac {-1}{s^{2}_{(n)}} * \n",
    "\\frac {\\partial L}{ \\partial s^{-1}_{(n)} } \\\\\n",
    "&= \\frac {1}{s_n}\n",
    "\\end{align*} \\\\\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient $\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } $\n",
    "$\n",
    "\\begin{align*}\n",
    "s_{(n)} &= \\sum\\limits ^{M-1}_{m=0} e_{(n)(m)} \\rightarrow \n",
    "\\frac { \\partial { s_{(n)} } } {\\partial e_{(n)(m=0)}} = 1\n",
    "\\\\\n",
    "p_{(n)(m=0)} &= \\frac {e_{(n)(m=0)} }{ s_{(n)} }\\rightarrow \n",
    "\\frac { \\partial { p_{(n)(m=0)} } } {\\partial e_{(n)(m=0)}} = \\frac {1}{s_{(n)}}\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } \n",
    "&= \\begin{bmatrix}  \n",
    "    \\frac { \\partial { s_{(n)} } } {\\partial e_{(n)(m=0)}} *  \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \\\\\n",
    "    + \\\\\n",
    "    \\frac { \\partial { p_{(n)(m=0)} } } {\\partial e_{(n)(m=0)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}  \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \\\\\n",
    "    + \\\\\n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } &= \n",
    "\\left[\n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "    + \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \n",
    "\\right]\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } &= \n",
    "\\begin{bmatrix}  \n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} }  \\\\\n",
    "    +  \\\\\n",
    "    \\frac {\\partial L }{ \\partial s_{(n)} } \n",
    "\\end{bmatrix} \\\\\n",
    "&= \\frac {-t_{(n)(m=0)}}{e_{(n)(m=0)} } + \\frac {1}{s_{n}}\n",
    "\\end{align*}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gardient $\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "e_{(n)(m)} &= exp(\\; a_{(n)(m)} \\; ) \\rightarrow \\frac { \\partial e_{(n)(m)} }{ \\partial a_{(n)(m)} } = e_{(n)(m)} \n",
    "\\\\\n",
    "e_{(n)(m=0)} &= exp(a_{(n)(m=0)}) \\rightarrow \\frac { \\partial e_{(n)(m=0)} }{ \\partial a_{(n)(m=0)} } = e_{(n)(m=0)} \n",
    "\\\\\n",
    "e_{(n)(m=1)} &= exp(a_{(n)(m=1)}) \\\\\n",
    "\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } \n",
    "&=   \\frac { \\partial e_{(n)(m=0)} }{ \\partial a_{(n)(m=0)} } * \n",
    "    \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \n",
    "= e_{(n)(m=0)} * \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } \n",
    "&= e_{(n)(m=0)} * \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \\\\\n",
    "&= -t_{(n)(m=0)} + \\frac { e_{(n)(m=0)} }{ s_{n} } \\\\\n",
    "&= p_{(n)(m=0)} -t_{(n)(m=0)} \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Log Loss\n",
    "\n",
    "A probability distribution $P(x)$ can be represented with its entropy $E(x) = \\sum\\limits_{x}  \\frac {p(x)}{log(p(x)} = - \\sum\\limits_{x} p(x) log(p(x))$. In the diagram, x: (0:dog, 1:cat, 2:fish, 3:bird) are labels and p(dog) is 0.5. When  a NN predicts an input x as a probability distribution $P(x)$, then the $E(x) = 1.75$. \n",
    "\n",
    "0. $p(dog)=\\frac {1}{2}$\n",
    "1. $p(cat)=\\frac {1}{4}$\n",
    "2. $p(fish)=\\frac {1}{8}$\n",
    "3. $p(bird)=\\frac {1}{8}$\n",
    "\n",
    "When the truth is that x is a dog, then the probability distribution of the truth $P(t)$ has the entropy $E(t) = 0$.\n",
    "\n",
    "0. $p(dog)=1$\n",
    "1. $p(cat)=0$\n",
    "2. $p(fish)=0$\n",
    "3. $p(bird)=0$\n",
    "\n",
    "The difference E(x) - E(t) = E(x) = 1.75 can be used as the distance or the error of the prediction from the truth. Need to understand further but  the actuall loss function is $E(x) = -tlog(p(x)) = -log(p(x))$ where p(x) is the probability from the softmax for the correct label.\n",
    "\n",
    "\n",
    "<img src=\"image/entropy.png\" align=\"left\" width=600/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.log() is ln based on the mathematical constant $e$ and its derivative $\\frac {\\partial log(x)}{\\partial x} = \\frac {1}{x}$.\n",
    "\n",
    "* [Logarithm](https://en.wikipedia.org/wiki/Logarithm)\n",
    "\n",
    "\n",
    "<img src=\"image/logarithm_plots.png\" align=\"left\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [ML Grossary - Loss Functions](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)\n",
    "\n",
    "<img src=\"image/cross_entropy_log_loss.png\" align=\"left\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cross_entropy_log_loss_input_combinations.xlsx](./common/cross_entropy_log_loss_input_combinations.xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import (\n",
    "    cross_entropy_log_loss,\n",
    "    OFFSET_LOG\n",
    ")\n",
    "lines = inspect.getsource(cross_entropy_log_loss)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For label using One Hot Encoding (OHE)\n",
    "For instance, if multi labels are (0,1,2,3,4) and each label is OHE, then the label for 2 is (0,0,1,0,0).   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product of matrix rows\n",
    "\n",
    "There is no formal operation to calculate the dot products of the rows from two matrices, but to calculate the diagonal of the matlix multiplication that also calculate non-diagonals. To avoid calculating non-diagonals, use [einsum](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html).\n",
    "\n",
    "* [Name of matrix operation of ```[A[0] dot B[0], A[1] dot B[1] ]``` from 2x2 matrices A, B](https://math.stackexchange.com/questions/4010721/name-of-matrix-operation-of-a0-dot-b0-a1-dot-b1-from-2x2-matrices-a)\n",
    "\n",
    "<img src=\"image/dot_products_of_matrix_rows.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(6).reshape(2,3)\n",
    "b = np.arange(0,-6,-1).reshape(2,3)\n",
    "c = [\n",
    "    np.inner(a[0], b[0]),\n",
    "    np.inner(a[1], b[1]),    \n",
    "]\n",
    "print(f\"a is \\n{a}\")\n",
    "print(f\"b.T is \\n{b.T}\\n\")\n",
    "fmt=f\"\"\"c[\n",
    "    np.inner(a[0], b[0]),\n",
    "    np.inner(a[1], b[1]),    \n",
    "] is {c}\\n\n",
    "\"\"\"\n",
    "print(fmt)\n",
    "\n",
    "# Use einsum\n",
    "e = np.einsum('ij,ji->i', a, b.T)\n",
    "fmt=\"np.einsum('ij,ji->i', a, b.T)\"\n",
    "print(f\"{fmt} is {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foward path (OHE)\n",
    "$\n",
    "\\text{ for one hot encoding labels }\n",
    "\\\\\n",
    "\\begin{align*}\n",
    "\\overset{ (N,) }{ J } &= - \\sum\\limits^{M-1}_{m=0} \n",
    "    \\left[ \\; \\;  \n",
    "        t_{(n)(m)} \\;  * \\;  np.log(p_{(n)(m)}) \\;\\;  \n",
    "    \\right]\n",
    "\\\\\n",
    "\\overset{ () }{ j_{(n)} } &= \\overset{ (M,) }{ T_{(n)} } \\cdot \\overset{ (M,) }{ P_{(n)} } \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient dL/dP\n",
    "\n",
    "Impact on L by the $dP$ from the softmax layer for one hot encoding labels.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac { \\partial L }{ \\partial P} }\n",
    "&= \\overset { (N,) }{ \\frac { \\partial L }{ \\partial J} } * \n",
    "\\overset { (N,M) }{ \n",
    "\\left(\n",
    " - \\frac { \\partial T } { \\partial P }\n",
    " \\right) \n",
    "} \n",
    "= - \\frac {1}{N }  \\frac { \\partial T } { \\partial P }\n",
    "\\\\\n",
    "\\frac {\\partial L }{\\partial p_{(n)(m=0)}} \n",
    "&= \\frac {\\partial L}{\\partial j_{(n)}} * \\frac {\\partial j_{(n)}} {\\partial p_{(n)(m=0)}} \n",
    "= \\frac {1}{N} \\frac { -t_{(n)(m=0)}}{ p_{(n)(m=0)} } \n",
    "=  \\frac {1}{N} \\left(\n",
    " -t_{(n)(m=0)} * \\frac { s_{(n)} }{ e_{(n)(m=0)} }\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For label using indexing \n",
    "For instance, if the multi labels are (0,1,2,3,4) then the index is 2 for the label 2. If the labels are (2,4,6,8,9), then the index is 3 for the label 8.  \n",
    "\n",
    "Use LP to select the probabilities from P for the corresponding labels. For instance, if the label is 2 (hence the index is 2) for X(n=0), and 4 for X(n=3), then the numpy tuple indexing selects ```P[n=0][m=2]``` and ```P[n=3][m=4] ```.\n",
    "\n",
    "```\n",
    "P[\n",
    "   (0, 3),\n",
    "   (2, 4)\n",
    "]\n",
    "```\n",
    "\n",
    "$\n",
    "\\text{ for index labels e.g. (5, 2, 0, 9, ...)}\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,) }{ J } = - np.sum(\\; np.log(LP), \\; axis = -1 \\;) \\\\\n",
    "LP = label\\_probability = P \\left[ \\\\\n",
    "\\quad ( \\; 0, \\; \\dots, \\;  {N-1}) , \\\\\n",
    "\\quad ( \\; t_{(n=0)} \\; , \\dots , \\; t_{(n=N-1)}) \\\\\n",
    "\\right]\n",
    "\\\\\n",
    "\\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward path\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ () }{ L } = \\frac {1}{N} \\sum\\limits^{N-1}_{n=0} \\overset{ () }{ j_{{(n)}} }\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gardient dL/dJ\n",
    "\n",
    "Impact on L by $dJ$ from the cross entropy log loss layer.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,) }{ \\frac {\\partial L}{\\partial J} }  &= \\frac {1}{N} \\overset{(N,)}{ones}\n",
    "\\\\\n",
    "\\frac {\\partial L}{\\partial j_{(n)} } &= \\frac {1}{N} \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dJ = np.ones(N) / N\n",
    "dJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [two_layer_net.ipynb defines the lambda with parameter W which is redundant #254](https://github.com/cs231n/cs231n.github.io/issues/254)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```numerical_jacobian(f, X)``` returns ```J``` of the same shape with ```X```. It takes each element in ```x``` in ```X```, and calculate ```(f(x+h) and f(x-h))/2h```. For ```cross_entropy_logg_loss()```, the expected numerical gradient is ```gn = (-np.log(p+h+e) + -np.log(p-h+e)) / (2*h)``` for each element ```p``` in ```P```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import (\n",
    "    numerical_jacobian,\n",
    "    OFFSET_DELTA\n",
    ")\n",
    "lines = inspect.getsource(numerical_jacobian)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Example gradients for the cross entropy log loss -t*log(p).\n",
    "# --------------------------------------------------------------------------------\n",
    "p =0.86270721\n",
    "h = OFFSET_DELTA\n",
    "e = OFFSET_LOG\n",
    "\n",
    "expected_gn = (-np.log(p+h+e) + np.log(p-h+e)) / (2*h)\n",
    "actual_gn = (cross_entropy_log_loss(p+h, 1) - cross_entropy_log_loss(p-h, 1)) / (2*h)\n",
    "print(f\"Expected numerical gradient={expected_gn}\")\n",
    "print(f\"Actual numerical gradient={actual_gn}\")\n",
    "print(f\"Expected analytical gradient -T/P={-1 / (p+e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.50924298 \n",
    "gn = (cross_entropy_log_loss(p+h, 1) - cross_entropy_log_loss(p-h, 1)) / (2*h)\n",
    "gn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification\n",
    "\n",
    "Use Matmul and CrossEntropyLogLoss layers to build a binary classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import (\n",
    "    weights,\n",
    "    transform_X_T,\n",
    "    sigmoid_cross_entropy_log_loss,\n",
    "    softmax_cross_entropy_log_loss\n",
    ")\n",
    "from data import (\n",
    "    linear_separable\n",
    ")\n",
    "from optimizer import (\n",
    "    Optimizer,\n",
    "    SGD\n",
    ")\n",
    "from network import (\n",
    "    train_binary_classifier\n",
    ")\n",
    "from drawing import (\n",
    "    COLOR_LABELS,   # labels to classify outside/0/red or inside/1/green.\n",
    "    plot_categorical_predictions\n",
    ")\n",
    "\n",
    "# Logging is enabled by calling logging.basicConfig\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.ERROR)\n",
    "Logger = logging.getLogger(\"train_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "%matplotlib notebook\n",
    "\n",
    "def draw_training(X, W, _ax=None, colors=['b']):\n",
    "    w0 = W[0]\n",
    "    w1 = W[1]\n",
    "    w2 = W[2]\n",
    "    \n",
    "    #_ax.set_xlim(-3, 3)\n",
    "    #_ax.set_ylim(-3, 3)\n",
    "    #_ax.set_title(label=f\"W: {W}\")\n",
    "\n",
    "    #_ax.scatter(X[T==0, 1], X[T==0, 2], c='red')\n",
    "    #_ax.scatter(X[T==1, 1], X[T==1, 2], c='green')\n",
    "    x = np.linspace(-3,3,100)\n",
    "    if _ax.lines:\n",
    "        for line in _ax.lines:\n",
    "            line.set_xdata(x)\n",
    "            y = -w1/w2 * x - w0 / w2\n",
    "            line.set_ydata(y)\n",
    "    else:\n",
    "        for color in colors:\n",
    "            y = -w1/w2 * x - w0 / w2\n",
    "            _ax.plot(x, y, color)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    _x = np.linspace(-3,3,100)\n",
    "    _y = -w1/w2 * x - w0 / w2\n",
    "    _ax.plot(_x, _y, label='linear')  # Plot some data on the _axes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X\n",
    "\n",
    "Training data is two dimensional plots that can be linearly separable with a line whose normal is $(w1, w2)$ and point is $b=-w0/w2$. The line is written as $X \\cdot W = 0$ where $W = (w0,w1,w2)$ and $X = (x0, x1, x2)$. $T$ are binary labels that tells if each plot is classfied as 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500    # Number of plots\n",
    "D = 3      # Number of features\n",
    "from data import (\n",
    "    linear_separable\n",
    ")\n",
    "X, T, V = linear_separable(d=D, n=N)\n",
    "#print(f\"X.shape {X.shape} T.shape {T.shape} W {V}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "ax.set_xlabel('x label')\n",
    "ax.set_ylabel('y label')\n",
    "ax.axis('equal')\n",
    "ax.set_title('Linarly seprable two dimensional plots')\n",
    "\n",
    "ax.scatter(X[T==0, 1], X[T==0, 2], c='red')\n",
    "ax.scatter(X[T==1, 1], X[T==1, 2], c='green')\n",
    "\n",
    "# Hyperplace (X-b)V = 0 -> x1V1 + x2V2 - bV2 = 0\n",
    "x = np.linspace(-3,3,100)\n",
    "y = -(V[1] / V[2]) * x - (V[0] / V[2])\n",
    "ax.plot(x, y)\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train binary classifiers\n",
    "1. Sigmoid binary classifier\n",
    "2. Softmax binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12,6))\n",
    "for i in range(2):\n",
    "    ax[i].scatter(X[T==0, 1], X[T==0, 2], c='red')\n",
    "    ax[i].scatter(X[T==1, 1], X[T==1, 2], c='green')\n",
    "    ax[i].set_xlabel('x label')\n",
    "    ax[i].set_ylabel('y label')\n",
    "    ax[i].axis('equal')\n",
    "    ax[i].set_xlim(-3, 3)\n",
    "    ax[i].set_ylim(-3, 3)\n",
    "    ax[i].grid()\n",
    "\n",
    "fig.suptitle('Trainig progress to be plotted here', fontsize=16)\n",
    "ax[0].set_title(\"sigmoid binary classifier\")\n",
    "ax[1].set_title(\"softmax binary classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid classifier training\n",
    "\n",
    "Plots in the previous cell."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(f\"Train a sigmoid classifier to find optimal W {tuple(V)} for the boundary.\")\n",
    "MAX_TEST_TIMES = 100\n",
    "\n",
    "M = 1\n",
    "W = weights.xavier(M, D)    # Xavier initialization for Sigmoid\n",
    "optimizer = SGD(lr=0.1)\n",
    "draw = partial(draw_training, X=X, _ax=ax[0])\n",
    "ax[0].set_xlim(-3, 3)\n",
    "ax[0].set_ylim(-3, 3)\n",
    "\n",
    "train_binary_classifier(\n",
    "    N=N,\n",
    "    D=D,\n",
    "    M=M,\n",
    "    X=X,\n",
    "    T=T,\n",
    "    W=W,\n",
    "    log_loss_function=sigmoid_cross_entropy_log_loss,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=MAX_TEST_TIMES,\n",
    "    test_numerical_gradient=False,\n",
    "    callback=draw\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%lprun \\\n",
    "    -T train_sigmoid_binary_classifier.log \\\n",
    "    -f train_binary_classifier \\\n",
    "    train_binary_classifier(\\\n",
    "        N=N,D=D,M=M,X=X,T=T,W=W,\\\n",
    "        log_loss_function=sigmoid_cross_entropy_log_loss, \\\n",
    "        optimizer=optimizer, \\\n",
    "        num_epochs=MAX_TEST_TIMES, \\\n",
    "        test_numerical_gradient=False \\\n",
    "    )\n",
    "\n",
    "print(open('train_sigmoid_classifier.log', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax classifier training\n",
    "Two class classification with softmax activation. \n",
    "Plots in the previous cell."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(f\"Train a softmax classifier to find optimal W {tuple(V)} for the boundary.\")\n",
    "MAX_TEST_TIMES = 200\n",
    "\n",
    "M = 2                      \n",
    "W = weights.he(M, D)\n",
    "optimizer = SGD(lr=0.1)\n",
    "draw = partial(draw_training, X=X, _ax=ax[1])\n",
    "ax[1].set_xlim(-3, 3)\n",
    "ax[1].set_ylim(-3, 3)\n",
    "\n",
    "train_binary_classifier(\n",
    "    N=N,\n",
    "    D=D,\n",
    "    M=M,\n",
    "    X=X,\n",
    "    T=T,\n",
    "    W=W,\n",
    "    log_loss_function=softmax_cross_entropy_log_loss,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=MAX_TEST_TIMES,\n",
    "    test_numerical_gradient=False, \n",
    "    callback=draw\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%lprun \\\n",
    "    -T train_softmax_binary_classifier.log \\\n",
    "    -f train_binary_classifier \\\n",
    "    train_binary_classifier(\\\n",
    "        N=N,D=D,M=M,X=X,T=T,W=W,\\\n",
    "        log_loss_function=softmax_cross_entropy_log_loss, \\\n",
    "        optimizer=optimizer, \\\n",
    "        num_epochs=MAX_TEST_TIMES, \\\n",
    "        test_numerical_gradient=False \n",
    "    )\n",
    "\n",
    "print(open('train_softmax_classifier.log', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Classification\n",
    "\n",
    "Use Matmul and CrossEntropyLogLoss layers to classify M categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import (\n",
    "    prediction_grid,\n",
    ")\n",
    "from data import (\n",
    "    linear_separable_sectors,\n",
    ")\n",
    "from network import (\n",
    "    train_matmul_relu_classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearly separable multiple categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data X and Label T\n",
    "Training data to lassify into M categories and labels T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train a categorical classifier\")\n",
    "MAX_TEST_TIMES = 400\n",
    "N = 300\n",
    "D = 3      # Dimension\n",
    "M = 3\n",
    "\n",
    "rotation = np.radians(35)\n",
    "# x0 = X[::,0] is the bias 1\n",
    "X, T, B = linear_separable_sectors(n=N, d=D, m=M, r=2, rotation=rotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot X, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radius of a circle within which to place plots.\n",
    "radius = 2   \n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Plot area\n",
    "# --------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "for i in range(2):\n",
    "    ax.set_xlabel('x label')\n",
    "    ax.set_ylabel('y label')\n",
    "    ax.axis('equal')\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-3, 3)\n",
    "    ax.grid()\n",
    "\n",
    "ax.set_title(f\"Categorical data of {M} classes\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Cirle within which to place random plots.\n",
    "# --------------------------------------------------------------------------------\n",
    "r = np.linspace(0, 2 * np.pi, 100)\n",
    "ax.plot(radius * np.cos(r), radius * np.sin(r), \"b--\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Classify plots (x, y) if inside the coverage sector\n",
    "# labels to classify outside/0/red or inside/1/green.\n",
    "# --------------------------------------------------------------------------------\n",
    "Y = COLOR_LABELS[\n",
    "    T\n",
    "]\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Plot color-classified points.\n",
    "# --------------------------------------------------------------------------------\n",
    "ax.scatter(X[::,1], X[::,2], marker='o', color=Y)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Plot sector bases\n",
    "# --------------------------------------------------------------------------------\n",
    "for i in range(B.shape[0]):\n",
    "    ax.plot((0, radius * B[i, 0]), (0, radius * B[i, 1]), COLOR_LABELS[i])\n",
    "\n",
    "# ax.legend()\n",
    "fig.suptitle('Categorical classifiation data', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on linearly separable multiple categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = weights.he(M, D)\n",
    "optimizer = SGD(lr=0.1)\n",
    "\n",
    "train_matmul_relu_classifier\n",
    "# W = train_classifier(\n",
    "W = train_matmul_relu_classifier(\n",
    "    N=N,\n",
    "    D=D,\n",
    "    M=M,\n",
    "    X=X,\n",
    "    T=T,\n",
    "    W=W,\n",
    "    log_loss_function=softmax_cross_entropy_log_loss,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=MAX_TEST_TIMES,\n",
    "    test_numerical_gradient=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run predictions\n",
    "\n",
    "Run preditions against the grid coordinates (x1, x2).\n",
    "```\n",
    "x1: X[:, 1].min() - 1 <= x1 <=  X[:, 1].max() + 1\n",
    "x2: X[:, 2].min() - 1 <= x2 <=  X[:, 2].max() + 1\n",
    "grid = np.meshgrid(x1, x2)\n",
    "\n",
    "# np.argmax(scores) selets the highest score for each data point in X.\n",
    "# e.g score[i] = [0.2, 8.2, 0.3], then np.argmax(scores[i]) selects index 1 as the prediction. \n",
    "# Then cluster of predition/label == 1 will form a contour.\n",
    "sores = grid @ W.T\n",
    "predictions = p.argmax(score, axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,5)) \n",
    "ax.set_xlabel('x label')\n",
    "ax.set_ylabel('y label')\n",
    "ax.axis('equal')\n",
    "ax.grid()\n",
    "ax.set_title(\"Predictions\")\n",
    "#ax.set_xlim(-3, 3)\n",
    "#ax.set_ylim(-3, 3)\n",
    "\n",
    "x_grid, y_grid, predictions = prediction_grid(X, W)\n",
    "plot_categorical_predictions(ax, [x_grid, y_grid], X, Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear separable spiral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import (\n",
    "    prediction_grid_2d\n",
    ")\n",
    "from network import (\n",
    "    train_two_layer_classifier\n",
    ")\n",
    "from data import (\n",
    "    spiral\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 3        # Number of category classes\n",
    "K = 100       # Number of data points per class\n",
    "N = M * K    # Number of entire data points\n",
    "D = 3        # Dimensions inluding bias\n",
    "\n",
    "\n",
    "# X[::,0] is bias\n",
    "X, T = spiral(K, D, M)\n",
    "\n",
    "# Y is colors as labels, instead of category indices.\n",
    "Y = COLOR_LABELS[T]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,5)) \n",
    "ax.scatter(X[:, 1], X[:, 2], c=Y, s=40)\n",
    "ax.set_xlim([-1,1])\n",
    "ax.set_ylim([-1,1])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on non-linear separable spiral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TEST_TIMES = 2000\n",
    "\n",
    "M1 = 50\n",
    "W1 = weights.he(M1, D)\n",
    "M2: int = 3                 # Number of categories to classify\n",
    "W2 = weights.he(M2, M1)\n",
    "optimizer = SGD(lr=0.3)\n",
    "\n",
    "X, T = transform_X_T(X, T)\n",
    "W1, W2, objective, prediction = train_two_layer_classifier(\n",
    "    N=N,\n",
    "    D=D,\n",
    "    X=X,\n",
    "    T=T,\n",
    "    M1=M1,\n",
    "    W1=W1,\n",
    "    M2=M2,\n",
    "    W2=W2,\n",
    "    log_loss_function=softmax_cross_entropy_log_loss,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=MAX_TEST_TIMES,\n",
    "    test_numerical_gradient=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X[:, 1].min(), X[:, 1].max()\n",
    "y_min, y_max = X[:, 2].min(), X[:, 2].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,5)) \n",
    "for i in range(2):\n",
    "    ax.set_xlabel('x label')\n",
    "    ax.set_ylabel('y label')\n",
    "    ax.axis('equal')\n",
    "    ax.grid()\n",
    "\n",
    "ax.set_title(\"Predictions\")\n",
    "#ax.set_xlim(-1, 1)\n",
    "#ax.set_ylim(-1, 1)\n",
    "\n",
    "x_grid, y_grid, predictions = prediction_grid_2d(x_min, x_max, y_min, y_max, prediction)\n",
    "plot_categorical_predictions(ax, [x_grid, y_grid], X, Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A NOT B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import (\n",
    "    set_in_a_radius,\n",
    "    set_in_A_not_B,\n",
    "    sets_of_circle_A_not_B\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 3\n",
    "__N = 300\n",
    "radius = 1\n",
    "circles, centres = sets_of_circle_A_not_B(radius=radius, ratio=1.2, m=M, n=__N)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6)) \n",
    "ax.set_xlabel('x label')\n",
    "ax.set_ylabel('y label')\n",
    "ax.axis('equal')\n",
    "ax.grid()\n",
    "r = np.linspace(0, 2*np.pi, 100)\n",
    "\n",
    "for i in range(M):\n",
    "    circle = circles[i]\n",
    "    if circle.size > 0:\n",
    "        x = centres[i][0]\n",
    "        y = centres[i][1]\n",
    "        ax.scatter(circle[::, 0], circle[::, 1], color=COLOR_LABELS[i])\n",
    "        ax.plot(\n",
    "            x + radius * np.cos(r), \n",
    "            y + radius * np.sin(r), \n",
    "            linestyle='dashed', \n",
    "            color=COLOR_LABELS[i]\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack(\n",
    "    [circles[i] for i in range(M)]\n",
    ")\n",
    "\n",
    "T = np.hstack(\n",
    "    [np.full(circles[i].shape[0], i) for i in range(M)]\n",
    ")\n",
    "N = T.shape[0]\n",
    "assert T.shape[0] == X.shape[0]\n",
    "\n",
    "# Shuffle the data\n",
    "indices = np.random.permutation(range(T.shape[0]))\n",
    "\n",
    "T = T[indices]\n",
    "X = X[indices]\n",
    "X = np.c_[\n",
    "    np.ones(X.shape[0]),\n",
    "    X\n",
    "]       # Add bias=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TEST_TIMES = 1000\n",
    "D = 3\n",
    "M1 = 200\n",
    "W1 = weights.he(M1, D)\n",
    "M2: int = 3                 # Number of categories to classify\n",
    "W2 = weights.he(M2, M1)\n",
    "optimizer = SGD(lr=0.3)\n",
    "\n",
    "#X, T = transform_X_T(X, T)\n",
    "W1, W2, objective, prediction = train_two_layer_classifier(\n",
    "    N=N,\n",
    "    D=D,\n",
    "    X=X,\n",
    "    T=T,\n",
    "    M1=M1,\n",
    "    W1=W1,\n",
    "    M2=M2,\n",
    "    W2=W2,\n",
    "    log_loss_function=softmax_cross_entropy_log_loss,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=MAX_TEST_TIMES,\n",
    "    test_numerical_gradient=False\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
