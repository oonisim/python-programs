{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* [CS231n: Convolutional Neural Networks for Visual Recognition 2017](http://cs231n.stanford.edu/2017/syllabus)\n",
    "    - [cs231n 2017 assignment #1 kNN, SVM, SoftMax, two-layer network](https://cs231n.github.io/assignments2017/assignment1/)\n",
    "    - [Training a Softmax Linear Classifier](https://cs231n.github.io/neural-networks-case-study)\n",
    "* [ゼロから作る Deep Learning](https://github.com/oreilly-japan/deep-learning-from-scratch)\n",
    "* [Mathematics for Machine Learning](https://mml-book.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network \n",
    "Simple one layer neural network classifier. Mathjax formula not fully supported in github, hence the formulas get corrupted.\n",
    "\n",
    "<img src=\"image/nn_diagram.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/nn_functions.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Optional,\n",
    "    Union,\n",
    "    List,\n",
    "    Dict,\n",
    "    Tuple\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python path\n",
    "Python path setup to avoid the relative imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Union,\n",
    "    List,\n",
    "    Callable\n",
    ")\n",
    "import inspect\n",
    "from functools import partial\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=80) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install line_profile memory_profiler\n",
    "%load_ext memory_profiler\n",
    "%load_ext line_profiler\n",
    "\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1    # Batch size\n",
    "D = 3    # Number of features in the input data\n",
    "M = 2    # Number of nodes in a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confiurations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X\n",
    "X is to have been standardized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T\n",
    "Labels for data X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For One Hot Encoding labels\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ T_{_{OHE}} } &= ( \n",
    "    \\overset{ (M,) }{ T_{(n=0)} }, \\; \\dots \\;, \\overset{ (M,) }{ T_{(n=N-1)} } \n",
    ") \n",
    "\\\\\n",
    "\\overset{ (N,M) }{ T_{_{OHE}} } = ( \n",
    "    \\overset{ (M,) }{ T_{(n=0)} }, \\dots , \\overset{ (M,) }{ T_{(n=N-1)} } \n",
    ") \n",
    "\\\\\n",
    "\\overset{ (M,) }{ T_{ _{OHE} (n)} } &= ( \\overset{ () }{ t_{(n)(m=0)} }, \\; \\dots \\;, \\overset{ () }{ t_{(n)(m=M-1)} })\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "### For index labels\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,) }{ T_{_{IDX}} } &= (\\overset{ () }{ t_{(n=0)} }, \\; \\dots \\;, \\overset{ () }{ t_{(n=N-1)} }) \\qquad \\text {for index labels }\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W\n",
    "Weight parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import (\n",
    "    xavier,\n",
    "    he,\n",
    "    uniform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization Layer\n",
    "Apply normalization or use batch normaliation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matmul layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer.matmul import Matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ Y } \n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "{ Y_{(n=0)} } \\\\\n",
    "\\vdots \\\\\n",
    "{ Y_{(n)} } \\\\\n",
    "\\vdots \\\\\n",
    "{ Y_{(n=N-1)} }\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\overset{ (N,D) }{ X } \\; @ \\; \\overset{ (D,M) }{ W^T }\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (M,) }{ Y_{(n)} } &= (y_{(n)(m=0)}, \\; \\dots, \\; y_{(n)(m)},  \\; \\dots, \\; y_{(n)(m=M-1)})\n",
    "\\\\\n",
    "\\overset{ () }{ y_{(n)(m)} }\n",
    "&= \\overset{ (D,) }{ X_{(n)} } \\cdot \\overset{ (D,) }{ W_{(m)}^T }\n",
    "= \\sum\\limits ^{D}_{d=0}  \\overset{ () }{ x_{(n)(d)} } * \\overset{ () }{ w_{(m)(d)} }\n",
    "\\\\\n",
    "_{(0 \\le d \\le D, \\; 0 \\le n \\lt N, \\; 0 \\le m \\lt M)}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dX\n",
    "\n",
    "Impact on L by $dX$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,D) }{ \\frac {\\partial L }{ \\partial X } }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial Y} }\n",
    "@ \\overset { (M,D) }{ W } \n",
    "\\end{align*}\n",
    "$\n",
    "<img src=\"image/nn_back_propagation_dL_dX.png\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dW.T\n",
    "Impact on L by $dW^T$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{ \\partial W^T } }\n",
    "= \\overset { (D,N) }{ X^T } \n",
    "@ \n",
    "\\overset { (N,M) }{ \\frac {\\partial L}{\\partial Y} }\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "<img src=\"image/nn_back_propagation_dL_dWT.png\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ A } &= \n",
    "activation \\left( \n",
    "    \\overset{ (N,M) }{ Y }  = \n",
    "    \\begin{bmatrix}\n",
    "    { Y_{(n=0)} } \\\\\n",
    "    \\vdots \\\\\n",
    "    { Y_{(n)} } \\\\\n",
    "    \\vdots \\\\\n",
    "    { Y_{(n=N-1)} }\n",
    "    \\end{bmatrix}\n",
    "\\right)\n",
    "\\\\\n",
    "\\overset{ (M,) }{ A_{(n)} } \n",
    "&= activation \\left( \\overset{ (M,) }{ Y_{(n) }} \\right)  \\\\\n",
    "&= (a_{(n)(m=0)}, \\; \\dots, \\; a_{(n)(m)},  \\; \\dots, \\; a_{(n)(m=M-1)})\n",
    "\\\\\n",
    "\\overset{ () }{ a_{(n)(m)} } &= activation \\left( \\overset{ () }{ y_{(n)(m)} } \\right)\n",
    "\\quad _{(0 \\le n \\lt N, \\; 0 \\le m \\lt M)}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dY\n",
    "\n",
    "Impact on L by dY from the matmul layer.\n",
    "\n",
    "$\n",
    "\\begin {align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{ \\partial Y } }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial A} } \n",
    "* \n",
    "\\overset { (N,M) }{ \\frac {\\partial A}{\\partial Y} }\n",
    "\\end {align*}\n",
    "$\n",
    "\n",
    "### For sigmoid activation\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset {(N,M)}{\\frac { \\partial L }{ \\partial Y} }\n",
    "&= \\frac { \\partial A }{ \\partial Y} * A * (1 - A)\n",
    "\\\\\n",
    "\\frac { \\partial y_{(n)(m)} } { \\partial a_{(n)(m)} }\n",
    "&= a_{(n)(m)} * (1 - a_{(n)(m)} )  \\\\ \n",
    "y_{(n)(m)} = sigmoid(a_{(n)(m)} )&=  \\frac {1}{ 1 + exp(y_{(n)(m)})}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "### For ReLU activation\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac { \\partial a_{(n)(m)} }{ \\partial y_{(n)(m)} }\n",
    "&= 1 \\quad y_{(n)(m)}  \\gt 0 \\\\\n",
    "&= 0 \\quad y_{(n)(m)}  \\le 0 \\\\\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax layer\n",
    "$C_n$ is to prevent the overflow at $np.exp()$.\n",
    "\n",
    "<img src=\"image/softmax.png\" align=\"left\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exp(x) can take all x values and produces a positive, which is required for log(y) that needs y > 0, hence fit-for-purpose to build a probability function.\n",
    "\n",
    "<img src=\"image/exp.gif\" align=\"left\" width=250/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax and Cross Entropy Log Loss are combined as the gradient results in a simple form $P - T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def softmax(X: Union[np.ndarray, float]) -> Union[np.ndarray, float]:\n",
      "    \"\"\"Softmax P = exp(X) / sum(exp(X))\n",
      "    Args:\n",
      "        X: batch input data of shape (N,M).\n",
      "            N: Batch size\n",
      "            M: Number of nodes\n",
      "    Returns:\n",
      "        P: Probability of shape (N,M)\n",
      "    \"\"\"\n",
      "    name = \"softmax\"\n",
      "    assert isinstance(X, float) or (isinstance(X, np.ndarray) and X.dtype == float), \\\n",
      "        \"X must be float or ndarray(dtype=float)\"\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # exp(x-c) to prevent the infinite exp(x) for a large value x, with c = max(x).\n",
      "    # keepdims=True to be able to broadcast.\n",
      "    # --------------------------------------------------------------------------------\n",
      "    C = np.max(X, axis=-1, keepdims=True)\n",
      "    exp = np.exp(X - C)\n",
      "    P = exp / np.sum(exp, axis=-1, keepdims=True)\n",
      "    Logger.debug(\"%s: X %s exp %s P %s\", name, X, exp, P)\n",
      "\n",
      "    return P\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from layer import CrossEntropyLogLoss\n",
    "from common import softmax\n",
    "\n",
    "lines = inspect.getsource(softmax)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,1) }{ C } &= np.max\\left( \n",
    "    \\overset{ (N,M) }{ A }, \\; axis=-1,  \\; keepdim=True \n",
    "\\right) \\\\\n",
    "&=  \n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n=0  )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n    )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n=N-1)} } \\end{bmatrix}\n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\overset{ (N,M) }{ EXP } &= np.exp \\left( \\overset{ (N,M) }{ A } - \\overset{ (N,1) }{ C } \\right)\n",
    "= np.exp \\left(\n",
    "    \\begin{bmatrix}\n",
    "    { A_{(n=0)} } - { C_{(n=0)} }\\\\\n",
    "    \\vdots \\\\\n",
    "    { A_{(n)} }   - { C_{(n)} }\\\\\n",
    "    \\vdots \\\\\n",
    "    { A_{(n=N-1)} } - { C_{(n=N-1)} }\\\\\n",
    "    \\end{bmatrix}\n",
    "\\right) \n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "    e_{(n=0)(m=0)}   & \\dots      & e_{(n=0)(m=M-1)}   \\\\  \n",
    "    \\vdots           & e_{(n)(m)} & \\vdots             \\\\\n",
    "    e_{(n=N-1)(m=0)} & \\dots      & e_{(n=N-1)(m=M-1)} \n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,1) }{ S } &= \\overset{ (N,1) }{ sum(EXP) } = np.sum \\left( \n",
    "    \\overset{ (N,M) }{ EXP }, \\; axis=-1,  \\; keepdim=True \n",
    "\\right)\n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n=0  )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n    )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n=N-1)} } \\end{bmatrix}\n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\overset{ () }{ s_{(n)} } &= \\sum\\limits ^{M-1}_{m=0} np.exp(\\; a_{(n)(m)} - c_{(n)} \\; )\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,M) }{ P } &= \\overset{ (N,M) }{ EXP }  \\;\\; / \\;\\; \\overset{ (N,1) }{ sum(EXP) } \n",
    "\\\\\n",
    "\\overset{ (N,) }{ P_{(n)} } &= (p_{(n)(m=0)}, \\; \\dots, \\; p_{(n)(m)} , \\; \\dots, \\; p_{(n)(m=M-1)})\n",
    "\\\\\n",
    "{ p_{(n)(m)} } \n",
    "&= \\frac {np.exp \\left( \n",
    "    { a_{(n)(m) } } - { c_{(n)} }) \\right) \n",
    "}\n",
    "{  \n",
    "np.sum \\left( \n",
    "    np.exp \\left( \n",
    "        a_{(n)(m) } - c_{(n)}\n",
    "    \\right)\n",
    "\\right) \n",
    "}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dA\n",
    "\n",
    "Impact on L by dA from the activation layer.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{\\partial A} }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial P} }\n",
    "* \n",
    "\\overset { (N,M) }{ \\frac {\\partial P }{\\partial A} } \n",
    "= \n",
    "\\frac {1}{N} (P - T)\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "$\n",
    "Jacobian \\; : \\; f \\circ g \\rightarrow Jf \\circ Jg\n",
    "\\\\\n",
    "\\begin{align*}\n",
    "\\\\\n",
    "L &= f(\\; p_{(n)(m=0)} \\;) = f( \\; g(\\;  a_{(n)(m=0)} \\; ) \\; ) \\quad : p = g(a) = softmax(a)\n",
    "\\\\\n",
    "\\frac {\\partial L} { \\partial a_{(n)(m=0)} }\n",
    "&= Jf(p) \\circ Jg(a) \n",
    "=  \\frac {\\partial L} { \\partial p_{(n)(m=0)} } * \\frac {\\partial  p_{(n)(m=0)}} { \\partial a_{(n)(m=0)} }\n",
    "\\\\\n",
    "&= \\frac {1}{N} \\left(\n",
    " p_{(n)(m=0)} -t_{(n)(m=0)}\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient from cross entropy log loss\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac { \\partial L }{ \\partial  p_{(n)(m=0)} }\n",
    "&= \\frac{-1}{N} t_{(n)(m=0)} * \\frac {s_{(n)}}{e_{(n)(m=0)}}\n",
    "\\\\\n",
    "\\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "&= \\frac{-1}{N} t_{(n)(m=1)} * \\frac {s_{(n)}}{e_{(n)(m=1)}}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Gradient $\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "p_{(n)(m=0)} &= \\frac {e_{(n)(m=0)} }{ s_{(n)} } \\\\\n",
    "p_{(n)(m=0)} &= f \\circ g_{(m=0)} = { s^{-1}_{(n)} } \\; * \\; { e_{(n)(m=0)} }\n",
    "\\rightarrow \\frac { \\partial  p_{(n)(m=0)} } { \\partial s^{-1}_{(n)} } = e_{(n)(m=0)}\n",
    "\\\\\n",
    "p_{(n)(m=1)} &= \\frac {e_{(n)(m=1)} }{ s_{(n)} } \\\\\n",
    "p_{(n)(m=1)} &= f \\circ g_{(m=1)} = { s^{-1}_{(n)} } \\; * \\; { e_{(n)(m=1)} }\n",
    "\\rightarrow \\frac { \\partial  p_{(n)(m=1)} } { \\partial s^{-1}_{(n)} } = e_{(n)(m=1)}\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "\\left[\n",
    "    \\frac { \\partial  p_{(n)(m=0)} } { \\partial s^{-1}_{(n)} } * \n",
    "    \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \n",
    "    +\n",
    "    \\frac { \\partial  p_{(n)(m=1)} } { \\partial s^{-1}_{(n)} } * \n",
    "    \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "\\right]\n",
    "\\\\\n",
    "&= \\sum\\limits^{M-1}_{m=0} \n",
    "    e_{(n)(m)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m)} } \n",
    "\\\\\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "    \\begin{bmatrix}\n",
    "    e_{(n)(m=0)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \\\\\n",
    "    + \\\\\n",
    "    e_{(n)(m=1)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "    \\end{bmatrix}\n",
    "\\\\\n",
    "&= -s_{(n)}(\\; t_{(n)(m=0)} + t_{(n)(m=1)} \\;) \\\\\n",
    "&= -s_{(n)}\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "\\left[\n",
    "    e_{(n)(m=0)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \n",
    "    + \n",
    "    e_{(n)(m=1)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "\\right]\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient $\\frac {\\partial L }{ \\partial { s_{(n)} } } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac {1} { s_{(n)} } &= s^{-1}_{(n)} \\rightarrow\n",
    "\\frac { \\partial { s^{-1}_{(n)} } } {\\partial s_{(n)}} = \\frac {-1}{s^{2}_{(n)}}\n",
    "\\\\\n",
    "\\frac {\\partial L }{ \\partial { s_{(n)} } } \n",
    "&=\n",
    "\\frac {-1}{s^{2}_{(n)}} * \n",
    "\\frac {\\partial L}{ \\partial s^{-1}_{(n)} } \\\\\n",
    "&= \\frac {1}{s_n}\n",
    "\\end{align*} \\\\\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient $\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } $\n",
    "$\n",
    "\\begin{align*}\n",
    "s_{(n)} &= \\sum\\limits ^{M-1}_{m=0} e_{(n)(m)} \\rightarrow \n",
    "\\frac { \\partial { s_{(n)} } } {\\partial e_{(n)(m=0)}} = 1\n",
    "\\\\\n",
    "p_{(n)(m=0)} &= \\frac {e_{(n)(m=0)} }{ s_{(n)} }\\rightarrow \n",
    "\\frac { \\partial { p_{(n)(m=0)} } } {\\partial e_{(n)(m=0)}} = \\frac {1}{s_{(n)}}\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } \n",
    "&= \\begin{bmatrix}  \n",
    "    \\frac { \\partial { s_{(n)} } } {\\partial e_{(n)(m=0)}} *  \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \\\\\n",
    "    + \\\\\n",
    "    \\frac { \\partial { p_{(n)(m=0)} } } {\\partial e_{(n)(m=0)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}  \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \\\\\n",
    "    + \\\\\n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } &= \n",
    "\\left[\n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "    + \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \n",
    "\\right]\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } &= \n",
    "\\begin{bmatrix}  \n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} }  \\\\\n",
    "    +  \\\\\n",
    "    \\frac {\\partial L }{ \\partial s_{(n)} } \n",
    "\\end{bmatrix} \\\\\n",
    "&= \\frac {-t_{(n)(m=0)}}{e_{(n)(m=0)} } + \\frac {1}{s_{n}}\n",
    "\\end{align*}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gardient $\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "e_{(n)(m)} &= exp(\\; a_{(n)(m)} \\; ) \\rightarrow \\frac { \\partial e_{(n)(m)} }{ \\partial a_{(n)(m)} } = e_{(n)(m)} \n",
    "\\\\\n",
    "e_{(n)(m=0)} &= exp(a_{(n)(m=0)}) \\rightarrow \\frac { \\partial e_{(n)(m=0)} }{ \\partial a_{(n)(m=0)} } = e_{(n)(m=0)} \n",
    "\\\\\n",
    "e_{(n)(m=1)} &= exp(a_{(n)(m=1)}) \\\\\n",
    "\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } \n",
    "&=   \\frac { \\partial e_{(n)(m=0)} }{ \\partial a_{(n)(m=0)} } * \n",
    "    \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \n",
    "= e_{(n)(m=0)} * \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } \n",
    "&= e_{(n)(m=0)} * \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \\\\\n",
    "&= -t_{(n)(m=0)} + \\frac { e_{(n)(m=0)} }{ s_{n} } \\\\\n",
    "&= p_{(n)(m=0)} -t_{(n)(m=0)} \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Log Loss\n",
    "\n",
    "A probability distribution $P(x)$ can be represented with its entropy $E(x) = \\sum\\limits_{x}  \\frac {p(x)}{log(p(x)} = - \\sum\\limits_{x} p(x) log(p(x))$. In the diagram, x: (0:dog, 1:cat, 2:fish, 3:bird) are labels and p(dog) is 0.5. When  a NN predicts an input x as a probability distribution $P(x)$, then the $E(x) = 1.75$. \n",
    "\n",
    "0. $p(dog)=\\frac {1}{2}$\n",
    "1. $p(cat)=\\frac {1}{4}$\n",
    "2. $p(fish)=\\frac {1}{8}$\n",
    "3. $p(bird)=\\frac {1}{8}$\n",
    "\n",
    "When the truth is that x is a dog, then the probability distribution of the truth $P(t)$ has the entropy $E(t) = 0$.\n",
    "\n",
    "0. $p(dog)=1$\n",
    "1. $p(cat)=0$\n",
    "2. $p(fish)=0$\n",
    "3. $p(bird)=0$\n",
    "\n",
    "The difference E(x) - E(t) = E(x) = 1.75 can be used as the distance or the error of the prediction from the truth. Need to understand further but  the actuall loss function is $E(x) = -tlog(p(x)) = -log(p(x))$ where p(x) is the probability from the softmax for the correct label.\n",
    "\n",
    "\n",
    "<img src=\"image/entropy.png\" align=\"left\" width=600/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.log() is ln based on the mathematical constant $e$ and its derivative $\\frac {\\partial log(x)}{\\partial x} = \\frac {1}{x}$.\n",
    "\n",
    "* [Logarithm](https://en.wikipedia.org/wiki/Logarithm)\n",
    "\n",
    "\n",
    "<img src=\"image/logarithm_plots.png\" align=\"left\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [ML Grossary - Loss Functions](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)\n",
    "\n",
    "<img src=\"image/cross_entropy_log_loss.png\" align=\"left\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cross_entropy_log_loss_input_combinations.xlsx](./common/cross_entropy_log_loss_input_combinations.xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cross_entropy_log_loss(\n",
      "        P: Union[np.ndarray, float],\n",
      "        T: Union[np.ndarray, int],\n",
      "        f: Callable = categorical_log_loss,\n",
      "        offset: float = OFFSET_LOG\n",
      ") -> np.ndarray:\n",
      "    \"\"\"Cross entropy log loss [ -t(n)(m) * log(p(n)(m)) ] for multi labels.\n",
      "    Args:\n",
      "        P: activation or probabilities from an activation function.\n",
      "        T: labels\n",
      "        f: Cross entropy log loss function f(P, T) where P is activation, T is label\n",
      "        offset: small number to avoid np.inf by log(0) by log(0+offset)\n",
      "\n",
      "    Returns:\n",
      "        J: Loss value of shape (N,), a loss value per batch.\n",
      "\n",
      "    NOTE:\n",
      "        Handle only the label whose value is True. The reason not to use non-labels to\n",
      "        calculate the loss is TBD.\n",
      "\n",
      "        See transform_X_T for the format and shape of P and T.\n",
      "    \"\"\"\n",
      "    name = \"cross_entropy_log_loss\"\n",
      "    P, T = transform_X_T(P, T)\n",
      "    if P.ndim == 0:\n",
      "        assert False, \"P.ndim needs (N,M) after transform_X_T(P, T)\"\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # P is scalar, T is a scalar binary OHE label. Return -t * log(p).\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # assert T.ndim == 0, \"P.ndim==0 requires T.ndim==0 but %s\" % T.shape\n",
      "        # return f(P, T, offset)\n",
      "\n",
      "    if (1 < P.ndim == T.ndim) and (P.shape[1] == T.shape[1] == 1):\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # This condition X:(N,1), T(N,1) tells T is the 2D binary OHE labels.\n",
      "        # T is 2D binary OHE labels e.g. T[[0],[1],[0]], P[[0.9],[0.1],[0.3]].\n",
      "        # Return -T * log(P)\n",
      "        # --------------------------------------------------------------------------------\n",
      "        return np.squeeze(f(P=P, T=T, offset=offset), axis=-1)    # Shape from (N,M) to (N,)\n",
      "\n",
      "    # ================================================================================\n",
      "    # Calculate Cross entropy log loss -t * log(p).\n",
      "    # Select an element P[n][t] at each row n which corresponds to the true label t.\n",
      "    # Use the Numpy tuple indexing. e.g. P[n=0][t=2] and P[n=3][t=4].\n",
      "    # P[\n",
      "    #   (0, 3),\n",
      "    #   (2, 4)     # The tuple sizes must be the same at all axes\n",
      "    # ]\n",
      "    #\n",
      "    # Tuple indexing selects only one element per row.\n",
      "    # Beware the numpy behavior difference between P[(n),(m)] and P[[n],[m]].\n",
      "    # https://stackoverflow.com/questions/66269684\n",
      "    # P[1,1]  and P[(0)(0)] results in a scalar value, HOWEVER, P[[1],[1]] in array.\n",
      "    #\n",
      "    # P shape can be (1,1), (1, M), (N, 1), (N, M), hence P[rows, cols] are:\n",
      "    # P (1,1) -> P[rows, cols] results in a 1D of  (1,).\n",
      "    # P (1,M) -> P[rows, cols] results in a 1D of  (1,)\n",
      "    # P (N,1) -> P[rows, cols] results in a 1D of  (N,)\n",
      "    # P (N,M) -> P[rows, cols] results in a 1D of  (N,)\n",
      "    #\n",
      "    # J shape matches with the P[rows, cols] shape.\n",
      "    # ================================================================================\n",
      "    N = batch_size = P.shape[0]\n",
      "    rows = np.arange(N)     # (N,)\n",
      "    cols = T                # Same shape (N,) with rows\n",
      "    assert rows.shape == cols.shape, \\\n",
      "        f\"np P indices need the same shape but rows {rows.shape} cols {cols.shape}.\"\n",
      "\n",
      "    _P = P[rows, cols]\n",
      "    Logger.debug(\"%s: N is [%s]\", name, N)\n",
      "    Logger.debug(\"%s: P.shape %s\", name, P.shape)\n",
      "    Logger.debug(\"%s: P[rows, cols].shape %s\", name, _P.shape)\n",
      "    Logger.debug(\"%s: P[rows, cols] is %s\", name, _P)\n",
      "\n",
      "    J = f(P=_P, T=int(1), offset=offset)\n",
      "\n",
      "    assert not np.all(np.isnan(J)), f\"log(x) caused nan for P \\n{P}.\"\n",
      "    Logger.debug(\"%s: J is [%s]\", name, J)\n",
      "    Logger.debug(\"%s: J.shape %s\\n\", name, J.shape)\n",
      "\n",
      "    assert (J.ndim > 0) and (0 < N == J.shape[0]), \\\n",
      "        f\"Loss J.shape is expected to be ({N},) but {J.shape}\"\n",
      "    return J\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from common import (\n",
    "    cross_entropy_log_loss,\n",
    "    OFFSET_LOG\n",
    ")\n",
    "lines = inspect.getsource(cross_entropy_log_loss)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For label using One Hot Encoding (OHE)\n",
    "For instance, if multi labels are (0,1,2,3,4) and each label is OHE, then the label for 2 is (0,0,1,0,0).   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product of matrix rows\n",
    "\n",
    "There is no formal operation to calculate the dot products of the rows from two matrices, but to calculate the diagonal of the matlix multiplication that also calculate non-diagonals. To avoid calculating non-diagonals, use [einsum](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html).\n",
    "\n",
    "* [Name of matrix operation of ```[A[0] dot B[0], A[1] dot B[1] ]``` from 2x2 matrices A, B](https://math.stackexchange.com/questions/4010721/name-of-matrix-operation-of-a0-dot-b0-a1-dot-b1-from-2x2-matrices-a)\n",
    "\n",
    "<img src=\"image/dot_products_of_matrix_rows.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is \n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "b.T is \n",
      "[[ 0 -3]\n",
      " [-1 -4]\n",
      " [-2 -5]]\n",
      "\n",
      "c[\n",
      "    np.inner(a[0], b[0]),\n",
      "    np.inner(a[1], b[1]),    \n",
      "] is [-5, -50]\n",
      "\n",
      "\n",
      "np.einsum('ij,ji->i', a, b.T) is [ -5 -50]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(6).reshape(2,3)\n",
    "b = np.arange(0,-6,-1).reshape(2,3)\n",
    "c = [\n",
    "    np.inner(a[0], b[0]),\n",
    "    np.inner(a[1], b[1]),    \n",
    "]\n",
    "print(f\"a is \\n{a}\")\n",
    "print(f\"b.T is \\n{b.T}\\n\")\n",
    "fmt=f\"\"\"c[\n",
    "    np.inner(a[0], b[0]),\n",
    "    np.inner(a[1], b[1]),    \n",
    "] is {c}\\n\n",
    "\"\"\"\n",
    "print(fmt)\n",
    "\n",
    "# Use einsum\n",
    "e = np.einsum('ij,ji->i', a, b.T)\n",
    "fmt=\"np.einsum('ij,ji->i', a, b.T)\"\n",
    "print(f\"{fmt} is {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foward path (OHE)\n",
    "$\n",
    "\\text{ for one hot encoding labels }\n",
    "\\\\\n",
    "\\begin{align*}\n",
    "\\overset{ (N,) }{ J } &= - \\sum\\limits^{M-1}_{m=0} \n",
    "    \\left[ \\; \\;  \n",
    "        t_{(n)(m)} \\;  * \\;  np.log(p_{(n)(m)}) \\;\\;  \n",
    "    \\right]\n",
    "\\\\\n",
    "\\overset{ () }{ j_{(n)} } &= \\overset{ (M,) }{ T_{(n)} } \\cdot \\overset{ (M,) }{ P_{(n)} } \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient dL/dP\n",
    "\n",
    "Impact on L by the $dP$ from the softmax layer for one hot encoding labels.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac { \\partial L }{ \\partial P} }\n",
    "&= \\overset { (N,) }{ \\frac { \\partial L }{ \\partial J} } * \n",
    "\\overset { (N,M) }{ \n",
    "\\left(\n",
    " - \\frac { \\partial T } { \\partial P }\n",
    " \\right) \n",
    "} \n",
    "= - \\frac {1}{N }  \\frac { \\partial T } { \\partial P }\n",
    "\\\\\n",
    "\\frac {\\partial L }{\\partial p_{(n)(m=0)}} \n",
    "&= \\frac {\\partial L}{\\partial j_{(n)}} * \\frac {\\partial j_{(n)}} {\\partial p_{(n)(m=0)}} \n",
    "= \\frac {1}{N} \\frac { -t_{(n)(m=0)}}{ p_{(n)(m=0)} } \n",
    "=  \\frac {1}{N} \\left(\n",
    " -t_{(n)(m=0)} * \\frac { s_{(n)} }{ e_{(n)(m=0)} }\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For label using indexing \n",
    "For instance, if the multi labels are (0,1,2,3,4) then the index is 2 for the label 2. If the labels are (2,4,6,8,9), then the index is 3 for the label 8.  \n",
    "\n",
    "Use LP to select the probabilities from P for the corresponding labels. For instance, if the label is 2 (hence the index is 2) for X(n=0), and 4 for X(n=3), then the numpy tuple indexing selects ```P[n=0][m=2]``` and ```P[n=3][m=4] ```.\n",
    "\n",
    "```\n",
    "P[\n",
    "   (0, 3),\n",
    "   (2, 4)\n",
    "]\n",
    "```\n",
    "\n",
    "$\n",
    "\\text{ for index labels e.g. (5, 2, 0, 9, ...)}\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,) }{ J } = - np.sum(\\; np.log(LP), \\; axis = -1 \\;) \\\\\n",
    "LP = label\\_probability = P \\left[ \\\\\n",
    "\\quad ( \\; 0, \\; \\dots, \\;  {N-1}) , \\\\\n",
    "\\quad ( \\; t_{(n=0)} \\; , \\dots , \\; t_{(n=N-1)}) \\\\\n",
    "\\right]\n",
    "\\\\\n",
    "\\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward path\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ () }{ L } = \\frac {1}{N} \\sum\\limits^{N-1}_{n=0} \\overset{ () }{ j_{{(n)}} }\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gardient dL/dJ\n",
    "\n",
    "Impact on L by $dJ$ from the cross entropy log loss layer.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,) }{ \\frac {\\partial L}{\\partial J} }  &= \\frac {1}{N} \\overset{(N,)}{ones}\n",
    "\\\\\n",
    "\\frac {\\partial L}{\\partial j_{(n)} } &= \\frac {1}{N} \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ = np.ones(N) / N\n",
    "dJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [two_layer_net.ipynb defines the lambda with parameter W which is redundant #254](https://github.com/cs231n/cs231n.github.io/issues/254)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```numerical_jacobian(f, X)``` returns ```J``` of the same shape with ```X```. It takes each element in ```x``` in ```X```, and calculate ```(f(x+h) and f(x-h))/2h```. For ```cross_entropy_logg_loss()```, the expected numerical gradient is ```gn = (-np.log(p+h+e) + -np.log(p-h+e)) / (2*h)``` for each element ```p``` in ```P```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import (\n",
    "    numerical_jacobian,\n",
    "    OFFSET_DELTA\n",
    ")\n",
    "lines = inspect.getsource(numerical_jacobian)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Example gradients for the cross entropy log loss -t*log(p).\n",
    "# --------------------------------------------------------------------------------\n",
    "p =0.86270721\n",
    "h = OFFSET_DELTA\n",
    "e = OFFSET_LOG\n",
    "\n",
    "expected_gn = (-np.log(p+h+e) + np.log(p-h+e)) / (2*h)\n",
    "actual_gn = (cross_entropy_log_loss(p+h, 1) - cross_entropy_log_loss(p-h, 1)) / (2*h)\n",
    "print(f\"Expected numerical gradient={expected_gn}\")\n",
    "print(f\"Actual numerical gradient={actual_gn}\")\n",
    "print(f\"Expected analytical gradient -T/P={-1 / (p+e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.50924298 \n",
    "gn = (cross_entropy_log_loss(p+h, 1) - cross_entropy_log_loss(p-h, 1)) / (2*h)\n",
    "gn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification\n",
    "\n",
    "Use Matmul and CrossEntropyLogLoss layers to build a binary classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import (\n",
    "    weights,\n",
    "    sigmoid_cross_entropy_log_loss,\n",
    "    softmax_cross_entropy_log_loss\n",
    ")\n",
    "from data import (\n",
    "    linear_separable\n",
    ")\n",
    "from optimizer import (\n",
    "    Optimizer,\n",
    "    SGD\n",
    ")\n",
    "from network import (\n",
    "    train_binary_classifier\n",
    ")\n",
    "from drawing import (\n",
    "    COLOR_LABELS,   # labels to classify outside/0/red or inside/1/green.\n",
    "    plot_categorical_predictions\n",
    ")\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "Logger = logging.getLogger(\"train_classifier\")\n",
    "Logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "%matplotlib notebook\n",
    "\n",
    "def draw_training(X, W, _ax=None, colors=['b']):\n",
    "    w0 = W[0]\n",
    "    w1 = W[1]\n",
    "    w2 = W[2]\n",
    "    \n",
    "    #_ax.set_xlim(-3, 3)\n",
    "    #_ax.set_ylim(-3, 3)\n",
    "    #_ax.set_title(label=f\"W: {W}\")\n",
    "\n",
    "    #_ax.scatter(X[T==0, 1], X[T==0, 2], c='red')\n",
    "    #_ax.scatter(X[T==1, 1], X[T==1, 2], c='green')\n",
    "    x = np.linspace(-3,3,100)\n",
    "    if _ax.lines:\n",
    "        for line in _ax.lines:\n",
    "            line.set_xdata(x)\n",
    "            y = -w1/w2 * x - w0 / w2\n",
    "            line.set_ydata(y)\n",
    "    else:\n",
    "        for color in colors:\n",
    "            y = -w1/w2 * x - w0 / w2\n",
    "            _ax.plot(x, y, color)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    _x = np.linspace(-3,3,100)\n",
    "    _y = -w1/w2 * x - w0 / w2\n",
    "    _ax.plot(_x, _y, label='linear')  # Plot some data on the _axes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X\n",
    "\n",
    "Training data is two dimensional plots that can be linearly separable with a line whose normal is $(w1, w2)$ and point is $b=-w0/w2$. The line is written as $X \\cdot W = 0$ where $W = (w0,w1,w2)$ and $X = (x0, x1, x2)$. $T$ are binary labels that tells if each plot is classfied as 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500    # Number of plots\n",
    "D = 3      # Number of features\n",
    "from data import (\n",
    "    linear_separable\n",
    ")\n",
    "X, T, V = linear_separable(d=D, n=N)\n",
    "#print(f\"X.shape {X.shape} T.shape {T.shape} W {V}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "ax.set_xlabel('x label')\n",
    "ax.set_ylabel('y label')\n",
    "ax.axis('equal')\n",
    "ax.set_title('Linarly seprable two dimensional plots')\n",
    "\n",
    "ax.scatter(X[T==0, 1], X[T==0, 2], c='red')\n",
    "ax.scatter(X[T==1, 1], X[T==1, 2], c='green')\n",
    "\n",
    "# Hyperplace (X-b)V = 0 -> x1V1 + x2V2 - bV2 = 0\n",
    "x = np.linspace(-3,3,100)\n",
    "y = -(V[1] / V[2]) * x - (V[0] / V[2])\n",
    "ax.plot(x, y)\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train binary classifiers\n",
    "1. Sigmoid binary classifier\n",
    "2. Softmax binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12,6))\n",
    "for i in range(2):\n",
    "    ax[i].scatter(X[T==0, 1], X[T==0, 2], c='red')\n",
    "    ax[i].scatter(X[T==1, 1], X[T==1, 2], c='green')\n",
    "    ax[i].set_xlabel('x label')\n",
    "    ax[i].set_ylabel('y label')\n",
    "    ax[i].axis('equal')\n",
    "    ax[i].set_xlim(-3, 3)\n",
    "    ax[i].set_ylim(-3, 3)\n",
    "    ax[i].grid()\n",
    "\n",
    "fig.suptitle('Trainig progress to be plotted here', fontsize=16)\n",
    "ax[0].set_title(\"sigmoid binary classifier\")\n",
    "ax[1].set_title(\"softmax binary classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid classifier training\n",
    "\n",
    "Plots in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train a sigmoid classifier to find optimal W {tuple(V)} for the boundary.\")\n",
    "MAX_TEST_TIMES = 100\n",
    "\n",
    "M = 1\n",
    "W = weights.xavier(M, D)    # Xavier initialization for Sigmoid\n",
    "optimizer = SGD(lr=0.1)\n",
    "draw = partial(draw_training, X=X, _ax=ax[0])\n",
    "ax[0].set_xlim(-3, 3)\n",
    "ax[0].set_ylim(-3, 3)\n",
    "\n",
    "train_binary_classifier(\n",
    "    N=N,\n",
    "    D=D,\n",
    "    M=M,\n",
    "    X=X,\n",
    "    T=T,\n",
    "    W=W,\n",
    "    log_loss_function=sigmoid_cross_entropy_log_loss,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=MAX_TEST_TIMES,\n",
    "    callback=draw\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%lprun \\\n",
    "    -T train_sigmoid_binary_classifier.log \\\n",
    "    -f train_binary_classifier \\\n",
    "    train_binary_classifier(\\\n",
    "        N=N,D=D,M=M,X=X,T=T,W=W,\\\n",
    "        log_loss_function=sigmoid_cross_entropy_log_loss, \\\n",
    "        optimizer=optimizer, \\\n",
    "        num_epochs=MAX_TEST_TIMES \\\n",
    "    )\n",
    "\n",
    "print(open('train_sigmoid_classifier.log', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax classifier training\n",
    "Two class classification with softmax activation. \n",
    "Plots in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train a softmax classifier to find optimal W {tuple(V)} for the boundary.\")\n",
    "MAX_TEST_TIMES = 200\n",
    "\n",
    "M = 2                      \n",
    "W = weights.he(M, D)\n",
    "optimizer = SGD(lr=0.1)\n",
    "draw = partial(draw_training, X=X, _ax=ax[1])\n",
    "ax[1].set_xlim(-3, 3)\n",
    "ax[1].set_ylim(-3, 3)\n",
    "\n",
    "train_binary_classifier(\n",
    "    N=N,\n",
    "    D=D,\n",
    "    M=M,\n",
    "    X=X,\n",
    "    T=T,\n",
    "    W=W,\n",
    "    log_loss_function=softmax_cross_entropy_log_loss,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=MAX_TEST_TIMES,\n",
    "    callback=draw\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%lprun \\\n",
    "    -T train_softmax_binary_classifier.log \\\n",
    "    -f train_binary_classifier \\\n",
    "    train_binary_classifier(\\\n",
    "        N=N,D=D,M=M,X=X,T=T,W=W,\\\n",
    "        log_loss_function=softmax_cross_entropy_log_loss, \\\n",
    "        optimizer=optimizer, \\\n",
    "        num_epochs=MAX_TEST_TIMES \\\n",
    "    )\n",
    "\n",
    "print(open('train_softmax_classifier.log', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Classification\n",
    "\n",
    "Use Matmul and CrossEntropyLogLoss layers to classify M categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import (\n",
    "    prediction_grid\n",
    ")\n",
    "from data import (\n",
    "    linear_separable_sectors,\n",
    "    spiral\n",
    ")\n",
    "from network import (\n",
    "    train_matmul_relu_classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearly separable multiple categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data X and Label T\n",
    "Training data to lassify into M categories and labels T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train a categorical classifier\")\n",
    "MAX_TEST_TIMES = 400\n",
    "N = 300\n",
    "D = 3      # Dimension\n",
    "M = 3\n",
    "\n",
    "rotation = np.radians(35)\n",
    "# x0 = X[::,0] is the bias 1\n",
    "X, T, B = linear_separable_sectors(n=N, d=D, m=M, r=2, rotation=rotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot X, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radius of a circle within which to place plots.\n",
    "radius = 2   \n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Plot area\n",
    "# --------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "for i in range(2):\n",
    "    ax.set_xlabel('x label')\n",
    "    ax.set_ylabel('y label')\n",
    "    ax.axis('equal')\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-3, 3)\n",
    "    ax.grid()\n",
    "\n",
    "ax.set_title(f\"Categorical data of {M} classes\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Cirle within which to place random plots.\n",
    "# --------------------------------------------------------------------------------\n",
    "r = np.linspace(0, 2 * np.pi, 100)\n",
    "ax.plot(radius * np.cos(r), radius * np.sin(r), \"b--\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Classify plots (x, y) if inside the coverage sector\n",
    "# labels to classify outside/0/red or inside/1/green.\n",
    "# --------------------------------------------------------------------------------\n",
    "Y = COLOR_LABELS[\n",
    "    T\n",
    "]\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Plot color-classified points.\n",
    "# --------------------------------------------------------------------------------\n",
    "ax.scatter(X[::,1], X[::,2], marker='o', color=Y)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Plot sector bases\n",
    "# --------------------------------------------------------------------------------\n",
    "for i in range(B.shape[0]):\n",
    "    ax.plot((0, radius * B[i, 0]), (0, radius * B[i, 1]), COLOR_LABELS[i])\n",
    "\n",
    "# ax.legend()\n",
    "fig.suptitle('Categorical classifiation data', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on linearly separable multiple categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = weights.he(M, D)\n",
    "optimizer = SGD(lr=0.1)\n",
    "\n",
    "train_matmul_relu_classifier\n",
    "# W = train_classifier(\n",
    "W = train_matmul_relu_classifier(\n",
    "    N=N,\n",
    "    D=D,\n",
    "    M=M,\n",
    "    X=X,\n",
    "    T=T,\n",
    "    W=W,\n",
    "    log_loss_function=softmax_cross_entropy_log_loss,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=MAX_TEST_TIMES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run predictions\n",
    "\n",
    "Run preditions against the grid coordinates (x1, x2).\n",
    "```\n",
    "x1: X[:, 1].min() - 1 <= x1 <=  X[:, 1].max() + 1\n",
    "x2: X[:, 2].min() - 1 <= x2 <=  X[:, 2].max() + 1\n",
    "grid = np.meshgrid(x1, x2)\n",
    "\n",
    "# np.argmax(scores) selets the highest score for each data point in X.\n",
    "# e.g score[i] = [0.2, 8.2, 0.3], then np.argmax(scores[i]) selects index 1 as the prediction. \n",
    "# Then cluster of predition/label == 1 will form a contour.\n",
    "sores = grid @ W.T\n",
    "predictions = p.argmax(score, axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,5)) \n",
    "ax.set_xlabel('x label')\n",
    "ax.set_ylabel('y label')\n",
    "ax.axis('equal')\n",
    "ax.grid()\n",
    "ax.set_title(\"Predictions\")\n",
    "#ax.set_xlim(-3, 3)\n",
    "#ax.set_ylim(-3, 3)\n",
    "\n",
    "x_grid, y_grid, predictions = prediction_grid(X, W)\n",
    "plot_categorical_predictions(ax, [x_grid, y_grid], X, Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear separable spiral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import (\n",
    "    train_two_layer_classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "M = 3        # Number of category classes\n",
    "K = 50       # Number of data points per class\n",
    "N = M * K    # Number of entire data points\n",
    "D = 3        # Dimensions inluding bias\n",
    "\n",
    "\n",
    "# X[::,0] is bias\n",
    "X, T = spiral(K, D, M)\n",
    "\n",
    "# Y is colors as labels, instead of category indices.\n",
    "Y = COLOR_LABELS[T]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,5)) \n",
    "ax.scatter(X[:, 1], X[:, 2], c=Y, s=40)\n",
    "ax.set_xlim([-1,1])\n",
    "ax.set_ylim([-1,1])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on non-linear separable spiral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1157584061883297\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[0]. Loss is [1.1157584061883297]\n",
      "WARNING:network.test_040_two_layer_classifier:Iteration [0]: Loss[1.1157584061883297] has not improved from the previous [1.1157584061883297] for 1 times.\n",
      "1.0997056260358367\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[1]. Loss is [1.0997056260358367]\n",
      "1.0862079111392318\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[2]. Loss is [1.0862079111392318]\n",
      "1.074663082374562\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[3]. Loss is [1.074663082374562]\n",
      "1.0647145324766742\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[4]. Loss is [1.0647145324766742]\n",
      "1.0561261147540137\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[5]. Loss is [1.0561261147540137]\n",
      "1.04863346378082\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[6]. Loss is [1.04863346378082]\n",
      "1.042011805608469\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[7]. Loss is [1.042011805608469]\n",
      "1.0360901467512031\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[8]. Loss is [1.0360901467512031]\n",
      "1.0305622512934822\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[9]. Loss is [1.0305622512934822]\n",
      "1.0253396760633637\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[10]. Loss is [1.0253396760633637]\n",
      "1.0204188362874687\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[11]. Loss is [1.0204188362874687]\n",
      "1.0157894619780439\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[12]. Loss is [1.0157894619780439]\n",
      "1.011384123923362\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[13]. Loss is [1.011384123923362]\n",
      "1.0071853873184065\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[14]. Loss is [1.0071853873184065]\n",
      "1.0032011419463993\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[15]. Loss is [1.0032011419463993]\n",
      "0.9993509496105679\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[16]. Loss is [0.9993509496105679]\n",
      "0.9955827767651173\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[17]. Loss is [0.9955827767651173]\n",
      "0.991895641769069\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[18]. Loss is [0.991895641769069]\n",
      "0.9882372623606064\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[19]. Loss is [0.9882372623606064]\n",
      "0.9845894370236814\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[20]. Loss is [0.9845894370236814]\n",
      "0.9809566833430957\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[21]. Loss is [0.9809566833430957]\n",
      "0.9773412695645851\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[22]. Loss is [0.9773412695645851]\n",
      "0.9736989467177659\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[23]. Loss is [0.9736989467177659]\n",
      "0.9700338302326204\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[24]. Loss is [0.9700338302326204]\n",
      "0.9663796343519216\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[25]. Loss is [0.9663796343519216]\n",
      "0.9626948422595583\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[26]. Loss is [0.9626948422595583]\n",
      "0.9589883928861784\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[27]. Loss is [0.9589883928861784]\n",
      "0.955260586085991\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[28]. Loss is [0.955260586085991]\n",
      "0.9515548273986911\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[29]. Loss is [0.9515548273986911]\n",
      "0.9480170083470372\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[30]. Loss is [0.9480170083470372]\n",
      "0.9444135556182789\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[31]. Loss is [0.9444135556182789]\n",
      "0.940790941310119\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[32]. Loss is [0.940790941310119]\n",
      "0.9371353628487733\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[33]. Loss is [0.9371353628487733]\n",
      "0.9334747237158215\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[34]. Loss is [0.9334747237158215]\n",
      "0.9297718937288266\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[35]. Loss is [0.9297718937288266]\n",
      "0.9259892686234351\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[36]. Loss is [0.9259892686234351]\n",
      "0.9221590226130029\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[37]. Loss is [0.9221590226130029]\n",
      "0.9183141046967255\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[38]. Loss is [0.9183141046967255]\n",
      "0.914402264179124\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[39]. Loss is [0.914402264179124]\n",
      "0.9104373692271417\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[40]. Loss is [0.9104373692271417]\n",
      "0.9063983486527047\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[41]. Loss is [0.9063983486527047]\n",
      "0.9022912744660302\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[42]. Loss is [0.9022912744660302]\n",
      "0.8981027187084563\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[43]. Loss is [0.8981027187084563]\n",
      "0.8938317428530522\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[44]. Loss is [0.8938317428530522]\n",
      "0.8894782766060475\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[45]. Loss is [0.8894782766060475]\n",
      "0.8850431124880761\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[46]. Loss is [0.8850431124880761]\n",
      "0.8805646751298044\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[47]. Loss is [0.8805646751298044]\n",
      "0.8760749299880753\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[48]. Loss is [0.8760749299880753]\n",
      "0.8715741349970263\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[49]. Loss is [0.8715741349970263]\n",
      "0.8670981794587146\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[50]. Loss is [0.8670981794587146]\n",
      "0.8625777649072838\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[51]. Loss is [0.8625777649072838]\n",
      "0.8580338032666823\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[52]. Loss is [0.8580338032666823]\n",
      "0.85351855630629\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[53]. Loss is [0.85351855630629]\n",
      "0.8490402420626142\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[54]. Loss is [0.8490402420626142]\n",
      "0.8446784805171145\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[55]. Loss is [0.8446784805171145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.840399551537637\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[56]. Loss is [0.840399551537637]\n",
      "0.8361982643782183\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[57]. Loss is [0.8361982643782183]\n",
      "0.8321409726306531\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[58]. Loss is [0.8321409726306531]\n",
      "0.8281214586115881\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[59]. Loss is [0.8281214586115881]\n",
      "0.8241485133733698\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[60]. Loss is [0.8241485133733698]\n",
      "0.8202237043591585\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[61]. Loss is [0.8202237043591585]\n",
      "0.8163441670404412\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[62]. Loss is [0.8163441670404412]\n",
      "0.8125132829073435\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[63]. Loss is [0.8125132829073435]\n",
      "0.8087507688221556\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[64]. Loss is [0.8087507688221556]\n",
      "0.8051119697034451\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[65]. Loss is [0.8051119697034451]\n",
      "0.8015755498056848\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[66]. Loss is [0.8015755498056848]\n",
      "0.7981099784860127\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[67]. Loss is [0.7981099784860127]\n",
      "0.7947342857978081\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[68]. Loss is [0.7947342857978081]\n",
      "0.791468295672276\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[69]. Loss is [0.791468295672276]\n",
      "0.7882797356300967\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[70]. Loss is [0.7882797356300967]\n",
      "0.7851691932366718\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[71]. Loss is [0.7851691932366718]\n",
      "0.7821374791306807\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[72]. Loss is [0.7821374791306807]\n",
      "0.7791872746958443\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[73]. Loss is [0.7791872746958443]\n",
      "0.7763219983032587\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[74]. Loss is [0.7763219983032587]\n",
      "0.7735382216571142\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[75]. Loss is [0.7735382216571142]\n",
      "0.7708534236646167\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[76]. Loss is [0.7708534236646167]\n",
      "0.7682624319011588\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[77]. Loss is [0.7682624319011588]\n",
      "0.7657596446590552\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[78]. Loss is [0.7657596446590552]\n",
      "0.7633504194451465\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[79]. Loss is [0.7633504194451465]\n",
      "0.761029470790248\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[80]. Loss is [0.761029470790248]\n",
      "0.7587864729731956\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[81]. Loss is [0.7587864729731956]\n",
      "0.7566193189555404\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[82]. Loss is [0.7566193189555404]\n",
      "0.7545318095446153\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[83]. Loss is [0.7545318095446153]\n",
      "0.7525222816007608\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[84]. Loss is [0.7525222816007608]\n",
      "0.7505839295083384\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[85]. Loss is [0.7505839295083384]\n",
      "0.7487274699917921\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[86]. Loss is [0.7487274699917921]\n",
      "0.7469360385319319\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[87]. Loss is [0.7469360385319319]\n",
      "0.7452127936796268\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[88]. Loss is [0.7452127936796268]\n",
      "0.7435513158821938\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[89]. Loss is [0.7435513158821938]\n",
      "0.7419496791860022\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[90]. Loss is [0.7419496791860022]\n",
      "0.7404059927322774\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[91]. Loss is [0.7404059927322774]\n",
      "0.7389194716086617\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[92]. Loss is [0.7389194716086617]\n",
      "0.7374867932924059\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[93]. Loss is [0.7374867932924059]\n",
      "0.736107387613765\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[94]. Loss is [0.736107387613765]\n",
      "0.7347764853733545\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[95]. Loss is [0.7347764853733545]\n",
      "0.7334937465178555\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[96]. Loss is [0.7334937465178555]\n",
      "0.7322580898011072\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[97]. Loss is [0.7322580898011072]\n",
      "0.7310669140761827\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[98]. Loss is [0.7310669140761827]\n",
      "0.7299170732831542\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[99]. Loss is [0.7299170732831542]\n",
      "0.7288097242090947\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[100]. Loss is [0.7288097242090947]\n",
      "0.7277412460791655\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[101]. Loss is [0.7277412460791655]\n",
      "0.7267148230565112\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[102]. Loss is [0.7267148230565112]\n",
      "0.7257248401558725\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[103]. Loss is [0.7257248401558725]\n",
      "0.7247677193528717\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[104]. Loss is [0.7247677193528717]\n",
      "0.7238441843342512\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[105]. Loss is [0.7238441843342512]\n",
      "0.7229499738439308\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[106]. Loss is [0.7229499738439308]\n",
      "0.722086520330791\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[107]. Loss is [0.722086520330791]\n",
      "0.7212505910855854\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[108]. Loss is [0.7212505910855854]\n",
      "0.7204431449517902\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[109]. Loss is [0.7204431449517902]\n",
      "0.7196613276099209\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[110]. Loss is [0.7196613276099209]\n",
      "0.7189027360633841\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[111]. Loss is [0.7189027360633841]\n",
      "0.718168583366039\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[112]. Loss is [0.718168583366039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7174561280714002\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[113]. Loss is [0.7174561280714002]\n",
      "0.7167637792990433\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[114]. Loss is [0.7167637792990433]\n",
      "0.7160916909719149\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[115]. Loss is [0.7160916909719149]\n",
      "0.7154391599841641\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[116]. Loss is [0.7154391599841641]\n",
      "0.7148046081993594\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[117]. Loss is [0.7148046081993594]\n",
      "0.7141863616621766\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[118]. Loss is [0.7141863616621766]\n",
      "0.7135846623244734\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[119]. Loss is [0.7135846623244734]\n",
      "0.7129985241195034\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[120]. Loss is [0.7129985241195034]\n",
      "0.7124271268482676\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[121]. Loss is [0.7124271268482676]\n",
      "0.7118697210476184\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[122]. Loss is [0.7118697210476184]\n",
      "0.7113256021197801\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[123]. Loss is [0.7113256021197801]\n",
      "0.7107941025777311\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[124]. Loss is [0.7107941025777311]\n",
      "0.7102752924653728\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[125]. Loss is [0.7102752924653728]\n",
      "0.709768960849286\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[126]. Loss is [0.709768960849286]\n",
      "0.7092735348307599\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[127]. Loss is [0.7092735348307599]\n",
      "0.7087884022373266\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[128]. Loss is [0.7087884022373266]\n",
      "0.7083130265963551\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[129]. Loss is [0.7083130265963551]\n",
      "0.7078469116323866\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[130]. Loss is [0.7078469116323866]\n",
      "0.7073897090600982\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[131]. Loss is [0.7073897090600982]\n",
      "0.706940909974576\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[132]. Loss is [0.706940909974576]\n",
      "0.7065000056311366\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[133]. Loss is [0.7065000056311366]\n",
      "0.7060665910632756\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[134]. Loss is [0.7060665910632756]\n",
      "0.7056402942865264\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[135]. Loss is [0.7056402942865264]\n",
      "0.7052227105226467\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[136]. Loss is [0.7052227105226467]\n",
      "0.7048118850642539\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[137]. Loss is [0.7048118850642539]\n",
      "0.704406622877148\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[138]. Loss is [0.704406622877148]\n",
      "0.7040083981915577\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[139]. Loss is [0.7040083981915577]\n",
      "0.7036154629756393\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[140]. Loss is [0.7036154629756393]\n",
      "0.7032269102852835\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[141]. Loss is [0.7032269102852835]\n",
      "0.7028447681149352\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[142]. Loss is [0.7028447681149352]\n",
      "0.7024674177226945\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[143]. Loss is [0.7024674177226945]\n",
      "0.7020938316468459\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[144]. Loss is [0.7020938316468459]\n",
      "0.7017262282152332\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[145]. Loss is [0.7017262282152332]\n",
      "0.7013627179879273\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[146]. Loss is [0.7013627179879273]\n",
      "0.7010037865267709\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[147]. Loss is [0.7010037865267709]\n",
      "0.7006510699514327\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[148]. Loss is [0.7006510699514327]\n",
      "0.7003013113958988\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[149]. Loss is [0.7003013113958988]\n",
      "0.6999569632994269\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[150]. Loss is [0.6999569632994269]\n",
      "0.6996162216050975\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[151]. Loss is [0.6996162216050975]\n",
      "0.6992792736484046\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[152]. Loss is [0.6992792736484046]\n",
      "0.6989461357471602\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[153]. Loss is [0.6989461357471602]\n",
      "0.698616709085634\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[154]. Loss is [0.698616709085634]\n",
      "0.698291488038434\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[155]. Loss is [0.698291488038434]\n",
      "0.6979682087647672\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[156]. Loss is [0.6979682087647672]\n",
      "0.6976491206530395\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[157]. Loss is [0.6976491206530395]\n",
      "0.6973313683470777\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[158]. Loss is [0.6973313683470777]\n",
      "0.6970179192969751\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[159]. Loss is [0.6970179192969751]\n",
      "0.696706522214186\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[160]. Loss is [0.696706522214186]\n",
      "0.6963991342192937\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[161]. Loss is [0.6963991342192937]\n",
      "0.6960939333887001\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[162]. Loss is [0.6960939333887001]\n",
      "0.6957914767434855\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[163]. Loss is [0.6957914767434855]\n",
      "0.6954903318016015\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[164]. Loss is [0.6954903318016015]\n",
      "0.6952054448395079\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[165]. Loss is [0.6952054448395079]\n",
      "0.6949262258007961\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[166]. Loss is [0.6949262258007961]\n",
      "0.6946530277996025\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[167]. Loss is [0.6946530277996025]\n",
      "0.6943829508224595\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[168]. Loss is [0.6943829508224595]\n",
      "0.6941146399119283\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[169]. Loss is [0.6941146399119283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6938493093482335\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[170]. Loss is [0.6938493093482335]\n",
      "0.6935923383267193\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[171]. Loss is [0.6935923383267193]\n",
      "0.6933305119201199\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[172]. Loss is [0.6933305119201199]\n",
      "0.6930790201072131\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[173]. Loss is [0.6930790201072131]\n",
      "0.6928237223265183\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[174]. Loss is [0.6928237223265183]\n",
      "0.6925731029267881\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[175]. Loss is [0.6925731029267881]\n",
      "0.6923278549860631\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[176]. Loss is [0.6923278549860631]\n",
      "0.6920727960975349\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[177]. Loss is [0.6920727960975349]\n",
      "0.6918190711344074\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[178]. Loss is [0.6918190711344074]\n",
      "0.6915670067005668\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[179]. Loss is [0.6915670067005668]\n",
      "0.6913161554986673\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[180]. Loss is [0.6913161554986673]\n",
      "0.691065730691804\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[181]. Loss is [0.691065730691804]\n",
      "0.6908166000984386\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[182]. Loss is [0.6908166000984386]\n",
      "0.6905685733903036\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[183]. Loss is [0.6905685733903036]\n",
      "0.6903221998961997\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[184]. Loss is [0.6903221998961997]\n",
      "0.6900707779327692\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[185]. Loss is [0.6900707779327692]\n",
      "0.689805492894877\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[186]. Loss is [0.689805492894877]\n",
      "0.6895415683019949\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[187]. Loss is [0.6895415683019949]\n",
      "0.6892806324225728\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[188]. Loss is [0.6892806324225728]\n",
      "0.6890204532541958\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[189]. Loss is [0.6890204532541958]\n",
      "0.6887604998956013\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[190]. Loss is [0.6887604998956013]\n",
      "0.6885032101582463\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[191]. Loss is [0.6885032101582463]\n",
      "0.6882457358139504\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[192]. Loss is [0.6882457358139504]\n",
      "0.6879959157545172\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[193]. Loss is [0.6879959157545172]\n",
      "0.6877477003786591\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[194]. Loss is [0.6877477003786591]\n",
      "0.6875063991077446\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[195]. Loss is [0.6875063991077446]\n",
      "0.6872633519145099\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[196]. Loss is [0.6872633519145099]\n",
      "0.687020634543004\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[197]. Loss is [0.687020634543004]\n",
      "0.6867802236836652\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[198]. Loss is [0.6867802236836652]\n",
      "0.686540049869079\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[199]. Loss is [0.686540049869079]\n",
      "0.6863009466528847\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[200]. Loss is [0.6863009466528847]\n",
      "0.6860627512507163\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[201]. Loss is [0.6860627512507163]\n",
      "0.685825685066389\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[202]. Loss is [0.685825685066389]\n",
      "0.6855882737931563\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[203]. Loss is [0.6855882737931563]\n",
      "0.685352059071386\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[204]. Loss is [0.685352059071386]\n",
      "0.6851217927971154\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[205]. Loss is [0.6851217927971154]\n",
      "0.6848928714058913\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[206]. Loss is [0.6848928714058913]\n",
      "0.684663191532738\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[207]. Loss is [0.684663191532738]\n",
      "0.6844346333645251\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[208]. Loss is [0.6844346333645251]\n",
      "0.6842069912881958\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[209]. Loss is [0.6842069912881958]\n",
      "0.6839786214153408\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[210]. Loss is [0.6839786214153408]\n",
      "0.6837517094966357\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[211]. Loss is [0.6837517094966357]\n",
      "0.6835249864644444\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[212]. Loss is [0.6835249864644444]\n",
      "0.6832978617167323\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[213]. Loss is [0.6832978617167323]\n",
      "0.6830727456711735\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[214]. Loss is [0.6830727456711735]\n",
      "0.6828463230680529\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[215]. Loss is [0.6828463230680529]\n",
      "0.6826214085820635\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[216]. Loss is [0.6826214085820635]\n",
      "0.6823966382989405\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[217]. Loss is [0.6823966382989405]\n",
      "0.6821716135392872\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[218]. Loss is [0.6821716135392872]\n",
      "0.6819482377645982\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[219]. Loss is [0.6819482377645982]\n",
      "0.6817236870386335\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[220]. Loss is [0.6817236870386335]\n",
      "0.6815012773027193\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[221]. Loss is [0.6815012773027193]\n",
      "0.6812794239611049\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[222]. Loss is [0.6812794239611049]\n",
      "0.681058293973102\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[223]. Loss is [0.681058293973102]\n",
      "0.6808376352208398\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[224]. Loss is [0.6808376352208398]\n",
      "0.6806180044804342\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[225]. Loss is [0.6806180044804342]\n",
      "0.6803988767543548\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[226]. Loss is [0.6803988767543548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6801805696268077\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[227]. Loss is [0.6801805696268077]\n",
      "0.6799625308034543\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[228]. Loss is [0.6799625308034543]\n",
      "0.6797451028554294\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[229]. Loss is [0.6797451028554294]\n",
      "0.6795287375796928\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[230]. Loss is [0.6795287375796928]\n",
      "0.6793152734014398\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[231]. Loss is [0.6793152734014398]\n",
      "0.6791011204053301\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[232]. Loss is [0.6791011204053301]\n",
      "0.6788889169678263\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[233]. Loss is [0.6788889169678263]\n",
      "0.678678001382396\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[234]. Loss is [0.678678001382396]\n",
      "0.6784684129199285\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[235]. Loss is [0.6784684129199285]\n",
      "0.6782592882968302\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[236]. Loss is [0.6782592882968302]\n",
      "0.6780496671046605\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[237]. Loss is [0.6780496671046605]\n",
      "0.6778411604481899\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[238]. Loss is [0.6778411604481899]\n",
      "0.67763312981408\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[239]. Loss is [0.67763312981408]\n",
      "0.6774246026716028\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[240]. Loss is [0.6774246026716028]\n",
      "0.6772171649055783\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[241]. Loss is [0.6772171649055783]\n",
      "0.6770102568040389\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[242]. Loss is [0.6770102568040389]\n",
      "0.6768028461965631\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[243]. Loss is [0.6768028461965631]\n",
      "0.6765964862230915\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[244]. Loss is [0.6765964862230915]\n",
      "0.6763910083982267\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[245]. Loss is [0.6763910083982267]\n",
      "0.6761855932314819\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[246]. Loss is [0.6761855932314819]\n",
      "0.6759809204015099\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[247]. Loss is [0.6759809204015099]\n",
      "0.6757773330303758\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[248]. Loss is [0.6757773330303758]\n",
      "0.6755730435289704\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[249]. Loss is [0.6755730435289704]\n",
      "0.6753694096978557\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[250]. Loss is [0.6753694096978557]\n",
      "0.6751666868432763\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[251]. Loss is [0.6751666868432763]\n",
      "0.6749643177227395\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[252]. Loss is [0.6749643177227395]\n",
      "0.6747616809764594\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[253]. Loss is [0.6747616809764594]\n",
      "0.6745597252983654\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[254]. Loss is [0.6745597252983654]\n",
      "0.6743590266653521\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[255]. Loss is [0.6743590266653521]\n",
      "0.674157604687066\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[256]. Loss is [0.674157604687066]\n",
      "0.6739568564195058\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[257]. Loss is [0.6739568564195058]\n",
      "0.6737567168240857\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[258]. Loss is [0.6737567168240857]\n",
      "0.673557485733683\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[259]. Loss is [0.673557485733683]\n",
      "0.67335775414097\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[260]. Loss is [0.67335775414097]\n",
      "0.6731586940589983\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[261]. Loss is [0.6731586940589983]\n",
      "0.6729601617183885\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[262]. Loss is [0.6729601617183885]\n",
      "0.6727626747562696\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[263]. Loss is [0.6727626747562696]\n",
      "0.6725646599632311\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[264]. Loss is [0.6725646599632311]\n",
      "0.6723673247731682\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[265]. Loss is [0.6723673247731682]\n",
      "0.6721704621084353\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[266]. Loss is [0.6721704621084353]\n",
      "0.6719742921049031\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[267]. Loss is [0.6719742921049031]\n",
      "0.6717787480398217\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[268]. Loss is [0.6717787480398217]\n",
      "0.6715829449043852\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[269]. Loss is [0.6715829449043852]\n",
      "0.671387826517053\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[270]. Loss is [0.671387826517053]\n",
      "0.6711931882912922\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[271]. Loss is [0.6711931882912922]\n",
      "0.6709991342049483\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[272]. Loss is [0.6709991342049483]\n",
      "0.6708058917638511\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[273]. Loss is [0.6708058917638511]\n",
      "0.6706123455860185\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[274]. Loss is [0.6706123455860185]\n",
      "0.6704194920988799\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[275]. Loss is [0.6704194920988799]\n",
      "0.6702271265128645\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[276]. Loss is [0.6702271265128645]\n",
      "0.6700352312979043\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[277]. Loss is [0.6700352312979043]\n",
      "0.6698438666217362\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[278]. Loss is [0.6698438666217362]\n",
      "0.6696539869582231\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[279]. Loss is [0.6696539869582231]\n",
      "0.6694638888655919\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[280]. Loss is [0.6694638888655919]\n",
      "0.6692744756601802\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[281]. Loss is [0.6692744756601802]\n",
      "0.6690855512265111\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[282]. Loss is [0.6690855512265111]\n",
      "0.6688971006187235\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[283]. Loss is [0.6688971006187235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.668709122933175\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[284]. Loss is [0.668709122933175]\n",
      "0.6685216185697843\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[285]. Loss is [0.6685216185697843]\n",
      "0.668334588158404\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[286]. Loss is [0.668334588158404]\n",
      "0.6681480324277275\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[287]. Loss is [0.6681480324277275]\n",
      "0.6679619521691671\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[288]. Loss is [0.6679619521691671]\n",
      "0.6677763482165426\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[289]. Loss is [0.6677763482165426]\n",
      "0.6675912214310966\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[290]. Loss is [0.6675912214310966]\n",
      "0.667406591863981\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[291]. Loss is [0.667406591863981]\n",
      "0.6672227315138486\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[292]. Loss is [0.6672227315138486]\n",
      "0.6670387782410031\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[293]. Loss is [0.6670387782410031]\n",
      "0.6668555284546096\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[294]. Loss is [0.6668555284546096]\n",
      "0.6666727788490628\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[295]. Loss is [0.6666727788490628]\n",
      "0.6664905156332657\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[296]. Loss is [0.6664905156332657]\n",
      "0.666308892851735\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[297]. Loss is [0.666308892851735]\n",
      "0.66612823067543\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[298]. Loss is [0.66612823067543]\n",
      "0.665948053524768\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[299]. Loss is [0.665948053524768]\n",
      "0.6657683594725438\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[300]. Loss is [0.6657683594725438]\n",
      "0.6655891484109839\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[301]. Loss is [0.6655891484109839]\n",
      "0.6654104206798224\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[302]. Loss is [0.6654104206798224]\n",
      "0.6652322244360805\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[303]. Loss is [0.6652322244360805]\n",
      "0.6650547053715743\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[304]. Loss is [0.6650547053715743]\n",
      "0.6648777151547155\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[305]. Loss is [0.6648777151547155]\n",
      "0.664701232536701\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[306]. Loss is [0.664701232536701]\n",
      "0.6645252525967014\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[307]. Loss is [0.6645252525967014]\n",
      "0.6643497736691036\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[308]. Loss is [0.6643497736691036]\n",
      "0.6641749806243896\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[309]. Loss is [0.6641749806243896]\n",
      "0.6640009795618828\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[310]. Loss is [0.6640009795618828]\n",
      "0.663827478228885\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[311]. Loss is [0.663827478228885]\n",
      "0.6636544737540118\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[312]. Loss is [0.6636544737540118]\n",
      "0.6634819650430838\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[313]. Loss is [0.6634819650430838]\n",
      "0.6633099514729639\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[314]. Loss is [0.6633099514729639]\n",
      "0.663138432578464\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[315]. Loss is [0.663138432578464]\n",
      "0.6629633019404504\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[316]. Loss is [0.6629633019404504]\n",
      "0.6627697532356636\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[317]. Loss is [0.6627697532356636]\n",
      "0.6625815878047348\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[318]. Loss is [0.6625815878047348]\n",
      "0.6623956279240031\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[319]. Loss is [0.6623956279240031]\n",
      "0.662210863813195\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[320]. Loss is [0.662210863813195]\n",
      "0.6620204034337372\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[321]. Loss is [0.6620204034337372]\n",
      "0.6618011160298599\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[322]. Loss is [0.6618011160298599]\n",
      "0.6615864713063537\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[323]. Loss is [0.6615864713063537]\n",
      "0.6613746493576009\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[324]. Loss is [0.6613746493576009]\n",
      "0.6611657447208379\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[325]. Loss is [0.6611657447208379]\n",
      "0.6609598606321998\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[326]. Loss is [0.6609598606321998]\n",
      "0.6607560791872773\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[327]. Loss is [0.6607560791872773]\n",
      "0.6605542423678747\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[328]. Loss is [0.6605542423678747]\n",
      "0.6603543898931553\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[329]. Loss is [0.6603543898931553]\n",
      "0.6601571172217624\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[330]. Loss is [0.6601571172217624]\n",
      "0.659961520462561\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[331]. Loss is [0.659961520462561]\n",
      "0.659767481068506\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[332]. Loss is [0.659767481068506]\n",
      "0.6595750154771494\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[333]. Loss is [0.6595750154771494]\n",
      "0.6593846285628782\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[334]. Loss is [0.6593846285628782]\n",
      "0.6591960496602173\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[335]. Loss is [0.6591960496602173]\n",
      "0.6590091930126545\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[336]. Loss is [0.6590091930126545]\n",
      "0.6588235504004882\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[337]. Loss is [0.6588235504004882]\n",
      "0.6586394255288966\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[338]. Loss is [0.6586394255288966]\n",
      "0.6584567496853871\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[339]. Loss is [0.6584567496853871]\n",
      "0.6582751447045225\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[340]. Loss is [0.6582751447045225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6580945878858757\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[341]. Loss is [0.6580945878858757]\n",
      "0.6579150791706809\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[342]. Loss is [0.6579150791706809]\n",
      "0.6577365398583435\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[343]. Loss is [0.6577365398583435]\n",
      "0.6575592349495618\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[344]. Loss is [0.6575592349495618]\n",
      "0.6573833483267154\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[345]. Loss is [0.6573833483267154]\n",
      "0.6572086603782911\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[346]. Loss is [0.6572086603782911]\n",
      "0.6570348275727265\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[347]. Loss is [0.6570348275727265]\n",
      "0.6568618244067262\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[348]. Loss is [0.6568618244067262]\n",
      "0.6566896301456664\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[349]. Loss is [0.6566896301456664]\n",
      "0.6565182263991364\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[350]. Loss is [0.6565182263991364]\n",
      "0.6563475964249151\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[351]. Loss is [0.6563475964249151]\n",
      "0.6561777248457987\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[352]. Loss is [0.6561777248457987]\n",
      "0.6560085974812379\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[353]. Loss is [0.6560085974812379]\n",
      "0.65584020121905\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[354]. Loss is [0.65584020121905]\n",
      "0.6556725239070306\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[355]. Loss is [0.6556725239070306]\n",
      "0.6555055542580749\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[356]. Loss is [0.6555055542580749]\n",
      "0.655339281766079\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[357]. Loss is [0.655339281766079]\n",
      "0.6551738569314589\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[358]. Loss is [0.6551738569314589]\n",
      "0.6550093290906308\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[359]. Loss is [0.6550093290906308]\n",
      "0.6548454686798033\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[360]. Loss is [0.6548454686798033]\n",
      "0.6546822660713203\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[361]. Loss is [0.6546822660713203]\n",
      "0.6545197131842405\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[362]. Loss is [0.6545197131842405]\n",
      "0.6543582204878989\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[363]. Loss is [0.6543582204878989]\n",
      "0.6541979496585496\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[364]. Loss is [0.6541979496585496]\n",
      "0.654038693773231\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[365]. Loss is [0.654038693773231]\n",
      "0.6538798823731147\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[366]. Loss is [0.6538798823731147]\n",
      "0.6537221848203707\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[367]. Loss is [0.6537221848203707]\n",
      "0.6535651587724695\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[368]. Loss is [0.6535651587724695]\n",
      "0.6534087798590874\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[369]. Loss is [0.6534087798590874]\n",
      "0.6532530306939394\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[370]. Loss is [0.6532530306939394]\n",
      "0.6530978970897025\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[371]. Loss is [0.6530978970897025]\n",
      "0.652943367257346\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[372]. Loss is [0.652943367257346]\n",
      "0.6527894312681934\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[373]. Loss is [0.6527894312681934]\n",
      "0.6526363061683251\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[374]. Loss is [0.6526363061683251]\n",
      "0.6524839757802945\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[375]. Loss is [0.6524839757802945]\n",
      "0.6523322144246925\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[376]. Loss is [0.6523322144246925]\n",
      "0.6521810146103371\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[377]. Loss is [0.6521810146103371]\n",
      "0.6520303708619415\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[378]. Loss is [0.6520303708619415]\n",
      "0.6518802784664336\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[379]. Loss is [0.6518802784664336]\n",
      "0.6517307331487177\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[380]. Loss is [0.6517307331487177]\n",
      "0.6515817309483276\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[381]. Loss is [0.6515817309483276]\n",
      "0.6514332681543503\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[382]. Loss is [0.6514332681543503]\n",
      "0.6512853412620763\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[383]. Loss is [0.6512853412620763]\n",
      "0.6511380365136248\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[384]. Loss is [0.6511380365136248]\n",
      "0.6509914801596676\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[385]. Loss is [0.6509914801596676]\n",
      "0.6508457358655789\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[386]. Loss is [0.6508457358655789]\n",
      "0.6507006032701176\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[387]. Loss is [0.6507006032701176]\n",
      "0.6505561237384169\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[388]. Loss is [0.6505561237384169]\n",
      "0.6504121568183611\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[389]. Loss is [0.6504121568183611]\n",
      "0.6502686973825693\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[390]. Loss is [0.6502686973825693]\n",
      "0.6501257421123653\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[391]. Loss is [0.6501257421123653]\n",
      "0.649983288171876\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[392]. Loss is [0.649983288171876]\n",
      "0.6498413329065028\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[393]. Loss is [0.6498413329065028]\n",
      "0.6496998737636464\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[394]. Loss is [0.6496998737636464]\n",
      "0.6495589082677939\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[395]. Loss is [0.6495589082677939]\n",
      "0.649418434009751\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[396]. Loss is [0.649418434009751]\n",
      "0.6492784486400393\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[397]. Loss is [0.6492784486400393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6491389498638513\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[398]. Loss is [0.6491389498638513]\n",
      "0.648999942566476\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[399]. Loss is [0.648999942566476]\n",
      "0.6488617387847905\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[400]. Loss is [0.6488617387847905]\n",
      "0.6487240158558154\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[401]. Loss is [0.6487240158558154]\n",
      "0.6485867692743031\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[402]. Loss is [0.6485867692743031]\n",
      "0.6484499962462907\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[403]. Loss is [0.6484499962462907]\n",
      "0.6483136944240141\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[404]. Loss is [0.6483136944240141]\n",
      "0.6481778616097975\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[405]. Loss is [0.6481778616097975]\n",
      "0.6480424956811315\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[406]. Loss is [0.6480424956811315]\n",
      "0.6479075945688388\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[407]. Loss is [0.6479075945688388]\n",
      "0.6477731562486506\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[408]. Loss is [0.6477731562486506]\n",
      "0.6476391787365475\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[409]. Loss is [0.6476391787365475]\n",
      "0.6475056600853912\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[410]. Loss is [0.6475056600853912]\n",
      "0.6473725983821478\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[411]. Loss is [0.6473725983821478]\n",
      "0.6472399917454623\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[412]. Loss is [0.6472399917454623]\n",
      "0.647107838323495\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[413]. Loss is [0.647107838323495]\n",
      "0.6469761362919636\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[414]. Loss is [0.6469761362919636]\n",
      "0.6468448838523639\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[415]. Loss is [0.6468448838523639]\n",
      "0.646714079230346\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[416]. Loss is [0.646714079230346]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6465837206742276\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[417]. Loss is [0.6465837206742276]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.646453806453634\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[418]. Loss is [0.646453806453634]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6463243348582466\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[419]. Loss is [0.6463243348582466]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.646195304196654\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[420]. Loss is [0.646195304196654]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.646066712795296\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[421]. Loss is [0.646066712795296]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6459385589974873\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[422]. Loss is [0.6459385589974873]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6458108411625204\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[423]. Loss is [0.6458108411625204]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6456835576648345\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[424]. Loss is [0.6456835576648345]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6455567068932482\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[425]. Loss is [0.6455567068932482]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6454302872502484\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[426]. Loss is [0.6454302872502484]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6453042971513329\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[427]. Loss is [0.6453042971513329]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6451787350243986\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[428]. Loss is [0.6451787350243986]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6450535993091744\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[429]. Loss is [0.6450535993091744]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6449288884566939\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[430]. Loss is [0.6449288884566939]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.644804600928804\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[431]. Loss is [0.644804600928804]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6446807351977084\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[432]. Loss is [0.6446807351977084]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6445572897455399\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[433]. Loss is [0.6445572897455399]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6444342630639637\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[434]. Loss is [0.6444342630639637]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.644311653653804\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[435]. Loss is [0.644311653653804]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6441894600246973\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[436]. Loss is [0.6441894600246973]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6440676806947662\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[437]. Loss is [0.6440676806947662]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.643946314190314\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[438]. Loss is [0.643946314190314]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6438253590455397\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[439]. Loss is [0.6438253590455397]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6437048138022686\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[440]. Loss is [0.6437048138022686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6435846770097009\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[441]. Loss is [0.6435846770097009]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6434649472241741\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[442]. Loss is [0.6434649472241741]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6433456230089409\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[443]. Loss is [0.6433456230089409]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6432267029339594\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[444]. Loss is [0.6432267029339594]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6431081855756945\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[445]. Loss is [0.6431081855756945]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6429900695169333\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[446]. Loss is [0.6429900695169333]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6428723533466085\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[447]. Loss is [0.6428723533466085]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6427550356596332\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[448]. Loss is [0.6427550356596332]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6426381150567446\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[449]. Loss is [0.6426381150567446]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.642521590144356\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[450]. Loss is [0.642521590144356]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6424054595344171\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[451]. Loss is [0.6424054595344171]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6422897218442828\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[452]. Loss is [0.6422897218442828]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6421743756965875\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[453]. Loss is [0.6421743756965875]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6420594197191275\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[454]. Loss is [0.6420594197191275]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6419448525447484\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[455]. Loss is [0.6419448525447484]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6418306728112407\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[456]. Loss is [0.6418306728112407]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.641716879161238\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[457]. Loss is [0.641716879161238]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6416034702421226\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[458]. Loss is [0.6416034702421226]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6414904447059354\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[459]. Loss is [0.6414904447059354]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.64137780120929\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[460]. Loss is [0.64137780120929]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6412655384132927\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[461]. Loss is [0.6412655384132927]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6411536549834644\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[462]. Loss is [0.6411536549834644]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6410421495896691\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[463]. Loss is [0.6410421495896691]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6409310209060433\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[464]. Loss is [0.6409310209060433]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6408202676109316\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[465]. Loss is [0.6408202676109316]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6407098883868241\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[466]. Loss is [0.6407098883868241]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.640599881920297\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[467]. Loss is [0.640599881920297]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6404902469019574\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[468]. Loss is [0.6404902469019574]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6403809820263887\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[469]. Loss is [0.6403809820263887]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6402720859921017\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[470]. Loss is [0.6402720859921017]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6401635575014862\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[471]. Loss is [0.6401635575014862]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6400553952607654\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[472]. Loss is [0.6400553952607654]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6399475979799525\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[473]. Loss is [0.6399475979799525]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6398401643728111\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[474]. Loss is [0.6398401643728111]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6397330931568151\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[475]. Loss is [0.6397330931568151]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6396263830531133\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[476]. Loss is [0.6396263830531133]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6395200327864937\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[477]. Loss is [0.6395200327864937]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6394140410853507\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[478]. Loss is [0.6394140410853507]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.639308406681655\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[479]. Loss is [0.639308406681655]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6392031283109225\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[480]. Loss is [0.6392031283109225]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6390982047121877\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[481]. Loss is [0.6390982047121877]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6389936346279766\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[482]. Loss is [0.6389936346279766]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.638889416804282\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[483]. Loss is [0.638889416804282]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6387856173056544\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[484]. Loss is [0.6387856173056544]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6386829168481953\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[485]. Loss is [0.6386829168481953]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6385798489587469\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[486]. Loss is [0.6385798489587469]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6384776561593076\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[487]. Loss is [0.6384776561593076]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6383756284157168\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[488]. Loss is [0.6383756284157168]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.638273789234653\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[489]. Loss is [0.638273789234653]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6381728939812013\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[490]. Loss is [0.6381728939812013]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.638071721086673\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[491]. Loss is [0.638071721086673]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6379711625392465\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[492]. Loss is [0.6379711625392465]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6378710272409116\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[493]. Loss is [0.6378710272409116]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6377715809991884\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[494]. Loss is [0.6377715809991884]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6376719714666085\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[495]. Loss is [0.6376719714666085]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6375745791568954\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[496]. Loss is [0.6375745791568954]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6374775467847811\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[497]. Loss is [0.6374775467847811]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6373808553764309\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[498]. Loss is [0.6373808553764309]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n",
      "0.6372845008236857\n",
      "INFO:network.test_040_two_layer_classifier:network.test_040_two_layer_classifier: iteration[499]. Loss is [0.6372845008236857]\n",
      "WARNING:SGD:SGD[SGD].update(): Gradient descent potentially stalling with dW < W/100.\n"
     ]
    }
   ],
   "source": [
    "MAX_TEST_TIMES = 500\n",
    "\n",
    "W1 = weights.uniform(M, D)\n",
    "W2 = weights.uniform(M, M)\n",
    "optimizer = SGD(lr=0.3)\n",
    "\n",
    "# W = train_classifier(\n",
    "W1, W2 = train_two_layer_classifier(\n",
    "    N=N,\n",
    "    D=D,\n",
    "    M=M,\n",
    "    X=X,\n",
    "    T=T,\n",
    "    W1=W1,\n",
    "    W2=W2,\n",
    "    log_loss_function=softmax_cross_entropy_log_loss,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=MAX_TEST_TIMES,\n",
    "    test_numerical_gradient=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = X@W1.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,5)) \n",
    "for i in range(2):\n",
    "    ax.set_xlabel('x label')\n",
    "    ax.set_ylabel('y label')\n",
    "    ax.axis('equal')\n",
    "    ax.grid()\n",
    "\n",
    "ax.set_title(\"Predictions\")\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(-1, 1)\n",
    "\n",
    "x_grid, y_grid, predictions = prediction_grid(Z, W2)\n",
    "plot_categorical_predictions(ax, [x_grid, y_grid], X, Y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A NOT B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import (\n",
    "    set_in_a_radius,\n",
    "    set_in_A_not_B,\n",
    "    sets_of_circle_A_not_B\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 3\n",
    "N = 300\n",
    "radius = 1\n",
    "circles, centres = sets_of_circle_A_not_B(radius=radius, ratio=1.2, m=M, n=N)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6)) \n",
    "ax.set_xlabel('x label')\n",
    "ax.set_ylabel('y label')\n",
    "ax.axis('equal')\n",
    "ax.grid()\n",
    "r = np.linspace(0, 2*np.pi, 100)\n",
    "\n",
    "for i in range(M):\n",
    "    circle = circles[i]\n",
    "    if circle.size > 0:\n",
    "        x = centres[i][0]\n",
    "        y = centres[i][1]\n",
    "        ax.scatter(circle[::, 0], circle[::, 1], color=COLOR_LABELS[i])\n",
    "        ax.plot(\n",
    "            x + radius * np.cos(r), \n",
    "            y + radius * np.sin(r), \n",
    "            linestyle='dashed', \n",
    "            color=COLOR_LABELS[i]\n",
    "        )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
