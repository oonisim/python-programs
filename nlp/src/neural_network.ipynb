{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network \n",
    "Simple one layer neural network classifier. Mathjax formula not fully supported in github, hence the formulas get corrupted.\n",
    "\n",
    "<img src=\"image/nn_diagram.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/nn_functions.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python path\n",
    "Python path setup to avoid the relative imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1    # Batch size\n",
    "D = 3    # Number of features in the input data\n",
    "M = 2    # Number of nodes in a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confiurations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.classifications import (\n",
    "    linear_separable\n",
    ")\n",
    "X, T, V = linear_separable(d=2, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f74f8433520>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[T==0, 1], X[T==0, 2], c='red')\n",
    "plt.scatter(X[T==1, 1], X[T==1, 2], c='green')\n",
    "\n",
    "# Hyperplace (X-b)V = 0 -> x1V1 + x2V2 - bV2 = 0\n",
    "x = np.linspace(-3,3,100)\n",
    "y = -(V[1] / V[2]) * x - (V[0] / V[2])\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T\n",
    "Labels for data X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For One Hot Encoding labels\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ T_{_{OHE}} } &= ( \n",
    "    \\overset{ (M,) }{ T_{(n=0)} }, \\; \\dots \\;, \\overset{ (M,) }{ T_{(n=N-1)} } \n",
    ") \n",
    "\\\\\n",
    "\\overset{ (N,M) }{ T_{_{OHE}} } = ( \n",
    "    \\overset{ (M,) }{ T_{(n=0)} }, \\dots , \\overset{ (M,) }{ T_{(n=N-1)} } \n",
    ") \n",
    "\\\\\n",
    "\\overset{ (M,) }{ T_{ _{OHE} (n)} } &= ( \\overset{ () }{ t_{(n)(m=0)} }, \\; \\dots \\;, \\overset{ () }{ t_{(n)(m=M-1)} })\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "### For index labels\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,) }{ T_{_{IDX}} } &= (\\overset{ () }{ t_{(n=0)} }, \\; \\dots \\;, \\overset{ () }{ t_{(n=N-1)} }) \\qquad \\text {for index labels }\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W\n",
    "Weight parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.weights import (\n",
    "    xavier,\n",
    "    he,\n",
    "    uniform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matmul layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer.matmul import Matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ Y } \n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "{ Y_{(n=0)} } \\\\\n",
    "\\vdots \\\\\n",
    "{ Y_{(n)} } \\\\\n",
    "\\vdots \\\\\n",
    "{ Y_{(n=N-1)} }\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\overset{ (N,D) }{ X } \\; @ \\; \\overset{ (D,M) }{ W^T }\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (M,) }{ Y_{(n)} } &= (y_{(n)(m=0)}, \\; \\dots, \\; y_{(n)(m)},  \\; \\dots, \\; y_{(n)(m=M-1)})\n",
    "\\\\\n",
    "\\overset{ () }{ y_{(n)(m)} }\n",
    "&= \\overset{ (D,) }{ X_{(n)} } \\cdot \\overset{ (D,) }{ W_{(m)}^T }\n",
    "= \\sum\\limits ^{D}_{d=0}  \\overset{ () }{ x_{(n)(d)} } * \\overset{ () }{ w_{(m)(d)} }\n",
    "\\\\\n",
    "_{(0 \\le d \\le D, \\; 0 \\le n \\lt N, \\; 0 \\le m \\lt M)}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dX\n",
    "\n",
    "Impact on L by $dX$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,D) }{ \\frac {\\partial L }{ \\partial X } }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial Y} }\n",
    "@ \\overset { (M,D) }{ W } \n",
    "\\end{align*}\n",
    "$\n",
    "<img src=\"image/nn_back_propagation_dL_dX.png\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dW.T\n",
    "Impact on L by $dW^T$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{ \\partial W^T } }\n",
    "= \\overset { (D,N) }{ X^T } \n",
    "@ \n",
    "\\overset { (N,M) }{ \\frac {\\partial L}{\\partial Y} }\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "<img src=\"image/nn_back_propagation_dL_dWT.png\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ A } &= \n",
    "activation \\left( \n",
    "    \\overset{ (N,M) }{ Y }  = \n",
    "    \\begin{bmatrix}\n",
    "    { Y_{(n=0)} } \\\\\n",
    "    \\vdots \\\\\n",
    "    { Y_{(n)} } \\\\\n",
    "    \\vdots \\\\\n",
    "    { Y_{(n=N-1)} }\n",
    "    \\end{bmatrix}\n",
    "\\right)\n",
    "\\\\\n",
    "\\overset{ (M,) }{ A_{(n)} } \n",
    "&= activation \\left( \\overset{ (M,) }{ Y_{(n) }} \\right)  \\\\\n",
    "&= (a_{(n)(m=0)}, \\; \\dots, \\; a_{(n)(m)},  \\; \\dots, \\; a_{(n)(m=M-1)})\n",
    "\\\\\n",
    "\\overset{ () }{ a_{(n)(m)} } &= activation \\left( \\overset{ () }{ y_{(n)(m)} } \\right)\n",
    "\\quad _{(0 \\le n \\lt N, \\; 0 \\le m \\lt M)}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dY\n",
    "\n",
    "Impact on L by dY from the matmul layer.\n",
    "\n",
    "$\n",
    "\\begin {align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{ \\partial Y } }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial A} } \n",
    "* \n",
    "\\overset { (N,M) }{ \\frac {\\partial A}{\\partial Y} }\n",
    "\\end {align*}\n",
    "$\n",
    "\n",
    "### For sigmoid activation\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset {(N,M)}{\\frac { \\partial L }{ \\partial Y} }\n",
    "&= \\frac { \\partial A }{ \\partial Y} * A * (1 - A)\n",
    "\\\\\n",
    "\\frac { \\partial y_{(n)(m)} } { \\partial a_{(n)(m)} }\n",
    "&= a_{(n)(m)} * (1 - a_{(n)(m)} )  \\\\ \n",
    "y_{(n)(m)} = sigmoid(a_{(n)(m)} )&=  \\frac {1}{ 1 + exp(y_{(n)(m)})}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "### For ReLU activation\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac { \\partial a_{(n)(m)} }{ \\partial y_{(n)(m)} }\n",
    "&= 1 \\quad y_{(n)(m)}  \\gt 0 \\\\\n",
    "&= 0 \\quad y_{(n)(m)}  \\le 0 \\\\\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax layer\n",
    "$C_n$ is to prevent the overflow at $np.exp()$.\n",
    "\n",
    "<img src=\"image/softmax.png\" align=\"left\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax and Cross Entropy Log Loss are combined as the gradient results in a simple form $P - T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def softmax(X: Union[np.ndarray, float]) -> Union[np.ndarray, float]:\n",
      "    \"\"\"Softmax P = exp(X) / sum(exp(X))\n",
      "    Args:\n",
      "        X: batch input data of shape (N,M).\n",
      "            N: Batch size\n",
      "            M: Number of nodes\n",
      "    Returns:\n",
      "        Probability P of shape (N,M)\n",
      "    \"\"\"\n",
      "    assert X.dtype == float\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # exp(x-c) to prevent the infinite exp(x) for a large value x, with c = max(x).\n",
      "    # keepdims=True to be able to broadcast.\n",
      "    # --------------------------------------------------------------------------------\n",
      "    C = np.max(X, axis=-1, keepdims=True)   # オーバーフロー対策\n",
      "    exp = np.exp(X - C)\n",
      "    return exp / np.sum(exp, axis=-1, keepdims=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from layer import SoftmaxWithLogLoss\n",
    "from common import softmax\n",
    "\n",
    "lines = inspect.getsource(softmax)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,1) }{ C } &= np.max\\left( \n",
    "    \\overset{ (N,M) }{ A }, \\; axis=-1,  \\; keepdim=True \n",
    "\\right) \\\\\n",
    "&=  \n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n=0  )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n    )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ c_{(n=N-1)} } \\end{bmatrix}\n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\overset{ (N,M) }{ EXP } &= np.exp \\left( \\overset{ (N,M) }{ A } - \\overset{ (N,1) }{ C } \\right)\n",
    "= np.exp \\left(\n",
    "    \\begin{bmatrix}\n",
    "    { A_{(n=0)} } - { C_{(n=0)} }\\\\\n",
    "    \\vdots \\\\\n",
    "    { A_{(n)} }   - { C_{(n)} }\\\\\n",
    "    \\vdots \\\\\n",
    "    { A_{(n=N-1)} } - { C_{(n=N-1)} }\\\\\n",
    "    \\end{bmatrix}\n",
    "\\right) \n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "    e_{(n=0)(m=0)}   & \\dots      & e_{(n=0)(m=M-1)}   \\\\  \n",
    "    \\vdots           & e_{(n)(m)} & \\vdots             \\\\\n",
    "    e_{(n=N-1)(m=0)} & \\dots      & e_{(n=N-1)(m=M-1)} \n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,1) }{ S } &= \\overset{ (N,1) }{ sum(EXP) } = np.sum \\left( \n",
    "    \\overset{ (N,M) }{ EXP }, \\; axis=-1,  \\; keepdim=True \n",
    "\\right)\n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n=0  )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n    )} } \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} \\overset{ () }{ s_{(n=N-1)} } \\end{bmatrix}\n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "\\overset{ () }{ s_{(n)} } &= \\sum\\limits ^{M-1}_{m=0} np.exp(\\; a_{(n)(m)} - c_{(n)} \\; )\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,M) }{ P } &= \\overset{ (N,M) }{ EXP }  \\;\\; / \\;\\; \\overset{ (N,1) }{ sum(EXP) } \n",
    "\\\\\n",
    "\\overset{ (N,) }{ P_{(n)} } &= (p_{(n)(m=0)}, \\; \\dots, \\; p_{(n)(m)} , \\; \\dots, \\; p_{(n)(m=M-1)})\n",
    "\\\\\n",
    "{ p_{(n)(m)} } \n",
    "&= \\frac {np.exp \\left( \n",
    "    { a_{(n)(m) } } - { c_{(n)} }) \\right) \n",
    "}\n",
    "{  \n",
    "np.sum \\left( \n",
    "    np.exp \\left( \n",
    "        a_{(n)(m) } - c_{(n)}\n",
    "    \\right)\n",
    "\\right) \n",
    "}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dL/dA\n",
    "\n",
    "Impact on L by dA from the activation layer.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac {\\partial L }{\\partial A} }\n",
    "= \\overset { (N,M) }{ \\frac {\\partial L}{\\partial P} }\n",
    "* \n",
    "\\overset { (N,M) }{ \\frac {\\partial P }{\\partial A} } \n",
    "= \n",
    "\\frac {1}{N} (P - T)\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "$\n",
    "Jacobian \\; : \\; f \\circ g \\rightarrow Jf \\circ Jg\n",
    "\\\\\n",
    "\\begin{align*}\n",
    "\\\\\n",
    "L &= f(\\; p_{(n)(m=0)} \\;) = f( \\; g(\\;  a_{(n)(m=0)} \\; ) \\; ) \\quad : p = g(a) = softmax(a)\n",
    "\\\\\n",
    "\\frac {\\partial L} { \\partial a_{(n)(m=0)} }\n",
    "&= Jf(p) \\circ Jg(a) \n",
    "=  \\frac {\\partial L} { \\partial p_{(n)(m=0)} } * \\frac {\\partial  p_{(n)(m=0)}} { \\partial a_{(n)(m=0)} }\n",
    "\\\\\n",
    "&= \\frac {1}{N} \\left(\n",
    " p_{(n)(m=0)} -t_{(n)(m=0)}\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient from cross entropy log loss\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac { \\partial L }{ \\partial  p_{(n)(m=0)} }\n",
    "&= \\frac{-1}{N} t_{(n)(m=0)} * \\frac {s_{(n)}}{e_{(n)(m=0)}}\n",
    "\\\\\n",
    "\\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "&= \\frac{-1}{N} t_{(n)(m=1)} * \\frac {s_{(n)}}{e_{(n)(m=1)}}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Gradient $\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "p_{(n)(m=0)} &= \\frac {e_{(n)(m=0)} }{ s_{(n)} } \\\\\n",
    "p_{(n)(m=0)} &= f \\circ g_{(m=0)} = { s^{-1}_{(n)} } \\; * \\; { e_{(n)(m=0)} }\n",
    "\\rightarrow \\frac { \\partial  p_{(n)(m=0)} } { \\partial s^{-1}_{(n)} } = e_{(n)(m=0)}\n",
    "\\\\\n",
    "p_{(n)(m=1)} &= \\frac {e_{(n)(m=1)} }{ s_{(n)} } \\\\\n",
    "p_{(n)(m=1)} &= f \\circ g_{(m=1)} = { s^{-1}_{(n)} } \\; * \\; { e_{(n)(m=1)} }\n",
    "\\rightarrow \\frac { \\partial  p_{(n)(m=1)} } { \\partial s^{-1}_{(n)} } = e_{(n)(m=1)}\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "\\left[\n",
    "    \\frac { \\partial  p_{(n)(m=0)} } { \\partial s^{-1}_{(n)} } * \n",
    "    \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \n",
    "    +\n",
    "    \\frac { \\partial  p_{(n)(m=1)} } { \\partial s^{-1}_{(n)} } * \n",
    "    \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "\\right]\n",
    "\\\\\n",
    "&= \\sum\\limits^{M-1}_{m=0} \n",
    "    e_{(n)(m)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m)} } \n",
    "\\\\\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "    \\begin{bmatrix}\n",
    "    e_{(n)(m=0)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \\\\\n",
    "    + \\\\\n",
    "    e_{(n)(m=1)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "    \\end{bmatrix}\n",
    "\\\\\n",
    "&= -s_{(n)}(\\; t_{(n)(m=0)} + t_{(n)(m=1)} \\;) \\\\\n",
    "&= -s_{(n)}\n",
    "\\\\\n",
    "\\frac { \\partial  L } { \\partial s^{-1}_{(n)} } \n",
    "&=\n",
    "\\left[\n",
    "    e_{(n)(m=0)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=0)} } \n",
    "    + \n",
    "    e_{(n)(m=1)}  * \\frac { \\partial L }{ \\partial  p_{(n)(m=1)} } \n",
    "\\right]\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient $\\frac {\\partial L }{ \\partial { s_{(n)} } } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac {1} { s_{(n)} } &= s^{-1}_{(n)} \\rightarrow\n",
    "\\frac { \\partial { s^{-1}_{(n)} } } {\\partial s_{(n)}} = \\frac {-1}{s^{2}_{(n)}}\n",
    "\\\\\n",
    "\\frac {\\partial L }{ \\partial { s_{(n)} } } \n",
    "&=\n",
    "\\frac {-1}{s^{2}_{(n)}} * \n",
    "\\frac {\\partial L}{ \\partial s^{-1}_{(n)} } \\\\\n",
    "&= \\frac {1}{s_n}\n",
    "\\end{align*} \\\\\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient $\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } $\n",
    "$\n",
    "\\begin{align*}\n",
    "s_{(n)} &= \\sum\\limits ^{M-1}_{m=0} e_{(n)(m)} \\rightarrow \n",
    "\\frac { \\partial { s_{(n)} } } {\\partial e_{(n)(m=0)}} = 1\n",
    "\\\\\n",
    "p_{(n)(m=0)} &= \\frac {e_{(n)(m=0)} }{ s_{(n)} }\\rightarrow \n",
    "\\frac { \\partial { p_{(n)(m=0)} } } {\\partial e_{(n)(m=0)}} = \\frac {1}{s_{(n)}}\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } \n",
    "&= \\begin{bmatrix}  \n",
    "    \\frac { \\partial { s_{(n)} } } {\\partial e_{(n)(m=0)}} *  \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \\\\\n",
    "    + \\\\\n",
    "    \\frac { \\partial { p_{(n)(m=0)} } } {\\partial e_{(n)(m=0)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}  \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \\\\\n",
    "    + \\\\\n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } &= \n",
    "\\left[\n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} } \n",
    "    + \n",
    "    \\frac {\\partial L }{ \\partial { s_{(n)} } } \n",
    "\\right]\n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial e_{(n)(m=0)} } &= \n",
    "\\begin{bmatrix}  \n",
    "    \\frac {1}{s_{(n)}} * \n",
    "    \\frac {\\partial L }{ \\partial p_{(n)(m=0)} }  \\\\\n",
    "    +  \\\\\n",
    "    \\frac {\\partial L }{ \\partial s_{(n)} } \n",
    "\\end{bmatrix} \\\\\n",
    "&= \\frac {-t_{(n)(m=0)}}{e_{(n)(m=0)} } + \\frac {1}{s_{n}}\n",
    "\\end{align*}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gardient $\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } $\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "e_{(n)(m)} &= exp(\\; a_{(n)(m)} \\; ) \\rightarrow \\frac { \\partial e_{(n)(m)} }{ \\partial a_{(n)(m)} } = e_{(n)(m)} \n",
    "\\\\\n",
    "e_{(n)(m=0)} &= exp(a_{(n)(m=0)}) \\rightarrow \\frac { \\partial e_{(n)(m=0)} }{ \\partial a_{(n)(m=0)} } = e_{(n)(m=0)} \n",
    "\\\\\n",
    "e_{(n)(m=1)} &= exp(a_{(n)(m=1)}) \\\\\n",
    "\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } \n",
    "&=   \\frac { \\partial e_{(n)(m=0)} }{ \\partial a_{(n)(m=0)} } * \n",
    "    \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \n",
    "= e_{(n)(m=0)} * \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \n",
    "\\\\\n",
    "\\frac {\\partial L}{ \\partial a_{(n)(m=0)} } \n",
    "&= e_{(n)(m=0)} * \\frac { \\partial L }{ \\partial e_{(n)(m=0)} } \\\\\n",
    "&= -t_{(n)(m=0)} + \\frac { e_{(n)(m=0)} }{ s_{n} } \\\\\n",
    "&= p_{(n)(m=0)} -t_{(n)(m=0)} \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Log Loss\n",
    "\n",
    "A probability distribution $P(x)$ can be represented with its entropy $E(x) = \\sum\\limits_{x}  \\frac {p(x)}{log(p(x)} = - \\sum\\limits_{x} p(x) log(p(x))$. In the diagram, x: (0:dog, 1:cat, 2:fish, 3:bird) are labels and p(dog) is 0.5. When  a NN predicts an input x as a probability distribution $P(x)$, then the $E(x) = 1.75$. \n",
    "\n",
    "0. $p(dog)=\\frac {1}{2}$\n",
    "1. $p(cat)=\\frac {1}{4}$\n",
    "2. $p(fish)=\\frac {1}{8}$\n",
    "3. $p(bird)=\\frac {1}{8}$\n",
    "\n",
    "When the truth is that x is a dog, then the probability distribution of the truth $P(t)$ has the entropy $E(t) = 0$.\n",
    "\n",
    "0. $p(dog)=1$\n",
    "1. $p(cat)=0$\n",
    "2. $p(fish)=0$\n",
    "3. $p(bird)=0$\n",
    "\n",
    "The difference E(x) - E(t) = E(x) = 1.75 can be used as the distance or the error of the prediction from the truth. Need to understand further but  the actuall loss function is $E(x) = -tlog(p(x)) = -log(p(x))$ where p(x) is the probability from the softmax for the correct label.\n",
    "\n",
    "\n",
    "<img src=\"image/entropy.png\" align=\"left\" width=600/><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.log() is ln based on the mathematical constant $e$ and its derivative $\\frac {\\partial log(x)}{\\partial x} = \\frac {1}{x}$.\n",
    "\n",
    "* [Logarithm](https://en.wikipedia.org/wiki/Logarithm)\n",
    "\n",
    "\n",
    "<img src=\"image/logarithm_plots.png\" align=\"left\" width=300/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cross_entropy_log_loss(\n",
      "        P: Union[np.ndarray, float],\n",
      "        T: Union[np.ndarray, int],\n",
      "        offset: float = OFFSET_FOR_LOG\n",
      ") -> np.ndarray:\n",
      "    \"\"\"Cross entropy log loss [ -t(n)(m) * log(p(n)(m)) ] for multi labels.\n",
      "    Assumption:\n",
      "        Label is integer 0 or 1 for an OHE label and any integer for an index label.\n",
      "\n",
      "    NOTE:\n",
      "        Handle only the label whose value is True. The reason not to use non-labels to\n",
      "        calculate the loss is TBD.\n",
      "\n",
      "    Args:\n",
      "        P: probabilities of shape (N,M) from soft-max layer where:\n",
      "            N is Batch size\n",
      "            M is Number of nodes\n",
      "        T: label either in OHE format of shape (N,M) or index format of shape (N,).\n",
      "           OHE: One Hot Encoding\n",
      "        offset: small number to avoid np.inf by log(0) by log(0+offset)\n",
      "    Returns:\n",
      "        J: Loss value of shape (N,), a loss value per batch.\n",
      "    \"\"\"\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # For scalar values, calculate -t * log(p) and return\n",
      "    # --------------------------------------------------------------------------------\n",
      "    if isinstance(P, (float, int)) or (isinstance(P, np.ndarray) and P.ndim == 0):\n",
      "        assert isinstance(T, (float, int)) \\\n",
      "                or (isinstance(P, np.ndarray) and P.ndim == 0), \\\n",
      "                \"T type(%s) needs scalar when P is scalar\" % type(T)\n",
      "        return -T * np.log(P+offset)\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # Handle P/T as (N,M) matrix to use P[[N], [M]] to select T=1 elements.\n",
      "    # --------------------------------------------------------------------------------\n",
      "    assert P.dtype == float\n",
      "    if P.ndim <= 1:\n",
      "        T = T.reshape(1, T.size)\n",
      "        P = P.reshape(1, P.size)\n",
      "\n",
      "    # Label is integer\n",
      "    T = T.astype(int)\n",
      "    assert T.shape[0] == P.shape[0], \\\n",
      "        f\"Batch size of T {T.shape[0]} and P {P.shape[0]} should be the same.\"\n",
      "\n",
      "    N = batch_size = P.shape[0]\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # Convert OHE format into index label format of shape (N,).\n",
      "    # T in OHE format has the shape (N,M) with P, hence same size N*M.\n",
      "    # --------------------------------------------------------------------------------\n",
      "    if T.size == P.size:\n",
      "        T = T.argmax(axis=1)\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # Calculate Cross entropy log loss -t * log(p).\n",
      "    # Select an element P[n][t] at each row n which corresponds to the true label t.\n",
      "    # Use the Numpy tuple indexing. e.g. P[n=0][t=2] and P[n=3][t=4].\n",
      "    # P[\n",
      "    #   (0, 3),\n",
      "    #   (2, 4)     # The tuple sizes must be the same at all axes\n",
      "    # ]\n",
      "    #\n",
      "    # Tuple indexing selects only one element per row.\n",
      "    # Beware the numpy behavior difference between P[(n),(m)] and P[[n],[m]].\n",
      "    # https://stackoverflow.com/questions/66269684\n",
      "    # P[1,1]  and P[(0)(0)] results in a scalar value, HOWEVER, P[[1],[1]] in array.\n",
      "    #\n",
      "    # P shape can be (1,1), (1, M), (N, 1), (N, M), hence P[rows, cols] are:\n",
      "    # P (1,1) -> P[rows, cols] results in a 1D of  (1,).\n",
      "    # P (1,M) -> P[rows, cols] results in a 1D of  (1,)\n",
      "    # P (N,1) -> P[rows, cols] results in a 1D of  (N,)\n",
      "    # P (N,M) -> P[rows, cols] results in a 1D of  (N,)\n",
      "    #\n",
      "    # J shape matches with the P[rows, cols] shape.\n",
      "    # --------------------------------------------------------------------------------\n",
      "    rows = np.arange(N)     # 1D tuple index for rows\n",
      "    cols = T                # 1D tuple index for columns\n",
      "    assert rows.ndim == cols.ndim == 1 and rows.size == cols.size, \\\n",
      "        f\"np tuple indices size need to be same but rows {rows.size} cols {cols.size}.\"\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # Log loss per batch. Log(0+e) prevents the infinitive value log(0).\n",
      "    # NOTE: numerical gradient check.\n",
      "    #   Numerical gradient calculate f(x+/-h) with a small h e.g. 1e-5.\n",
      "    #   However, when x=0 and h >> e, f(0-h)=log(e-h) is np.nan as because x in log(x)\n",
      "    #   cannot be < 0.\n",
      "    # --------------------------------------------------------------------------------\n",
      "    assert np.all((P[rows, cols] + offset) > 0), \\\n",
      "        \"x for log(x) needs to be > 0 but %s.\" % (P[rows, cols] + offset)\n",
      "\n",
      "    _P = P[rows, cols]\n",
      "    Logger.debug(\"P[rows, cols] is %s\" % P[rows, cols])\n",
      "    J = -np.log(_P + offset)\n",
      "    assert not np.all(np.isnan(J))\n",
      "\n",
      "    Logger.debug(\"P.shape %s\", P.shape)\n",
      "    Logger.debug(\"P[rows, cols].shape %s\", P[rows, cols].shape)\n",
      "    Logger.debug(\"N is [%s]\", N)\n",
      "    Logger.debug(\"J is [%s]\", J)\n",
      "    Logger.debug(\"J.shape %s\\n\", J.shape)\n",
      "\n",
      "    assert 0 < N == J.shape[0], \\\n",
      "        \"Loss J.shape is expected to be (%s,) but %s\" % (N, J.shape)\n",
      "    return J\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from common import (\n",
    "    cross_entropy_log_loss,\n",
    "    OFFSET_FOR_LOG\n",
    ")\n",
    "lines = inspect.getsource(cross_entropy_log_loss)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For label using One Hot Encoding (OHE)\n",
    "For instance, if multi labels are (0,1,2,3,4) and each label is OHE, then the label for 2 is (0,0,1,0,0).   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product of matrix rows\n",
    "\n",
    "There is no formal operation to calculate the dot products of the rows from two matrices, but to calculate the diagonal of the matlix multiplication that also calculate non-diagonals. To avoid calculating non-diagonals, use [einsum](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html).\n",
    "\n",
    "* [Name of matrix operation of ```[A[0] dot B[0], A[1] dot B[1] ]``` from 2x2 matrices A, B](https://math.stackexchange.com/questions/4010721/name-of-matrix-operation-of-a0-dot-b0-a1-dot-b1-from-2x2-matrices-a)\n",
    "\n",
    "<img src=\"image/dot_products_of_matrix_rows.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is \n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "b.T is \n",
      "[[ 0 -3]\n",
      " [-1 -4]\n",
      " [-2 -5]]\n",
      "\n",
      "c[\n",
      "    np.inner(a[0], b[0]),\n",
      "    np.inner(a[1], b[1]),    \n",
      "] is [-5, -50]\n",
      "\n",
      "\n",
      "np.einsum('ij,ji->i', a, b.T) is [ -5 -50]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(6).reshape(2,3)\n",
    "b = np.arange(0,-6,-1).reshape(2,3)\n",
    "c = [\n",
    "    np.inner(a[0], b[0]),\n",
    "    np.inner(a[1], b[1]),    \n",
    "]\n",
    "print(f\"a is \\n{a}\")\n",
    "print(f\"b.T is \\n{b.T}\\n\")\n",
    "fmt=f\"\"\"c[\n",
    "    np.inner(a[0], b[0]),\n",
    "    np.inner(a[1], b[1]),    \n",
    "] is {c}\\n\n",
    "\"\"\"\n",
    "print(fmt)\n",
    "\n",
    "# Use einsum\n",
    "e = np.einsum('ij,ji->i', a, b.T)\n",
    "fmt=\"np.einsum('ij,ji->i', a, b.T)\"\n",
    "print(f\"{fmt} is {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foward path (OHE)\n",
    "$\n",
    "\\text{ for one hot encoding labels }\n",
    "\\\\\n",
    "\\begin{align*}\n",
    "\\overset{ (N,) }{ J } &= - \\sum\\limits^{M-1}_{m=0} \n",
    "    \\left[ \\; \\;  \n",
    "        t_{(n)(m)} \\;  * \\;  np.log(p_{(n)(m)}) \\;\\;  \n",
    "    \\right]\n",
    "\\\\\n",
    "\\overset{ () }{ j_{(n)} } &= \\overset{ (M,) }{ T_{(n)} } \\cdot \\overset{ (M,) }{ P_{(n)} } \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient dL/dP\n",
    "\n",
    "Impact on L by the $dP$ from the softmax layer for one hot encoding labels.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,M) }{ \\frac { \\partial L }{ \\partial P} }\n",
    "&= \\overset { (N,) }{ \\frac { \\partial L }{ \\partial J} } * \n",
    "\\overset { (N,M) }{ \n",
    "\\left(\n",
    " - \\frac { \\partial T } { \\partial P }\n",
    " \\right) \n",
    "} \n",
    "= - \\frac {1}{N }  \\frac { \\partial T } { \\partial P }\n",
    "\\\\\n",
    "\\frac {\\partial L }{\\partial p_{(n)(m=0)}} \n",
    "&= \\frac {\\partial L}{\\partial j_{(n)}} * \\frac {\\partial j_{(n)}} {\\partial p_{(n)(m=0)}} \n",
    "= \\frac {1}{N} \\frac { -t_{(n)(m=0)}}{ p_{(n)(m=0)} } \n",
    "=  \\frac {1}{N} \\left(\n",
    " -t_{(n)(m=0)} * \\frac { s_{(n)} }{ e_{(n)(m=0)} }\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For label using indexing \n",
    "For instance, if the multi labels are (0,1,2,3,4) then the index is 2 for the label 2. If the labels are (2,4,6,8,9), then the index is 3 for the label 8.  \n",
    "\n",
    "Use LP to select the probabilities from P for the corresponding labels. For instance, if the label is 2 (hence the index is 2) for X(n=0), and 4 for X(n=3), then the numpy tuple indexing selects ```P[n=0][m=2]``` and ```P[n=3][m=4] ```.\n",
    "\n",
    "```\n",
    "P[\n",
    "   (0, 3),\n",
    "   (2, 4)\n",
    "]\n",
    "```\n",
    "\n",
    "$\n",
    "\\text{ for index labels e.g. (5, 2, 0, 9, ...)}\n",
    "\\\\\n",
    "\\\\\n",
    "\\overset{ (N,) }{ J } = - np.sum(\\; np.log(LP), \\; axis = -1 \\;) \\\\\n",
    "LP = label\\_probability = P \\left[ \\\\\n",
    "\\quad ( \\; 0, \\; \\dots, \\;  {N-1}) , \\\\\n",
    "\\quad ( \\; t_{(n=0)} \\; , \\dots , \\; t_{(n=N-1)}) \\\\\n",
    "\\right]\n",
    "\\\\\n",
    "\\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward path\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ () }{ L } = \\frac {1}{N} \\sum\\limits^{N-1}_{n=0} \\overset{ () }{ j_{{(n)}} }\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gardient dL/dJ\n",
    "\n",
    "Impact on L by $dJ$ from the cross entropy log loss layer.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset { (N,) }{ \\frac {\\partial L}{\\partial J} }  &= \\frac {1}{N} \\overset{(N,)}{ones}\n",
    "\\\\\n",
    "\\frac {\\partial L}{\\partial j_{(n)} } &= \\frac {1}{N} \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ = np.ones(N) / N\n",
    "dJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [two_layer_net.ipynb defines the lambda with parameter W which is redundant #254](https://github.com/cs231n/cs231n.github.io/issues/254)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def numerical_jacobian(\n",
      "        f: Callable[[np.ndarray], np.ndarray],\n",
      "        X: Union[np.ndarray, float],\n",
      "        delta: float = OFFSET_FOR_DELTA\n",
      ") -> np.ndarray:\n",
      "    \"\"\"Calculate Jacobian matrix J numerically with (f(X+h) - f(X-h)) / 2h\n",
      "    Jacobian matrix element Jpq = df/dXpq, the impact on J by the\n",
      "    small difference to Xpq where p is row index and q is col index of J.\n",
      "\n",
      "    Args:\n",
      "        f: Y=f(X) where Y is a scalar or shape() array.\n",
      "        X: input of shame (N, M), or (N,) or ()\n",
      "        delta: small delta value to calculate the f value for X+/-h\n",
      "    Returns:\n",
      "        J: Jacobian matrix that has the same shape of X.\n",
      "    \"\"\"\n",
      "    X = np.array(X, dtype=float) if isinstance(X, (float, int)) else X\n",
      "    J = np.zeros_like(X, dtype=float)\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # (x+h) or (x-h) may cause an invalid value area for the function f.\n",
      "    # e.g log loss tries to offset x=0 by adding a small value e as log(0+e).\n",
      "    # However because e=1e-7 << h=1e-5, f(x-h) causes nan due to log(x < 0)\n",
      "    # as x needs to be > 0 for log.\n",
      "    #\n",
      "    # X and tmp must be float, or it will be int causing float calculation fail.\n",
      "    # e.g. f(1-h) = log(1-h) causes log(0) instead of log(1-h).\n",
      "    # --------------------------------------------------------------------------------\n",
      "    assert (X.dtype == float), \"X must be float type\"\n",
      "    assert delta > 0.0\n",
      "\n",
      "    it = np.nditer(X, flags=['multi_index'], op_flags=['readwrite'])\n",
      "    while not it.finished:\n",
      "        idx = it.multi_index\n",
      "        tmp: float = X[idx]\n",
      "\n",
      "        X[idx] = tmp + delta\n",
      "        fx1: float = f(X)  # f(x+h)\n",
      "        assert not np.isnan(fx1), \\\n",
      "            \"numerical delta f(x+h) caused nan for f %s for X %s\" \\\n",
      "            % (f, (tmp + delta))\n",
      "\n",
      "        X[idx] = tmp - delta\n",
      "        fx2: float = f(X)  # f(x-h)\n",
      "        assert not np.isnan(fx2), \\\n",
      "            \"numerical delta f(x-h) caused nan for f %s for X %s\" \\\n",
      "            % (f, (tmp - delta))\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Set the gradient element scalar value or shape()\n",
      "        # --------------------------------------------------------------------------------\n",
      "        g = (fx1 - fx2) / (2 * delta)\n",
      "        assert g.size == 1, \"The f function needs to return scalar or shape ()\"\n",
      "        J[idx] = g\n",
      "\n",
      "        X[idx] = tmp\n",
      "        it.iternext()\n",
      "\n",
      "    return J\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from common import (\n",
    "    numerical_jacobian,\n",
    "    OFFSET_FOR_DELTA\n",
    ")\n",
    "lines = inspect.getsource(numerical_jacobian)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3437487342346522\n",
      "0.3437515546871296\n",
      "-1.410226238701684\n"
     ]
    }
   ],
   "source": [
    "p = 0.70910508\n",
    "h = OFFSET_FOR_DELTA\n",
    "e = OFFSET_FOR_DELTA\n",
    "type(p+h+e)\n",
    "print(left:=-np.log(p+e+h))\n",
    "print(right:=-np.log(p+e-h))\n",
    "print((left-right)/(2*h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0999993949765637e-06"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(1.00000+1.1e-06 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999979000044101"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / (1.000001+1.1e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_log_loss(p+e, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cross_entropy_log_loss(p+e+h, 1) - cross_entropy_log_loss(p+e-h, 1)) / (2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.34939492200687716 - 0.34939634021704463) / (2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [\n",
    "    [0,0,1],\n",
    "    [0,1,0]\n",
    "]\n",
    "np.argmax(a, axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=np.array([0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(T).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = np.array(0)\n",
    "T.reshape(())\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(np.array(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
