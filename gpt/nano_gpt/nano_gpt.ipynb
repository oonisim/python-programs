{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df3f532-40a2-4049-844f-12f0cea1127a",
   "metadata": {},
   "source": [
    "# Nano GPT\n",
    "\n",
    "Nano GPT implementation by Andrej Karpathy.\n",
    "\n",
    "* [nanoGPT](https://github.com/karpathy/nanoGPT)\n",
    "* [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n",
    "* [nanogpt-lecture](https://github.com/karpathy/ng-video-lecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5710adfc-3fb3-4e3e-857c-331a5ab58489",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T01:05:22.316374112Z",
     "start_time": "2024-01-02T01:05:22.300633784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f507fe2d-3931-491a-a14a-25ba5edb12d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T01:05:24.138290904Z",
     "start_time": "2024-01-02T01:05:22.305989324Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from bigram import (\n",
    "    V,\n",
    "    B,\n",
    "    T,\n",
    "    C,\n",
    "    get_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d9b0fe-8ac6-4ca3-b237-610999b0b055",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Using tinyshakespeare as the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e53b20-2177-458e-9f98-60ce2135ef19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T01:05:24.796234523Z",
     "start_time": "2024-01-02T01:05:24.140384661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-02 12:05:24--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1115394 (1.1M) [text/plain]\r\n",
      "Saving to: ‘input.txt.1’\r\n",
      "\r\n",
      "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.1s    \r\n",
      "\r\n",
      "2024-01-02 12:05:24 (10.3 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13271d1f-2fc5-4632-b867-01988054c251",
   "metadata": {},
   "source": [
    "# Terminologies\n",
    "\n",
    "* B: Batch size\n",
    "* T: Time steps or Sequence length (e.g. 512 for bert input sequence)\n",
    "* C: Channel or Feature (channel perhaps because Andrej is from CNN background?). ```C=2``` two features in each x."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45745a2a-c062-40af-90a2-65d372419312",
   "metadata": {},
   "source": [
    "## Batch Input\n",
    "\n",
    "<img src=\"./image/gpt_batch.jpeg\" align=\"left\" width=750/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891cf9afa4fb5110",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Bigram Language Model\n",
    "\n",
    "The Bigram Language Model is **NOT using context of size T** but using the current token to predict next token, hence **Bi**gram. The objective of a Language Model is to use the historycal context, but here in the Bigram Model, Andrej is building very simple Markov Chain process to **generate next token only from the current token** for the sake of explaning the basic idea of token generation with bare naked possible way as he mentioned at [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY?t=2077) and [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY?t=2261).\n",
    "\n",
    "> right now the history is not used, so it looks silly (but eventually history will be used). ...\n",
    "> \n",
    "> Given the previous context of whatever generated, we only look at the very last character (```[:, -1, :]```) to make the prediciton of what comes next.\n",
    "\n",
    "* [Let's build GPT: from scratch, in code, spelled out - simplest baseline](https://youtu.be/kCc8FmEb1nY?t=1383)\n",
    "\n",
    "The ```idx:shape(B,T)``` is the idices to extract ```T``` number embedding vectors for the tokens. The ```token_embedding_table``` is e.g. a table of word embeddings in Word2Vec for each word in the vocabulary of the language. Andrej is using ```vocab_size``` as the dimension of the token embedding vector dimensions, hence the table has ```(number of words in language, dimensions)==(vocab_size, vocab_size)```. \n",
    "\n",
    "The reason using ```vocab_size``` as the dimensions is because Andrej simlifies the classification head which predicts the next token. Usually there is a **fully connected (FC)** layer that reduces the higher dimension down to N classes to predict, then apply softmax and argmax to select the hightest probability token index. Here, he skipped **FC** and directly generate N class outputs where ```N==vocab size``` so that the model can directly predict which word in the vocabulary to come next as GPT output.\n",
    "\n",
    "<img src=\"./image/andrej_gpt_dev_idx.png\" align=\"left\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbb48c7-84df-45e9-8bd7-cf5169c2894c",
   "metadata": {},
   "source": [
    "The ```generate``` function is the mechanism to continuously generate the next tokens from the given context (prompt). In this Bigram Language Model, the (last) token predicts the next token, hence take the last token embedding vector with ```[:, -1, :]``` to get the next token id via softmax and argmax (```torch.multinomial(, num_samples=1)```) as the next token.\n",
    "\n",
    "<img src=\"./image/text_generation_from_prompt.png\" align=\"left\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18a1f7e-d2e9-4290-9b1f-2f16cdf9afe7",
   "metadata": {},
   "source": [
    "This corresponds with GPT generates succeeding sentences given a prompt (context). \n",
    "\n",
    "* [Let's build GPT: from scratch, in code, spelled out.](https://youtu.be/kCc8FmEb1nY?t=1934)\n",
    "\n",
    "By giving an index to one token as the first ```idx``` to the ```generate``` function, it will continuously generates ```max_new_tokens```.\n",
    "\n",
    "```\n",
    "# 0 as the index to the first token in the embedding table\n",
    "first_token_index: int = torch.zeros((1, 1)  \n",
    "\n",
    "# [0] to get first batch\n",
    "first_batch = decode(m.generate(idx=first_token_index, dtype=torch.long), max_new_tokens=100)[0].tolist())  \n",
    "print(first_batch)\n",
    "```\n",
    "\n",
    "<img src=\"./image/next_token_generation.jpeg\" align=\"left\" width=750/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a326dbfa-e644-4685-b0b0-263e83bd76f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T01:05:25.208379741Z",
     "start_time": "2024-01-02T01:05:25.182606312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_batch(split):\n",
      "    # generate a small batch of data of inputs x and targets y\n",
      "    data = train_data if split == 'train' else val_data\n",
      "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
      "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
      "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
      "    x, y = x.to(device), y.to(device)\n",
      "    return x, y\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(get_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26538e1e-a74e-4e1c-8268-bfa4d86240ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 8 65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(B,T,C)\n",
    "x, y = get_batch('train')\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07acdfac-5f13-438e-844b-1d124d115735",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "(The Transformer implementaion starting at [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY?t=2268))\n",
    "\n",
    "Transformer generates a graph network between position-encoded tokens.\n",
    "\n",
    "1. Get un-connected tokens as a sequence (e.g. sentence)\n",
    "2. Wires connections among tokens by having looked at the co-occurrances of them in billions of sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c610bd-1648-42cb-8233-afdd978e2a31",
   "metadata": {},
   "source": [
    "## Attention \n",
    "\n",
    "$$Attention(Q,K,V)=softmax(\\frac {QK^T}{\\sqrt {d_k}})$$\n",
    "\n",
    "\n",
    "* [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY?t=4301)\n",
    "* [Building a GPT](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-)\n",
    "\n",
    "> - Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "> - There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "> - Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "> - In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "> - \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "> - \"Scaled\" attention additional divides `similarity` by ```1/sqrt(head_size)```. This makes it so when input Q,K are unit variance, `similarity` will be unit variance too and Softmax will stay diffuse and not saturate too much.\n",
    "> \n",
    "> <img src=\"./image/transformer_attention_as_communication.png\" align=\"left\" width=250/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084ccb5c-b89e-44a2-8434-b128b3cbfe1e",
   "metadata": {},
   "source": [
    "## Dot Product\n",
    "\n",
    "Transformer uses Scaled Dot Product Attention. Refresh the memory on what dot-product transformation does and where they are used.\n",
    "\n",
    "* Similarity = Q@K\n",
    "* Attention Valuye = Similarity@V\n",
    "\n",
    "<img src=\"./image/transformer_self_attention_flow.jpeg\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062c57c2-4217-49fc-ad75-fa040f3c8924",
   "metadata": {},
   "source": [
    "## Q and K (Similarity Score)\n",
    "\n",
    "For every token ```Q``` in a sequence, calculate the relation/communication with other token ```K``` in the sequence (for GPT, only previous tokens). This builds the graph network of Self Attention.\n",
    "\n",
    "\n",
    "|Similarity Score (Q & K)| Proabability as Softmax |\n",
    "|---|---|\n",
    "|<img src=\"./image/transformer_dot_product_attention_similarity_score.jpeg\" align=\"left\" width=500/>|<img src=\"./image/transformer_dot_product_attention.png\" align=\"left\" width=175/>|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb90c4a3-efdd-4658-9e30-d770c737c90d",
   "metadata": {},
   "source": [
    "\n",
    "### Scale by $\\sqrt{d_k}$\n",
    "\n",
    "As in the name **Scaled** Dot-Product Attention, the similarity score is normalized by $\\sqrt{d_k}$ to manage the variance.\n",
    "\n",
    "* [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY?t=4616)\n",
    "\n",
    "> If you have unit gausian input of mean 0 and $W_K$ and $W_V$ are unit gaussian, and if you calculate the ```similarity``` naively, the variance is the order of the head size $d_k$ (e.g. approx 16 if $d_k$ == 16). By standardizing the ```similarity``` score by $\\sqrt{d_k}$ the variance of the ```similarity``` socre will be normal (approx 1.0).\n",
    "\n",
    "Otherwise, softmax will pickup the nodes with larger values, hence only specific nodes in the sequence will be incorporated into the BoW. We want to consider the communication among every nodes if there is, not specific ones only.\n",
    "\n",
    "#### Without scaling\n",
    "\n",
    "When the similarity score is not normalized/scaled by $\\sqrt{d_k}$, the softmax becomes peaky like one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88651d9f-56b2-4ed2-9047-03dfc99ca314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance:14.592000007629395, softmax:tensor([7.8424e-03, 3.1967e-04, 3.8844e-02, 6.4541e-05, 9.5293e-01])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<StemContainer object of 3 artists>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head_size = 16\n",
    "naive_score = torch.tensor([0.1, -0.1, 0.2, -0.2, 0.4]) * head_size\n",
    "# Note that variance is close to the head_size\n",
    "print(f\"variance:{naive_score.var()}, softmax:{torch.softmax(naive_score, dim=-1)}\")\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.grid()\n",
    "plt.stem(range(len(naive_score)), torch.softmax(naive_score, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8cda58-08ac-4c0e-9bff-f2333221f3b2",
   "metadata": {},
   "source": [
    "#### With scaling\n",
    "\n",
    "By scale/normalize, the softmax will be smoothed/diffused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fce13dc5-0ee8-414e-bdb7-64856d6356eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance:0.9120000004768372, softmax:tensor([0.1524, 0.0685, 0.2273, 0.0459, 0.5059])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<StemContainer object of 3 artists>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scaled_score = naive_score / torch.sqrt(torch.tensor(head_size))\n",
    "print(f\"variance:{scaled_score.var()}, softmax:{torch.softmax(scaled_score, dim=-1)}\")\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.grid()\n",
    "plt.stem(range(len(scaled_score)), torch.softmax(scaled_score, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca3a079b-5d19-4564-afa8-fae846795501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see a single Head perform self-attention\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# B: batch size\n",
    "# T: time steps or number of tokens to iterate or sequencee size\n",
    "# C: channels or embedding vector dimension or features\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "head_size = 16\n",
    "    \n",
    "Wk = nn.Linear(C, head_size, bias=False)\n",
    "Wq = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "score2 = None\n",
    "def calculate_similarity_score(x):\n",
    "    k = Wk(x)   # (B, T, head_size)\n",
    "    q = Wq(x)   # (B, T, head_size)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Scaled Similarity between k and q as the first MatMul:\n",
    "    # (B, T, head_size) @ (B, head_size, T) ---> (B, T, T)\n",
    "    # same can be done with einsum ```score = torch.einsum(\"btd,bsd->bts\", q, k)```.\n",
    "    # score of each node is scaled by sqrt(head_size) so that the variance is approx 1.\n",
    "    # --------------------------------------------------------------------------------\n",
    "    score =  q @ k.transpose(-2, -1) / torch.sqrt(head_size, dim=-1)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # mask to make uni-direction (left to right only) for GPT.\n",
    "    # For bi-directional e.g .BERT, skip the masking.\n",
    "    # --------------------------------------------------------------------------------\n",
    "    tril = torch.tril(torch.ones(T, T))\n",
    "    score = score.masked_fill(tril == 0, float('-inf'))\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Normalize by softmax. exp(-inf) = 0 masks the score to make it uni-directional.\n",
    "    # --------------------------------------------------------------------------------\n",
    "    score = F.softmax(score, dim=-1)\n",
    "\n",
    "    return score    # shape:(B, T, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db11fb7-d368-4556-ab1a-056d0b0b8478",
   "metadata": {},
   "source": [
    "## V (Bow/Bag of Words)\n",
    "\n",
    "One way to generate the inter-connections among the tokens to distill their knowledges or relations is ```BoW``` by averaging them feature-wise/axis=-1.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./image/transformer_dot_product_attention_bow.png\" align=\"left\" width=700/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e061adeb-8b2b-4783-9a4e-f9f499e3db0a",
   "metadata": {},
   "source": [
    "Note that the initially the value of similarity is random or ```(1.0, 1/2, 1/3, ...)``` but eventually it gets trained to memorize the relations among position-encoded tokens.\n",
    "\n",
    "* [Let's build GPT: from scratch, in code, spelled out.](https://youtu.be/kCc8FmEb1nY?t=3814)\n",
    "\n",
    "> Different will find other tokens more or less interesting and we want that data dependent. If I/token is a vowel, I am looking for consonants in my past and want to know what consonants were. And I want the information to flow to me (connection). This is the problem that Self Attention solves.\n",
    "\n",
    "<img src=\"./image/self_attention.jpeg\" align=\"left\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff9381-ea71-4a56-8695-dbd0e14ade54",
   "metadata": {},
   "source": [
    "### Purpose of  using $W_V$\n",
    "\n",
    "$v$ looks to be a proxy of $x$ but what transformation or meaning does $W_V$ gives by having transformation from $x$ to $v$? (Note: ```x``` in the diagram above is actually $v$ as $v=x@W{_V}{^T}$).\n",
    "\n",
    "* [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY?t=4258)\n",
    "\n",
    "> $x$ is like a private information to a token. For the purpose of the single attention head, $v$ is what I give for you to communicate with if you find me interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49fc2741-1a1a-46db-a830-5b68b0fd6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wv = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "def calculate_attension_value(score, x):\n",
    "    v = Wv(x)            # (B,T,C) @ (C,head_size) -> (B,T,head_size)\n",
    "    value = score @ v    # (B,T,T) @ (B,T,head_size) -> (B,T,head_size)\n",
    "\n",
    "    return value         # (B,T,head_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afeee82-9b7e-43b0-a992-317437528550",
   "metadata": {},
   "source": [
    "### Example A(Q,K,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7574252-3199-409c-95d7-d59d3b34985a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(B,T,C)\n",
    "\n",
    "similarity_score = calculate_similarity_score(x)\n",
    "similarity_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81ed6987-87c3-462b-b793-b457de0b98dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_value = calculate_attension_value(similarity_score, x)\n",
    "attention_value.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29037d96-27c9-482e-84e1-be8e17e18218",
   "metadata": {},
   "source": [
    "## Multi Attention Head\n",
    "\n",
    "Divide the embedding vector q, k, v into ```h``` number of segmenets and apply self attention to each segment in parallel respectively.\n",
    "\n",
    "* <img src=\"./image/transformer_paper_multi_head_attention.png\" align=\"left\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef7f98-f117-4ae5-8485-25ba4f3f488b",
   "metadata": {},
   "source": [
    "* [Transformers Explained Visually (Part 3): Multi-head Attention, deep dive](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)\n",
    "\n",
    "<img src=\"./image/transformer_multi_head_attention.png\" align=\"left\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea5fac-4b0e-453b-a073-5e49dab46628",
   "metadata": {},
   "source": [
    "## Pointwise Feed Forward\n",
    "\n",
    "<img src=\"./image/transformer_paper_pointwise_feedforward.png\" align=\"left\" width=700/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79042876-a9c4-4464-9f49-a6af1fda4527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
