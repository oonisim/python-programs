{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3475838",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d35f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import datetime\n",
    "import calendar\n",
    "from typing import (\n",
    "    List,\n",
    "    Dict,\n",
    "    Tuple,\n",
    "    Any,\n",
    "    Optional,\n",
    "    Callable,\n",
    "    Iterable,\n",
    ")\n",
    "\n",
    "import dateutil\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56f5329",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99dfcce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.ERROR)\n",
    "logger = logging.getLogger(\"analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a43a6e7",
   "metadata": {},
   "source": [
    "## PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0ddf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(f\"{os.getcwd()}/../../../lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3406af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from constant import (\n",
    "    TYPE_FLOAT,\n",
    ")\n",
    "from util_datetime import (\n",
    "    get_datetime_components,\n",
    "    convert_date_into_datetime,\n",
    "    convert_time_into_timedelta,\n",
    "    convert_date_time_into_datetime,\n",
    "    parse_date_string,\n",
    "    parse_time_string,\n",
    "    get_dates_from_string,\n",
    "    has_date_in_string,\n",
    "    parse_datetime_string,\n",
    "    get_epoch_from_datetime,\n",
    "    get_epoch_from_string,\n",
    "    get_seconds_between_datetimes,\n",
    "    get_datetime_after_duration,\n",
    "    get_elapsed_time,\n",
    "    get_holidays,\n",
    "    get_cyclic_time_of_day,\n",
    "    get_cyclic_day_of_week,\n",
    "    get_cyclic_month_of_year,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2e9f37",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17da9def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5b0177",
   "metadata": {},
   "source": [
    "## Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26e8f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_SUPPLIER_CODE: str = 'supplier_code'\n",
    "COLUMN_FACILITY: str = 'facility'\n",
    "COLUMN_START_TIME: str = \"start_date_time\"\n",
    "COLUMN_END_TIME: str = \"end_date_time\"\n",
    "COLUMN_PROCESS_DATE: str = 'process_date'\n",
    "COLUMN_PROCESS_TIME: str = \"process_time\"\n",
    "COLUMN_INPUT: str = \"input\"\n",
    "COLUMN_OUTPUT: str = \"output\"\n",
    "    \n",
    "PATH_TO_DATA: str = \"../data/september.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f058a0",
   "metadata": {},
   "source": [
    "---\n",
    "# Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eff2ffe",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File ../data/december.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m info_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPATH_TO_DATA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacility\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCategoricalDtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msupplierCode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCategoricalDtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m info_df\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m~/venv/ml/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/ml/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/ml/lib/python3.9/site-packages/pandas/io/json/_json.py:733\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    731\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "File \u001b[0;32m~/venv/ml/lib/python3.9/site-packages/pandas/io/json/_json.py:818\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlines:\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows can only be passed if lines=True\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 818\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_from_filepath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[0;32m~/venv/ml/lib/python3.9/site-packages/pandas/io/json/_json.py:874\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    866\u001b[0m     filepath_or_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[1;32m    873\u001b[0m ):\n\u001b[0;32m--> 874\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filepath_or_buffer\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File ../data/december.json does not exist"
     ]
    }
   ],
   "source": [
    "info_df = pd.read_json(\n",
    "    PATH_TO_DATA,\n",
    "    convert_dates=['date'],\n",
    "    dtype={\n",
    "        \"facility\": pd.CategoricalDtype(),\n",
    "        \"supplierCode\": pd.CategoricalDtype(),\n",
    "    }\n",
    ")\n",
    "info_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca04c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df['facility'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f10c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df['supplierCode'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46fab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df['supplier'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2439d415",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a31d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read columns as strings without auto-detect/convert.\n",
    "raw_df = pd.read_json(\n",
    "    PATH_TO_DATA,\n",
    "    convert_dates=False\n",
    ")\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40971d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cd1c60",
   "metadata": {},
   "source": [
    "\n",
    "## Start Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cc76cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_date_time(row: pd.Series) -> pd.Series:\n",
    "    name: str = \"get_start_date_time()\"\n",
    "    logger.debug(\"%s: row type[%s] row[%s]\", name, type(row), row)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # if timeStart column is not valid, the row is invalid as there should be no way \n",
    "    # to recover start time.\n",
    "    # --------------------------------------------------------------------------------\n",
    "    if pd.isnull(row['timeStart']): \n",
    "        return np.nan\n",
    "    \n",
    "    \n",
    "    start_date_time: datetime.datetime = np.nan\n",
    "\n",
    "    # Begin start_date_time extraction\n",
    "    if has_date_in_string(row['timeStart']):\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # timeStart already includes date.\n",
    "        # --------------------------------------------------------------------------------\n",
    "        start_date_time = parse_datetime_string(row['timeStart'])\n",
    "        logger.debug(\"%s: start_date_time is [%s]\", name, start_date_time)\n",
    "        \n",
    "    else:\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # timeStart has no date, then (date, startTime) columns must be valid.\n",
    "        # Otherwise the row is invalid.\n",
    "        # --------------------------------------------------------------------------------\n",
    "        if pd.isnull(row['date']):\n",
    "            # --------------------------------------------------------------------------------\n",
    "            # Invalid row. Return NaN as the start_date_time to mark the row as invalid.\n",
    "            # --------------------------------------------------------------------------------\n",
    "            start_date_time = np.nan\n",
    "\n",
    "        else:\n",
    "            # --------------------------------------------------------------------------------\n",
    "            # Date from 'date' column, and omit the time part.\n",
    "            # --------------------------------------------------------------------------------\n",
    "            _date_time: datetime.datetime = parse_datetime_string(row['date'])\n",
    "            assert isinstance(_date_time, datetime.datetime)\n",
    "            \n",
    "            _date: datetime.date = _date_time.date()\n",
    "            logger.debug(\"%s: date is [%s]\", name, _date)\n",
    "\n",
    "            # --------------------------------------------------------------------------------\n",
    "            # Time from 'timeStart' column\n",
    "            # --------------------------------------------------------------------------------\n",
    "            _temp_date_time: datetime.datetime = parse_datetime_string(row['timeStart'])\n",
    "            assert isinstance(_temp_date_time, datetime.datetime)\n",
    "            \n",
    "            _time: datetime.time = _temp_date_time.time()\n",
    "            logger.debug(\"%s: start_time is [%s]\", name, _time)\n",
    "            \n",
    "            start_date_time = convert_date_time_into_datetime(_date, _time)\n",
    "            logger.debug(\"%s: start_date_time is [%s]\", name, start_date_time)\n",
    "\n",
    "    # End start_date_time extraction\n",
    "            \n",
    "    return start_date_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f89ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_time: pd.Series = raw_df.apply(func=get_start_date_time, axis=1)\n",
    "start_date_time.name = COLUMN_START_TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b610b0",
   "metadata": {},
   "source": [
    "## End Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5e34d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_end_time_df: pd.DataFrame = pd.DataFrame({\n",
    "    'start_date_time': start_date_time,\n",
    "    'processTime': raw_df['processTime'],\n",
    "    'timeEnd': raw_df['timeEnd']\n",
    "})\n",
    "# interim_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49f4af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_end_date_time(row: pd.Series) -> pd.Series:\n",
    "    name: str = \"get_end_date_time()\"\n",
    "    logger.debug(\"%s: row type[%s] row[%s]\", name, type(row), row)\n",
    "\n",
    "    assert isinstance(row[COLUMN_START_TIME], datetime.datetime), \\\n",
    "        f\"expected start_date_time as datetime.datetime, got [{type(row[COLUMN_START_TIME])}].\"\n",
    "    start_date_time: datetime.datetime = row[COLUMN_START_TIME]\n",
    "    \n",
    "    # --------------------------------------------------------------------------------\n",
    "    # if timeEnd and processTime columns are not valid, the row is invalid\n",
    "    # --------------------------------------------------------------------------------\n",
    "    if pd.isnull(row['timeEnd']) and pd.isnull(row['processTime']): \n",
    "        return np.nan\n",
    "    \n",
    "    # Begin end_date_time extraction\n",
    "    end_date_time: datetime.datetime = np.nan\n",
    "        \n",
    "    if not pd.isnull(row['timeEnd']):\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Acquire end_date_time from (start_date_time, timeEnd)\n",
    "        # --------------------------------------------------------------------------------\n",
    "        if has_date_in_string(row['timeEnd']):\n",
    "            # --------------------------------------------------------------------------------\n",
    "            # timeEnd already includes date.\n",
    "            # --------------------------------------------------------------------------------\n",
    "            end_date_time = parse_datetime_string(row['timeEnd'])\n",
    "            logger.debug(\"%s: end_date_time is [%s]\", name, end_date_time)\n",
    "\n",
    "        else:\n",
    "            # --------------------------------------------------------------------------------\n",
    "            # timeEnd has no date, hence it is only time expression, and date is from start_time.\n",
    "            # --------------------------------------------------------------------------------\n",
    "            _dummy_end_date_time: datetime.datetime = parse_datetime_string(row['timeEnd'])\n",
    "            assert isinstance(_dummy_end_date_time, datetime.datetime)\n",
    "            \n",
    "            # take only time part\n",
    "            _time: datetime.time = _dummy_end_date_time.time()\n",
    "            logger.debug(\"%s: timeEnd is [%s]\", name, _time)\n",
    "            \n",
    "            assert isinstance(row['start_date_time'], datetime.datetime)\n",
    "            _date = row['start_date_time'].date()\n",
    "            \n",
    "            end_date_time = convert_date_time_into_datetime(\n",
    "                date_in_year=_date,\n",
    "                time_in_day=_time\n",
    "            )\n",
    "            \n",
    "            # Make sure timeEnd is after start_time in case the processing is crossing midnight.\n",
    "            if end_date_time <= start_date_time:\n",
    "                logger.warning(\n",
    "                    \"%S: end_date_time [%s] is before start_date_time [%s]\", \n",
    "                    end_date_time, start_date_time\n",
    "                )\n",
    "                # Advance the end_date_time with 24h.\n",
    "                end_date_time = end_date_time + datetime.timedelta(days=1)\n",
    "                logger.debug(\"%s: end_date_time is [%s]\", name, end_date_time)\n",
    "\n",
    "    else:\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Acquire end_date_time from (start_date_time, processTime)\n",
    "        # [Assumption] processTime is hh:mm:yy and mm is less than 60.\n",
    "        # --------------------------------------------------------------------------------\n",
    "        try:\n",
    "            assert isinstance(row['processTime'], str), \\\n",
    "                f\"expected row['processTime'] of type str, got [{type(row['processTime'])}]\"\n",
    "\n",
    "            _time: datetime.time = parse_time_string(row['processTime'])\n",
    "            delta: datetime.timedelta = convert_time_into_timedelta(time_in_day=_time)\n",
    "            end_date_time = start_date_time + delta\n",
    "            \n",
    "        except ValueError as e:\n",
    "            logging.error(\"%s: invalid time expression [%s]\", name, row['processTime'])\n",
    "            end_date_time = np.nan\n",
    "        \n",
    "    # End start_date_time extraction\n",
    "            \n",
    "    return end_date_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab156489",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date_time: pd.Series = interim_end_time_df.apply(func=get_end_date_time, axis=1)\n",
    "end_date_time.name = COLUMN_END_TIME\n",
    "interim_end_time_df.insert(\n",
    "    loc=len(interim_end_time_df.columns),\n",
    "    column=COLUMN_END_TIME,\n",
    "    value=end_date_time,\n",
    "    allow_duplicates=False\n",
    ")\n",
    "# interim_end_time_df\n",
    "del interim_end_time_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea51c8e",
   "metadata": {},
   "source": [
    "## Process Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab67c715",
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_process_time_df: pd.DataFrame = pd.DataFrame({\n",
    "    COLUMN_START_TIME: start_date_time,\n",
    "    COLUMN_END_TIME: end_date_time,\n",
    "})\n",
    "# interim_process_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591af9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_process_time(row: pd.Series) -> pd.Series:\n",
    "    \"\"\"Get proces time in seconds as TYPE_FLOAT\n",
    "    Process time as end time - start time.\n",
    "    \n",
    "    Return: process time taken in second (as TYPE_FLOAT)\n",
    "    \"\"\"\n",
    "    assert isinstance(row[COLUMN_START_TIME], datetime.datetime)\n",
    "    assert isinstance(row[COLUMN_END_TIME], datetime.datetime)\n",
    "    \n",
    "    delta: datetime.timedelta = row[COLUMN_END_TIME] - row[COLUMN_START_TIME]\n",
    "    assert isinstance(delta, datetime.timedelta)\n",
    "    \n",
    "    return TYPE_FLOAT(delta.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e2077",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_time: pd.Series = interim_process_time_df.apply(func=get_process_time, axis=1)\n",
    "process_time.name = COLUMN_PROCESS_TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3febef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_process_time_df.insert(\n",
    "    loc=len(interim_process_time_df.columns),\n",
    "    column=COLUMN_PROCESS_TIME,\n",
    "    value=process_time,\n",
    "    allow_duplicates=False\n",
    ")\n",
    "\n",
    "# interim_process_time_df\n",
    "del interim_process_time_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d6fca",
   "metadata": {},
   "source": [
    "## Process Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c02fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_process_date(row: pd.Series):\n",
    "    _date_time: datetime.datetime = np.nan\n",
    "        \n",
    "    if not pd.isnull(row['start_date_time']):\n",
    "        _date_time = convert_date_into_datetime(row['start_date_time'].date())\n",
    "     \n",
    "    return _date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25f74c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_date: pd.Series = start_date_time.to_frame().apply(func=get_process_date, axis=1)\n",
    "process_date.name = COLUMN_PROCESS_DATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1311623",
   "metadata": {},
   "source": [
    "## Supplier Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f9a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_from_supplier(supplier: str):\n",
    "    supplier_to_code = {\n",
    "        \"mary therese\": \"mat\",\n",
    "        \"mary\": \"mar\",\n",
    "        'mary anne': \"maa\",\n",
    "        'mary jane': 'maj',\n",
    "    }\n",
    "    return supplier_to_code.get(supplier.lower(), np.nan)\n",
    "    \n",
    "    \n",
    "def get_supplier_code(row: pd.DataFrame):\n",
    "    if row['supplier'] not in (np.nan, None):\n",
    "        return get_code_from_supplier(row['supplier'])\n",
    "    else:\n",
    "        return row['supplierCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea544f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "supplier_code: pd.Series = raw_df.apply(func=get_supplier_code, axis=1)\n",
    "supplier_code.name = COLUMN_SUPPLIER_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c803f871",
   "metadata": {},
   "source": [
    "---\n",
    "# Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc233c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df: pd.DataFrame = pd.DataFrame({\n",
    "    COLUMN_FACILITY: raw_df['facility'],\n",
    "    COLUMN_SUPPLIER_CODE: supplier_code,\n",
    "    COLUMN_START_TIME: start_date_time,\n",
    "    COLUMN_PROCESS_TIME: process_time,\n",
    "    COLUMN_INPUT: raw_df['suppliedM3'].astype(TYPE_FLOAT),\n",
    "    COLUMN_OUTPUT: raw_df['recoveredM3'].astype(TYPE_FLOAT),\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c108c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fa8cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d970b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['supplier_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3633309",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['facility'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0276f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
