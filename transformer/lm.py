"""Decoder-only Language Model (GPT-style). Generated by Claude Code.
This module implements a decoder-only Transformer for language modeling,
which is the architecture used by GPT-2, GPT-3, LLaMA, Mistral, etc.

Architecture:
    Input tokens -> Embedding + Positional Encoding -> N x DecoderLayers -> Projection

    Unlike the encoder-decoder Transformer (model.py), this uses only the decoder
    with causal self-attention (no cross-attention to encoder).

Usage:
    from scratch.lm import LanguageModel
    from transformers import GPT2Tokenizer

    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    model = LanguageModel(vocab_size=tokenizer.vocab_size)

    # Training: get log probabilities
    log_probabilities = model.forward(input_ids)  # (B, T, V)
    loss = criterion(log_probabilities.view(-1, V), target.view(-1))

    # Inference: generate text
    with model:
        output = model.generate(prompt_ids, max_length=100)
"""
from typing import Optional

import logging
import torch
from torch import Tensor, nn

from constant import (
    TYPE_FLOAT,
    DIM_MODEL,
    DIM_PWFF_HIDDEN,
    NUM_LAYERS,
    NUM_HEADS,
    MAX_TIME_STEPS,
    DROPOUT_RATIO,
)
from common import Projection
from decoder import Decoder


logger = logging.getLogger(__name__)


class LanguageModel(nn.Module):
    """Decoder-only Transformer for language modeling (GPT-style).

    This model uses causal self-attention only (no encoder, no cross-attention).
    Each token can only attend to previous tokens, enabling autoregressive generation.

    Architecture:
        tokens -> Decoder(memory=None) -> Projection -> log_probabilities

    Example tokenizer configuration (GPT-2):
        model.end_token = tokenizer.eos_token_id  # 50256
    """

    def __init__(
            self,
            vocab_size: int,
            d_model: int = DIM_MODEL,
            num_heads: int = NUM_HEADS,
            num_layers: int = NUM_LAYERS,
            d_ff: int = DIM_PWFF_HIDDEN,
            max_seq_len: int = MAX_TIME_STEPS,
            dropout: float = DROPOUT_RATIO,
            dtype: torch.dtype = TYPE_FLOAT
    ):
        """Initialize the LanguageModel.

        Args:
            vocab_size: Size of the vocabulary.
            d_model: Dimension of the model embedding vector.
            num_heads: Number of attention heads.
            num_layers: Number of decoder layers.
            d_ff: Hidden dimension of position-wise feed-forward network.
            max_seq_len: Maximum sequence length (context window).
            dropout: Dropout rate.
            dtype: Data type for weights.
        """
        super().__init__()
        self.d_model = d_model
        self.max_seq_len = max_seq_len
        self.vocab_size = vocab_size

        # Decoder with causal self-attention only (no cross-attention)
        self.decoder = Decoder(
            vocabulary_size=vocab_size,
            num_layers=num_layers,
            num_heads=num_heads,
            d_model=d_model,
            dtype=dtype,
            d_ff=d_ff,
            max_time_steps=max_seq_len,
            bias=True,
            p_drop=dropout
        )

        # Final layer norm (Pre-LayerNorm architecture)
        self.final_norm = nn.LayerNorm(d_model, dtype=dtype)

        # Output projection to vocabulary
        self.projection = Projection(
            d_model=d_model,
            num_classes=vocab_size,
            dtype=dtype,
            bias=True
        )

        # Token for stopping generation
        self.end_token: Optional[int] = None
        # Detect CUDA at initialization and move model to that device
        # This makes the model ready to accept inputs on GPU by default.
        self.device: torch.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def __enter__(self) -> 'LanguageModel':
        """Context manager: set model to eval mode for inference."""
        self.eval()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb) -> bool:
        """Context manager: restore train mode."""
        self.train()
        return False

    def forward(self, x: Tensor) -> Tensor:
        """Forward pass for training.

        Computes log probabilities for next token prediction at each position.
        $$ \\log P(x_t | x_{<t}) $$

        Args:
            x: Input token IDs of shape (B, T).

        Returns:
            Log probabilities of shape (B, T, V).
        """
        if x.ndim != 2:
            raise ValueError(f"Expected (B, T), got {x.shape}")
        if x.device != self._device():
            raise RuntimeError(
                f"Input tensor x on device {x.device} but model on {self._device()}."
            )

        # Decoder without memory (decoder-only mode)
        hidden = self.decoder(y=x, memory=None)
        hidden = self.final_norm(hidden)

        # Project to vocabulary and return log probabilities
        log_probabilities = self.projection(y=hidden)
        return log_probabilities

    @torch.no_grad()
    def generate(
            self,
            prompt: torch.Tensor,
            max_length: int,
            temperature: float = 1.0,
            top_k: Optional[int] = None,
            top_p: Optional[float] = None
    ) -> torch.Tensor:
        """
        Generate text continuation from a prompt using autoregressive sampling.

        This function implements the core text generation loop used by language models
        like GPT. Starting from a prompt (seed text), it predicts one token at a time,
        appending each prediction to build up the full generated sequence.

        Mathematical Foundation:
            At each step t, we sample the next token from:

            $$P(x_t | x_{<t}) = \text{softmax}(f_\theta(x_{<t}) / \tau)$$

            Where:
            - x_{<t} = all tokens generated so far (context)
            - f_θ = neural network (the transformer)
            - τ = temperature (controls randomness)

        Args:
            prompt: Starting tokens, shape (batch_size, prompt_length) or (prompt_length,)
                Example: [2045, 318, 257] representing "This is a"

            max_length: Maximum total sequence length (prompt + generated)
                Example: 100 means generate up to 100 tokens total

            temperature: Sampling temperature (controls randomness)
                - τ = 0.1: Very deterministic (always picks most likely token)
                - τ = 1.0: Standard sampling (default)
                - τ = 2.0: Very random (explores unlikely tokens)
                Higher values increase diversity but may reduce coherence.

            top_k: Keep only top-k most likely tokens before sampling
                Example: top_k=50 means only consider 50 most probable next tokens
                This prevents sampling from the long tail of unlikely tokens.

            top_p: Nucleus sampling - keep tokens comprising top p% of probability mass
                Example: top_p=0.9 means keep smallest set of tokens that sum to 90% probability
                This adaptively adjusts vocabulary size based on confidence.

        Returns:
            Generated token sequence, shape (batch_size, generated_length)
            where generated_length ≤ max_length

        Generation Strategy:
            This implements "autoregressive sampling" - the standard approach where:
            1. Model sees all previous tokens
            2. Predicts probability distribution over next token
            3. Sample one token from that distribution
            4. Append sampled token to sequence
            5. Repeat until done

        Example:
            Prompt: "The capital of France is"
            Step 1: Model predicts P(next | "The capital of France is")
                    → Samples "Paris" (high probability)
            Step 2: Model predicts P(next | "The capital of France is Paris")
                    → Samples "," (punctuation likely)
            Step 3: Model predicts P(next | "The capital of France is Paris,")
                    → Samples "which" (continuation)
            ...and so on until max_length or end token reached.
        """
        if prompt.device != self._device():
            raise RuntimeError(
                f"Prompt tensor is on device {prompt.device} but model on {self._device()}."
            )

        # =========================================================================
        # STEP 1: PREPARE INPUT BATCH
        # =========================================================================

        # Handle single sequence input by adding batch dimension
        # Language models process batches (multiple sequences simultaneously)
        # If user provides single sequence, we reshape to batch of size 1
        if prompt.ndim == 1:
            # Input shape: (sequence_length,)
            # Example: [2045, 318, 257] - three tokens
            prompt = prompt.unsqueeze(0)
            # Output shape: (1, sequence_length)
            # Example: [[2045, 318, 257]] - batch of 1 sequence

        # Extract batch size for managing parallel generation
        # We may be generating multiple sequences simultaneously
        batch_size = prompt.shape[0]
        # Example: If prompt.shape = (4, 10), batch_size = 4
        #          (generating 4 different sequences in parallel)

        # Clone prompt to create working copy
        # We'll append generated tokens to this as we go
        # Clone ensures we don't modify the original prompt tensor
        generated_sequence = prompt.clone()
        # Shape: (batch_size, prompt_length)
        # This will grow: (batch_size, prompt_length + 1), then (batch_size, prompt_length + 2), etc.

        # =========================================================================
        # STEP 2: AUTOREGRESSIVE GENERATION LOOP
        # =========================================================================

        # Generate tokens one at a time until we reach max_length
        # The range determines how many NEW tokens to generate
        # If prompt has 10 tokens and max_length=100, we generate 90 more tokens
        number_of_tokens_to_generate = max_length - prompt.shape[1]

        for generation_step in range(number_of_tokens_to_generate):
            # ---------------------------------------------------------------------
            # STEP 2A: PREPARE MODEL INPUT (SLIDING WINDOW)
            # ---------------------------------------------------------------------

            # Extract the most recent tokens that fit in model's context window
            # Language models have maximum sequence length (e.g., 2048 tokens for GPT-2)
            # If our generated sequence exceeds this, we only feed the last N tokens

            # Why? Transformer attention is O(n²) in sequence length
            # Feeding 10,000 tokens would be computationally prohibitive
            # So we use a sliding window of the most recent context

            model_input_sequence = generated_sequence[:, -self.max_seq_len:]
            # Shape: (batch_size, min(current_length, max_seq_len))

            # Example progression:
            # Step 1: generated_sequence has 10 tokens → use all 10
            # Step 50: generated_sequence has 60 tokens → use all 60
            # Step 2000: generated_sequence has 2010 tokens → use last 2048 only
            #            (if max_seq_len = 2048)

            # ---------------------------------------------------------------------
            # STEP 2B: GET MODEL PREDICTIONS
            # ---------------------------------------------------------------------

            # Forward pass through the neural network
            # Input: Token sequence of shape (batch_size, sequence_length)
            # Output: Probability distribution over vocabulary for EACH position

            log_probabilities_all_positions = self.forward(model_input_sequence)
            # Shape: (batch_size, sequence_length, vocabulary_size)

            # Example with vocabulary_size=50,000:
            # If input has 10 tokens, output is (batch_size, 10, 50000)
            # Each of the 10 positions gets a 50,000-dim probability distribution

            # We only care about the LAST position (the next token to generate)
            # All previous positions are already decided (they're in our prompt/generated text)
            next_token_logits = log_probabilities_all_positions[:, -1, :]
            # Shape: (batch_size, vocabulary_size)
            # This is the model's prediction for what token should come next

            # ---------------------------------------------------------------------
            # STEP 2C: APPLY TEMPERATURE SCALING
            # ---------------------------------------------------------------------

            # Temperature controls the "sharpness" of the probability distribution
            #
            # Mathematical effect:
            #   P(token) ∝ exp(logit / temperature)
            #
            # Low temperature (τ < 1):
            #   - Sharpens distribution (makes high-prob tokens even higher)
            #   - Model becomes more confident/deterministic
            #   - Example: "The capital of France is ___"
            #              With τ=0.1, "Paris" might have 99% probability
            #
            # High temperature (τ > 1):
            #   - Flattens distribution (makes all tokens more equally likely)
            #   - Model becomes more random/creative
            #   - Example: With τ=2.0, "Paris" might only have 40% probability,
            #              giving more chance to alternatives like "Lyon", "Marseille"

            next_token_logits = next_token_logits / temperature
            # Shape unchanged: (batch_size, vocabulary_size)
            # But values are now scaled by temperature

            # ---------------------------------------------------------------------
            # STEP 2D: APPLY TOP-K FILTERING (OPTIONAL)
            # ---------------------------------------------------------------------

            # Top-K sampling: Keep only the K most likely tokens, zero out the rest
            # This prevents sampling from the "long tail" of very unlikely tokens

            if top_k is not None:
                # Find the K highest logit values for each sequence in batch
                top_k_values, _ = torch.topk(next_token_logits, top_k)
                # Shape: (batch_size, top_k)

                # Extract the K-th highest value (the threshold)
                # Tokens with logits below this threshold will be masked out
                threshold_value_per_sequence = top_k_values[:, -1, None]
                # Shape: (batch_size, 1)
                # Example: If top_k=50, this is the 50th highest logit

                # Mask out (set to -infinity) all tokens below the threshold
                # Why -infinity? Because after softmax, exp(-∞) = 0 probability
                next_token_logits[next_token_logits < threshold_value_per_sequence] = float('-inf')

                # Result: Only top-k tokens remain as candidates
                # Example: With vocabulary_size=50,000 and top_k=50,
                #          49,950 tokens now have -∞ logits (zero probability)

            # ---------------------------------------------------------------------
            # STEP 2E: APPLY TOP-P (NUCLEUS) FILTERING (OPTIONAL)
            # ---------------------------------------------------------------------

            # Top-P (nucleus) sampling: Keep smallest set of tokens that sum to P% probability
            # This is ADAPTIVE - the number of kept tokens changes based on confidence

            # Why better than top-k?
            # - When model is confident: Keeps few tokens (sharp distribution)
            # - When model is uncertain: Keeps many tokens (flat distribution)

            # Example:
            # Scenario 1 (confident): "The capital of France is ___"
            #   - "Paris" has 80% probability
            #   - With top_p=0.9, we keep "Paris" + a few alternatives = 3 tokens
            #
            # Scenario 2 (uncertain): "The color of the ___"
            #   - Top token only has 15% probability (many plausible colors)
            #   - With top_p=0.9, we might keep 20 tokens to reach 90% mass

            if top_p is not None:
                next_token_logits = self._apply_top_p(next_token_logits, top_p)
                # This function (defined elsewhere) implements nucleus sampling
                # Masks out tokens outside the nucleus (cumulative probability < top_p)

            # ---------------------------------------------------------------------
            # STEP 2F: CONVERT LOGITS TO PROBABILITIES
            # ---------------------------------------------------------------------

            # Softmax converts raw logits (unbounded real numbers) to probabilities
            #
            # Mathematical formula:
            #   P(token_i) = exp(logit_i) / Σ_j exp(logit_j)
            #
            # Properties:
            #   - All probabilities are between 0 and 1
            #   - All probabilities sum to 1
            #   - Higher logits → higher probabilities

            probabilities_next_token = torch.softmax(next_token_logits, dim=-1)
            # Shape: (batch_size, vocabulary_size)
            # Each row is a probability distribution over the vocabulary

            # Example values after softmax:
            # [0.45, 0.23, 0.12, 0.08, 0.05, 0.03, 0.02, 0.01, 0.01, 0.00, ...]
            # (sums to 1.0, all values between 0 and 1)

            # Note: Tokens that were masked with -∞ now have probability 0
            # because exp(-∞) = 0

            # ---------------------------------------------------------------------
            # STEP 2G: SAMPLE NEXT TOKEN
            # ---------------------------------------------------------------------

            # Multinomial sampling: Randomly choose one token according to probabilities
            # This is the stochastic (random) part of text generation

            # Why sample instead of taking argmax (most likely token)?
            # - Argmax is deterministic: Same input always gives same output
            # - Sampling adds diversity: Can generate different texts from same prompt
            # - Allows model to explore creative/unexpected continuations

            next_token = torch.multinomial(probabilities_next_token, num_samples=1)
            # Shape: (batch_size, 1)
            # Each sequence in batch gets one sampled token

            # Example:
            # If probabilities = [0.45, 0.23, 0.12, 0.08, ...]
            # Token 0 is sampled 45% of the time
            # Token 1 is sampled 23% of the time
            # Token 2 is sampled 12% of the time
            # etc.

            # ---------------------------------------------------------------------
            # STEP 2H: APPEND TOKEN TO SEQUENCE
            # ---------------------------------------------------------------------

            # Concatenate the newly sampled token to our growing sequence
            # This token becomes part of the context for the next generation step

            generated_sequence = torch.cat([generated_sequence, next_token], dim=1)
            # Shape increases: (batch_size, length) → (batch_size, length + 1)

            # Example progression:
            # Step 0: [2045, 318, 257]           (prompt: "This is a")
            # Step 1: [2045, 318, 257, 1332]     (added "good")
            # Step 2: [2045, 318, 257, 1332, 1110]  (added "idea")
            # Step 3: [2045, 318, 257, 1332, 1110, 13]  (added ".")

            # ---------------------------------------------------------------------
            # STEP 2I: CHECK FOR EARLY STOPPING
            # ---------------------------------------------------------------------

            # End-of-sequence (EOS) token signals natural completion
            # Common in many tasks: translation, summarization, dialogue

            # Example EOS tokens:
            # - "<|endoftext|>" in GPT models
            # - "</s>" in many encoder-decoder models
            # - Custom tokens like "<END>"

            if self.end_token is not None:
                # Check if ALL sequences in batch generated the EOS token
                # .all() ensures we only stop when every sequence is done
                # (for batch processing, we continue until all are complete)

                all_sequences_ended = (next_token == self.end_token).all()

                if all_sequences_ended:
                    # All sequences have naturally concluded
                    # No need to generate more tokens
                    break

            # If not all sequences ended, continue to next generation step

        # =========================================================================
        # STEP 3: RETURN GENERATED SEQUENCE
        # =========================================================================

        # Return the complete generated sequence
        # This includes both the original prompt AND all generated tokens
        return generated_sequence
        # Shape: (batch_size, final_length)
        # where final_length ≤ max_length


    def _apply_top_p(
            self,
            logits: torch.Tensor,
            top_p: float
    ) -> torch.Tensor:
        """
        Apply nucleus (top-p) sampling by masking tokens outside the nucleus.

        Nucleus sampling keeps the smallest set of tokens whose cumulative
        probability exceeds top_p, masking out the rest.

        Algorithm:
            1. Convert logits to probabilities
            2. Sort probabilities descending
            3. Compute cumulative sum
            4. Find cutoff where cumulative sum exceeds top_p
            5. Mask all tokens beyond cutoff

        Mathematical Formulation:
            Let tokens be sorted by probability: p₁ ≥ p₂ ≥ p₃ ≥ ...

            Find smallest k such that: Σᵢ₌₁ᵏ pᵢ ≥ top_p

            Keep tokens {1, 2, ..., k}, mask the rest

        Example:
            top_p = 0.9

            Original probabilities (sorted):
            [0.45, 0.23, 0.12, 0.08, 0.05, 0.03, 0.02, 0.01, 0.01, ...]

            Cumulative sum:
            [0.45, 0.68, 0.80, 0.88, 0.93, ...]
                                        ↑ First to exceed 0.9

            Result: Keep first 5 tokens, mask all others

        Why Adaptive?
            - High confidence (sharp distribution): Few tokens kept
            - Low confidence (flat distribution): Many tokens kept
            - This is unlike top-k which always keeps exactly k tokens

        Args:
            logits: Raw model outputs, shape (batch_size, vocabulary_size)
            top_p: Probability threshold, typically 0.9 or 0.95

        Returns:
            Masked logits where tokens outside nucleus are set to -inf
        """

        # Convert logits to probabilities
        # Need probabilities (not logits) to compute cumulative mass
        probabilities = torch.softmax(logits, dim=-1)
        # Shape: (batch_size, vocabulary_size)

        # Sort probabilities in descending order (highest probability first)
        # This allows us to accumulate from most likely to least likely
        sorted_probabilities, sorted_indices = torch.sort(
            probabilities, descending=True, dim=-1
        )
        # sorted_probabilities: (batch_size, vocabulary_size) - sorted values
        # sorted_indices: (batch_size, vocabulary_size) - original positions

        # Compute cumulative sum of sorted probabilities
        # This tells us "what fraction of total probability mass have we seen so far"
        cumulative_probabilities = torch.cumsum(sorted_probabilities, dim=-1)
        # Shape: (batch_size, vocabulary_size)

        # Example:
        # sorted_probabilities = [0.45, 0.23, 0.12, 0.08, ...]
        # cumulative_probabilities = [0.45, 0.68, 0.80, 0.88, ...]

        # Find positions where cumulative probability exceeds top_p threshold
        # These are tokens OUTSIDE the nucleus (to be masked)
        tokens_to_remove = cumulative_probabilities > top_p
        # Shape: (batch_size, vocabulary_size) - boolean mask

        # Shift mask right by one position
        # Why? We want to KEEP the token that pushes us over top_p
        # Without shift: If token 5 brings us to 0.93, we'd remove it
        # With shift: We keep token 5, remove token 6 onwards
        tokens_to_remove[:, 1:] = tokens_to_remove[:, :-1].clone()
        tokens_to_remove[:, 0] = False  # Always keep the highest probability token

        # Unsort the mask back to original vocabulary order
        # sorted_indices told us how to sort; now we reverse that sorting
        # This is necessary because we need to mask the ORIGINAL logits
        nucleus_mask = torch.zeros_like(logits, dtype=torch.bool)
        nucleus_mask.scatter_(dim=-1, index=sorted_indices, src=tokens_to_remove)
        # Shape: (batch_size, vocabulary_size)

        # Apply mask: Set tokens outside nucleus to -infinity
        # After softmax, exp(-∞) = 0, so these tokens have zero probability
        logits[nucleus_mask] = float('-inf')

        # Shape unchanged: (batch_size, vocabulary_size)
        # But now only nucleus tokens have finite logits
        return logits

    def _device(self) -> torch.device:
        """Return the device where the model parameters or buffers live.

        Falls back to CPU if no parameters/buffers are present.
        """
        # Prefer explicit self.device if set
        if hasattr(self, "device") and isinstance(self.device, torch.device):
            return self.device
        # Otherwise fall back to parameters/buffers
        try:
            return next(self.parameters()).device
        except StopIteration:
            try:
                return next(self.buffers()).device
            except StopIteration:
                return torch.device("cpu")


if __name__ == "__main__":
    print(__doc__)
