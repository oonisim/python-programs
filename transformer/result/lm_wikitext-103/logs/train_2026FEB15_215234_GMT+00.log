============================================================
Language Model Training
============================================================
Dataset: wikitext-103
Device: cuda
Model: d=256, h=4, L=4
Training: epochs=10, batch=32, lr=0.0003

  Train tokens: 117,920,140
  Val tokens: 247,289
  Train sequences: 460,625
  Early stopping enabled (patience=5)
Weight update monitoring enabled (sample_size=1024, interval=10000 steps)

============================================================
Training...
============================================================
Training on cuda for 10 epochs
Gradient flow monitoring enabled: decoder (4 layers)
  Monitor at snapshots: True
  Monitor interval: 10000
  Monitor at epochs: False
  Norm type: l2
  Epoch 0 | Step 1/14394 | Loss: 10.8955
  Epoch 0 | Step 100/14394 | Loss: 7.3246
  Epoch 0 | Step 200/14394 | Loss: 6.8887
  Epoch 0 | Step 300/14394 | Loss: 6.6139
  Epoch 0 | Step 400/14394 | Loss: 6.3433
  Epoch 0 | Step 500/14394 | Loss: 6.1814
  Epoch 0 | Step 600/14394 | Loss: 6.1284
  Epoch 0 | Step 700/14394 | Loss: 6.0710
  Epoch 0 | Step 800/14394 | Loss: 5.9584
  Epoch 0 | Step 900/14394 | Loss: 5.9414
  Epoch 0 | Step 1000/14394 | Loss: 5.9238
  Epoch 0 | Step 1100/14394 | Loss: 5.6955
  Epoch 0 | Step 1200/14394 | Loss: 5.8603
  Epoch 0 | Step 1300/14394 | Loss: 5.6856
  Epoch 0 | Step 1400/14394 | Loss: 5.6413
  Epoch 0 | Step 1500/14394 | Loss: 5.8077
  Epoch 0 | Step 1600/14394 | Loss: 5.7094
  Epoch 0 | Step 1700/14394 | Loss: 5.6366
  Epoch 0 | Step 1800/14394 | Loss: 5.4962
  Epoch 0 | Step 1900/14394 | Loss: 5.5860
  Epoch 0 | Step 2000/14394 | Loss: 5.5893
  Epoch 0 | Step 2100/14394 | Loss: 5.4864
  Epoch 0 | Step 2200/14394 | Loss: 5.6051
  Epoch 0 | Step 2300/14394 | Loss: 5.3757
  Epoch 0 | Step 2400/14394 | Loss: 5.5332
  Epoch 0 | Step 2500/14394 | Loss: 5.3542
  Epoch 0 | Step 2600/14394 | Loss: 5.4885
  Epoch 0 | Step 2700/14394 | Loss: 5.3403
  Epoch 0 | Step 2800/14394 | Loss: 5.3030
  Epoch 0 | Step 2900/14394 | Loss: 5.3078
  Epoch 0 | Step 3000/14394 | Loss: 5.3755
  Epoch 0 | Step 3100/14394 | Loss: 5.3522
  Epoch 0 | Step 3200/14394 | Loss: 5.4593
  Epoch 0 | Step 3300/14394 | Loss: 5.2815
  Epoch 0 | Step 3400/14394 | Loss: 5.2060
  Epoch 0 | Step 3500/14394 | Loss: 5.1900
  Epoch 0 | Step 3600/14394 | Loss: 5.1943
  Epoch 0 | Step 3700/14394 | Loss: 5.1000
  Epoch 0 | Step 3800/14394 | Loss: 5.2596
  Epoch 0 | Step 3900/14394 | Loss: 5.3049
  Epoch 0 | Step 4000/14394 | Loss: 5.0278
  Epoch 0 | Step 4100/14394 | Loss: 5.2006
  Epoch 0 | Step 4200/14394 | Loss: 5.1069
  Epoch 0 | Step 4300/14394 | Loss: 5.1051
  Epoch 0 | Step 4400/14394 | Loss: 5.1354
  Epoch 0 | Step 4500/14394 | Loss: 5.0664
  Epoch 0 | Step 4600/14394 | Loss: 5.1588
  Epoch 0 | Step 4700/14394 | Loss: 5.0709
  Epoch 0 | Step 4800/14394 | Loss: 5.1038
  Epoch 0 | Step 4900/14394 | Loss: 4.9398
  Epoch 0 | Step 5000/14394 | Loss: 5.2392

======================================================================
SANITY CHECK AT STEP 5000
======================================================================
Current LR: 3.00e-04
Input shape: torch.Size([32, 256])
Target shape: torch.Size([32, 256])
Non-ignored labels: 8192/8192 (100.0%)
Sequence length T: 256
======================================================================

  Epoch 0 | Step 5100/14394 | Loss: 5.0830
  Epoch 0 | Step 5200/14394 | Loss: 5.0949
  Epoch 0 | Step 5300/14394 | Loss: 5.0144
  Epoch 0 | Step 5400/14394 | Loss: 5.0699
  Epoch 0 | Step 5500/14394 | Loss: 4.9617
  Epoch 0 | Step 5600/14394 | Loss: 4.9708
  Epoch 0 | Step 5700/14394 | Loss: 4.8392
  Epoch 0 | Step 5800/14394 | Loss: 5.0015
  Epoch 0 | Step 5900/14394 | Loss: 4.9919
  Epoch 0 | Step 6000/14394 | Loss: 5.0092
  Epoch 0 | Step 6100/14394 | Loss: 4.8255
  Epoch 0 | Step 6200/14394 | Loss: 4.8177
  Epoch 0 | Step 6300/14394 | Loss: 4.7880
  Epoch 0 | Step 6400/14394 | Loss: 4.9425
  Epoch 0 | Step 6500/14394 | Loss: 4.8634
  Epoch 0 | Step 6600/14394 | Loss: 4.9565
  Epoch 0 | Step 6700/14394 | Loss: 4.9889
  Epoch 0 | Step 6800/14394 | Loss: 5.0715
  Epoch 0 | Step 6900/14394 | Loss: 4.9424
  Epoch 0 | Step 7000/14394 | Loss: 4.7530
  Epoch 0 | Step 7100/14394 | Loss: 4.8091
  Epoch 0 | Step 7200/14394 | Loss: 4.8768
  Epoch 0 | Step 7300/14394 | Loss: 5.0544
  Epoch 0 | Step 7400/14394 | Loss: 4.9141
  Epoch 0 | Step 7500/14394 | Loss: 4.8547
  Epoch 0 | Step 7600/14394 | Loss: 4.7044
  Epoch 0 | Step 7700/14394 | Loss: 4.8028
  Epoch 0 | Step 7800/14394 | Loss: 4.7595
  Epoch 0 | Step 7900/14394 | Loss: 4.8657
  Epoch 0 | Step 8000/14394 | Loss: 4.7720
  Epoch 0 | Step 8100/14394 | Loss: 4.6442
  Epoch 0 | Step 8200/14394 | Loss: 4.8583
  Epoch 0 | Step 8300/14394 | Loss: 4.7878
  Epoch 0 | Step 8400/14394 | Loss: 4.6910
  Epoch 0 | Step 8500/14394 | Loss: 4.7191
  Epoch 0 | Step 8600/14394 | Loss: 4.8147
  Epoch 0 | Step 8700/14394 | Loss: 4.7762
  Epoch 0 | Step 8800/14394 | Loss: 4.9106
  Epoch 0 | Step 8900/14394 | Loss: 4.8061
  Epoch 0 | Step 9000/14394 | Loss: 4.9154
  Epoch 0 | Step 9100/14394 | Loss: 4.7238
  Epoch 0 | Step 9200/14394 | Loss: 4.7905
  Epoch 0 | Step 9300/14394 | Loss: 4.8264
  Epoch 0 | Step 9400/14394 | Loss: 4.9802
  Epoch 0 | Step 9500/14394 | Loss: 4.9076
  Epoch 0 | Step 9600/14394 | Loss: 4.8499
  Epoch 0 | Step 9700/14394 | Loss: 4.8030
  Epoch 0 | Step 9800/14394 | Loss: 4.9155
  Epoch 0 | Step 9900/14394 | Loss: 4.8379
  ✓ Validation at step 10000: All weights and gradients valid
  Epoch 0 | Step 10000/14394 | Loss: 4.7913
  Gradient flow stats saved with snapshot:
    Block: decoder
    Mean gain: 1.0950
    Healthy: 3, Damping: 0, Amplifying: 0
Snapshot saved: result/lm_wikitext-103/snapshots/snapshot_epoch_0000_step_009999_20260215_221903.pt

======================================================================
SANITY CHECK AT STEP 10000
======================================================================
Current LR: 3.00e-04
Input shape: torch.Size([32, 256])
Target shape: torch.Size([32, 256])
Non-ignored labels: 8192/8192 (100.0%)
Sequence length T: 256
======================================================================

  Epoch 0 | Step 10100/14394 | Loss: 4.7991
  Epoch 0 | Step 10200/14394 | Loss: 4.8654
  Epoch 0 | Step 10300/14394 | Loss: 4.9227
  Epoch 0 | Step 10400/14394 | Loss: 4.7880
  Epoch 0 | Step 10500/14394 | Loss: 4.8082
  Epoch 0 | Step 10600/14394 | Loss: 4.8104
  Epoch 0 | Step 10700/14394 | Loss: 4.8356
  Epoch 0 | Step 10800/14394 | Loss: 4.7542
  Epoch 0 | Step 10900/14394 | Loss: 4.7525
  Epoch 0 | Step 11000/14394 | Loss: 4.6720
  Epoch 0 | Step 11100/14394 | Loss: 4.5637
  Epoch 0 | Step 11200/14394 | Loss: 4.8223
  Epoch 0 | Step 11300/14394 | Loss: 4.6385
  Epoch 0 | Step 11400/14394 | Loss: 4.5364
  Epoch 0 | Step 11500/14394 | Loss: 4.7540
  Epoch 0 | Step 11600/14394 | Loss: 4.5901
  Epoch 0 | Step 11700/14394 | Loss: 4.8885
  Epoch 0 | Step 11800/14394 | Loss: 4.6391
  Epoch 0 | Step 11900/14394 | Loss: 4.8305
  Epoch 0 | Step 12000/14394 | Loss: 4.7929
  Epoch 0 | Step 12100/14394 | Loss: 4.6087
  Epoch 0 | Step 12200/14394 | Loss: 4.7631
  Epoch 0 | Step 12300/14394 | Loss: 4.7125
  Epoch 0 | Step 12400/14394 | Loss: 4.7216
  Epoch 0 | Step 12500/14394 | Loss: 4.7822
  Epoch 0 | Step 12600/14394 | Loss: 4.6416
  Epoch 0 | Step 12700/14394 | Loss: 4.7361
  Epoch 0 | Step 12800/14394 | Loss: 4.7873
  Epoch 0 | Step 12900/14394 | Loss: 4.8211
  Epoch 0 | Step 13000/14394 | Loss: 4.7243
  Epoch 0 | Step 13100/14394 | Loss: 4.8149
  Epoch 0 | Step 13200/14394 | Loss: 4.7092
  Epoch 0 | Step 13300/14394 | Loss: 4.7249
  Epoch 0 | Step 13400/14394 | Loss: 4.7201
  Epoch 0 | Step 13500/14394 | Loss: 4.5060
  Epoch 0 | Step 13600/14394 | Loss: 4.6561
  Epoch 0 | Step 13700/14394 | Loss: 4.6039
  Epoch 0 | Step 13800/14394 | Loss: 4.6243
  Epoch 0 | Step 13900/14394 | Loss: 4.6684
  Epoch 0 | Step 14000/14394 | Loss: 4.6475
  Epoch 0 | Step 14100/14394 | Loss: 4.5478
  Epoch 0 | Step 14200/14394 | Loss: 4.6835
  Epoch 0 | Step 14300/14394 | Loss: 4.5983
Epoch 0 | Train Loss: 5.0782 | Val Loss: 4.4228
  Early stopping: new best loss 4.4228 at epoch 0
  Gradient flow stats saved with snapshot:
    Block: decoder
    Mean gain: 1.1472
    Healthy: 3, Damping: 0, Amplifying: 0
Snapshot saved: result/lm_wikitext-103/snapshots/snapshot_epoch_0000_step_000000_20260215_223006.pt
  Epoch 1 | Step 1/14394 | Loss: 4.4386
  Epoch 1 | Step 100/14394 | Loss: 4.7016
  Epoch 1 | Step 200/14394 | Loss: 4.7281
  Epoch 1 | Step 300/14394 | Loss: 4.5414
  Epoch 1 | Step 400/14394 | Loss: 4.6311
  Epoch 1 | Step 500/14394 | Loss: 4.5389
  Epoch 1 | Step 600/14394 | Loss: 4.7764

======================================================================
SANITY CHECK AT STEP 15000
======================================================================
Current LR: 2.93e-04
Input shape: torch.Size([32, 256])
Target shape: torch.Size([32, 256])
Non-ignored labels: 8192/8192 (100.0%)
Sequence length T: 256
======================================================================

  Epoch 1 | Step 700/14394 | Loss: 4.7232
  Epoch 1 | Step 800/14394 | Loss: 4.6065
  Epoch 1 | Step 900/14394 | Loss: 4.6218
  Epoch 1 | Step 1000/14394 | Loss: 4.5922
  Epoch 1 | Step 1100/14394 | Loss: 4.6661
  Epoch 1 | Step 1200/14394 | Loss: 4.5709
  Epoch 1 | Step 1300/14394 | Loss: 4.6754
  Epoch 1 | Step 1400/14394 | Loss: 4.7103
  Epoch 1 | Step 1500/14394 | Loss: 4.6323
  Epoch 1 | Step 1600/14394 | Loss: 4.6184
  Epoch 1 | Step 1700/14394 | Loss: 4.6254
  Epoch 1 | Step 1800/14394 | Loss: 4.5224
  Epoch 1 | Step 1900/14394 | Loss: 4.5684
  Epoch 1 | Step 2000/14394 | Loss: 4.4362
  Epoch 1 | Step 2100/14394 | Loss: 4.7292
  Epoch 1 | Step 2200/14394 | Loss: 4.6460
  Epoch 1 | Step 2300/14394 | Loss: 4.4610
  Epoch 1 | Step 2400/14394 | Loss: 4.5190
  Epoch 1 | Step 2500/14394 | Loss: 4.3076
  Epoch 1 | Step 2600/14394 | Loss: 4.7352
  Epoch 1 | Step 2700/14394 | Loss: 4.5744
  Epoch 1 | Step 2800/14394 | Loss: 4.5472
  Epoch 1 | Step 2900/14394 | Loss: 4.7658
  Epoch 1 | Step 3000/14394 | Loss: 4.6994
  Epoch 1 | Step 3100/14394 | Loss: 4.5376
  Epoch 1 | Step 3200/14394 | Loss: 4.5814
  Epoch 1 | Step 3300/14394 | Loss: 4.3971
  Epoch 1 | Step 3400/14394 | Loss: 4.5544
  Epoch 1 | Step 3500/14394 | Loss: 4.5643
  Epoch 1 | Step 3600/14394 | Loss: 4.5034
  Epoch 1 | Step 3700/14394 | Loss: 4.5501
  Epoch 1 | Step 3800/14394 | Loss: 4.5906
  Epoch 1 | Step 3900/14394 | Loss: 4.7016
  Epoch 1 | Step 4000/14394 | Loss: 4.6708
  Epoch 1 | Step 4100/14394 | Loss: 4.6742
  Epoch 1 | Step 4200/14394 | Loss: 4.5084
  Epoch 1 | Step 4300/14394 | Loss: 4.5903
  Epoch 1 | Step 4400/14394 | Loss: 4.7748
  Epoch 1 | Step 4500/14394 | Loss: 4.6950
  Epoch 1 | Step 4600/14394 | Loss: 4.5706
  Epoch 1 | Step 4700/14394 | Loss: 4.5639
  Epoch 1 | Step 4800/14394 | Loss: 4.5853
  Epoch 1 | Step 4900/14394 | Loss: 4.5955
  Epoch 1 | Step 5000/14394 | Loss: 4.6262
  Epoch 1 | Step 5100/14394 | Loss: 4.4313
  Epoch 1 | Step 5200/14394 | Loss: 4.5009
  Epoch 1 | Step 5300/14394 | Loss: 4.4159
  Epoch 1 | Step 5400/14394 | Loss: 4.4627
  Epoch 1 | Step 5500/14394 | Loss: 4.4937
  Epoch 1 | Step 5600/14394 | Loss: 4.5725
  ✓ Validation at step 20000: All weights and gradients valid

======================================================================
SANITY CHECK AT STEP 20000
======================================================================
Current LR: 2.93e-04
Input shape: torch.Size([32, 256])
Target shape: torch.Size([32, 256])
Non-ignored labels: 8192/8192 (100.0%)
Sequence length T: 256
======================================================================

  Epoch 1 | Step 5700/14394 | Loss: 4.4897
  Epoch 1 | Step 5800/14394 | Loss: 4.6147
  Epoch 1 | Step 5900/14394 | Loss: 4.5787
  Epoch 1 | Step 6000/14394 | Loss: 4.8300
  Epoch 1 | Step 6100/14394 | Loss: 4.3958
  Epoch 1 | Step 6200/14394 | Loss: 4.5526
  Epoch 1 | Step 6300/14394 | Loss: 4.4837
  Epoch 1 | Step 6400/14394 | Loss: 4.5677
  Epoch 1 | Step 6500/14394 | Loss: 4.6721
  Epoch 1 | Step 6600/14394 | Loss: 4.6539
  Epoch 1 | Step 6700/14394 | Loss: 4.5031
  Epoch 1 | Step 6800/14394 | Loss: 4.6104
  Epoch 1 | Step 6900/14394 | Loss: 4.4346
  Epoch 1 | Step 7000/14394 | Loss: 4.5030
  Epoch 1 | Step 7100/14394 | Loss: 4.6073
  Epoch 1 | Step 7200/14394 | Loss: 4.6713
  Epoch 1 | Step 7300/14394 | Loss: 4.5232
  Epoch 1 | Step 7400/14394 | Loss: 4.5744
  Epoch 1 | Step 7500/14394 | Loss: 4.5976
  Epoch 1 | Step 7600/14394 | Loss: 4.5482
  Epoch 1 | Step 7700/14394 | Loss: 4.5721
  Epoch 1 | Step 7800/14394 | Loss: 4.6643
  Epoch 1 | Step 7900/14394 | Loss: 4.3941
  Epoch 1 | Step 8000/14394 | Loss: 4.4422
  Epoch 1 | Step 8100/14394 | Loss: 4.5512
  Epoch 1 | Step 8200/14394 | Loss: 4.5969
  Epoch 1 | Step 8300/14394 | Loss: 4.6987
  Epoch 1 | Step 8400/14394 | Loss: 4.4327
  Epoch 1 | Step 8500/14394 | Loss: 4.6547
  Epoch 1 | Step 8600/14394 | Loss: 4.4902
  Epoch 1 | Step 8700/14394 | Loss: 4.6947
  Epoch 1 | Step 8800/14394 | Loss: 4.4032
  Epoch 1 | Step 8900/14394 | Loss: 4.5329
  Epoch 1 | Step 9000/14394 | Loss: 4.5299
  Epoch 1 | Step 9100/14394 | Loss: 4.5204
  Epoch 1 | Step 9200/14394 | Loss: 4.6488
  Epoch 1 | Step 9300/14394 | Loss: 4.5731
  Epoch 1 | Step 9400/14394 | Loss: 4.5876
  Epoch 1 | Step 9500/14394 | Loss: 4.6996
  Epoch 1 | Step 9600/14394 | Loss: 4.6804
  Epoch 1 | Step 9700/14394 | Loss: 4.5226
  Epoch 1 | Step 9800/14394 | Loss: 4.3570
  Epoch 1 | Step 9900/14394 | Loss: 4.5266
  Epoch 1 | Step 10000/14394 | Loss: 4.5747
  Gradient flow stats saved with snapshot:
    Block: decoder
    Mean gain: 1.1764
    Healthy: 3, Damping: 0, Amplifying: 0
Snapshot saved: result/lm_wikitext-103/snapshots/snapshot_epoch_0001_step_009999_20260215_225516.pt
  Epoch 1 | Step 10100/14394 | Loss: 4.5070
  Epoch 1 | Step 10200/14394 | Loss: 4.5127
  Epoch 1 | Step 10300/14394 | Loss: 4.3574
  Epoch 1 | Step 10400/14394 | Loss: 4.6152
  Epoch 1 | Step 10500/14394 | Loss: 4.5640
  Epoch 1 | Step 10600/14394 | Loss: 4.4599

======================================================================
SANITY CHECK AT STEP 25000
======================================================================
Current LR: 2.93e-04
Input shape: torch.Size([32, 256])
Target shape: torch.Size([32, 256])
Non-ignored labels: 8192/8192 (100.0%)
Sequence length T: 256
======================================================================

  Epoch 1 | Step 10700/14394 | Loss: 4.5881
  Epoch 1 | Step 10800/14394 | Loss: 4.4935
  Epoch 1 | Step 10900/14394 | Loss: 4.6908
  Epoch 1 | Step 11000/14394 | Loss: 4.3737
  Epoch 1 | Step 11100/14394 | Loss: 4.5625
  Epoch 1 | Step 11200/14394 | Loss: 4.5186
  Epoch 1 | Step 11300/14394 | Loss: 4.6901
  Epoch 1 | Step 11400/14394 | Loss: 4.6556
  Epoch 1 | Step 11500/14394 | Loss: 4.2950
  Epoch 1 | Step 11600/14394 | Loss: 4.7212
  Epoch 1 | Step 11700/14394 | Loss: 4.5141
  Epoch 1 | Step 11800/14394 | Loss: 4.5006
  Epoch 1 | Step 11900/14394 | Loss: 4.3399
  Epoch 1 | Step 12000/14394 | Loss: 4.5006
  Epoch 1 | Step 12100/14394 | Loss: 4.4981
  Epoch 1 | Step 12200/14394 | Loss: 4.5375
  Epoch 1 | Step 12300/14394 | Loss: 4.4617
  Epoch 1 | Step 12400/14394 | Loss: 4.4436
  Epoch 1 | Step 12500/14394 | Loss: 4.5316
  Epoch 1 | Step 12600/14394 | Loss: 4.5524
  Epoch 1 | Step 12700/14394 | Loss: 4.3500
  Epoch 1 | Step 12800/14394 | Loss: 4.3229
  Epoch 1 | Step 12900/14394 | Loss: 4.6250
  Epoch 1 | Step 13000/14394 | Loss: 4.3975
  Epoch 1 | Step 13100/14394 | Loss: 4.5767
  Epoch 1 | Step 13200/14394 | Loss: 4.4870
  Epoch 1 | Step 13300/14394 | Loss: 4.4968
  Epoch 1 | Step 13400/14394 | Loss: 4.5662
  Epoch 1 | Step 13500/14394 | Loss: 4.5052
  Epoch 1 | Step 13600/14394 | Loss: 4.6815
  Epoch 1 | Step 13700/14394 | Loss: 4.5432
  Epoch 1 | Step 13800/14394 | Loss: 4.4322
  Epoch 1 | Step 13900/14394 | Loss: 4.5075
  Epoch 1 | Step 14000/14394 | Loss: 4.5782
  Epoch 1 | Step 14100/14394 | Loss: 4.7529
  Epoch 1 | Step 14200/14394 | Loss: 4.4186
  Epoch 1 | Step 14300/14394 | Loss: 4.6918
Epoch 1 | Train Loss: 4.5492 | Val Loss: 4.2732
  Early stopping: new best loss 4.2732 at epoch 1
  Gradient flow stats saved with snapshot:
    Block: decoder
    Mean gain: 1.1940
    Healthy: 3, Damping: 0, Amplifying: 0
Snapshot saved: result/lm_wikitext-103/snapshots/snapshot_epoch_0001_step_000000_20260215_230626.pt
Cleaned up 1 old snapshots
  Epoch 2 | Step 1/14394 | Loss: 4.4456
  Epoch 2 | Step 100/14394 | Loss: 4.5079
  Epoch 2 | Step 200/14394 | Loss: 4.1926
  Epoch 2 | Step 300/14394 | Loss: 4.3695
  Epoch 2 | Step 400/14394 | Loss: 4.4335
  Epoch 2 | Step 500/14394 | Loss: 4.4268
  Epoch 2 | Step 600/14394 | Loss: 4.5424
  Epoch 2 | Step 700/14394 | Loss: 4.2822
  Epoch 2 | Step 800/14394 | Loss: 4.4911
  Epoch 2 | Step 900/14394 | Loss: 4.4732
  Epoch 2 | Step 1000/14394 | Loss: 4.5737
  Epoch 2 | Step 1100/14394 | Loss: 4.5781
  Epoch 2 | Step 1200/14394 | Loss: 4.4856
  ✓ Validation at step 30000: All weights and gradients valid

======================================================================
SANITY CHECK AT STEP 30000
======================================================================
Current LR: 2.71e-04
Input shape: torch.Size([32, 256])
Target shape: torch.Size([32, 256])
Non-ignored labels: 8192/8192 (100.0%)
Sequence length T: 256
======================================================================

  Epoch 2 | Step 1300/14394 | Loss: 4.5406
  Epoch 2 | Step 1400/14394 | Loss: 4.5560
  Epoch 2 | Step 1500/14394 | Loss: 4.5148
  Epoch 2 | Step 1600/14394 | Loss: 4.3596
  Epoch 2 | Step 1700/14394 | Loss: 4.4391
  Epoch 2 | Step 1800/14394 | Loss: 4.3735
  Epoch 2 | Step 1900/14394 | Loss: 4.6242
  Epoch 2 | Step 2000/14394 | Loss: 4.4331
  Epoch 2 | Step 2100/14394 | Loss: 4.4762
  Epoch 2 | Step 2200/14394 | Loss: 4.3354
  Epoch 2 | Step 2300/14394 | Loss: 4.4109
  Epoch 2 | Step 2400/14394 | Loss: 4.5631
  Epoch 2 | Step 2500/14394 | Loss: 4.1305
  Epoch 2 | Step 2600/14394 | Loss: 4.5727
  Epoch 2 | Step 2700/14394 | Loss: 4.4819
  Epoch 2 | Step 2800/14394 | Loss: 4.3851
  Epoch 2 | Step 2900/14394 | Loss: 4.5449
  Epoch 2 | Step 3000/14394 | Loss: 4.4748
  Epoch 2 | Step 3100/14394 | Loss: 4.5033
  Epoch 2 | Step 3200/14394 | Loss: 4.3756
  Epoch 2 | Step 3300/14394 | Loss: 4.3592
  Epoch 2 | Step 3400/14394 | Loss: 4.3779
  Epoch 2 | Step 3500/14394 | Loss: 4.3880
  Epoch 2 | Step 3600/14394 | Loss: 4.5124
  Epoch 2 | Step 3700/14394 | Loss: 4.4436
  Epoch 2 | Step 3800/14394 | Loss: 4.5196
  Epoch 2 | Step 3900/14394 | Loss: 4.5037
  Epoch 2 | Step 4000/14394 | Loss: 4.4404
  Epoch 2 | Step 4100/14394 | Loss: 4.1758
  Epoch 2 | Step 4200/14394 | Loss: 4.5524
  Epoch 2 | Step 4300/14394 | Loss: 4.6657
  Epoch 2 | Step 4400/14394 | Loss: 4.5333
  Epoch 2 | Step 4500/14394 | Loss: 4.3256
  Epoch 2 | Step 4600/14394 | Loss: 4.6398
  Epoch 2 | Step 4700/14394 | Loss: 4.3907
  Epoch 2 | Step 4800/14394 | Loss: 4.5091
  Epoch 2 | Step 4900/14394 | Loss: 4.6531
  Epoch 2 | Step 5000/14394 | Loss: 4.4523
  Epoch 2 | Step 5100/14394 | Loss: 4.5251
  Epoch 2 | Step 5200/14394 | Loss: 4.3814
  Epoch 2 | Step 5300/14394 | Loss: 4.5521
  Epoch 2 | Step 5400/14394 | Loss: 4.4295
  Epoch 2 | Step 5500/14394 | Loss: 4.3882
  Epoch 2 | Step 5600/14394 | Loss: 4.5068
  Epoch 2 | Step 5700/14394 | Loss: 4.2600
  Epoch 2 | Step 5800/14394 | Loss: 4.5568
  Epoch 2 | Step 5900/14394 | Loss: 4.5583
  Epoch 2 | Step 6000/14394 | Loss: 4.5995
  Epoch 2 | Step 6100/14394 | Loss: 4.3915
  Epoch 2 | Step 6200/14394 | Loss: 4.5286

======================================================================
SANITY CHECK AT STEP 35000
======================================================================
Current LR: 2.71e-04
Input shape: torch.Size([32, 256])
Target shape: torch.Size([32, 256])
Non-ignored labels: 8192/8192 (100.0%)
Sequence length T: 256
======================================================================

  Epoch 2 | Step 6300/14394 | Loss: 4.3605
  Epoch 2 | Step 6400/14394 | Loss: 4.5354
  Epoch 2 | Step 6500/14394 | Loss: 4.5111
  Epoch 2 | Step 6600/14394 | Loss: 4.4485
  Epoch 2 | Step 6700/14394 | Loss: 4.4637
  Epoch 2 | Step 6800/14394 | Loss: 4.3372
  Epoch 2 | Step 6900/14394 | Loss: 4.3945
  Epoch 2 | Step 7000/14394 | Loss: 4.4565
  Epoch 2 | Step 7100/14394 | Loss: 4.5990
  Epoch 2 | Step 7200/14394 | Loss: 4.3175
  Epoch 2 | Step 7300/14394 | Loss: 4.4214
  Epoch 2 | Step 7400/14394 | Loss: 4.5503
  Epoch 2 | Step 7500/14394 | Loss: 4.4178
  Epoch 2 | Step 7600/14394 | Loss: 4.5805
  Epoch 2 | Step 7700/14394 | Loss: 4.3215
  Epoch 2 | Step 7800/14394 | Loss: 4.3953
  Epoch 2 | Step 7900/14394 | Loss: 4.6937
  Epoch 2 | Step 8000/14394 | Loss: 4.4249
  Epoch 2 | Step 8100/14394 | Loss: 4.4495
  Epoch 2 | Step 8200/14394 | Loss: 4.6660
  Epoch 2 | Step 8300/14394 | Loss: 4.3611
  Epoch 2 | Step 8400/14394 | Loss: 4.4352
  Epoch 2 | Step 8500/14394 | Loss: 4.4374
  Epoch 2 | Step 8600/14394 | Loss: 4.5295
  Epoch 2 | Step 8700/14394 | Loss: 4.3958
  Epoch 2 | Step 8800/14394 | Loss: 4.4638
  Epoch 2 | Step 8900/14394 | Loss: 4.4312
  Epoch 2 | Step 9000/14394 | Loss: 4.6091
  Epoch 2 | Step 9100/14394 | Loss: 4.3909
  Epoch 2 | Step 9200/14394 | Loss: 4.4699
  Epoch 2 | Step 9300/14394 | Loss: 4.4456
  Epoch 2 | Step 9400/14394 | Loss: 4.4911
  Epoch 2 | Step 9500/14394 | Loss: 4.4901
  Epoch 2 | Step 9600/14394 | Loss: 4.6527
  Epoch 2 | Step 9700/14394 | Loss: 4.3744
  Epoch 2 | Step 9800/14394 | Loss: 4.4467
  Epoch 2 | Step 9900/14394 | Loss: 4.4961
  Epoch 2 | Step 10000/14394 | Loss: 4.4779
  Gradient flow stats saved with snapshot:
    Block: decoder
    Mean gain: 1.1837
    Healthy: 3, Damping: 0, Amplifying: 0
Snapshot saved: result/lm_wikitext-103/snapshots/snapshot_epoch_0002_step_009999_20260215_233142.pt
Cleaned up 1 old snapshots
  Epoch 2 | Step 10100/14394 | Loss: 4.4204
  Epoch 2 | Step 10200/14394 | Loss: 4.4463
  Epoch 2 | Step 10300/14394 | Loss: 4.3186
  Epoch 2 | Step 10400/14394 | Loss: 4.4449
  Epoch 2 | Step 10500/14394 | Loss: 4.4120
  Epoch 2 | Step 10600/14394 | Loss: 4.4135
  Epoch 2 | Step 10700/14394 | Loss: 4.5083
  Epoch 2 | Step 10800/14394 | Loss: 4.4418
  Epoch 2 | Step 10900/14394 | Loss: 4.3749
  Epoch 2 | Step 11000/14394 | Loss: 4.4824
  Epoch 2 | Step 11100/14394 | Loss: 4.4835
  Epoch 2 | Step 11200/14394 | Loss: 4.4198
  ✓ Validation at step 40000: All weights and gradients valid

======================================================================
SANITY CHECK AT STEP 40000
======================================================================
Current LR: 2.71e-04
Input shape: torch.Size([32, 256])
Target shape: torch.Size([32, 256])
Non-ignored labels: 8192/8192 (100.0%)
Sequence length T: 256
======================================================================

  Epoch 2 | Step 11300/14394 | Loss: 4.4152
  Epoch 2 | Step 11400/14394 | Loss: 4.4213
  Epoch 2 | Step 11500/14394 | Loss: 4.3127
  Epoch 2 | Step 11600/14394 | Loss: 4.4464
  Epoch 2 | Step 11700/14394 | Loss: 4.3532
  Epoch 2 | Step 11800/14394 | Loss: 4.4521
  Epoch 2 | Step 11900/14394 | Loss: 4.5307
  Epoch 2 | Step 12000/14394 | Loss: 4.3983
  Epoch 2 | Step 12100/14394 | Loss: 4.4617
  Epoch 2 | Step 12200/14394 | Loss: 4.3759
  Epoch 2 | Step 12300/14394 | Loss: 4.2465
  Epoch 2 | Step 12400/14394 | Loss: 4.4348
  Epoch 2 | Step 12500/14394 | Loss: 4.4056
  Epoch 2 | Step 12600/14394 | Loss: 4.4085
  Epoch 2 | Step 12700/14394 | Loss: 4.4933
  Epoch 2 | Step 12800/14394 | Loss: 4.4498
  Epoch 2 | Step 12900/14394 | Loss: 4.2663
  Epoch 2 | Step 13000/14394 | Loss: 4.3777
  Epoch 2 | Step 13100/14394 | Loss: 4.2876
  Epoch 2 | Step 13200/14394 | Loss: 4.5519
  Epoch 2 | Step 13300/14394 | Loss: 4.6247
  Epoch 2 | Step 13400/14394 | Loss: 4.6183
  Epoch 2 | Step 13500/14394 | Loss: 4.2982
  Epoch 2 | Step 13600/14394 | Loss: 4.5172
  Epoch 2 | Step 13700/14394 | Loss: 4.3836
  Epoch 2 | Step 13800/14394 | Loss: 4.4844
  Epoch 2 | Step 13900/14394 | Loss: 4.3006
  Epoch 2 | Step 14000/14394 | Loss: 4.4259
  Epoch 2 | Step 14100/14394 | Loss: 4.4043
  Epoch 2 | Step 14200/14394 | Loss: 4.3261
  Epoch 2 | Step 14300/14394 | Loss: 4.4900
Epoch 2 | Train Loss: 4.4575 | Val Loss: 4.2125
  Early stopping: new best loss 4.2125 at epoch 2
  Gradient flow stats saved with snapshot:
    Block: decoder
    Mean gain: 1.1447
    Healthy: 3, Damping: 0, Amplifying: 0
Snapshot saved: result/lm_wikitext-103/snapshots/snapshot_epoch_0002_step_000000_20260215_234246.pt
Cleaned up 1 old snapshots
  Epoch 3 | Step 1/14394 | Loss: 4.3519
  Epoch 3 | Step 100/14394 | Loss: 4.4072
  Epoch 3 | Step 200/14394 | Loss: 4.5255
  Epoch 3 | Step 300/14394 | Loss: 4.2463
  Epoch 3 | Step 400/14394 | Loss: 4.4852
  Epoch 3 | Step 500/14394 | Loss: 4.4595
  Epoch 3 | Step 600/14394 | Loss: 4.3506
  Epoch 3 | Step 700/14394 | Loss: 4.3763
  Epoch 3 | Step 800/14394 | Loss: 4.4982
  Epoch 3 | Step 900/14394 | Loss: 4.3975
  Epoch 3 | Step 1000/14394 | Loss: 4.4216
  Epoch 3 | Step 1100/14394 | Loss: 4.6278
  Epoch 3 | Step 1200/14394 | Loss: 4.5007
  Epoch 3 | Step 1300/14394 | Loss: 4.4679
  Epoch 3 | Step 1400/14394 | Loss: 4.3787
  Epoch 3 | Step 1500/14394 | Loss: 4.3473
  Epoch 3 | Step 1600/14394 | Loss: 4.4393
  Epoch 3 | Step 1700/14394 | Loss: 4.4291
  Epoch 3 | Step 1800/14394 | Loss: 4.3757

======================================================================
SANITY CHECK AT STEP 45000
======================================================================
Current LR: 2.38e-04
Input shape: torch.Size([32, 256])
Target shape: torch.Size([32, 256])
Non-ignored labels: 8192/8192 (100.0%)
Sequence length T: 256
======================================================================

  Epoch 3 | Step 1900/14394 | Loss: 4.4745
  Epoch 3 | Step 2000/14394 | Loss: 4.4143
  Epoch 3 | Step 2100/14394 | Loss: 4.3826
  Epoch 3 | Step 2200/14394 | Loss: 4.4291
  Epoch 3 | Step 2300/14394 | Loss: 4.6656
  Epoch 3 | Step 2400/14394 | Loss: 4.1680
  Epoch 3 | Step 2500/14394 | Loss: 4.4046
  Epoch 3 | Step 2600/14394 | Loss: 4.3888
  Epoch 3 | Step 2700/14394 | Loss: 4.4158
  Epoch 3 | Step 2800/14394 | Loss: 4.3758
  Epoch 3 | Step 2900/14394 | Loss: 4.3726
  Epoch 3 | Step 3000/14394 | Loss: 4.4834
  Epoch 3 | Step 3100/14394 | Loss: 4.3724
  Epoch 3 | Step 3200/14394 | Loss: 4.3227
  Epoch 3 | Step 3300/14394 | Loss: 4.4029
  Epoch 3 | Step 3400/14394 | Loss: 4.4344
  Epoch 3 | Step 3500/14394 | Loss: 4.3519
  Epoch 3 | Step 3600/14394 | Loss: 4.3125
  Epoch 3 | Step 3700/14394 | Loss: 4.3298
  Epoch 3 | Step 3800/14394 | Loss: 4.3664
  Epoch 3 | Step 3900/14394 | Loss: 4.3951
  Epoch 3 | Step 4000/14394 | Loss: 4.3080
  Epoch 3 | Step 4100/14394 | Loss: 4.4284
  Epoch 3 | Step 4200/14394 | Loss: 4.3020
  Epoch 3 | Step 4300/14394 | Loss: 4.4277
  Epoch 3 | Step 4400/14394 | Loss: 4.4264
  Epoch 3 | Step 4500/14394 | Loss: 4.2876
  Epoch 3 | Step 4600/14394 | Loss: 4.4637
  Epoch 3 | Step 4700/14394 | Loss: 4.5426
  Epoch 3 | Step 4800/14394 | Loss: 4.5581
  Epoch 3 | Step 4900/14394 | Loss: 4.5035
  Epoch 3 | Step 5000/14394 | Loss: 4.5489
  Epoch 3 | Step 5100/14394 | Loss: 4.4797
  Epoch 3 | Step 5200/14394 | Loss: 4.4443
  Epoch 3 | Step 5300/14394 | Loss: 4.3480
  Epoch 3 | Step 5400/14394 | Loss: 4.4434
  Epoch 3 | Step 5500/14394 | Loss: 4.3873
  Epoch 3 | Step 5600/14394 | Loss: 4.4831
  Epoch 3 | Step 5700/14394 | Loss: 4.3481
  Epoch 3 | Step 5800/14394 | Loss: 4.4223
  Epoch 3 | Step 5900/14394 | Loss: 4.3702
  Epoch 3 | Step 6000/14394 | Loss: 4.5022
  Epoch 3 | Step 6100/14394 | Loss: 4.3705
  Epoch 3 | Step 6200/14394 | Loss: 4.3321
  Epoch 3 | Step 6300/14394 | Loss: 4.3480
  Epoch 3 | Step 6400/14394 | Loss: 4.5091
  Epoch 3 | Step 6500/14394 | Loss: 4.4904
  Epoch 3 | Step 6600/14394 | Loss: 4.5352
  Epoch 3 | Step 6700/14394 | Loss: 4.4505
  Epoch 3 | Step 6800/14394 | Loss: 4.2914
  ✓ Validation at step 50000: All weights and gradients valid

======================================================================
SANITY CHECK AT STEP 50000
======================================================================
Current LR: 2.38e-04
Input shape: torch.Size([32, 256])
Target shape: torch.Size([32, 256])
Non-ignored labels: 8192/8192 (100.0%)
Sequence length T: 256
======================================================================

  Epoch 3 | Step 6900/14394 | Loss: 4.3571
  Epoch 3 | Step 7000/14394 | Loss: 4.3348
  Epoch 3 | Step 7100/14394 | Loss: 4.3761
  Epoch 3 | Step 7200/14394 | Loss: 4.2903
  Epoch 3 | Step 7300/14394 | Loss: 4.3843
  Epoch 3 | Step 7400/14394 | Loss: 4.2798
  Epoch 3 | Step 7500/14394 | Loss: 4.4519
  Epoch 3 | Step 7600/14394 | Loss: 4.2635
  Epoch 3 | Step 7700/14394 | Loss: 4.3819
  Epoch 3 | Step 7800/14394 | Loss: 4.3531
  Epoch 3 | Step 7900/14394 | Loss: 4.5914
  Epoch 3 | Step 8000/14394 | Loss: 4.3660
  Epoch 3 | Step 8100/14394 | Loss: 4.3048
  Epoch 3 | Step 8200/14394 | Loss: 4.3961
  Epoch 3 | Step 8300/14394 | Loss: 4.2974
  Epoch 3 | Step 8400/14394 | Loss: 4.3590
  Epoch 3 | Step 8500/14394 | Loss: 4.3538
  Epoch 3 | Step 8600/14394 | Loss: 4.4896
  Epoch 3 | Step 8700/14394 | Loss: 4.4814
  Epoch 3 | Step 8800/14394 | Loss: 4.5071
  Epoch 3 | Step 8900/14394 | Loss: 4.4842
  Epoch 3 | Step 9000/14394 | Loss: 4.4041
  Epoch 3 | Step 9100/14394 | Loss: 4.4862
  Epoch 3 | Step 9200/14394 | Loss: 4.3289
  Epoch 3 | Step 9300/14394 | Loss: 4.5289
  Epoch 3 | Step 9400/14394 | Loss: 4.3923
  Epoch 3 | Step 9500/14394 | Loss: 4.3429
  Epoch 3 | Step 9600/14394 | Loss: 4.5505
  Epoch 3 | Step 9700/14394 | Loss: 4.3738
  Epoch 3 | Step 9800/14394 | Loss: 4.4080
  Epoch 3 | Step 9900/14394 | Loss: 4.4415
  Epoch 3 | Step 10000/14394 | Loss: 4.3985
  Gradient flow stats saved with snapshot:
    Block: decoder
    Mean gain: 1.1389
    Healthy: 3, Damping: 0, Amplifying: 0
Snapshot saved: result/lm_wikitext-103/snapshots/snapshot_epoch_0003_step_009999_20260216_000749.pt
Cleaned up 1 old snapshots
  Epoch 3 | Step 10100/14394 | Loss: 4.2479
  Epoch 3 | Step 10200/14394 | Loss: 4.3282
  Epoch 3 | Step 10300/14394 | Loss: 4.5045
  Epoch 3 | Step 10400/14394 | Loss: 4.4009
  Epoch 3 | Step 10500/14394 | Loss: 4.5908
  Epoch 3 | Step 10600/14394 | Loss: 4.5825
  Epoch 3 | Step 10700/14394 | Loss: 4.5218
  Epoch 3 | Step 10800/14394 | Loss: 4.4699
  Epoch 3 | Step 10900/14394 | Loss: 4.2704
  Epoch 3 | Step 11000/14394 | Loss: 4.3563
  Epoch 3 | Step 11100/14394 | Loss: 4.3721
  Epoch 3 | Step 11200/14394 | Loss: 4.3072
  Epoch 3 | Step 11300/14394 | Loss: 4.3790
  Epoch 3 | Step 11400/14394 | Loss: 4.4549
  Epoch 3 | Step 11500/14394 | Loss: 4.3491
  Epoch 3 | Step 11600/14394 | Loss: 4.4781
  Epoch 3 | Step 11700/14394 | Loss: 4.4787
  Epoch 3 | Step 11800/14394 | Loss: 4.4637

======================================================================
SANITY CHECK AT STEP 55000
======================================================================
Current LR: 2.38e-04
Input shape: torch.Size([32, 256])
Target shape: torch.Size([32, 256])
Non-ignored labels: 8192/8192 (100.0%)
Sequence length T: 256
======================================================================

  Epoch 3 | Step 11900/14394 | Loss: 4.3406
  Epoch 3 | Step 12000/14394 | Loss: 4.4475
  Epoch 3 | Step 12100/14394 | Loss: 4.3646
  Epoch 3 | Step 12200/14394 | Loss: 4.3901
  Epoch 3 | Step 12300/14394 | Loss: 4.3893
  Epoch 3 | Step 12400/14394 | Loss: 4.3625
  Epoch 3 | Step 12500/14394 | Loss: 4.5317
  Epoch 3 | Step 12600/14394 | Loss: 4.3664
  Epoch 3 | Step 12700/14394 | Loss: 4.4169
  Epoch 3 | Step 12800/14394 | Loss: 4.3985
  Epoch 3 | Step 12900/14394 | Loss: 4.4097
  Epoch 3 | Step 13000/14394 | Loss: 4.3483
  Epoch 3 | Step 13100/14394 | Loss: 4.3611
  Epoch 3 | Step 13200/14394 | Loss: 4.3575
  Epoch 3 | Step 13300/14394 | Loss: 4.3931
  Epoch 3 | Step 13400/14394 | Loss: 4.3264
  Epoch 3 | Step 13500/14394 | Loss: 4.3757
  Epoch 3 | Step 13600/14394 | Loss: 4.7110
  Epoch 3 | Step 13700/14394 | Loss: 4.4922
  Epoch 3 | Step 13800/14394 | Loss: 4.3834
  Epoch 3 | Step 13900/14394 | Loss: 4.5072
  Epoch 3 | Step 14000/14394 | Loss: 4.4219
  Epoch 3 | Step 14100/14394 | Loss: 4.2333
  Epoch 3 | Step 14200/14394 | Loss: 4.3865
  Epoch 3 | Step 14300/14394 | Loss: 4.4263
Epoch 3 | Train Loss: 4.4078 | Val Loss: 4.1854
  Early stopping: new best loss 4.1854 at epoch 3
  Gradient flow stats saved with snapshot:
    Block: decoder
    Mean gain: 1.1496
    Healthy: 3, Damping: 0, Amplifying: 0
Snapshot saved: result/lm_wikitext-103/snapshots/snapshot_epoch_0003_step_000000_20260216_001852.pt
Cleaned up 1 old snapshots
  Epoch 4 | Step 1/14394 | Loss: 4.4258
  Epoch 4 | Step 100/14394 | Loss: 4.4223
  Epoch 4 | Step 200/14394 | Loss: 4.5295
  Epoch 4 | Step 300/14394 | Loss: 4.4739
  Epoch 4 | Step 400/14394 | Loss: 4.3213
  Epoch 4 | Step 500/14394 | Loss: 4.4865
  Epoch 4 | Step 600/14394 | Loss: 4.2229
  Epoch 4 | Step 700/14394 | Loss: 4.4160
  Epoch 4 | Step 800/14394 | Loss: 4.3776
  Epoch 4 | Step 900/14394 | Loss: 4.2743
  Epoch 4 | Step 1000/14394 | Loss: 4.4594
  Epoch 4 | Step 1100/14394 | Loss: 4.4426
  Epoch 4 | Step 1200/14394 | Loss: 4.3265
  Epoch 4 | Step 1300/14394 | Loss: 4.4077
  Epoch 4 | Step 1400/14394 | Loss: 4.3632
  Epoch 4 | Step 1500/14394 | Loss: 4.4271
  Epoch 4 | Step 1600/14394 | Loss: 4.4070
  Epoch 4 | Step 1700/14394 | Loss: 4.5864
  Epoch 4 | Step 1800/14394 | Loss: 4.4742
  Epoch 4 | Step 1900/14394 | Loss: 4.4397
  Epoch 4 | Step 2000/14394 | Loss: 4.3957
  Epoch 4 | Step 2100/14394 | Loss: 4.4567
  Epoch 4 | Step 2200/14394 | Loss: 4.3249
  Epoch 4 | Step 2300/14394 | Loss: 4.3685
  Epoch 4 | Step 2400/14394 | Loss: 4.4970
  ✓ Validation at step 60000: All weights and gradients valid

======================================================================
SANITY CHECK AT STEP 60000
======================================================================
Current LR: 1.96e-04
Input shape: torch.Size([32, 256])
Target shape: torch.Size([32, 256])
Non-ignored labels: 8192/8192 (100.0%)
Sequence length T: 256
======================================================================

  Epoch 4 | Step 2500/14394 | Loss: 4.3897
  Epoch 4 | Step 2600/14394 | Loss: 4.3711
  Epoch 4 | Step 2700/14394 | Loss: 4.3745
  Epoch 4 | Step 2800/14394 | Loss: 4.4630
  Epoch 4 | Step 2900/14394 | Loss: 4.5367
  Epoch 4 | Step 3000/14394 | Loss: 4.3989
  Epoch 4 | Step 3100/14394 | Loss: 4.1565
  Epoch 4 | Step 3200/14394 | Loss: 4.4817
  Epoch 4 | Step 3300/14394 | Loss: 4.3423
  Epoch 4 | Step 3400/14394 | Loss: 4.4427
  Epoch 4 | Step 3500/14394 | Loss: 4.2305
  Epoch 4 | Step 3600/14394 | Loss: 4.4380
  Epoch 4 | Step 3700/14394 | Loss: 4.5700
  Epoch 4 | Step 3800/14394 | Loss: 4.2567
  Epoch 4 | Step 3900/14394 | Loss: 4.4114
  Epoch 4 | Step 4000/14394 | Loss: 4.3938
  Epoch 4 | Step 4100/14394 | Loss: 4.4457
  Epoch 4 | Step 4200/14394 | Loss: 4.2876
  Epoch 4 | Step 4300/14394 | Loss: 4.5682
  Epoch 4 | Step 4400/14394 | Loss: 4.0401
  Epoch 4 | Step 4500/14394 | Loss: 4.4296
  Epoch 4 | Step 4600/14394 | Loss: 4.5502
  Epoch 4 | Step 4700/14394 | Loss: 4.3011
  Epoch 4 | Step 4800/14394 | Loss: 4.5420
  Epoch 4 | Step 4900/14394 | Loss: 4.3673
  Epoch 4 | Step 5000/14394 | Loss: 4.4479
  Epoch 4 | Step 5100/14394 | Loss: 4.4935
  Epoch 4 | Step 5200/14394 | Loss: 4.4333
  Epoch 4 | Step 5300/14394 | Loss: 4.4973
  Epoch 4 | Step 5400/14394 | Loss: 4.2191
  Epoch 4 | Step 5500/14394 | Loss: 4.2053
  Epoch 4 | Step 5600/14394 | Loss: 4.4256
  Epoch 4 | Step 5700/14394 | Loss: 4.4866
  Epoch 4 | Step 5800/14394 | Loss: 4.4411
  Epoch 4 | Step 5900/14394 | Loss: 4.3672
  Epoch 4 | Step 6000/14394 | Loss: 4.4774
  Epoch 4 | Step 6100/14394 | Loss: 4.5182
  Epoch 4 | Step 6200/14394 | Loss: 4.3990
  Epoch 4 | Step 6300/14394 | Loss: 4.3884
  Epoch 4 | Step 6400/14394 | Loss: 4.3715
  Epoch 4 | Step 6500/14394 | Loss: 4.3898
  Epoch 4 | Step 6600/14394 | Loss: 4.4617
  Epoch 4 | Step 6700/14394 | Loss: 4.3429
  Epoch 4 | Step 6800/14394 | Loss: 4.1790
  Epoch 4 | Step 6900/14394 | Loss: 4.2496
  Epoch 4 | Step 7000/14394 | Loss: 4.4980
  Epoch 4 | Step 7100/14394 | Loss: 4.4025
  Epoch 4 | Step 7200/14394 | Loss: 4.5938
  Epoch 4 | Step 7300/14394 | Loss: 4.2901
  Epoch 4 | Step 7400/14394 | Loss: 4.2922

======================================================================
SANITY CHECK AT STEP 65000
======================================================================
Current LR: 1.96e-04
Input shape: torch.Size([32, 256])
Target shape: torch.Size([32, 256])
Non-ignored labels: 8192/8192 (100.0%)
Sequence length T: 256
======================================================================

  Epoch 4 | Step 7500/14394 | Loss: 4.3345
  Epoch 4 | Step 7600/14394 | Loss: 4.3943
  Epoch 4 | Step 7700/14394 | Loss: 4.4043
  Epoch 4 | Step 7800/14394 | Loss: 4.3273
  Epoch 4 | Step 7900/14394 | Loss: 4.4515
  Epoch 4 | Step 8000/14394 | Loss: 4.5166
  Epoch 4 | Step 8100/14394 | Loss: 4.2244
  Epoch 4 | Step 8200/14394 | Loss: 4.4848
  Epoch 4 | Step 8300/14394 | Loss: 4.3132
  Epoch 4 | Step 8400/14394 | Loss: 4.4036
  Epoch 4 | Step 8500/14394 | Loss: 4.3179
  Epoch 4 | Step 8600/14394 | Loss: 4.5477
  Epoch 4 | Step 8700/14394 | Loss: 4.2760
  Epoch 4 | Step 8800/14394 | Loss: 4.2838
  Epoch 4 | Step 8900/14394 | Loss: 4.1368
  Epoch 4 | Step 9000/14394 | Loss: 4.3843
  Epoch 4 | Step 9100/14394 | Loss: 4.5002
  Epoch 4 | Step 9200/14394 | Loss: 4.2426
  Epoch 4 | Step 9300/14394 | Loss: 4.4123
  Epoch 4 | Step 9400/14394 | Loss: 4.4596
  Epoch 4 | Step 9500/14394 | Loss: 4.3947
  Epoch 4 | Step 9600/14394 | Loss: 4.4473
  Epoch 4 | Step 9700/14394 | Loss: 4.3263
  Epoch 4 | Step 9800/14394 | Loss: 4.1869
  Epoch 4 | Step 9900/14394 | Loss: 4.3117
  Epoch 4 | Step 10000/14394 | Loss: 4.1591
  Gradient flow stats saved with snapshot:
    Block: decoder
    Mean gain: 1.1468
    Healthy: 3, Damping: 0, Amplifying: 0
Snapshot saved: result/lm_wikitext-103/snapshots/snapshot_epoch_0004_step_009999_20260216_004400.pt
Cleaned up 1 old snapshots
  Epoch 4 | Step 10100/14394 | Loss: 4.3686
  Epoch 4 | Step 10200/14394 | Loss: 4.2403
  Epoch 4 | Step 10300/14394 | Loss: 4.3319
  Epoch 4 | Step 10400/14394 | Loss: 4.3221
  Epoch 4 | Step 10500/14394 | Loss: 4.1747
  Epoch 4 | Step 10600/14394 | Loss: 4.4711
  Epoch 4 | Step 10700/14394 | Loss: 4.4144
  Epoch 4 | Step 10800/14394 | Loss: 4.4073
  Epoch 4 | Step 10900/14394 | Loss: 4.1851
  Epoch 4 | Step 11000/14394 | Loss: 4.4319
  Epoch 4 | Step 11100/14394 | Loss: 4.4036
  Epoch 4 | Step 11200/14394 | Loss: 4.4460
  Epoch 4 | Step 11300/14394 | Loss: 4.3698
  Epoch 4 | Step 11400/14394 | Loss: 4.2045
  Epoch 4 | Step 11500/14394 | Loss: 4.2732
  Epoch 4 | Step 11600/14394 | Loss: 4.4446
  Epoch 4 | Step 11700/14394 | Loss: 4.2115
  Epoch 4 | Step 11800/14394 | Loss: 4.3110
  Epoch 4 | Step 11900/14394 | Loss: 4.4332
  Epoch 4 | Step 12000/14394 | Loss: 4.4636
  Epoch 4 | Step 12100/14394 | Loss: 4.4578
  Epoch 4 | Step 12200/14394 | Loss: 4.2091
  Epoch 4 | Step 12300/14394 | Loss: 4.2382
  Epoch 4 | Step 12400/14394 | Loss: 4.3943
  ✓ Validation at step 70000: All weights and gradients valid

======================================================================
SANITY CHECK AT STEP 70000
======================================================================
Current LR: 1.96e-04
Input shape: torch.Size([32, 256])
Target shape: torch.Size([32, 256])
Non-ignored labels: 8192/8192 (100.0%)
Sequence length T: 256
======================================================================

  Epoch 4 | Step 12500/14394 | Loss: 4.4522
  Epoch 4 | Step 12600/14394 | Loss: 4.2847
  Epoch 4 | Step 12700/14394 | Loss: 4.3980
  Epoch 4 | Step 12800/14394 | Loss: 4.3784
  Epoch 4 | Step 12900/14394 | Loss: 4.4978
  Epoch 4 | Step 13000/14394 | Loss: 4.4847
  Epoch 4 | Step 13100/14394 | Loss: 4.3350
  Epoch 4 | Step 13200/14394 | Loss: 4.2038
  Epoch 4 | Step 13300/14394 | Loss: 4.2393
  Epoch 4 | Step 13400/14394 | Loss: 4.1893
  Epoch 4 | Step 13500/14394 | Loss: 4.3249
  Epoch 4 | Step 13600/14394 | Loss: 4.3769
  Epoch 4 | Step 13700/14394 | Loss: 4.3700
  Epoch 4 | Step 13800/14394 | Loss: 4.2838
  Epoch 4 | Step 13900/14394 | Loss: 4.2319
  Epoch 4 | Step 14000/14394 | Loss: 4.4905
  Epoch 4 | Step 14100/14394 | Loss: 4.4734
  Epoch 4 | Step 14200/14394 | Loss: 4.3377
  Epoch 4 | Step 14300/14394 | Loss: 4.3639
Epoch 4 | Train Loss: 4.3789 | Val Loss: 4.1545
  Early stopping: new best loss 4.1545 at epoch 4
  Gradient flow stats saved with snapshot:
    Block: decoder
    Mean gain: 1.1279
    Healthy: 3, Damping: 0, Amplifying: 0
Snapshot saved: result/lm_wikitext-103/snapshots/snapshot_epoch_0004_step_000000_20260216_005503.pt
Cleaned up 1 old snapshots
  Epoch 5 | Step 1/14394 | Loss: 4.3667
  Epoch 5 | Step 100/14394 | Loss: 4.2621
  Epoch 5 | Step 200/14394 | Loss: 4.5118
  Epoch 5 | Step 300/14394 | Loss: 4.3771
  Epoch 5 | Step 400/14394 | Loss: 4.2994
  Epoch 5 | Step 500/14394 | Loss: 4.2658
  Epoch 5 | Step 600/14394 | Loss: 4.4197
  Epoch 5 | Step 700/14394 | Loss: 4.2276
  Epoch 5 | Step 800/14394 | Loss: 4.4465
  Epoch 5 | Step 900/14394 | Loss: 4.3363
  Epoch 5 | Step 1000/14394 | Loss: 4.4698
  Epoch 5 | Step 1100/14394 | Loss: 4.3052
  Epoch 5 | Step 1200/14394 | Loss: 4.3598
  Epoch 5 | Step 1300/14394 | Loss: 4.3968
  Epoch 5 | Step 1400/14394 | Loss: 4.3776
  Epoch 5 | Step 1500/14394 | Loss: 4.3245
  Epoch 5 | Step 1600/14394 | Loss: 4.3977
  Epoch 5 | Step 1700/14394 | Loss: 4.4336
  Epoch 5 | Step 1800/14394 | Loss: 4.3360
  Epoch 5 | Step 1900/14394 | Loss: 4.2708
  Epoch 5 | Step 2000/14394 | Loss: 4.3083
  Epoch 5 | Step 2100/14394 | Loss: 4.1934
  Epoch 5 | Step 2200/14394 | Loss: 4.3586
  Epoch 5 | Step 2300/14394 | Loss: 4.3420
  Epoch 5 | Step 2400/14394 | Loss: 4.3524
  Epoch 5 | Step 2500/14394 | Loss: 4.4437
  Epoch 5 | Step 2600/14394 | Loss: 4.3787
  Epoch 5 | Step 2700/14394 | Loss: 4.4186
  Epoch 5 | Step 2800/14394 | Loss: 4.3543
  Epoch 5 | Step 2900/14394 | Loss: 4.2967
  Epoch 5 | Step 3000/14394 | Loss: 4.4015

======================================================================
SANITY CHECK AT STEP 75000
======================================================================
Current LR: 1.50e-04
Input shape: torch.Size([32, 256])
Target shape: torch.Size([32, 256])
Non-ignored labels: 8192/8192 (100.0%)
Sequence length T: 256
======================================================================

  Epoch 5 | Step 3100/14394 | Loss: 4.5544
  Epoch 5 | Step 3200/14394 | Loss: 4.4401
  Epoch 5 | Step 3300/14394 | Loss: 4.3145
  Epoch 5 | Step 3400/14394 | Loss: 4.2322
  Epoch 5 | Step 3500/14394 | Loss: 4.3113
  Epoch 5 | Step 3600/14394 | Loss: 4.4495
  Epoch 5 | Step 3700/14394 | Loss: 4.2598
  Epoch 5 | Step 3800/14394 | Loss: 4.3359
  Epoch 5 | Step 3900/14394 | Loss: 4.1594
  Epoch 5 | Step 4000/14394 | Loss: 4.3920
  Epoch 5 | Step 4100/14394 | Loss: 4.3063
  Epoch 5 | Step 4200/14394 | Loss: 4.2789
  Epoch 5 | Step 4300/14394 | Loss: 4.3958
  Epoch 5 | Step 4400/14394 | Loss: 4.3477
  Epoch 5 | Step 4500/14394 | Loss: 4.2845
  Epoch 5 | Step 4600/14394 | Loss: 4.3107
  Epoch 5 | Step 4700/14394 | Loss: 4.2326
  Epoch 5 | Step 4800/14394 | Loss: 4.3151
  Epoch 5 | Step 4900/14394 | Loss: 4.2655
  Epoch 5 | Step 5000/14394 | Loss: 4.3352
  Epoch 5 | Step 5100/14394 | Loss: 4.3380
  Epoch 5 | Step 5200/14394 | Loss: 4.1817
  Epoch 5 | Step 5300/14394 | Loss: 4.5188
  Epoch 5 | Step 5400/14394 | Loss: 4.3942
  Epoch 5 | Step 5500/14394 | Loss: 4.3713
  Epoch 5 | Step 5600/14394 | Loss: 4.3979
  Epoch 5 | Step 5700/14394 | Loss: 4.2333
  Epoch 5 | Step 5800/14394 | Loss: 4.1781
  Epoch 5 | Step 5900/14394 | Loss: 4.4276
  Epoch 5 | Step 6000/14394 | Loss: 4.2121
  Epoch 5 | Step 6100/14394 | Loss: 4.4815
  Epoch 5 | Step 6200/14394 | Loss: 4.2483
  Epoch 5 | Step 6300/14394 | Loss: 4.4047
  Epoch 5 | Step 6400/14394 | Loss: 4.5454
  Epoch 5 | Step 6500/14394 | Loss: 4.3383
  Epoch 5 | Step 6600/14394 | Loss: 4.3583
  Epoch 5 | Step 6700/14394 | Loss: 4.3267
  Epoch 5 | Step 6800/14394 | Loss: 4.2635
