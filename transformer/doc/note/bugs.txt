================================================================================
Training System - Open Issues and Backlogs
================================================================================

Status: All previously tracked issues have been resolved ✅

This file tracks open issues that need fixing. Currently there are no open
issues tracked in this file.

For completed fixes, see:
- doc/change/v0.5/FIX_COMPLETE.md
- doc/change/v0.5/EMA_LOSS_MONITOR_COMPLETE.md
- doc/change/v0.5/WIGHT_UPDATE_MONITOR_COMPLETE.md
- doc/change/v0.5/OVERFITTING_DETECTION_COMPLETE.md
- doc/change/v0.5/INTEGRATION_COMPLETE.md
- doc/change/v0.5/WARMUP_SCHEDULER_TEST_COVERAGE_COMPLETE.md
- doc/change/v0.5/LM_SCHEDULER_STEPPING_FIX_COMPLETE.md
- doc/change/v0.5/TRANSLATION_PAD_MASK_TEST_FIX_COMPLETE.md
- doc/change/v0.5/TARGET_PADDING_MASK_COMPLETE.md

For implementation guides, see:
- doc/implementation/DATASET_CHUNKING.md
- doc/implementation/EMA_LOSS_MONITOR.md
- doc/implementation/WEIGHT_MONITOR_INTEGRATION.md
- doc/implementation/GRADIENT_MONITOR.md
- doc/implementation/OVERFITTING_DETECTION.md
- doc/implementation/CALLBACKS.md

================================================================================


[PRIORITY] Short issue description
File: path/to/file.py:line
Date: YYYY-MM-DD

Issue:
  Detailed description of the problem

Expected:
  Expected behavior

Suggested fix:
  Optional suggested solution

================================================================================
Last updated: 2026-02-16
================================================================================
Here’s a focused review of the issues mentioned under `/doc`, and whether they’re resolved for seq2seq training via `train_translation.py` + `model.py`.

**Summary**
- All doc-listed issues are now resolved for seq2seq training via `train_translation.py` + `model.py`.
- ✅ FIXED (2026-02-16): Target padding mask now properly applied in decoder self-attention.

**Doc Issues vs Current Seq2Seq Code**

1. **Sliding window / stride=1 chunking (LM)**
- **Doc:** `doc/implementation/DATASET_CHUNKING.md`
- **Status for seq2seq:** Not applicable. This is LM‑only and does not affect translation data.
- **Evidence:** Translation uses paired sequences in `TranslationDataLoaderFactory`, not windowed chunks.
  - `/Users/oonisim/home/repository/git/oonisim/python-programs/transformer/src/training/loader_translation.py`

2. **Attention attending to PAD tokens**
- **Doc:** `doc/change/v0.5/FIX_COMPLETE.md` (Attention Padding Mask implemented)
- **Status:** ✅ FULLY RESOLVED (2026-02-16)
- **Resolved for encoder/cross-attention:** Yes.
  - `translation_collate_fn` returns `source_pad_mask`.
  - `Trainer._train_one_step` forwards `source_pad_mask` into model.
  - `ScaledDotProductAttention` accepts `padding_mask`.
  - `/Users/oonisim/home/repository/git/oonisim/python-programs/transformer/src/training/loader_translation.py`
  - `/Users/oonisim/home/repository/git/oonisim/python-programs/transformer/src/training/trainer.py`
  - `/Users/oonisim/home/repository/git/oonisim/python-programs/transformer/src/model/common.py`
  - `/Users/oonisim/home/repository/git/oonisim/python-programs/transformer/src/model/encoder.py`
  - `/Users/oonisim/home/repository/git/oonisim/python-programs/transformer/src/model/model.py`
- **✅ FIXED - Decoder self-attention:** `DecodeLayer` now accepts and applies `target_pad_mask` for causal self-attention.
  - `Trainer` shifts and passes `target_pad_mask` to model.
  - All attention types now properly masked: encoder self-attention, decoder self-attention, cross-attention.
  - `/Users/onishima/Documents/home/python-programs/transformer/src/model/decoder.py`
  - `/Users/onishima/Documents/home/python-programs/transformer/src/training/trainer.py`
  - See: `doc/change/v0.5/TARGET_PADDING_MASK_COMPLETE.md`

3. **EOS/BOS masked when `pad_token_id == eos_token_id`**
- **Doc:** `doc/change/v0.5/FIX_COMPLETE.md` (masking by length, not by token ID)
- **Status:** Resolved in collate.
- **Evidence:** `translation_collate_fn` builds masks from sequence lengths, not token IDs.
  - `/Users/oonisim/home/repository/git/oonisim/python-programs/transformer/src/training/loader_translation.py`

4. **EOS unlearnable because loss ignores pad token**
- **Doc:** `doc/change/v0.5/FIX_COMPLETE.md` (use ignore_index=-100 + mask)
- **Status:** Resolved.
- **Evidence:** Criterion uses `ignore_index=LABEL_IGNORE_VALUE` and trainer masks `decoder_target` by `target_pad_mask`.
  - `/Users/oonisim/home/repository/git/oonisim/python-programs/transformer/src/training/train_translation.py`
  - `/Users/oonisim/home/repository/git/oonisim/python-programs/transformer/src/training/trainer.py`

5. **Decoder input corruption by `LABEL_IGNORE_VALUE`**
- **Doc:** `doc/change/v0.5/FIX_COMPLETE.md` (mask after shift)
- **Status:** Resolved.
- **Evidence:** Collate returns clean `target_ids`; trainer masks `decoder_target` only.
  - `/Users/oonisim/home/repository/git/oonisim/python-programs/transformer/src/training/loader_translation.py`
  - `/Users/oonisim/home/repository/git/oonisim/python-programs/transformer/src/training/trainer.py`

6. **Missing BOS token for target**
- **Doc:** `doc/change/v0.5/FIX_COMPLETE.md`
- **Status:** Resolved.
- **Evidence:** Target is prepended with BOS (set to EOS) and appended with EOS.
  - `/Users/oonisim/home/repository/git/oonisim/python-programs/transformer/src/training/loader_translation.py`

7. **Source padding mask in `generate()`**
- **Doc:** `doc/change/v0.5/FIX_COMPLETE.md`
- **Status:** Resolved.
- **Evidence:** `Transformer.generate()` accepts and forwards `source_pad_mask`.
  - `/Users/oonisim/home/repository/git/oonisim/python-programs/transformer/src/model/model.py`

8. **Eval mode restoration in `generate()`/`evaluate()`**
- **Doc:** `doc/change/v0.5/FIX_COMPLETE.md`
- **Status:** Resolved.
- **Evidence:** Both `generate()` and `evaluate()` save/restore training state.
  - `/Users/oonisim/home/repository/git/oonisim/python-programs/transformer/src/model/model.py`

================================================================================
Test Issues
================================================================================

test_translation_source_pad_mask_does_not_mask_real_eos_when_pad_equals_eos

  Location: src/test/test_training_pitfalls.py

  What it tests: When pad_token_id == eos_token_id, the source padding mask should not hide real EOS tokens.

source_ids[0] = [1, 2, 0]     # Original length 3, 0 is real EOS
  source_ids[1] = [5, 0, 0]     # Original length 2, first 0 is real EOS, second is padding
  mask =          [[F, F, F],
                   [F, F, T]]

  The test expects all EOS tokens (including padding) to not be masked, but position [1,2] is actual padding added by pad_sequence. The current implementation correctly masks it based on length.

  The test appears to have a flaw - it's checking that padding tokens (which happen to be eos_id) should not be masked, which defeats the purpose of masking. Let me check if this test should be marked as xfail or if there's documentation
  about it:


  Why it fails:
  # Test creates sequences:
  batch[0]: source_ids = [1, 2, 0]  # 0 is real EOS (length 3)
  batch[1]: source_ids = [5, 0]     # 0 is real EOS (length 2)

  # After padding to max length (3):
  source_ids[0] = [1, 2, 0]     # All real tokens
  source_ids[1] = [5, 0, 0]     # First 0 is real EOS, second 0 is PADDING

  # Current mask (based on sequence lengths):
  mask = [[F, F, F],    # Batch 0: all real
          [F, F, T]]    # Batch 1: last position is padding

  # Test expects: All positions with token_id==0 should have mask=False
  # But position [1,2] IS actual padding, so mask=True is correct!

  The Issue: The test has a conceptual flaw. It expects that ALL tokens with value eos_id (including artificial padding) should not be masked. But the current implementation correctly distinguishes between:
  - Real EOS: Position [1,1] - not masked ✓
  - Padding that happens to be eos_id: Position [1,2] - correctly masked ✓

  The implementation is working as intended. The test seems to be checking for an impossible requirement (distinguishing padding from real EOS purely by token value, which is impossible when they're the same value).

  This is likely a test bug, not an implementation bug. The length-based masking approach in translation_collate_fn is the correct solution.
================================================================================
Recent Fixes (2026-02-16)
================================================================================

✅ LanguageModelTrainer Scheduler Stepping Fix
   - Bug: LanguageModelTrainer._train_one_step() was not stepping scheduler per batch
   - Impact: Warmup schedules did not work for language model training
   - Fixed: Added _step_scheduler_if_configured() call after optimizer.step()
   - Location: src/training/trainer.py lines 1191-1192
   - Test: test_lm_scheduler_steps_per_batch_when_enabled now passes
   - See: doc/change/v0.5/LM_SCHEDULER_STEPPING_FIX_COMPLETE.md

✅ Translation Padding Mask Test Fix
   - Bug: Test expected impossible behavior (all eos_id tokens not masked)
   - Impact: Test was failing despite correct implementation
   - Fixed: Test now validates length-based masking correctly
   - Location: src/test/test_training_pitfalls.py
   - Test: test_translation_source_pad_mask_does_not_mask_real_eos_when_pad_equals_eos now passes
   - See: doc/change/v0.5/TRANSLATION_PAD_MASK_TEST_FIX_COMPLETE.md

All Tests Passing: 11/11 ✓
