[P2] generate() does not validate max_length <= decoder_max_time_steps, which can blow up inside positional encoding mid‑generation. File: model.py
[P2] LanguageModelDataset.__len__ is off by one and discards a valid last sample, including the len(tokens) == seq_len + 1 case. File: loader.py 
[P2] LanguageModel.generate doesn’t validate temperature (<=0 → NaN/inf scores). File: lm.py 
[P2] top_k is unvalidated, so invalid values crash torch.topk. File: lm.py 
[P2] top_p is unvalidated; out‑of‑range values can mask all tokens and yield NaNs. File: lm.py.
[P3] max_length < prompt_len silently returns the prompt unchanged. File: lm.py.