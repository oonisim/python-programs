[P2] Attention Padding Mask (NOT IMPLEMENTED)
     Status: Documented but not implemented

     What exists:
     - Documentation in loader_translation.py (lines 122-129) explains why
       padding masks are needed for attention

     What's missing:
     1. translation_collate_fn() doesn't create/return padding masks
        Need: source_pad_mask = (source_padded == source_pad_id)

     2. ScaledDotProductAttention.forward() has no padding_mask parameter
        Current: forward(q, k, v, return_similarities=False)
        Need: forward(q, k, v, padding_mask=None, return_similarities=False)

     3. No mask application before softmax in attention computation
        Need: similarities.masked_fill(padding_mask, float('-inf'))

     Impact: Padded tokens are attended to in variable-length batches,
     wasting attention mass and contaminating representations

     File: common.py lines 399-519
