● The Warmup Problem Explained:

  With warmup_steps=1000:

  At step 100, the LR multiplier would be:
  lr_multiplier = current_step / warmup_steps = 100 / 1000 = 0.1
  actual_lr = 0.0003 × 0.1 = 0.00003  # 10x smaller!

  At step 500:
  lr_multiplier = 500 / 1000 = 0.5
  actual_lr = 0.0003 × 0.5 = 0.00015  # Still 2x smaller!

  At steps 0-999, the LR was too small → Almost no weight updates → Loss stayed near log(V)

  Summary:

  ┌──────────────────┬─────────────────┬──────────────────┬────────────────┐
  │  Configuration   │   Step 100 LR   │ Loss at step 100 │     Result     │
  ├──────────────────┼─────────────────┼──────────────────┼────────────────┤
  │ With warmup=1000 │ 0.00003 (3e-05) │ ~10.9 (stuck)    │ ❌ No learning │
  ├──────────────────┼─────────────────┼──────────────────┼────────────────┤
  │ With warmup=0    │ 0.0003 (3e-04)  │ 6.73 (dropping)  │ ✅ Learning!   │
  └──────────────────┴─────────────────┴──────────────────┴────────────────┘

  The fix: Disabled warmup so LR starts at full 0.0003 from step 0, allowing immediate learning.


Root Cause Analysis

  1. Model is severely underpowered for WikiText-103

  Current: 16M params (d=256, h=4, L=4, d_ff=512)
  Dataset: ~118M tokens (WikiText-103)

  Comparison to baselines:
  - GPT-2 Small: 117M params, WikiText-103 perplexity: ~35-40
  - Your model: 16M params, perplexity: ~70

  Your model is 7× smaller than GPT-2 Small, which already struggles with WikiText-103.

  2. Sequence length mismatch

  Config: max_seq_len = 256
  Actual data: sequences up to 1063 tokens
  Warning in log: "Token indices sequence length is longer than the specified maximum sequence length"

  This truncates context, preventing the model from learning long-range dependencies.

  3. Training configuration issues

  - d_ff=512: Should be 4× d_model (1024, not 512) per Transformer paper
  - max_seq_len=256: Too short for WikiText-103
  - No warmup: LR scheduler jumps straight to 3e-4

  ---
  Recommendations

  Option A: Scale up the model (Recommended)

  For WikiText-103, you need a significantly larger model:

  # Minimum viable config for WikiText-103
  d_model = 512      # 2× current
  num_heads = 8      # 2× current
  num_layers = 6     # 1.5× current
  d_ff = 2048        # 4× d_model
  max_seq_len = 512  # 2× current
  # Results in ~45-50M params

  This should achieve perplexity ~50-60 with proper training.

  Option B: Switch to WikiText-2

  Your current 16M model is better sized for WikiText-2 (~2M tokens):

  python training/train_lm.py --dataset wikitext --epochs 10

  Expected perplexity: ~30-40 (much better than current 70)

  Option C: Significantly scale up for good WikiText-103 results

  To match GPT-2 Small performance (~35-40 perplexity):

  d_model = 768      # 3× current
  num_heads = 12
  num_layers = 12    # 3× current
  d_ff = 3072        # 4× d_model
  max_seq_len = 1024
  # Results in ~117M params (like GPT-2 Small)

  ---
  Other improvements to apply:

  1. ✅ Fix d_ff ratio: Changed from 512 to 4 × d_model (2048) - IMPLEMENTED
  2. ✅ Fix max_seq_len: Increased to 512 for Option A - IMPLEMENTED
  3. ✅ Add LR warmup: Warmup for 1000-4000 steps - IMPLEMENTED
  4. ✅ Proper LR: 3e-4 default, 6e-4 with warmup available - IMPLEMENTED
  5. ❌ Save model config in checkpoints: Currently missing

    Your poor model performance (perplexity ~70, incoherent text) was likely caused by MULTIPLE issues:

  1. ✅ Fixed bugs (EOS learning, eval mode, padding mask, EOS masking) - RESOLVED
  2. ✅ Model too small (16M params for WikiText-103) - Option A implemented (45-50M)
  3. ✅ Incorrect hyperparameters (d_ff=512 instead of 1024) - Fixed in Option A
  4. ✅ Short context (max_seq_len=256) - Fixed in Option A (512)

  Current Status (After All Fixes):

  Option A is now FULLY IMPLEMENTED and ready to use:
    - Model preset: --model_preset small
    - Parameters: ~45-50M (3× original)
    - d_ff ratio: 2048 (4× d_model) ✓
    - max_seq_len: 512 (2× original) ✓
    - LR warmup: 1000 steps default ✓
    - Expected perplexity: ~50-60 on WikiText-103

  Usage:
    ./run/run_train_lm.sh --preset small
    ./run/run_train_lm.sh --preset small --warmup 4000 --lr 6e-4

  See doc/note/option_a_training.md for complete guide.


