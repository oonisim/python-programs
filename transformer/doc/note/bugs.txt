  5. EOS Learning Fix (train_lm.py, train_translation.py)

  ✅ Correct ⚠️ INCOMPLETE
  - Changed ignore_index=pad_token_id → ignore_index=LABEL_IGNORE_VALUE
  - Comprehensive comments explain the bug
  - CRITICAL MISSING PIECE: Actual masking logic not yet implemented!

  ⚠️ What's Missing:

  The labels still contain pad_token_id values. We need to mask them to -100:

  # In trainer._train_one_step() or collate function:
  pad_mask = (decoder_target == pad_token_id)
  decoder_target = decoder_target.masked_fill(pad_mask, LABEL_IGNORE_VALUE)

  Without this, nothing is ignored and the fix has no effect yet.

[P2] Attention has no padding mask support; padded tokens will be attended to in variable‑length batches. File: common.py lines 399–519.
[P2] Translation training skips learning the first target token because there is no BOS/START token in target_ids before shifting. File: trainer.py lines 317–319.
[P2] EOS is effectively ignored during translation loss because pad_token is set to EOS and used as ignore_index, so the model never learns to end sequences. File: train_translation.py lines 255–263.