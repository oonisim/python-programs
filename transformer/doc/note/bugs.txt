You can estimate it directly from your **throughput**:

\[
\text{epoch time} \approx \frac{\text{steps per epoch}}{\text{steps processed per hour}}
\]

From your log:
- steps per epoch: **3,684,989**

After ~5 hours you were around step ~23,700 (from the snippet), so:

- steps/hour \(\approx 23{,}700 / 5 \approx 4{,}740\)
- epoch hours \(\approx 3{,}684{,}989 / 4{,}740 \approx 777\) hours
  \(\approx 32\) days

So at the current rate, **one epoch would take on the order of weeks**, not hours.

That strongly suggests your “steps per epoch” is not what you think (likely your dataset is generating an enormous number of training windows, e.g., sliding by 1 token). For GPT-style LM training you normally chunk into fixed blocks with a stride equal to the block size (or similar), giving far fewer steps.

### What to do
1) Print how many batches your DataLoader actually has:
```python
print("len(train_loader) =", len(train_loader))
```

2) Check your dataset windowing/stride:
- If you create sequences with stride 1 (sliding window), you’ll get ~one sequence per token → millions of batches → epoch takes forever.
- Use block chunking with stride = block_size (e.g., 512), not 1.

If you paste:
- `len(train_loader)`
- your dataset’s block/stride logic
I can tell you exactly how to change it so an epoch is a reasonable length.





Evidence in code

Scheduler creation uses warmup in train_lm.py (warmup steps are based on total steps).
Scheduler stepping happens in Trainer._post_epoch_operations() once per epoch in trainer.py.
So your observation is consistent, but the actual bug is that warmup is implemented as step‑based while the scheduler is epoch‑stepped.

Why disabling warmup “fixes” it
With warmup off, you fall back to CosineAnnealingLR, which is intended to be stepped per epoch, so it behaves correctly. That’s why you see immediate learning.

Bottom line
Warmup isn’t “too big”; it’s being applied at the wrong cadence (epochs instead of steps). That’s why loss stays stuck near 10.

If you want, I can propose the exact change to step the warmup scheduler per batch (without applying it to the cosine‑per‑epoch path).

● The Warmup Problem Explained:

  With warmup_steps=1000:

  At step 100, the LR multiplier would be:
  lr_multiplier = current_step / warmup_steps = 100 / 1000 = 0.1
  actual_lr = 0.0003 × 0.1 = 0.00003  # 10x smaller!

  At step 500:
  lr_multiplier = 500 / 1000 = 0.5
  actual_lr = 0.0003 × 0.5 = 0.00015  # Still 2x smaller!

  At steps 0-999, the LR was too small → Almost no weight updates → Loss stayed near log(V)

  Summary:

  ┌──────────────────┬─────────────────┬──────────────────┬────────────────┐
  │  Configuration   │   Step 100 LR   │ Loss at step 100 │     Result     │
  ├──────────────────┼─────────────────┼──────────────────┼────────────────┤
  │ With warmup=1000 │ 0.00003 (3e-05) │ ~10.9 (stuck)    │ ❌ No learning │
  ├──────────────────┼─────────────────┼──────────────────┼────────────────┤
  │ With warmup=0    │ 0.0003 (3e-04)  │ 6.73 (dropping)  │ ✅ Learning!   │
  └──────────────────┴─────────────────┴──────────────────┴────────────────┘

  The fix: Disabled warmup so LR starts at full 0.0003 from step 0, allowing immediate learning.


 The remaining known issues from prior rounds are:
  ┌──────────────────────────────────────────┬───────────────────┬──────────┐
  │                  Issue                   │     File:Line     │ Severity │
  ├──────────────────────────────────────────┼───────────────────┼──────────┤
  │ evaluate() doesn't restore training mode │ model.py:562      │ MEDIUM   │
  ├──────────────────────────────────────────┼───────────────────┼──────────┤
  │ Missing space in error message           │ model.py:600-601  │ LOW      │
  ├──────────────────────────────────────────┼───────────────────┼──────────┤
  │ Invalid pylint directive (missing colon) │ lm.py:364         │ LOW      │
  ├──────────────────────────────────────────┼───────────────────┼──────────┤
  │ Off-by-one in get_stats()                │ loader.py:255,259 │ LOW      │
  └──────────────────────────────────────────┴───────────────────┴──────────┘
  No new issues beyond these. Would you like me to fix the evaluate() one?



Root Cause Analysis

  1. Model is severely underpowered for WikiText-103

  Current: 16M params (d=256, h=4, L=4, d_ff=512)
  Dataset: ~118M tokens (WikiText-103)

  Comparison to baselines:
  - GPT-2 Small: 117M params, WikiText-103 perplexity: ~35-40
  - Your model: 16M params, perplexity: ~70

  Your model is 7× smaller than GPT-2 Small, which already struggles with WikiText-103.

  2. Sequence length mismatch

  Config: max_seq_len = 256
  Actual data: sequences up to 1063 tokens
  Warning in log: "Token indices sequence length is longer than the specified maximum sequence length"

  This truncates context, preventing the model from learning long-range dependencies.

  3. Training configuration issues

  - d_ff=512: Should be 4× d_model (1024, not 512) per Transformer paper
  - max_seq_len=256: Too short for WikiText-103
  - No warmup: LR scheduler jumps straight to 3e-4

  ---
  Recommendations

  Option A: Scale up the model (Recommended)

  For WikiText-103, you need a significantly larger model:

  # Minimum viable config for WikiText-103
  d_model = 512      # 2× current
  num_heads = 8      # 2× current
  num_layers = 6     # 1.5× current
  d_ff = 2048        # 4× d_model
  max_seq_len = 512  # 2× current
  # Results in ~45-50M params

  This should achieve perplexity ~50-60 with proper training.

  Option B: Switch to WikiText-2

  Your current 16M model is better sized for WikiText-2 (~2M tokens):

  python training/train_lm.py --dataset wikitext --epochs 10

  Expected perplexity: ~30-40 (much better than current 70)

  Option C: Significantly scale up for good WikiText-103 results

  To match GPT-2 Small performance (~35-40 perplexity):

  d_model = 768      # 3× current
  num_heads = 12
  num_layers = 12    # 3× current
  d_ff = 3072        # 4× d_model
  max_seq_len = 1024
  # Results in ~117M params (like GPT-2 Small)

  ---
  Other improvements to apply:

  1. ✅ Fix d_ff ratio: Changed from 512 to 4 × d_model (2048) - IMPLEMENTED
  2. ✅ Fix max_seq_len: Increased to 512 for Option A - IMPLEMENTED
  3. ✅ Add LR warmup: Warmup for 1000-4000 steps - IMPLEMENTED
  4. ✅ Proper LR: 3e-4 default, 6e-4 with warmup available - IMPLEMENTED
  5. ❌ Save model config in checkpoints: Currently missing

    Your poor model performance (perplexity ~70, incoherent text) was likely caused by MULTIPLE issues:

  1. ✅ Fixed bugs (EOS learning, eval mode, padding mask, EOS masking) - RESOLVED
  2. ✅ Model too small (16M params for WikiText-103) - Option A implemented (45-50M)
  3. ✅ Incorrect hyperparameters (d_ff=512 instead of 1024) - Fixed in Option A
  4. ✅ Short context (max_seq_len=256) - Fixed in Option A (512)

  Current Status (After All Fixes):

  Option A is now FULLY IMPLEMENTED and ready to use:
    - Model preset: --model_preset small
    - Parameters: ~45-50M (3× original)
    - d_ff ratio: 2048 (4× d_model) ✓
    - max_seq_len: 512 (2× original) ✓
    - LR warmup: 1000 steps default ✓
    - Expected perplexity: ~50-60 on WikiText-103

  Usage:
    ./run/run_train_lm.sh --preset small
    ./run/run_train_lm.sh --preset small --warmup 4000 --lr 6e-4

  See doc/note/option_a_training.md for complete guide.


