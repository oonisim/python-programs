[P2] Translation training skips learning the first target token because there is no BOS/START token in target_ids before shifting. File: trainer.py lines 317–319.
[P2] Attention has no padding mask support; padded tokens will be attended to in variable‑length batches. File: common.py lines 399–519.
