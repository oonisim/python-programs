{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b49c5f55",
   "metadata": {},
   "source": [
    "# KeyBERT Mechanism\n",
    "\n",
    "* [Keyword Extraction with BERT](https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea)\n",
    "\n",
    "The author of KeyBERT explains the keyword extraction using BERT (Sentence BERT).\n",
    "\n",
    "> Although there are many great papers and solutions out there that use BERT-embeddings (e.g., 1, 2, 3, ), I could not find a simple and easy-to-use BERT-based solution. Instead, I decide to create KeyBERT a minimal and easy-to-use keyword extraction technique that leverages BERT embeddings. Now, the main topic of this article will not be the use of KeyBERT but a tutorial on how to use BERT to create your own keyword extraction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2adfb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69e89bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a61fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "Supervised learning is the machine learning task of \n",
    "learning a function that maps an input to an output based \n",
    "on example input-output pairs.[1] It infers a function \n",
    "from labeled training data consisting of a set of \n",
    "training examples.[2] In supervised learning, each \n",
    "example is a pair consisting of an input object \n",
    "(typically a vector) and a desired output value (also \n",
    "called the supervisory signal). A supervised learning \n",
    "algorithm analyzes the training data and produces an \n",
    "inferred function, which can be used for mapping new \n",
    "examples. An optimal scenario will allow for the algorithm \n",
    "to correctly determine the class labels for unseen \n",
    "instances. This requires the learning algorithm to  \n",
    "generalize from the training data to unseen situations \n",
    "in a 'reasonable' way (see inductive bias).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9294d4be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_gram_range = (1, 2)\n",
    "stop_words = \"english\"\n",
    "\n",
    "# Extract candidate words/phrases\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "candidates = count.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "188ee7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['algorithm', 'algorithm analyzes', 'algorithm correctly',\n",
       "       'algorithm generalize', 'allow', 'allow algorithm', 'analyzes',\n",
       "       'analyzes training', 'based', 'based example'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a6af9f",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3aff38c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "612af217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 768)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa40489",
   "metadata": {},
   "source": [
    "# Document to words similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59bc5534",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c08cb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm analyzes',\n",
       " 'supervised learning',\n",
       " 'machine learning',\n",
       " 'learning machine',\n",
       " 'learning algorithm']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33673830",
   "metadata": {},
   "source": [
    "# Diversification\n",
    "\n",
    "1. Max Sum Similarity\n",
    "2. Maximal Marginal Relevance\n",
    "\n",
    "\n",
    "## Max Sum Similarity\n",
    "\n",
    "> maximize the candidate similarity to the document whilst minimizing the similarity between candidates.\n",
    "> <img src=\"./image/max_sum_similarity.png\" align=\"left\" width=700/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e61dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sum_sim(doc_embedding, word_embeddings, words, top_n, nr_candidates):\n",
    "    # Calculate distances and extract keywords\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # Get top_n words as candidates based on cosine similarity\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # Calculate the combination of words that are the least similar to each other\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "204e0233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['training data',\n",
       " 'algorithm generalize',\n",
       " 'supervised learning',\n",
       " 'machine learning',\n",
       " 'learning algorithm']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(\n",
    "    doc_embedding,\n",
    "    candidate_embeddings,\n",
    "    candidates,\n",
    "    top_n=5,\n",
    "    nr_candidates=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b385d73e",
   "metadata": {},
   "source": [
    "## Maximal Marginal Relevance (MMR)\n",
    "\n",
    "> MMR tries to minimize redundancy and maximize the diversity of results in text summarization tasks. Fortunately, a keyword extraction algorithm called EmbedRank has implemented a version of MMR that allows us to use it for diversifying our keywords/keyphrases.\n",
    "\n",
    "> <img src=\"./image/mmr.png\" align=\"left\" width=700/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "570ae4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmr(doc_embedding, word_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de76f152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning algorithm',\n",
       " 'machine learning',\n",
       " 'supervised learning',\n",
       " 'learning machine',\n",
       " 'algorithm generalize']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(\n",
    "    doc_embedding,\n",
    "    candidate_embeddings,\n",
    "    candidates,\n",
    "    top_n=5,\n",
    "    diversity=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f1919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
