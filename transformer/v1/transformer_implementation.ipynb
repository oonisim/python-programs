{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df3f532-40a2-4049-844f-12f0cea1127a",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "## Transformer\n",
    "\n",
    "* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "* [On Layer Normalization in the Transformer Architecture](https://arxiv.org/pdf/2002.04745.pdf)\n",
    "* [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)\n",
    "\n",
    "## Nano GPT\n",
    "\n",
    "Nano GPT implementation by Andrej Karpathy.\n",
    "\n",
    "### Nano GPT YouTube Lecture version\n",
    "\n",
    "This is for lecture only. The Github is different from the proper implementation of Nano GPT github. Do not confuse/mix two diffent Github repositories.\n",
    "\n",
    "* [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n",
    "* [Github - nanogpt-lecture](https://github.com/karpathy/ng-video-lecture)\n",
    "\n",
    "> Code created in the Neural Networks: Zero To Hero video lecture series, specifically on the first lecture on nanoGPT. Publishing here as a Github repo so people can easily hack it, walk through the git log history of it, etc.\n",
    "\n",
    "\n",
    "### Nano GPT implementation version\n",
    "* [Github nanoGPT](https://github.com/karpathy/nanoGPT)\n",
    "\n",
    "## Resources\n",
    "\n",
    "* [Understanding Large Language Models](https://magazine.sebastianraschka.com/p/understanding-large-language-models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee3a8cd-ee4d-4c61-a49a-28ab53917e81",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5710adfc-3fb3-4e3e-857c-331a5ab58489",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T01:05:22.316374112Z",
     "start_time": "2024-01-02T01:05:22.300633784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d6b471b-a6cb-4145-9400-5381c11b1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "DIR = os.path.dirname(os.path.abspath('..'))\n",
    "if DIR not in sys.path:\n",
    "    sys.path.append(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f507fe2d-3931-491a-a14a-25ba5edb12d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T01:05:24.138290904Z",
     "start_time": "2024-01-02T01:05:22.305989324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from bigram import (\n",
    "    get_batch,\n",
    ")\n",
    "\n",
    "from transformer.v1 import (\n",
    "    TYPE_FLOAT,\n",
    "    DROPOUT_RATIO,\n",
    "    initialize_weights,\n",
    "    split,\n",
    "    calculate_dot_product_similarities,\n",
    "    scale,\n",
    "    mask,\n",
    "    softmax,\n",
    "    calculate_attentions,\n",
    "    MultiHeadAttention,\n",
    "    ScaledDotProductAttention,\n",
    "    PositionwiseFeedForward,\n",
    "    PositionalEncoding,\n",
    "    EncodeLayer,\n",
    "    Encoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d056634-e1b3-4a21-a698-2f1f18837a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_printoptions(profile=\"full\")\n",
    "torch.set_printoptions(edgeitems=4)\n",
    "torch.set_printoptions(threshold=100)\n",
    "torch.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a77adc0-895c-431f-97a8-f8a9949dba5d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d9b0fe-8ac6-4ca3-b237-610999b0b055",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Using tinyshakespeare as the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2e53b20-2177-458e-9f98-60ce2135ef19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T01:05:24.796234523Z",
     "start_time": "2024-01-02T01:05:24.140384661Z"
    }
   },
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13271d1f-2fc5-4632-b867-01988054c251",
   "metadata": {},
   "source": [
    "# Terminologies\n",
    "\n",
    "* B: Batch size\n",
    "* T: Time steps or Sequence length (e.g. 512 for bert input sequence)\n",
    "* C: Channel or Feature (channel perhaps because Andrej is from CNN background?). ```C=2``` two features in each x."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45745a2a-c062-40af-90a2-65d372419312",
   "metadata": {},
   "source": [
    "## Batch Input\n",
    "\n",
    "<img src=\"./image/gpt_batch.jpeg\" align=\"left\" width=750/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891cf9afa4fb5110",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Bigram Language Model\n",
    "\n",
    "The Bigram Language Model is **NOT using context of size T** but using the current token to predict next token, hence **Bi**gram. The objective of a Language Model is to use the historycal context, but here in the Bigram Model, Andrej is building very simple Markov Chain process to **generate next token only from the current token** for the sake of explaning the basic idea of token generation with bare naked possible way as he mentioned at [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY?t=2077) and [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY?t=2261).\n",
    "\n",
    "> right now the history is not used, so it looks silly (but eventually history will be used). ...\n",
    "> \n",
    "> Given the previous context of whatever generated, we only look at the very last character (```[:, -1, :]```) to make the prediciton of what comes next.\n",
    "\n",
    "* [Let's build GPT: from scratch, in code, spelled out - simplest baseline](https://youtu.be/kCc8FmEb1nY?t=1383)\n",
    "\n",
    "The ```idx:shape(B,T)``` is the idices to extract ```T``` number embedding vectors for the tokens. The ```token_embedding_table``` is e.g. a table of word embeddings in Word2Vec for each word in the vocabulary of the language. Andrej is using ```vocab_size``` as the dimension of the token embedding vector dimensions, hence the table has ```(number of words in language, dimensions)==(vocab_size, vocab_size)```. \n",
    "\n",
    "The reason using ```vocab_size``` as the dimensions is because Andrej simlifies the classification head which predicts the next token. Usually there is a **fully connected (FC)** layer that reduces the higher dimension down to N classes to predict, then apply softmax and argmax to select the hightest probability token index. Here, he skipped **FC** and directly generate N class outputs where ```N==vocab size``` so that the model can directly predict which word in the vocabulary to come next as GPT output.\n",
    "\n",
    "<img src=\"./image/andrej_gpt_dev_idx.png\" align=\"left\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbb48c7-84df-45e9-8bd7-cf5169c2894c",
   "metadata": {},
   "source": [
    "The ```generate``` function is the mechanism to continuously generate the next tokens from the given context (prompt). In this Bigram Language Model, the (last) token predicts the next token, hence take the last token embedding vector with ```[:, -1, :]``` to get the next token id via softmax and argmax (```torch.multinomial(, num_samples=1)```) as the next token.\n",
    "\n",
    "<img src=\"./image/text_generation_from_prompt.png\" align=\"left\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18a1f7e-d2e9-4290-9b1f-2f16cdf9afe7",
   "metadata": {},
   "source": [
    "This corresponds with GPT generates succeeding sentences given a prompt (context). \n",
    "\n",
    "* [Let's build GPT: from scratch, in code, spelled out.](https://youtu.be/kCc8FmEb1nY?t=1934)\n",
    "\n",
    "By giving an index to one token as the first ```idx``` to the ```generate``` function, it will continuously generates ```max_new_tokens```.\n",
    "\n",
    "```\n",
    "# 0 as the index to the first token in the embedding table\n",
    "first_token_index: int = torch.zeros((1, 1)  \n",
    "\n",
    "# [0] to get first batch\n",
    "first_batch = decode(m.generate(idx=first_token_index, dtype=torch.long), max_new_tokens=100)[0].tolist())  \n",
    "print(first_batch)\n",
    "```\n",
    "\n",
    "<img src=\"./image/next_token_generation.jpeg\" align=\"left\" width=750/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a326dbfa-e644-4685-b0b0-263e83bd76f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T01:05:25.208379741Z",
     "start_time": "2024-01-02T01:05:25.182606312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_batch(split):\n",
      "    # generate a small batch of data of inputs x and targets y\n",
      "    data = train_data if split == 'train' else val_data\n",
      "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
      "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
      "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
      "    x, y = x.to(device), y.to(device)\n",
      "    return x, y\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(get_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29c7d2-2f41-44d4-a20d-7428b75679dc",
   "metadata": {},
   "source": [
    "---\n",
    "# Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c610bd-1648-42cb-8233-afdd978e2a31",
   "metadata": {},
   "source": [
    "## Attention \n",
    "\n",
    "$$Attention(Q,K,V)=softmax(\\frac {QK^T}{\\sqrt {d_k}})$$\n",
    "\n",
    "\n",
    "* [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY?t=4301)\n",
    "* [Building a GPT](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-)\n",
    "\n",
    "> - Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "> - There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "> - Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "> - In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "> - \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "> - \"Scaled\" attention additional divides `similarity` by ```1/sqrt(head_size)```. This makes it so when input Q,K are unit variance, `similarity` will be unit variance too and Softmax will stay diffuse and not saturate too much.\n",
    "> \n",
    "> <img src=\"./image/transformer_attention_as_communication.png\" align=\"left\" width=250/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084ccb5c-b89e-44a2-8434-b128b3cbfe1e",
   "metadata": {},
   "source": [
    "## Dot Product\n",
    "\n",
    "Transformer uses Scaled Dot Product Attention. Refresh the memory on what dot-product transformation does and where they are used.\n",
    "\n",
    "* Similarity = Q@K\n",
    "* Attention Valuye = Similarity@V\n",
    "\n",
    "<img src=\"./image/transformer_self_attention_flow.jpeg\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a82f527-74bc-433a-a80e-6035e0ffc020",
   "metadata": {},
   "source": [
    "## Layer Normazliation and Residual Dropout\n",
    "\n",
    "Original paper applied Dropout to the Sub-Layer (Multi Head Attention) before Residual Connection and Layer Normalization. This is called **Post Normalization**. \n",
    "\n",
    "> dropout to the output of each sub-layer, **before** it is added to the sub-layer input (x) and (layer) normalized.\n",
    "\n",
    "\n",
    "<img src=\"./image/transformer_residual_dropout.png\" align=\"left\" width=800/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a16d1e-eb71-4107-9b9f-9e9374c44982",
   "metadata": {},
   "source": [
    "However, recent approach is **Pre Normalization** where LayerNorm is applied to the input x into the sub-layer as explained in [Let's build GPT: from scratch, in code, spelled out.](https://youtu.be/kCc8FmEb1nY?t=5729)\n",
    "\n",
    "> Very few details about the Transformer have changed in the last five years, but there is something slightly departs from the original paper. You see that Add and Norm is applied **after** the transformation (Multi Head Attention). But now it is more common to apply LayerNorm before the transformation, so there is a reshuffling of the Layer Norm. This is called **pre-norm formulation** and that is the one we are going to implement as well.\n",
    "\n",
    "It is proposed in [On Layer Normalization in the Transformer Architecture](https://arxiv.org/pdf/2002.04745.pdf).\n",
    "\n",
    "> On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.\n",
    "> \n",
    "> <img src=\"./image/pre-ln-transformer.png\" align=\"left\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12829ce-8c36-4edf-a78b-1115e7537433",
   "metadata": {},
   "source": [
    "* [Review — Pre-LN Transformer: On Layer Normalization in the Transformer Architecture\n",
    "Pre-LN Transformer, Warm-Up Stage is Skipped](https://sh-tsang.medium.com/review-pre-ln-transformer-on-layer-normalization-in-the-transformer-architecture-b6c91a89e9ab)\n",
    "* [About LayerNorm Variants in the Original Transformer Paper, and Some Other Interesting Historical Tidbits About LLMs](https://magazine.sebastianraschka.com/p/why-the-original-transformer-figure)\n",
    "\n",
    "<img src=\"./image/post_ln_to_pre_ln_transformer.jpeg\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65889fb-7fd8-428d-8f59-f363593943af",
   "metadata": {},
   "source": [
    "---\n",
    "# Transformer (Pre-LN)\n",
    "\n",
    "(The Transformer implementaion starting at [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY?t=2268))\n",
    "\n",
    "Transformer generates a graph network between position-encoded tokens.\n",
    "\n",
    "1. Get un-connected tokens as a sequence (e.g. sentence)\n",
    "2. Wires connections among tokens by having looked at the co-occurrances of them in billions of sequences.\n",
    "\n",
    "\n",
    "<img src=\"./image/transformer_pre_ln.png\" align=\"left\" width=700/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d405130-72cb-4c9a-a1e8-610c75385ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_B = 1    # Batch size\n",
    "_H = 2    # Number of heads\n",
    "_T = 4    # Time steps / Sequence length\n",
    "_D = 8    # Model vector dimension d_model\n",
    "d_ff = _D * 4    # Pointwise Feed Forward hidden layer dimenssion\n",
    "\n",
    "\n",
    "# Token Embedding\n",
    "embedding = torch.nn.Embedding(\n",
    "    num_embeddings=_T,\n",
    "    embedding_dim=_D\n",
    ")\n",
    "initialize_weights(module=embedding)\n",
    "DOe = torch.nn.Dropout(p=DROPOUT_RATIO)\n",
    "\n",
    "# Linear projections at Attention\n",
    "Wq = torch.nn.Linear(_D, _D, bias=True, dtype=TYPE_FLOAT)\n",
    "Wk = torch.nn.Linear(_D, _D, bias=True, dtype=TYPE_FLOAT)\n",
    "Wv = torch.nn.Linear(_D, _D, bias=True, dtype=TYPE_FLOAT)\n",
    "Wo = torch.nn.Linear(_D, _D, bias=True, dtype=TYPE_FLOAT)\n",
    "initialize_weights(Wq)\n",
    "initialize_weights(Wk)\n",
    "initialize_weights(Wv)\n",
    "initialize_weights(Wo, output_projection=True)\n",
    "\n",
    "# LayerNorm and Dropout for attention\n",
    "LNa = torch.nn.LayerNorm(normalized_shape=_D, eps=1e-5, bias=True, dtype=TYPE_FLOAT)\n",
    "DOa = torch.nn.Dropout(p=DROPOUT_RATIO)\n",
    "\n",
    "# Linear projections at Position Wise Feed Forward\n",
    "W1 = torch.nn.Linear(_D, _D, bias=True, dtype=TYPE_FLOAT)\n",
    "W2 = torch.nn.Linear(_D, _D, bias=True, dtype=TYPE_FLOAT)\n",
    "initialize_weights(module=W1)\n",
    "initialize_weights(module=W2, output_projection=True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "# LayerNorm and Dropout for position-wise feed-forward\n",
    "LNp = torch.nn.LayerNorm(normalized_shape=_D, eps=1e-5, bias=True, dtype=TYPE_FLOAT)\n",
    "DOp = torch.nn.Dropout(p=DROPOUT_RATIO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30c54df3-652e-4500-afa0-7da533e39cbb",
   "metadata": {},
   "source": [
    "# Input Embedding\n",
    "\n",
    "<img src=\"./image/transformer_embedding.png\" align=\"left\" width=750/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7a59db0-e643-493e-aca1-2cce2fea84c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0783, -0.0493, -0.0126,  0.0971,  0.0180, -0.0240,  0.0173, -0.0438],\n",
       "         [-0.0881,  0.0563, -0.0498, -0.0340, -0.0721,  0.1201, -0.0698, -0.0276],\n",
       "         [-0.0517, -0.0372,  0.0044,  0.0297, -0.0276,  0.0674, -0.0460, -0.0416],\n",
       "         [-0.0794,  0.0020, -0.0036,  0.0382, -0.0055,  0.1043, -0.0670,  0.0783]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.arange(0, _B * _T).view(_B, _T)    # Token IDs / Indices to token embedding vectors\n",
    "x = embedding(indices) * math.sqrt(_D)\n",
    "x.shape == (_B, _T, _D)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6c3481-2ead-42fa-b5d5-909644bbae50",
   "metadata": {},
   "source": [
    "# Positional Encoding and Dropout\n",
    "\n",
    "Position encoding vector is added to the token embedding vector. Dropout is applied to the position added embedding.\n",
    "\n",
    "<img src=\"./image/transformer_dropout_to_embedding.png\" align=\"left\" width=750/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e92b98d0-d126-45cb-9aba-48763ee55067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,  9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
       "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,  9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
       "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,  9.9955e-01,  3.0000e-03,  1.0000e+00]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PositionalEncoding(max_time_steps=_T, d_model=_D)\n",
    "positions = pe(x)\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4384651e-6ed6-4cb1-bfda-390938a31ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0870,  1.0564, -0.0140,  1.2191,  0.0200,  0.0000,  0.0192,  0.0000],\n",
       "         [ 0.8371,  0.6629,  0.0556,  1.0678, -0.0690,  1.2445, -0.0765,  1.0804],\n",
       "         [ 0.9529, -0.5038,  0.2256,  1.1220, -0.0085,  1.1858, -0.0489,  1.0648],\n",
       "         [ 0.0686, -1.0977,  0.3244,  1.1040,  0.0000,  1.2266, -0.0711,  1.1981]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = DOe(x + positions)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f5c8b-1c1c-4416-b0f7-b65c4b746968",
   "metadata": {},
   "source": [
    "# Pre Layer Normalization\n",
    "\n",
    "Laye Normalization is applied to the input to the sub-layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb3d0ed3-32b9-430d-a7ee-9e775c621775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7278,  1.5600, -0.5818,  1.8856, -0.5136, -0.5537, -0.5152, -0.5537],\n",
       "         [ 0.4591,  0.1213, -1.0565,  0.9066, -1.2982,  1.2493, -1.3127,  0.9311],\n",
       "         [ 0.7387, -1.6306, -0.4442,  1.0137, -0.8250,  1.1174, -0.8908,  0.9207],\n",
       "         [-0.3660, -1.9157, -0.0262,  1.0096, -0.4572,  1.1725, -0.5517,  1.1347]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = LNa(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a33f04-8732-4e44-a712-c9d8521c1cb6",
   "metadata": {},
   "source": [
    "# Split for Multi Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dae53b5-9a76-4534-93aa-cd04509d3435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0076, -0.0204,  0.0750, -0.1073],\n",
       "          [-0.0076, -0.0264,  0.0124, -0.0203],\n",
       "          [ 0.0347,  0.0079, -0.0192,  0.0299],\n",
       "          [ 0.0113,  0.0318, -0.0036,  0.0251]],\n",
       "\n",
       "         [[ 0.0610, -0.0066, -0.0240, -0.0667],\n",
       "          [ 0.0291,  0.0549,  0.0408, -0.1296],\n",
       "          [-0.0150,  0.0546,  0.0594, -0.1035],\n",
       "          [-0.0006,  0.0117,  0.0272, -0.0461]]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = split(Wq(x), h=_H)\n",
    "k = split(Wk(x), h=_H)\n",
    "v = split(Wv(x), h=_H)\n",
    "\n",
    "print(k.shape)    # (B,H,T,d_k)\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48959382-d37d-43e9-a57e-70b849dfa0ef",
   "metadata": {},
   "source": [
    "# Scaled Dot Product Attention\n",
    "\n",
    "<img src=\"./image/transformer_attention.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f5c27a2-b74f-4fbf-9fa2-ecb53ee4b080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ScaledDotProductAttention(nn.Module):\n",
      "    \"\"\"\n",
      "    Class to implement Scaled Dot Product Attention (Figure 2 left in the paper).\n",
      "    \"\"\"\n",
      "    def __init__(self, do_mask: bool, max_time_steps: Optional[int]):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            max_time_steps: max sequence length or time steps T\n",
      "        \"\"\"\n",
      "        mask_matrix: Optional[Tensor]\n",
      "        super().__init__()\n",
      "        if do_mask:\n",
      "            mask_matrix = torch.tril(torch.ones(max_time_steps, max_time_steps)) == 0\n",
      "        else:\n",
      "            mask_matrix = None\n",
      "\n",
      "        self.register_buffer(\"mask_matrix\", mask_matrix)\n",
      "        assert (\n",
      "            (not do_mask and self.mask_matrix is None) or\n",
      "            (do_mask and self.mask_matrix.ndim == 2 and self.mask_matrix.shape[-1] == max_time_steps)\n",
      "        )\n",
      "\n",
      "    def forward(\n",
      "            self,\n",
      "            q: Tensor,\n",
      "            k: Tensor,\n",
      "            v: Tensor,\n",
      "    ):\n",
      "        \"\"\"Calculate the scaled dot product attention.\n",
      "        Args:\n",
      "            q: query of shape (B,h,T,d)\n",
      "            k: key of shape (B,h,T,d)\n",
      "            v: value of shape (B,h,T,d)\n",
      "        \"\"\"\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # First MatMul in the Scaled Dot Product Attention to calculate the similarities\n",
      "        # matrix between (q,k) for every (q,k) combinations in Q, K.\n",
      "        # This is cartesian product matrix of shape (T, T) for every head and batch.\n",
      "        # The number of features in similarities matrix is B*H*T*T which will be\n",
      "        # (32 * 8 * 512 * 512) which is 64M. Each feature has 512 / H = 64 dimensions\n",
      "        # of float32, hence the size is 16G bytes of memory requirement.\n",
      "        # --------------------------------------------------------------------------------\n",
      "        similarities: Tensor = calculate_dot_product_similarities(\n",
      "            query=q,\n",
      "            key=k,\n",
      "        )\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Scale (standardize) the dot product similarity matrix with its standard deviation.\n",
      "        # --------------------------------------------------------------------------------\n",
      "        d_k = k.shape[-1]  # head size\n",
      "        similarities = scale(similarities=similarities, d_k=d_k)\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Mask if required\n",
      "        # --------------------------------------------------------------------------------\n",
      "        if self.mask_matrix is not None:\n",
      "            similarities = mask(similarities=similarities, mask_matrix=self.mask_matrix)\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Normalize by softmax.\n",
      "        # --------------------------------------------------------------------------------\n",
      "        similarities = softmax(similarities, dim=-1)\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Second MatMul to generate attention value for each token in sequence of length T\n",
      "        # --------------------------------------------------------------------------------\n",
      "        attentions: Tensor = calculate_attentions(\n",
      "            similarities=similarities,\n",
      "            values=v\n",
      "        )   # shape: (B,H,T,d)\n",
      "\n",
      "        return attentions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(ScaledDotProductAttention))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062c57c2-4217-49fc-ad75-fa040f3c8924",
   "metadata": {},
   "source": [
    "## First MatMul with Q and K (Calculate Similarity Score)\n",
    "\n",
    "For every token ```Q``` in a sequence, calculate the relation/communication with other token ```K``` in the sequence (for GPT, only previous tokens). This builds the graph network of Self Attention.\n",
    "\n",
    "\n",
    "|Similarity Score (Q & K)| Proabability as Softmax |\n",
    "|---|---|\n",
    "|<img src=\"./image/transformer_dot_product_attention_similarity_score.jpeg\" align=\"left\" width=500/>|<img src=\"./image/transformer_dot_product_attention.png\" align=\"left\" width=175/>|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "243b5688-b6fd-4565-be09-c4bcd13cde7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def calculate_dot_product_similarities(\n",
      "        query: Tensor,\n",
      "        key: Tensor,\n",
      ") -> Tensor:\n",
      "    \"\"\"\n",
      "    Calculate similarity scores between queries and keys using dot product.\n",
      "\n",
      "    Args:\n",
      "        query: embedding vector of query of shape (B, h, T, d_k)\n",
      "        key: embedding vector of key of shape (B, h, T, d_k)\n",
      "\n",
      "    Returns: Similarities (closeness) between q and k of shape (B, h, T, T) where\n",
      "        last (T, T) represents relations between all query elements in T sequence\n",
      "        against all key elements in T sequence. If T is people in an organization,\n",
      "        (T,T) represents all (cartesian product) social connections among them.\n",
      "        The relation considers d_k number of features.\n",
      "    \"\"\"\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # Relationship between k and q as the first MatMul using dot product similarity:\n",
      "    # (B, h, T, d_k) @ (B, hH, d_k, T) ---> (B, h, T, T)\n",
      "    # --------------------------------------------------------------------------------\n",
      "    similarities = query @ key.transpose(-2, -1)            # dot product\n",
      "    return similarities                                     # shape:(B, h, T, T)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(calculate_dot_product_similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e7166be-4618-430f-a086-7fc506889941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 4, 4])\n",
      "tensor([[[[ 0.0026,  0.0028,  0.0024,  0.0011],\n",
      "          [ 0.0028,  0.0140,  0.0089,  0.0031],\n",
      "          [ 0.0024,  0.0089,  0.0096,  0.0079],\n",
      "          [ 0.0011,  0.0031,  0.0079,  0.0108]],\n",
      "\n",
      "         [[ 0.0048,  0.0003, -0.0069, -0.0085],\n",
      "          [ 0.0003,  0.0024,  0.0032,  0.0037],\n",
      "          [-0.0069,  0.0032,  0.0179,  0.0214],\n",
      "          [-0.0085,  0.0037,  0.0214,  0.0265]]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "similarities = calculate_dot_product_similarities(query=q, key=q)\n",
    "print(similarities.shape)    # (B,H,T,T)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb90c4a3-efdd-4658-9e30-d770c737c90d",
   "metadata": {},
   "source": [
    "\n",
    "## Scale by $\\sqrt{d_k}$\n",
    "\n",
    "As in the name **Scaled** Dot-Product Attention, the similarity score is normalized by $\\sqrt{d_k}$ to manage the variance where $d_k$ is the dimension of the key vector $k$ (which is the same with that of query).\n",
    "\n",
    "Suppose the positionally encoded token vector $x$ has dimension $D$, the shape of $W_K$ is ```(M, D)```. Then key $k = x:(D,) @ W^T_K:(D,M)$ has the shape $(M,)$, which is $d_k$. The variance of the pdorduct $Q\\cdot K^T$ is $d_k$. The variance of two zero-mean normal distributions is:\n",
    "\n",
    "[Variance of product of multiple independent random variables](https://stats.stackexchange.com/questions/52646/)\n",
    "\n",
    "$${\\rm Var}(XY) = E(X^2Y^2) − (E(XY))^2={\\rm Var}(X){\\rm Var}(Y)+{\\rm Var}(X)(E(Y))^2+{\\rm Var}(Y)(E(X))^2$$  $$E(X)=E(Y)=0$$\n",
    "\n",
    "<img src=\"./image/variance_of_q@k.jpeg\" align=\"left\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11de16b9-7a7c-45f7-a829-a44930675019",
   "metadata": {},
   "source": [
    "* [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY?t=4616)\n",
    "\n",
    "> If you have unit gausian input of mean 0 and $W_K$ and $W_V$ are unit gaussian, and if you calculate the ```similarity``` naively, the variance is the order of the head size $d_k$ (e.g. approx 16 if $d_k$ == 16). By standardizing the ```similarity``` score by $\\sqrt{d_k}$ the variance of the ```similarity``` socre will be normal (approx 1.0).\n",
    "\n",
    "Otherwise, softmax will pickup the nodes with larger values, hence only specific nodes in the sequence will be incorporated into the BoW. We want to consider the communication among every nodes if there is, not specific ones only.\n",
    "\n",
    "### Without scaling\n",
    "\n",
    "When the similarity score is not normalized/scaled by $\\sqrt{d_k}$, the softmax becomes **peaky** like one hot encoding, which is beneficial for classification (amplify the high score signal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88651d9f-56b2-4ed2-9047-03dfc99ca314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance:14.592000007629395, softmax:tensor([7.8424e-03, 3.1967e-04, 3.8844e-02, 6.4541e-05, 9.5293e-01])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<StemContainer object of 3 artists>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head_size = 16\n",
    "naive_score = torch.tensor([0.1, -0.1, 0.2, -0.2, 0.4]) * head_size\n",
    "# Note that variance is close to the head_size\n",
    "print(f\"variance:{naive_score.var()}, softmax:{torch.softmax(naive_score, dim=-1)}\")\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.grid()\n",
    "plt.stem(range(len(naive_score)), torch.softmax(naive_score, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8cda58-08ac-4c0e-9bff-f2333221f3b2",
   "metadata": {},
   "source": [
    "### With scaling\n",
    "\n",
    "By scale/normalize, the softmax will be smoothed/diffused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fce13dc5-0ee8-414e-bdb7-64856d6356eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance:0.9120000004768372, softmax:tensor([0.1524, 0.0685, 0.2273, 0.0459, 0.5059])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<StemContainer object of 3 artists>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scaled_score = naive_score / torch.sqrt(torch.tensor(head_size))\n",
    "print(f\"variance:{scaled_score.var()}, softmax:{torch.softmax(scaled_score, dim=-1)}\")\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.grid()\n",
    "plt.stem(range(len(scaled_score)), torch.softmax(scaled_score, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2491f5-fb3d-416f-adf4-ad85b7ebb2a4",
   "metadata": {},
   "source": [
    "### Code for Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "055405d1-fa1c-40e3-abdb-23e965ef48fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def scale(\n",
      "        similarities: Tensor,\n",
      "        d_k: int\n",
      ") -> Tensor:\n",
      "    \"\"\"\n",
      "    Standardize the variance of the dot product similarities using the standard deviation\n",
      "    of the dot product of the normal distributions std=sqrt(d_k) so that the variance will\n",
      "    be 1.0 approx.\n",
      "\n",
      "    Citation:\n",
      "    > While for small values of dk the two mechanisms perform similarly, additive attention\n",
      "    > outperforms dot product attention without scaling for larger values of dk [3].\n",
      "    > We suspect that for large values of d_k, the dot products grow large in magnitude,\n",
      "    > pushing the softmax function into regions where it has extremely small gradients.\n",
      "    > To counteract this effect, we scale the dot products by sqrt(d_k).\n",
      "\n",
      "    The last (T, T) of the shape (B,h,T,T) is the matrix that represents the similarities\n",
      "    as the dot product between (q,k) from every q from sequence length T and k from the\n",
      "    sequence length T. The dimensions of q and k are both d_k, and q, k are expected to\n",
      "    follow normal distribution where the mean is 0 and variance is 1. The variance of the\n",
      "    two normal distributions q, k is expected to be d_k. Hence, standardize the (T,T)\n",
      "    with its standard deviation std=sqrt(d_k) so that the variance will be approximately 1.\n",
      "    Then, the later softmax will be smoothed out so that not to pick up higher value.\n",
      "\n",
      "    Args:\n",
      "        similarities: Similarities matrix shape (B, h, T, T)\n",
      "        d_k: dimension of the\n",
      "\n",
      "    Returns: scaled similarities matrix of shape (B, h, T, T)\n",
      "    \"\"\"\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # Scaling factor to standardize (div by standard deviation) the product q@k.T\n",
      "    # of two zero centered normal distributions q, k. The variance of the product\n",
      "    # is head_size d_k. See https://stats.stackexchange.com/a/52699/105137.\n",
      "    # --------------------------------------------------------------------------------\n",
      "    std = torch.sqrt(torch.tensor(d_k, dtype=TYPE_FLOAT))   # standard deviation\n",
      "\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # Scale similarities of each head by std so that the variance is approx 1.\n",
      "    # Scaling regularize the softmax output so as not to overfit to features, by which\n",
      "    # features in query and key can relate among themselves better.\n",
      "    # Otherwise, features with higher value will be peaked by softmax, (which is good\n",
      "    # for use as classification head but not for Bag of Words to incorporate features\n",
      "    # to make them related), hence only specific features in query and key will be\n",
      "    # connected.\n",
      "    # --------------------------------------------------------------------------------\n",
      "    scaled = similarities / std                             # scaled dot product\n",
      "    return scaled\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02f98016-2abb-476d-b8b0-28f3e20d4ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0013,  0.0014,  0.0012,  0.0005],\n",
       "          [ 0.0014,  0.0070,  0.0044,  0.0015],\n",
       "          [ 0.0012,  0.0044,  0.0048,  0.0039],\n",
       "          [ 0.0005,  0.0015,  0.0039,  0.0054]],\n",
       "\n",
       "         [[ 0.0024,  0.0002, -0.0035, -0.0042],\n",
       "          [ 0.0002,  0.0012,  0.0016,  0.0019],\n",
       "          [-0.0035,  0.0016,  0.0089,  0.0107],\n",
       "          [-0.0042,  0.0019,  0.0107,  0.0133]]]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_similarities = scale(similarities=similarities, d_k=_D/_H)\n",
    "scaled_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7d6ae-64db-4d47-b900-57f6c5b61803",
   "metadata": {},
   "source": [
    "## Mask\n",
    "\n",
    "Optional. After calculating ```(T,T)``` matrix of similarities from q to k, mask the matrix to prevent the communications with future time steps by replacing the similarity values with the ```-inf```, meaning there is **no relationship** from ```q```. Then softmax will make the contribution from the ```-inf``` to zero. This is the same with blocking relations between ```q``` and ```k```.\n",
    "\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2825b93b-4d6b-41a5-a4d6-d05c908a9f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [False, False,  True,  True],\n",
       "        [False, False, False,  True],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mask matrix to decide which element in (T,T) matrix to mask\n",
    "mask_matrix = torch.tril(torch.ones(_T,_T)) == 0\n",
    "mask_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "595f4378-ed66-423c-baad-a7fc337363a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0026,    -inf,    -inf,    -inf],\n",
       "          [ 0.0028,  0.0140,    -inf,    -inf],\n",
       "          [ 0.0024,  0.0089,  0.0096,    -inf],\n",
       "          [ 0.0011,  0.0031,  0.0079,  0.0108]],\n",
       "\n",
       "         [[ 0.0048,    -inf,    -inf,    -inf],\n",
       "          [ 0.0003,  0.0024,    -inf,    -inf],\n",
       "          [-0.0069,  0.0032,  0.0179,    -inf],\n",
       "          [-0.0085,  0.0037,  0.0214,  0.0265]]]], grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mask the similarity matrix element of future steps with -inf\n",
    "\n",
    "masked_similarities = torch.clone(similarities).masked_fill(mask=mask_matrix, value=float('-inf'))\n",
    "masked_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f4af2a2-e1b7-4c57-9ae5-fe06fc34d701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4972, 0.5028, 0.0000, 0.0000],\n",
       "          [0.3318, 0.3340, 0.3342, 0.0000],\n",
       "          [0.2488, 0.2493, 0.2505, 0.2513]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4995, 0.5005, 0.0000, 0.0000],\n",
       "          [0.3295, 0.3328, 0.3377, 0.0000],\n",
       "          [0.2452, 0.2482, 0.2526, 0.2539]]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Softmax will supress the contirubion from -inf similarity values\n",
    "F.softmax(masked_similarities, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bbfb41-e777-404e-85f2-d1382970b0ea",
   "metadata": {},
   "source": [
    "### Code for Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e365ea95-84a2-423e-9140-c26a5c1450fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def mask(\n",
      "    similarities: Tensor,\n",
      "    mask_matrix: Tensor\n",
      ") -> Tensor:\n",
      "    \"\"\"\n",
      "    Args:\n",
      "        similarities: matrix to mask of shape (B,H,T,T)\n",
      "        mask_matrix: boolean matrix of which elements in (T,T) to mask fill.\n",
      "\n",
      "    Returns: masked similarity matrix\n",
      "    \"\"\"\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # mask to make uni-direction (left to right only) for algorithm such as GPT.\n",
      "    # Skip masking for bi-directional e.g .BERT,\n",
      "    # --------------------------------------------------------------------------------\n",
      "    # exp(-inf) = 0 masks the similarities so that it will be uni-directional.\n",
      "    assert (\n",
      "        similarities.ndim == 4 and                              # (B,H,T,T)\n",
      "        similarities.shape[-2] == similarities.shape[-1] and\n",
      "        similarities.shape[-1] == mask_matrix.shape[-1]\n",
      "    )\n",
      "    masked = similarities.masked_fill(mask=mask_matrix, value=float('-inf'))\n",
      "    return masked\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46e5b0d8-8e1a-43e2-a0d0-338d939a5673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0026,    -inf,    -inf,    -inf],\n",
       "          [ 0.0028,  0.0140,    -inf,    -inf],\n",
       "          [ 0.0024,  0.0089,  0.0096,    -inf],\n",
       "          [ 0.0011,  0.0031,  0.0079,  0.0108]],\n",
       "\n",
       "         [[ 0.0048,    -inf,    -inf,    -inf],\n",
       "          [ 0.0003,  0.0024,    -inf,    -inf],\n",
       "          [-0.0069,  0.0032,  0.0179,    -inf],\n",
       "          [-0.0085,  0.0037,  0.0214,  0.0265]]]], grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_similarities = mask(similarities=similarities, mask_matrix=mask_matrix)\n",
    "masked_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a271a753-0956-4b7b-9c20-cae90c4efb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4972, 0.5028, 0.0000, 0.0000],\n",
       "          [0.3318, 0.3340, 0.3342, 0.0000],\n",
       "          [0.2488, 0.2493, 0.2505, 0.2513]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4995, 0.5005, 0.0000, 0.0000],\n",
       "          [0.3295, 0.3328, 0.3377, 0.0000],\n",
       "          [0.2452, 0.2482, 0.2526, 0.2539]]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_similarities = softmax(masked_similarities, dim=-1)\n",
    "normalized_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db11fb7-d368-4556-ab1a-056d0b0b8478",
   "metadata": {},
   "source": [
    "## Second MatMul with V (Calculate Bag of Words Attention Value)\n",
    "\n",
    "One way to generate the inter-connections among the tokens to distill their knowledges or relations is ```BoW``` by averaging them feature-wise/axis=-1.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./image/transformer_dot_product_attention_bow.png\" align=\"left\" width=700/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e061adeb-8b2b-4783-9a4e-f9f499e3db0a",
   "metadata": {},
   "source": [
    "Note that the initially the value of similarity is random or ```(1.0, 1/2, 1/3, ...)``` but eventually it gets trained to memorize the relations among position-encoded tokens.\n",
    "\n",
    "* [Let's build GPT: from scratch, in code, spelled out.](https://youtu.be/kCc8FmEb1nY?t=3814)\n",
    "\n",
    "> Different will find other tokens more or less interesting and we want that data dependent. If I/token is a vowel, I am looking for consonants in my past and want to know what consonants were. And I want the information to flow to me (connection). This is the problem that Self Attention solves.\n",
    "\n",
    "<img src=\"./image/self_attention.jpeg\" align=\"left\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff9381-ea71-4a56-8695-dbd0e14ade54",
   "metadata": {},
   "source": [
    "### Purpose of  using $W_V$\n",
    "\n",
    "$v$ looks to be a proxy of $x$ but what transformation or meaning does $W_V$ gives by having transformation from $x$ to $v$? (Note: ```x``` in the diagram above is actually $v$ as $v=x@W{_V}{^T}$).\n",
    "\n",
    "* [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY?t=4258)\n",
    "\n",
    "> $x$ is like a private information to a token. For the purpose of the single attention head, $v$ is what I give for you to communicate with if you find me interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49421e67-b3f2-43a0-b9e0-41ab241817cf",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b47923ef-9b0c-4ade-91f6-90a9e1ce09e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def calculate_attentions(\n",
      "        similarities,\n",
      "        values\n",
      "):\n",
      "    \"\"\"\n",
      "    For every q element, create a Bag of Words that encodes the relationships with\n",
      "    other elements (including itself) in T, using (q,k) relationship value as the\n",
      "    strength of the relationships.\n",
      "\n",
      "    Citation:\n",
      "    > On each of these projected versions of queries, keys and values we then perform\n",
      "    > the attention function in parallel, yielding d_v-dimensional output values.\n",
      "\n",
      "    ```\n",
      "    bows = []\n",
      "    for row in similarities:                    # similarity matrix of shape (T,T)\n",
      "        bow = sum([                             # bow:shape(d_v,)\n",
      "            # each column in row is (q,k) similarity score s\n",
      "            s*v for (s,v) in zip(row,values)    # k:shape(), v:shape(d_v,)\n",
      "=        ])\n",
      "        bows.append(bow)                        # bows:shape(T,d_v)\n",
      "    ```\n",
      "\n",
      "    Args:\n",
      "        similarities: q to k relationship strength matrix of shape (B, h, T, T)\n",
      "        values: elements of sequence with length T of shape (B, h, T, d_v)\n",
      "\n",
      "    Returns: Bag of Words for every q element of shape (B, h, T, d_v)\n",
      "    \"\"\"\n",
      "    return similarities @ values     # (B,h,T,T) @ (B,h,T,d_v) -> (B,h,T,d_v)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(calculate_attentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a4aa90d-f685-4c1b-b90a-5e493756d6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0252, -0.0309, -0.0251, -0.0567],\n",
       "          [ 0.0306, -0.0221,  0.0235, -0.0293],\n",
       "          [ 0.0178, -0.0226,  0.0428,  0.0120],\n",
       "          [-0.0047, -0.0216,  0.0461,  0.0445]],\n",
       "\n",
       "         [[ 0.0393,  0.0054,  0.0070, -0.0624],\n",
       "          [ 0.0052,  0.0427, -0.0217, -0.0740],\n",
       "          [-0.0110,  0.0511, -0.0106, -0.0645],\n",
       "          [-0.0124,  0.0461, -0.0127, -0.0548]]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attensions = calculate_attentions(similarities=normalized_similarities, values=v)\n",
    "print(attensions.shape)    # (B,H,T,d_v)\n",
    "attensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a332bafc-a932-4a45-9399-12601a71ca39",
   "metadata": {},
   "source": [
    "### Verify ScaledDotProductAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd60950a-64bd-4b2a-a124-aca111e59712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0252, -0.0309, -0.0251, -0.0567],\n",
       "          [ 0.0306, -0.0222,  0.0233, -0.0294],\n",
       "          [ 0.0178, -0.0226,  0.0427,  0.0120],\n",
       "          [-0.0046, -0.0216,  0.0460,  0.0443]],\n",
       "\n",
       "         [[ 0.0393,  0.0054,  0.0070, -0.0624],\n",
       "          [ 0.0053,  0.0427, -0.0217, -0.0740],\n",
       "          [-0.0107,  0.0509, -0.0107, -0.0646],\n",
       "          [-0.0121,  0.0460, -0.0128, -0.0550]]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdpa = ScaledDotProductAttention(do_mask=True, max_time_steps=_T)\n",
    "sdpa(q=q,k=k,v=v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29037d96-27c9-482e-84e1-be8e17e18218",
   "metadata": {},
   "source": [
    "# Multi Attention Head\n",
    "\n",
    "Divide the embedding vector q, k, v into ```h``` number of segmenets and apply self attention to each segment in parallel respectively.\n",
    "\n",
    "* <img src=\"./image/transformer_paper_multi_head_attention.png\" align=\"left\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ddc47c-f539-4bd9-a7e7-27d87bbd9a38",
   "metadata": {},
   "source": [
    "\n",
    "|<img src=\"./image/transformer_multi_head_attentions.png\" align=\"left\" width=200/>   | \n",
    "<img src=\"./image/transformer_multi_head_attention_formula.png\" align=\"left\" width=700/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef7f98-f117-4ae5-8485-25ba4f3f488b",
   "metadata": {},
   "source": [
    "* [Transformers Explained Visually (Part 3): Multi-head Attention, deep dive](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)\n",
    "\n",
    "<img src=\"./image/transformer_multi_head_attention.png\" align=\"left\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d81e2d-efae-4c53-9517-87f430d03197",
   "metadata": {},
   "source": [
    "### Concatenate multiple outputs from Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03238ec0-1b00-4714-a6de-97060a919fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0252, -0.0309, -0.0251, -0.0567,  0.0393,  0.0054,  0.0070, -0.0624],\n",
       "         [ 0.0306, -0.0221,  0.0235, -0.0293,  0.0052,  0.0427, -0.0217, -0.0740],\n",
       "         [ 0.0178, -0.0226,  0.0428,  0.0120, -0.0110,  0.0511, -0.0106, -0.0645],\n",
       "         [-0.0047, -0.0216,  0.0461,  0.0445, -0.0124,  0.0461, -0.0127, -0.0548]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attensions = attensions.transpose(2,1)  # (B,T,H,d_v)\n",
    "attentions = attensions.reshape(_B,_T,-1)\n",
    "print(attentions.shape)    # (B,T,D)\n",
    "attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e6e80-a5b6-4e2e-b38b-5fd799af1c52",
   "metadata": {},
   "source": [
    "### Linear Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fd009f4-8b76-4962-96a8-859446ac0a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0053e-03, -6.3895e-04, -1.4211e-04, -3.0041e-04, -7.8671e-04,  2.2353e-04,  1.5253e-04,  1.0346e-04],\n",
       "         [-1.3506e-03,  2.9461e-04,  7.0554e-05, -1.8622e-04, -7.2441e-04,  5.7259e-04,  4.7959e-04, -2.1396e-04],\n",
       "         [-1.0541e-03,  7.3774e-04,  2.8586e-05, -1.5152e-04, -5.9939e-04,  5.8687e-04,  5.9211e-04, -5.6746e-05],\n",
       "         [-5.9562e-04,  8.4529e-04, -1.2009e-04, -1.3679e-04, -5.0914e-04,  6.2729e-04,  5.0754e-04, -4.2058e-05]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions = Wo(attentions)\n",
    "print(attentions.shape)    # (B,T,D)\n",
    "attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f06caa-8a74-4974-890a-8186891d2066",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58c863d8-2dd4-43e9-8a29-bcd78de2afe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class MultiHeadAttention(nn.Module):\n",
      "    \"\"\"\n",
      "    Class to implement Multi Head Attention (Figure 2 right in the paper).\n",
      "    Citation:\n",
      "    > The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "    > sub-layers. The first is a multi-head self-attention mechanism, ... To facilitate\n",
      "    > these residual connections, all sub-layers in the model, as well as the embedding\n",
      "    > layers, produce outputs of dimension d_model = 512.\n",
      "\n",
      "    > Instead of performing a single attention function with d_model dimensional\n",
      "    > keys, values and queries, we found it beneficial to linearly project\n",
      "    > the queries, keys and values h times with different, learned linear projections\n",
      "    > to d_k, d_k and d_v dimensions, respectively.\n",
      "    > On each of these projected versions of queries, keys and values we then perform\n",
      "    > the attention function in parallel, yielding d_v dimensional output values.\n",
      "    > In this work we employ h = 8 parallel attention layers, or heads. For each of\n",
      "    > these we use dk = dv = dmodel /h = 64.\n",
      "    \"\"\"\n",
      "    @property\n",
      "    def D(self) -> int:     # pylint: disable=invalid-name\n",
      "        \"\"\"Dimensions (number of features) of the embedding vector d_model.\n",
      "        \"\"\"\n",
      "        return self._D\n",
      "\n",
      "    @property\n",
      "    def H(self) -> int:     # pylint: disable=invalid-name\n",
      "        \"\"\"Number of attention heads\"\"\"\n",
      "        return self._H\n",
      "\n",
      "    @property\n",
      "    def T(self) -> int:     # pylint: disable=invalid-name\n",
      "        \"\"\"Max time steps or max sequence length\"\"\"\n",
      "        return self._T\n",
      "\n",
      "    def __init__(\n",
      "            self,\n",
      "            num_heads: int = 8,\n",
      "            d_model: int = 512,\n",
      "            dtype: type = TYPE_FLOAT,\n",
      "            do_mask: bool = False,\n",
      "            max_time_steps: int = 512,\n",
      "            bias: bool = True,\n",
      "    ):\n",
      "        \"\"\"Multi Head Attention initialization.\n",
      "        Args:\n",
      "            num_heads: number of attention heads\n",
      "            d_model: dimensions of the model embedding vector.\n",
      "            dtype: data type\n",
      "            do_mask: True when execute masking to not calculate attention with future time steps\n",
      "            max_time_steps: max sequence length or time steps T.\n",
      "            bias: Ture if learning the additive bias at the linear layer.\n",
      "        \"\"\"\n",
      "        super().__init__()\n",
      "        self._D: int = d_model              # pylint: disable=invalid-name\n",
      "        self._H: int = num_heads            # pylint: disable=invalid-name\n",
      "        self._T: int = max_time_steps       # pylint: disable=invalid-name\n",
      "\n",
      "        # To transfer embedded token of dim_input features to Q space of d_model features\n",
      "        self.Wq: nn.Module = nn.Linear(     # pylint: disable=invalid-name\n",
      "            in_features=d_model,\n",
      "            out_features=d_model,\n",
      "            dtype=dtype,\n",
      "            bias=bias\n",
      "        )\n",
      "        initialize_weights(module=self.Wq)\n",
      "        # To transfer embedded token of dim_input features to K space of d_model features\n",
      "        self.Wk: nn.Module = nn.Linear(     # pylint: disable=invalid-name\n",
      "            in_features=d_model,\n",
      "            out_features=d_model,\n",
      "            dtype=dtype,\n",
      "            bias=bias\n",
      "        )\n",
      "        initialize_weights(module=self.Wk)\n",
      "        # To transfer embedded token of dim_input features to V space of d_model features\n",
      "        self.Wv: nn.Module = nn.Linear(     # pylint: disable=invalid-name\n",
      "            in_features=d_model,\n",
      "            out_features=d_model,\n",
      "            dtype=dtype,\n",
      "            bias=bias\n",
      "        )\n",
      "        initialize_weights(module=self.Wv)\n",
      "        # Project to apply to the concatenated output of Self Dot Product Attention\n",
      "        self.Wo: nn.Module = nn.Linear(     # pylint: disable=invalid-name\n",
      "            in_features=d_model,\n",
      "            out_features=d_model,\n",
      "            dtype=dtype,\n",
      "            bias=bias\n",
      "        )\n",
      "        initialize_weights(module=self.Wo, output_projection=True)\n",
      "        self.scaled_dot_product_attention: nn.Module = ScaledDotProductAttention(\n",
      "            do_mask=do_mask,\n",
      "            max_time_steps=max_time_steps\n",
      "        )\n",
      "\n",
      "    def forward(\n",
      "            self,\n",
      "            x\n",
      "    ):\n",
      "        \"\"\"Run multi head attention\n",
      "        Args:\n",
      "            x: input embedding vector of shape (B,T,D)\n",
      "\n",
      "        Returns: Attention values of shape (B,T,D)\n",
      "        \"\"\"\n",
      "        # pylint: disable=invalid-name\n",
      "        assert x.ndim == 3\n",
      "        B, T, _D = x.shape      # Batch, Tokens (or sequence length), Dimension\n",
      "        assert _D == self.D, \\\n",
      "            f\"input vector dimension is invalid, expected [{self.D}], got [{_D}].\"\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Transfer x into Q, K, V spaces. Corresponds to the first 'Linear' layers in\n",
      "        # the figure 2 of the original paper. In the paper, there are H number of Linear\n",
      "        # layers Q, K, V respectively, but no need to physically split into H number of\n",
      "        # Linear layers. Instead, use one Linear layer Wq, Wk, Wv for Q, K, V respectively.\n",
      "        # --------------------------------------------------------------------------------\n",
      "        q: Tensor = self.Wq(x)   # Transfer to Q space. Shape=(B, T, D)\n",
      "        k: Tensor = self.Wk(x)   # Transfer to K space. Shape=(B, T, D)\n",
      "        v: Tensor = self.Wv(x)   # Transfer to V space. Shape=(B, T, D)\n",
      "        assert q.shape == (B, self.T, self.D)\n",
      "        assert k.shape == (B, self.T, self.D)\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Split into H segments for multiple heads to attend.\n",
      "        # --------------------------------------------------------------------------------\n",
      "        q = split(x=q, h=self.H)    # (B, H, T, d)\n",
      "        k = split(x=k, h=self.H)    # (B, H, T, d)\n",
      "        v = split(x=v, h=self.H)    # (B, H, T, d)\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Calculate self attention values\n",
      "        # --------------------------------------------------------------------------------\n",
      "        attentions: Tensor = self.scaled_dot_product_attention(q=q, k=k, v=v)\n",
      "        assert attentions.shape == (B, self.H, T, self.D/self.H)\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Concatenate outputs from heads into the model output with reshape.\n",
      "        # First (B,H,T,d)->(B,T,H,d) then to (B,T,D)\n",
      "        # \"RuntimeError: view size is not compatible with input tensor's size and strid\"\n",
      "        # --------------------------------------------------------------------------------\n",
      "        attentions = attentions.transpose(2, 1).contiguous().view(B, T, -1)\n",
      "        assert attentions.shape == (B, T, self.D)\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Last Wo Linear projection\n",
      "        # --------------------------------------------------------------------------------\n",
      "        attentions = self.Wo(attentions)    # (B,T,D)@(D,D) -> (B,T,D)\n",
      "        assert attentions.shape == (B, T, self.D)\n",
      "\n",
      "        return attentions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(MultiHeadAttention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c85ff1bb-3431-4400-97ea-18e140b3e297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0053e-03, -6.3895e-04, -1.4211e-04, -3.0041e-04, -7.8671e-04,  2.2353e-04,  1.5253e-04,  1.0346e-04],\n",
       "         [-1.3495e-03,  2.9180e-04,  6.9362e-05, -1.8733e-04, -7.2412e-04,  5.7177e-04,  4.7865e-04, -2.1367e-04],\n",
       "         [-1.0546e-03,  7.3452e-04,  2.8880e-05, -1.5071e-04, -6.0229e-04,  5.8708e-04,  5.9047e-04, -5.6654e-05],\n",
       "         [-5.9982e-04,  8.4165e-04, -1.1867e-04, -1.3648e-04, -5.1327e-04,  6.2751e-04,  5.0727e-04, -4.2274e-05]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the MultiHeadAttention output\n",
    "mha = MultiHeadAttention(\n",
    "    num_heads=_H,\n",
    "    d_model=_D,\n",
    "    dtype=TYPE_FLOAT,\n",
    "    do_mask=True,\n",
    "    max_time_steps=_T,\n",
    "    bias=True,\n",
    ")\n",
    "mha.Wq = Wq\n",
    "mha.Wk = Wk\n",
    "mha.Wv = Wv\n",
    "mha.Wo = Wo\n",
    "\n",
    "\n",
    "attentions = mha(x)\n",
    "attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a48aa65-5906-4e4f-a32c-c5145dd419ca",
   "metadata": {},
   "source": [
    "## Dropout and Residual Connection\n",
    "\n",
    "Dropout is applied to the output of each sub-layer and added to input x to the sub-layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29033c75-8d27-49b4-9708-45b9b9f54ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5365,  3.2934, -1.2281,  3.9807, -1.0842, -1.1688, -1.0877, -1.1688],\n",
       "         [ 0.9692,  0.2561, -2.2304,  1.9138, -1.2982,  2.6374, -2.7713,  1.9657],\n",
       "         [ 1.5594, -3.4423, -0.4442,  2.1401, -1.7416,  2.3590, -1.8806,  1.9438],\n",
       "         [-0.7727, -1.9157, -0.0553,  2.1315, -0.9652,  2.4754, -1.1647,  2.3955]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x + DOa(x)   # DO NOT use x += DOa(x) as it is in-place operation for PyTorch.\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5150f0df-6568-4c82-b5b5-daf101c955bb",
   "metadata": {},
   "source": [
    "### \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea5fac-4b0e-453b-a073-5e49dab46628",
   "metadata": {},
   "source": [
    "# Positionwise Feed Forward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d30f0-4942-44e5-9192-f9fd3676f252",
   "metadata": {},
   "source": [
    "## Pre Layer Normalization\n",
    "\n",
    "Laye Normalization is applied to the input to the sub-layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f62d1f6-a9d0-4421-8bcf-88e67ad7aa0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7278,  1.5601, -0.5818,  1.8856, -0.5136, -0.5537, -0.5152, -0.5537],\n",
       "         [ 0.4104,  0.0395, -1.2539,  0.9017, -0.7690,  1.2781, -1.5353,  0.9287],\n",
       "         [ 0.7166, -1.6766, -0.2421,  0.9945, -0.8629,  1.0993, -0.9294,  0.9006],\n",
       "         [-0.6208, -1.3039, -0.1921,  1.1148, -0.7358,  1.3203, -0.8550,  1.2725]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = LNp(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebdcb72-92dc-4204-8fb3-aa237772b8da",
   "metadata": {},
   "source": [
    "## Feed Forward \n",
    "\n",
    "Apply a wider single hidden layer neural network with ReLU activation. Here the features in the attention vector of each token can be amplified or supressed by weights, and multiple combination of features can form a new feature as the output. Then back to d_model dimensions.\n",
    "\n",
    "<img src=\"./image/transformer_paper_pointwise_feedforward.png\" align=\"left\" width=700/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5c303c0-4a54-42b8-acca-c4de7338894f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.2628e-04, -1.7654e-04, -1.8576e-05,  2.5299e-04, -8.3870e-05, -9.8290e-05, -5.5770e-04, -1.3000e-04],\n",
       "         [-6.9182e-04,  1.5459e-03,  2.0443e-04,  3.1186e-04,  1.0443e-03, -3.2111e-04, -1.1316e-03,  4.6202e-04],\n",
       "         [-2.0228e-04,  8.6413e-04,  6.0100e-04,  2.7781e-04,  1.4059e-03, -7.7712e-04, -1.2969e-03,  8.1715e-04],\n",
       "         [-5.5683e-04,  1.2445e-03,  2.4765e-04,  2.3819e-04,  1.1877e-03, -6.6652e-04, -9.5419e-04,  3.1880e-04]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedforward = W2(relu(W1(x)))\n",
    "feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526b697-02a9-4ba7-9d2d-da34570a1183",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e5cc56c-2c6b-4286-9a9a-1213b828a38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class PositionwiseFeedForward(nn.Module):\n",
      "    \"\"\"Class to implementation of Position-wise Feed-Forward Networks.\n",
      "    This is a single hidden layer nural network with ReLU activation.\n",
      "    \"\"\"\n",
      "    def __init__(\n",
      "            self,\n",
      "            d_model: int = 512,\n",
      "            d_ff: int = 2048,\n",
      "            dtype: type = TYPE_FLOAT,\n",
      "            bias: bool = True,\n",
      "    ):\n",
      "        \"\"\"Initialize the class\n",
      "        Args:\n",
      "            d_model: dimensions of the model embedding vector.\n",
      "            d_ff: dimensions of the hidden layer output vector\n",
      "            dtype: data type\n",
      "            bias: True to learn additive bias in the layer.\n",
      "        \"\"\"\n",
      "        super().__init__()\n",
      "        self.W1: nn.Module = nn.Linear(     # pylint: disable=invalid-name\n",
      "            in_features=d_model, out_features=d_ff, bias=bias, dtype=dtype\n",
      "        )\n",
      "        initialize_weights(module=self.W1)\n",
      "        self.relu = nn.ReLU()\n",
      "        self.W2: nn.Module = nn.Linear(     # pylint: disable=invalid-name\n",
      "            in_features=d_ff, out_features=d_model, bias=bias, dtype=dtype\n",
      "        )\n",
      "        initialize_weights(module=self.W2, output_projection=True)\n",
      "\n",
      "    def forward(self, x):\n",
      "        \"\"\"Feed-forward neural network forward propagation\n",
      "        Citation:\n",
      "        > each of the layers in our encoder and decoder contains a fully connected\n",
      "        > feed-forward network, which is applied to each position separately and\n",
      "        > identically. This consists of two linear transformations with a ReLU\n",
      "        > activation in between.\n",
      "        > Another way of describing this is as two convolutions with kernel size 1.\n",
      "        > The dimensionality of input and output is dmodel = 512, and the inner-layer\n",
      "        > has dimensionality of d_ff = 2048.\n",
      "\n",
      "        Args:\n",
      "            x: input embedding vector of shape (B,T,D)\n",
      "\n",
      "        Returns: output embedding vector of shape (B,T,D)\n",
      "        \"\"\"\n",
      "        return self.W2(self.relu(self.W1(x)))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(PositionwiseFeedForward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df6d2641-a097-4eab-9938-97ba8459627f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.2628e-04, -1.7654e-04, -1.8576e-05,  2.5299e-04, -8.3870e-05, -9.8290e-05, -5.5770e-04, -1.3000e-04],\n",
       "         [-6.9182e-04,  1.5459e-03,  2.0443e-04,  3.1186e-04,  1.0443e-03, -3.2111e-04, -1.1316e-03,  4.6202e-04],\n",
       "         [-2.0228e-04,  8.6413e-04,  6.0100e-04,  2.7781e-04,  1.4059e-03, -7.7712e-04, -1.2969e-03,  8.1715e-04],\n",
       "         [-5.5683e-04,  1.2445e-03,  2.4765e-04,  2.3819e-04,  1.1877e-03, -6.6652e-04, -9.5419e-04,  3.1880e-04]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the PositionwiseFeedForward output.\n",
    "pwff = PositionwiseFeedForward(d_model=_D, d_ff=d_ff, dtype=TYPE_FLOAT, bias=True)\n",
    "pwff.W1 = W1\n",
    "pwff.W2 = W2\n",
    "pwff.relu = relu\n",
    "\n",
    "feedforward = pwff(x)\n",
    "feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35abab71-f498-42fe-9bf8-27ecaae6edd8",
   "metadata": {},
   "source": [
    "## Dropout and Residual Connection\n",
    "\n",
    "Dropout is applied to the output of each sub-layer and added to input x to the sub-layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1641fb9d-9a19-4cd6-b57b-c743a83e2136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7283,  1.5599, -0.5818,  1.8859, -0.5137, -0.5538, -0.5158, -0.5538],\n",
       "         [ 0.4096,  0.0412, -1.2537,  0.9021, -0.7679,  1.2777, -1.5366,  0.9287],\n",
       "         [ 0.7164, -1.6766, -0.2421,  0.9948, -0.8613,  1.0984, -0.9308,  0.9006],\n",
       "         [-0.6214, -1.3025, -0.1918,  1.1150, -0.7345,  1.3195, -0.8561,  1.2729]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = x + DOp(feedforward)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbc10b4-18fd-45a2-8611-c6067002774a",
   "metadata": {},
   "source": [
    "---\n",
    "# Encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35f0277d-bd61-4cd6-805d-a77d5b389588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Encoder(nn.Module):\n",
      "    \"\"\"Class to implement Transformer Encoder.\n",
      "    Citation:\n",
      "    > In addition, we apply dropout to the sums of the embeddings and\n",
      "    > the positional encodings in both the encoder and decoder stacks.\n",
      "    > For the base model, we use a rate p_drop = 0.1.\n",
      "    \"\"\"\n",
      "    @property\n",
      "    def D(self) -> int:     # pylint: disable=invalid-name\n",
      "        \"\"\"Dimension of the model embedding vector\n",
      "        \"\"\"\n",
      "        return self._D\n",
      "\n",
      "    def __init__(\n",
      "            self,\n",
      "            vocabulary_size: int,\n",
      "            num_layers: int = 6,\n",
      "            num_heads: int = 8,\n",
      "            d_model: int = 512,\n",
      "            dtype: type = TYPE_FLOAT,\n",
      "            d_ff: int = 2048,\n",
      "            do_mask: bool = False,\n",
      "            max_time_steps: int = 512,\n",
      "            bias: bool = True,\n",
      "            p_drop: float = 0.1,\n",
      "            eps: float = 1e-5\n",
      "    ):\n",
      "        super().__init__()\n",
      "        self._D: int = d_model      # pylint: disable=invalid-name\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Token embeddings\n",
      "        # --------------------------------------------------------------------------------\n",
      "        self.embedding: nn.Embedding = nn.Embedding(\n",
      "            num_embeddings=vocabulary_size,\n",
      "            embedding_dim=d_model\n",
      "        )\n",
      "        initialize_weights(module=self.embedding)\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Position encoded vectors\n",
      "        # --------------------------------------------------------------------------------\n",
      "        self.positional_encoding: PositionalEncoding = PositionalEncoding(\n",
      "            d_model=d_model,\n",
      "            max_time_steps=max_time_steps,\n",
      "        )\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Dropout for the sums of the embeddings and the positional encodings\n",
      "        # --------------------------------------------------------------------------------\n",
      "        self.dropout: nn.Dropout = nn.Dropout(p=p_drop)\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Encoder layers\n",
      "        # --------------------------------------------------------------------------------\n",
      "        self.layers: nn.ModuleList = nn.ModuleList([\n",
      "            EncodeLayer(\n",
      "                num_heads=num_heads, d_model=d_model, dtype=dtype, d_ff=d_ff,\n",
      "                do_mask=do_mask, max_time_steps=max_time_steps, bias=bias,\n",
      "                p_drop=p_drop, eps=eps\n",
      "            ) for _ in range(num_layers)\n",
      "        ])\n",
      "\n",
      "    def forward(self, indices: Tensor):\n",
      "        \"\"\"Encode\n",
      "        Citation:\n",
      "        > 3.4 Embeddings and Softmax:\n",
      "        > Similarly to other sequence transduction models, we use learned embeddings\n",
      "        > to convert the input tokens and output tokens to vectors of dimension d_model.\n",
      "        > ... In the embedding layers, we multiply those weights by sqrt(dmodel).\n",
      "\n",
      "        > In addition, we apply dropout to the sums of the embeddings and the\n",
      "        > positional encodings in both the encoder and decoder stacks.\n",
      "\n",
      "        Args:\n",
      "            indices: indices to tokens\n",
      "        \"\"\"\n",
      "        assert torch.is_tensor(indices) and indices.ndim == 2\n",
      "        B, T = indices.shape        # pylint: disable=invalid-name\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Token Embeddings multiplied by sqrt(d_model).\n",
      "        # --------------------------------------------------------------------------------\n",
      "        x = self.embedding(indices) * math.sqrt(self.D)     # x.shape=(B,T,D)\n",
      "        assert x.shape == (B, T, self.D)\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Add Positional Encoding followed by dropout.\n",
      "        # DO NOT use += as it is in-place operation that can cause back-prop issue.\n",
      "        # https://stackoverflow.com/a/68600205/4281353\n",
      "        # https://crazyoscarchang.github.io/2018/10/04/in-pytorch-not-the-same/\n",
      "        # --------------------------------------------------------------------------------\n",
      "        x = self.dropout(x + self.positional_encoding(x))   # (B,T,D) + (1,T,D)\n",
      "        assert x.shape == (B, T, self.D)\n",
      "\n",
      "        # --------------------------------------------------------------------------------\n",
      "        # Encoder layers\n",
      "        # --------------------------------------------------------------------------------\n",
      "        for _layer in self.layers:\n",
      "            x = _layer(x)\n",
      "\n",
      "        assert x.shape == (B, T, self.D)\n",
      "        return x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(Encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24f8c5da-bfdc-4424-8e4f-486eeb595a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    vocabulary_size=_T,\n",
    "    num_layers=1,\n",
    "    num_heads=_H,\n",
    "    d_model=_D,\n",
    "    dtype=TYPE_FLOAT,\n",
    "    d_ff=d_ff,\n",
    "    do_mask=True,\n",
    "    max_time_steps=_T,\n",
    "    bias=True,\n",
    "    p_drop=DROPOUT_RATIO,\n",
    "    eps=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c39be46b-9057-442d-bd23-75476bbdef18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.8852e-04,  1.3107e+00,  1.4481e-01, -1.3164e-03, -1.2593e-01,  1.1124e+00, -8.5213e-04,  1.2034e+00],\n",
       "         [ 9.0303e-01,  5.2553e-01,  4.4227e-02, -8.7120e-04,  2.6867e-02,  1.2918e+00, -7.9886e-02, -6.1426e-04],\n",
       "         [ 1.1071e+00, -4.0805e-01,  2.5929e-01,  1.0013e+00,  1.2094e-03,  1.1218e+00, -4.6482e-02,  1.1084e+00],\n",
       "         [ 1.5859e-01, -1.1256e+00,  2.4217e-01,  1.0903e+00, -9.8602e-03,  1.2136e+00,  8.7672e-02,  1.0859e+00]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3675312-5622-4292-b7b7-c8264c716435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
