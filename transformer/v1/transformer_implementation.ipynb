{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df3f532-40a2-4049-844f-12f0cea1127a",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "## Transformer\n",
    "\n",
    "* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "* [On Layer Normalization in the Transformer Architecture](https://arxiv.org/pdf/2002.04745.pdf)\n",
    "* [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)\n",
    "\n",
    "## Nano GPT\n",
    "\n",
    "Nano GPT implementation by Andrej Karpathy.\n",
    "\n",
    "### Nano GPT YouTube Lecture version\n",
    "\n",
    "This is for lecture only. The Github is different from the proper implementation of Nano GPT github. Do not confuse/mix two diffent Github repositories.\n",
    "\n",
    "* [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n",
    "* [Github - nanogpt-lecture](https://github.com/karpathy/ng-video-lecture)\n",
    "\n",
    "> Code created in the Neural Networks: Zero To Hero video lecture series, specifically on the first lecture on nanoGPT. Publishing here as a Github repo so people can easily hack it, walk through the git log history of it, etc.\n",
    "\n",
    "\n",
    "### Nano GPT implementation version\n",
    "* [Github nanoGPT](https://github.com/karpathy/nanoGPT)\n",
    "\n",
    "## Resources\n",
    "\n",
    "* [Understanding Large Language Models](https://magazine.sebastianraschka.com/p/understanding-large-language-models)\n",
    "* [Building a Transformer with PyTorch](https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee3a8cd-ee4d-4c61-a49a-28ab53917e81",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5710adfc-3fb3-4e3e-857c-331a5ab58489",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T01:05:22.316374112Z",
     "start_time": "2024-01-02T01:05:22.300633784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d6b471b-a6cb-4145-9400-5381c11b1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "DIR = os.path.dirname(os.path.abspath('..'))\n",
    "if DIR not in sys.path:\n",
    "    sys.path.append(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f507fe2d-3931-491a-a14a-25ba5edb12d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T01:05:24.138290904Z",
     "start_time": "2024-01-02T01:05:22.305989324Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from transformer.v1 import (\n",
    "    TYPE_FLOAT,\n",
    "    DROPOUT_RATIO,\n",
    "    initialize_weights,\n",
    "    initialize_embedding_weights,\n",
    "    split,\n",
    "    calculate_dot_product_similarities,\n",
    "    scale,\n",
    "    mask,\n",
    "    calculate_attention_values,\n",
    "    MultiHeadAttention,\n",
    "    ScaledDotProductAttention,\n",
    "    PositionwiseFeedForward,\n",
    "    PositionalEncoding,\n",
    "    EncodeLayer,\n",
    "    Encoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d056634-e1b3-4a21-a698-2f1f18837a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_printoptions(profile=\"full\")\n",
    "torch.set_printoptions(edgeitems=4)\n",
    "torch.set_printoptions(threshold=100)\n",
    "torch.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a77adc0-895c-431f-97a8-f8a9949dba5d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13271d1f-2fc5-4632-b867-01988054c251",
   "metadata": {},
   "source": [
    "# Terminologies\n",
    "\n",
    "* B: Batch size\n",
    "* T: Time steps or Sequence length (e.g. 512 for bert input sequence)\n",
    "* C: Channel or Feature (channel perhaps because Andrej is from CNN background?). ```C=2``` two features in each x."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45745a2a-c062-40af-90a2-65d372419312",
   "metadata": {},
   "source": [
    "## Batch Input\n",
    "\n",
    "<img src=\"./image/gpt_batch.jpeg\" align=\"left\" width=750/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29c7d2-2f41-44d4-a20d-7428b75679dc",
   "metadata": {},
   "source": [
    "---\n",
    "# Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c610bd-1648-42cb-8233-afdd978e2a31",
   "metadata": {},
   "source": [
    "## Attention \n",
    "\n",
    "$$Attention(Q,K,V)=softmax(\\frac {QK^T}{\\sqrt {d_k}})$$\n",
    "\n",
    "\n",
    "* [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY?t=4301)\n",
    "* [Building a GPT](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-)\n",
    "\n",
    "> - Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "> - There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "> - Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "> - In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "> - \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "> - \"Scaled\" attention additional divides `similarity` by ```1/sqrt(head_size)```. This makes it so when input Q,K are unit variance, `similarity` will be unit variance too and Softmax will stay diffuse and not saturate too much.\n",
    "> \n",
    "> <img src=\"./image/transformer_attention_as_communication.png\" align=\"left\" width=250/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084ccb5c-b89e-44a2-8434-b128b3cbfe1e",
   "metadata": {},
   "source": [
    "## Dot Product\n",
    "\n",
    "Transformer uses Scaled Dot Product Attention. Refresh the memory on what dot-product transformation does and where they are used.\n",
    "\n",
    "* Similarity = Q@K\n",
    "* Attention Valuye = Similarity@V\n",
    "\n",
    "<img src=\"./image/transformer_self_attention_flow.jpeg\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a82f527-74bc-433a-a80e-6035e0ffc020",
   "metadata": {},
   "source": [
    "## Layer Normazliation and Residual Dropout\n",
    "\n",
    "Original paper applied Dropout to the Sub-Layer (Multi Head Attention) before Residual Connection and Layer Normalization. This is called **Post Normalization**. \n",
    "\n",
    "> dropout to the output of each sub-layer, **before** it is added to the sub-layer input (x) and (layer) normalized.\n",
    "\n",
    "\n",
    "<img src=\"./image/transformer_residual_dropout.png\" align=\"left\" width=800/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a16d1e-eb71-4107-9b9f-9e9374c44982",
   "metadata": {},
   "source": [
    "However, recent approach is **Pre Normalization** where LayerNorm is applied to the input x into the sub-layer as explained in [Let's build GPT: from scratch, in code, spelled out.](https://youtu.be/kCc8FmEb1nY?t=5729)\n",
    "\n",
    "> Very few details about the Transformer have changed in the last five years, but there is something slightly departs from the original paper. You see that Add and Norm is applied **after** the transformation (Multi Head Attention). But now it is more common to apply LayerNorm before the transformation, so there is a reshuffling of the Layer Norm. This is called **pre-norm formulation** and that is the one we are going to implement as well.\n",
    "\n",
    "It is proposed in [On Layer Normalization in the Transformer Architecture](https://arxiv.org/pdf/2002.04745.pdf).\n",
    "\n",
    "> On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.\n",
    "> \n",
    "> <img src=\"./image/pre-ln-transformer.png\" align=\"left\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12829ce-8c36-4edf-a78b-1115e7537433",
   "metadata": {},
   "source": [
    "* [Review â€” Pre-LN Transformer: On Layer Normalization in the Transformer Architecture\n",
    "Pre-LN Transformer, Warm-Up Stage is Skipped](https://sh-tsang.medium.com/review-pre-ln-transformer-on-layer-normalization-in-the-transformer-architecture-b6c91a89e9ab)\n",
    "* [About LayerNorm Variants in the Original Transformer Paper, and Some Other Interesting Historical Tidbits About LLMs](https://magazine.sebastianraschka.com/p/why-the-original-transformer-figure)\n",
    "\n",
    "<img src=\"./image/post_ln_to_pre_ln_transformer.jpeg\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65889fb-7fd8-428d-8f59-f363593943af",
   "metadata": {},
   "source": [
    "---\n",
    "# Transformer (Pre-LN)\n",
    "\n",
    "(The Transformer implementaion starting at [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY?t=2268))\n",
    "\n",
    "Transformer generates a graph network between position-encoded tokens.\n",
    "\n",
    "1. Get un-connected tokens as a sequence (e.g. sentence)\n",
    "2. Wires connections among tokens by having looked at the co-occurrances of them in billions of sequences.\n",
    "\n",
    "\n",
    "<img src=\"./image/transformer_pre_ln.png\" align=\"left\" width=700/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d405130-72cb-4c9a-a1e8-610c75385ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_B = 1    # Batch size\n",
    "_H = 2    # Number of heads\n",
    "_T = 4    # Time steps / Sequence length\n",
    "_D = 8    # Model vector dimension d_model\n",
    "d_ff = _D * 4    # Position-wise Feed Forward hidden layer dimenssion\n",
    "\n",
    "\n",
    "# Token Embedding\n",
    "embedding = torch.nn.Embedding(\n",
    "    num_embeddings=_T,\n",
    "    embedding_dim=_D\n",
    ")\n",
    "initialize_embedding_weights(module=embedding)\n",
    "DOe = torch.nn.Dropout(p=DROPOUT_RATIO)\n",
    "\n",
    "# Linear projections at Attention\n",
    "Wq = torch.nn.Linear(_D, _D, bias=True, dtype=TYPE_FLOAT)\n",
    "Wk = torch.nn.Linear(_D, _D, bias=True, dtype=TYPE_FLOAT)\n",
    "Wv = torch.nn.Linear(_D, _D, bias=True, dtype=TYPE_FLOAT)\n",
    "Wo = torch.nn.Linear(_D, _D, bias=True, dtype=TYPE_FLOAT)\n",
    "initialize_weights(Wq, d_model=_D)\n",
    "initialize_weights(Wk, d_model=_D)\n",
    "initialize_weights(Wv, d_model=_D)\n",
    "initialize_weights(Wo, d_model=_D, output_projection=True)\n",
    "\n",
    "# LayerNorm and Dropout for attention\n",
    "LNa = torch.nn.LayerNorm(normalized_shape=_D, eps=1e-5, bias=True, dtype=TYPE_FLOAT)\n",
    "DOa = torch.nn.Dropout(p=DROPOUT_RATIO)\n",
    "\n",
    "# Linear projections at Position Wise Feed Forward\n",
    "W1 = torch.nn.Linear(_D, _D, bias=True, dtype=TYPE_FLOAT)\n",
    "W2 = torch.nn.Linear(_D, _D, bias=True, dtype=TYPE_FLOAT)\n",
    "initialize_weights(module=W1, d_model=_D)\n",
    "initialize_weights(module=W2, d_model=_D, output_projection=True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "# LayerNorm and Dropout for position-wise feed-forward\n",
    "LNp = torch.nn.LayerNorm(normalized_shape=_D, eps=1e-5, bias=True, dtype=TYPE_FLOAT)\n",
    "DOp = torch.nn.Dropout(p=DROPOUT_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bce9616-d2ff-4ebd-8447-212cc991fd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0170, 0.0207, 0.0195, 0.0284], grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(embedding.weight, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18a907dd-7626-41da-ac65-2a2b4eaa303a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2184, 0.2326, 0.3188, 0.2912, 0.2524, 0.2617, 0.2292, 0.2004], grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(Wk.weight, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c933b886-e866-4c9b-8eaa-0d2db6629283",
   "metadata": {},
   "source": [
    "----\n",
    "# Input Sequece \n",
    "\n",
    "Input(sentence or time series) is a sequence of integer indices to embedding vectors. With ```B``` number of inputs, the shape is ```(B, T)```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b1213f-b749-448c-a797-2ec8d86c0a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.arange(0, _B * _T).view(_B, _T)    # Token IDs / Indices to token embedding vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30c54df3-652e-4500-afa0-7da533e39cbb",
   "metadata": {},
   "source": [
    "# Input Embeddings\n",
    "\n",
    "Extract embedding vectors ```x``` for tokens in the sequeces as shape ```(B, T, D)``` where ```D``` is the embedding vector dimensions (number of features).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6be53-8fa6-4794-b047-65e0620f830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x: torch.Tensor = embedding(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7917a776-c27b-4ad5-85c2-0ebd210666c2",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./image/transformer_embedding.png\" align=\"left\" width=750/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f74b2-8073-4f74-be7f-73c993f1b152",
   "metadata": {},
   "source": [
    "Multiply the  sequence embeddings ```x``` by $\\sqrt {d_{model} }$ as per the paper, which increase the variance of each sequence embeddings by $ d_{model}$. Not clear why this is required. In the original paper:\n",
    "\n",
    "1. Multi Head Attention layer directly calculate ```Q```, ```K```, ```V``` with ```x``` (after PE) in linear layers.\n",
    "2. Weights are commonly normalized with ```std=0.02``` as used in BERT (0.02 is empilical value).\n",
    "3. Hence, the variance of ```Q``` will be  $0.0004 x $ instead of $1$ if $W_q$ is normalized with Xavier. However, it is known empilically that . Assumption is the authors found empilically it is better to \n",
    "\n",
    "Also, **this will be unnecessary when using the pre-layer normalization** (**LN**) as LN will cancel this normalization by its own standardization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8096929c-4e6a-48fd-8c0a-c58416f13f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4525483399593905"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.02 * math.sqrt(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a59db0-e643-493e-aca1-2cce2fea84c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x * math.sqrt(_D)\n",
    "assert x.shape == (_B, _T, _D)\n",
    "print(f\"x.std:{torch.std(x, dim=-1)}\")\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6c3481-2ead-42fa-b5d5-909644bbae50",
   "metadata": {},
   "source": [
    "# Positional Encoding and Dropout\n",
    "\n",
    "Position encoding vector is added to the token embedding vector. Dropout is applied to the position added embedding.\n",
    "\n",
    "<img src=\"./image/transformer_dropout_to_embedding.png\" align=\"left\" width=750/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92b98d0-d126-45cb-9aba-48763ee55067",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PositionalEncoding(max_time_steps=_T, d_model=_D)\n",
    "positions = pe(x)\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4384651e-6ed6-4cb1-bfda-390938a31ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = DOe(x + positions)\n",
    "print(f\"x.std:{torch.std(x, dim=-1)}\")\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f5c8b-1c1c-4416-b0f7-b65c4b746968",
   "metadata": {},
   "source": [
    "# Pre Layer Normalization\n",
    "\n",
    "Laye Normalization is applied to the input to the sub-layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3d0ed3-32b9-430d-a7ee-9e775c621775",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LNa(x)\n",
    "print(f\"x.std:{torch.std(x, dim=-1)}\")\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a33f04-8732-4e44-a712-c9d8521c1cb6",
   "metadata": {},
   "source": [
    "# Split for Multi Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dae53b5-9a76-4534-93aa-cd04509d3435",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = split(Wq(x), h=_H)\n",
    "k = split(Wk(x), h=_H)\n",
    "v = split(Wv(x), h=_H)\n",
    "\n",
    "print(f\"k.shape:{k.shape}, k.std:{torch.std(k, dim=-1)}\")    # (B,H,T,d_k)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48959382-d37d-43e9-a57e-70b849dfa0ef",
   "metadata": {},
   "source": [
    "# Scaled Dot Product Attention\n",
    "\n",
    "<img src=\"./image/transformer_attention.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5c27a2-b74f-4fbf-9fa2-ecb53ee4b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inspect.getsource(ScaledDotProductAttention))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062c57c2-4217-49fc-ad75-fa040f3c8924",
   "metadata": {},
   "source": [
    "## First MatMul with Q and K (Calculate Similarity Score)\n",
    "\n",
    "For every token ```Q``` in a sequence, calculate the relationships (similarities) with other tokens in  ```K``` (for GPT, only with previous tokens). This builds a graph network of Self Attentions in which the strength among tokens in a sequence is represented as a graph in a matrix of shape ```(T, T)```.\n",
    "\n",
    "|Similarity Score (Q & K)| Proabability as Softmax |\n",
    "|---|---|\n",
    "|<img src=\"./image/transformer_dot_product_attention_similarity_score.jpeg\" align=\"left\" width=500/>|<img src=\"./image/transformer_dot_product_attention.png\" align=\"left\" width=175/>|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b5688-b6fd-4565-be09-c4bcd13cde7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inspect.getsource(calculate_dot_product_similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7166be-4618-430f-a086-7fc506889941",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = calculate_dot_product_similarities(query=q, key=q)\n",
    "print(similarities.shape)    # (B,H,T,T)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb90c4a3-efdd-4658-9e30-d770c737c90d",
   "metadata": {},
   "source": [
    "\n",
    "## Scale by $\\sqrt{d_k}$\n",
    "\n",
    "### Control the variance\n",
    "\n",
    "As in the name **Scaled** Dot-Product Attention, the similarity score is normalized by $\\sqrt{d_k}$ to manage the variance where $d_k$ is the dimension of the key vector $k$ (which is the same with that of query).\n",
    "\n",
    "Providing $W$ is initialized with Xavier so that the variance of $X@W^T$ will be 1.0. Suppose the positionally encoded token vector $x$ has dimension $D$, the shape of $W_K$ is ```(M, D)```. Then key $k = x:(D,) @ W^T_K:(D,M)$ has the shape $(M,)$, which is $d_k$. The variance of the pdorduct $Q\\cdot K^T$ is $d_k$. The variance of two zero-mean normal distributions is:\n",
    "\n",
    "[Variance of product of multiple independent random variables](https://stats.stackexchange.com/questions/52646/)\n",
    "\n",
    "$${\\rm Var}(XY) = E(X^2Y^2) âˆ’ (E(XY))^2={\\rm Var}(X){\\rm Var}(Y)+{\\rm Var}(X)(E(Y))^2+{\\rm Var}(Y)(E(X))^2$$  $$E(X)=E(Y)=0$$\n",
    "\n",
    "<img src=\"./image/variance_of_q@k.jpeg\" align=\"left\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11de16b9-7a7c-45f7-a829-a44930675019",
   "metadata": {},
   "source": [
    "* [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY?t=4616)\n",
    "\n",
    "> If you have unit gausian input of mean 0 and $W_K$ and $W_V$ are unit gaussian, and if you calculate the ```similarity``` naively, the variance is the order of the head size $d_k$ (e.g. approx 16 if $d_k$ == 16). By standardizing the ```similarity``` score by $\\sqrt{d_k}$ the variance of the ```similarity``` socre will be normal (approx 1.0).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aa6170-aadf-4bb9-9bd9-0e8367c01d08",
   "metadata": {},
   "source": [
    "### Soften the softmax outputs\n",
    "\n",
    "#### Effect of exponential\n",
    "\n",
    "Exponential has amplify/supress effects when the signal has large variance with their values being negative and positive. Hence, only the large positive signal value gets amplified and others towards negatives get supressed.\n",
    "\n",
    "This is desirable to predict one class so that only one class stands out. But to incorporate all the signals, this amplify/supression is undesirable. By standardizing the signals (scale with the std of the signals), the exponentials get softened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a642538-f9ae-43e6-b7f7-9a47ac8e68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3),  tight_layout=True) \n",
    "plt.grid()\n",
    "\n",
    "head_size = 16\n",
    "normalized_qk = torch.tensor(sorted([ 0.0838, -0.7539,  0.5026, -1.1728,  1.3405]))\n",
    "naive_qk = normalized_qk * torch.sqrt(torch.tensor(head_size))\n",
    "\n",
    "# Variance=head_size=d_k\n",
    "\n",
    "_x = np.linspace(naive_qk[0]-0.5, naive_qk[-1]+0.5)\n",
    "axes[0].plot(_x, np.exp(_x), color='k', linewidth=0.5)\n",
    "axes[0].scatter(naive_qk, torch.exp(naive_qk), color='r')\n",
    "axes[0].set_title(\"large variance with negative/positive range\")\n",
    "axes[0].grid(linestyle='-', linewidth=1)\n",
    "\n",
    "# Variance = 1\n",
    "\n",
    "_x = np.linspace(normalized_qk[0]-0.5, normalized_qk[-1]+0.5)\n",
    "axes[1].plot(_x, np.exp(_x), color='k', linewidth=0.5)\n",
    "axes[1].scatter(normalized_qk, torch.exp(normalized_qk))\n",
    "axes[1].grid(linestyle='-', linewidth=1)\n",
    "axes[1].set_title(\"standardized variance 1 with negative/positive range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550459d3-5061-45bd-a2e4-cafca663e5a2",
   "metadata": {},
   "source": [
    "\n",
    "#### Without scaling\n",
    "\n",
    "When the similarity score is not normalized/scaled by $\\sqrt{d_k}$, the softmax becomes **peaky** like one hot encoding, which is beneficial for classification (amplify the high score signal). However, for self attention, softmax will pickup the nodes with larger values, hence only specific nodes in the sequence will be incorporated into the BoW. We want to consider the communication among every nodes if there is, not specific ones only.\n",
    "\n",
    "* [Let's build GPT: from scratch, in code, spelled out.](https://youtu.be/kCc8FmEb1nY?t=4694)\n",
    "\n",
    "> The problem here, because of the softmax, if the $Q@K$ takes very positive or very negative numbers inside it, softmax will converge towards one hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88651d9f-56b2-4ed2-9047-03dfc99ca314",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_size = 16\n",
    "normalized_qk = torch.tensor([ 0.0838, -0.7539,  0.5026, -1.1728,  1.3405])\n",
    "print(f\"VAR[qk]={torch.var(normalized_qk)}\")\n",
    "\n",
    "# Note that variance of Q@K is close to the head_size d_k\n",
    "# # scale with std to make the variance==head_size\n",
    "naive_qk = normalized_qk * torch.sqrt(torch.tensor(head_size))\n",
    "print(f\"variance:{naive_qk.var()}, softmax:{torch.softmax(naive_qk, dim=-1)}\")\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.grid()\n",
    "plt.stem(range(len(naive_qk)), torch.softmax(naive_qk, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8cda58-08ac-4c0e-9bff-f2333221f3b2",
   "metadata": {},
   "source": [
    "#### With scaling\n",
    "\n",
    "By scale/normalize, the softmax will be smoothed/diffused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce13dc5-0ee8-414e-bdb7-64856d6356eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_qk = naive_qk / torch.sqrt(torch.tensor(head_size))\n",
    "print(f\"variance:{scaled_qk.var()}, softmax:{torch.softmax(naive_qk, dim=-1)}\")\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.grid()\n",
    "plt.stem(range(len(scaled_qk)), torch.softmax(scaled_qk, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2491f5-fb3d-416f-adf4-ad85b7ebb2a4",
   "metadata": {},
   "source": [
    "### Code for Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055405d1-fa1c-40e3-abdb-23e965ef48fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inspect.getsource(scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f98016-2abb-476d-b8b0-28f3e20d4ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_similarities = scale(similarities=similarities, d_k=_D/_H)\n",
    "scaled_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7d6ae-64db-4d47-b900-57f6c5b61803",
   "metadata": {},
   "source": [
    "## Mask\n",
    "\n",
    "Optional. After calculating ```(T,T)``` matrix of similarities from q to k, mask the matrix to prevent the communications with future time steps by replacing the similarity values with the ```-inf```, meaning there is **no relationship** from ```q```. Then softmax will make the contribution from the ```-inf``` to zero. This is the same with blocking relations between ```q``` and ```k```.\n",
    "\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825b93b-4d6b-41a5-a4d6-d05c908a9f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask matrix to decide which element in (T,T) matrix to mask\n",
    "mask_matrix = torch.tril(torch.ones(_T,_T)) == 0\n",
    "mask_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f4378-ed66-423c-baad-a7fc337363a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask the similarity matrix element of future steps with -inf\n",
    "\n",
    "masked_similarities = torch.clone(similarities).masked_fill(mask=mask_matrix, value=float('-inf'))\n",
    "masked_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4af2a2-e1b7-4c57-9ae5-fe06fc34d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax will supress the contirubion from -inf similarity values\n",
    "F.softmax(masked_similarities, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bbfb41-e777-404e-85f2-d1382970b0ea",
   "metadata": {},
   "source": [
    "### Code for Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e365ea95-84a2-423e-9140-c26a5c1450fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inspect.getsource(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e5b0d8-8e1a-43e2-a0d0-338d939a5673",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_similarities = mask(similarities=similarities, mask_matrix=mask_matrix)\n",
    "masked_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271a753-0956-4b7b-9c20-cae90c4efb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_similarities = F.softmax(masked_similarities, dim=-1)\n",
    "print(f\"normalized_similarities.std:{torch.std(normalized_similarities, dim=-1)}\")\n",
    "\n",
    "normalized_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db11fb7-d368-4556-ab1a-056d0b0b8478",
   "metadata": {},
   "source": [
    "## Second MatMul with V (Calculate Bag of Words Attention Value)\n",
    "\n",
    "One way to generate the inter-connections among the tokens to distill their knowledges or relations is ```BoW``` by averaging them feature-wise/axis=-1.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./image/transformer_dot_product_attention_bow.png\" align=\"left\" width=700/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e061adeb-8b2b-4783-9a4e-f9f499e3db0a",
   "metadata": {},
   "source": [
    "Note that the initially the value of similarity is random or ```(1.0, 1/2, 1/3, ...)``` but eventually it gets trained to memorize the relations among position-encoded tokens.\n",
    "\n",
    "* [Let's build GPT: from scratch, in code, spelled out.](https://youtu.be/kCc8FmEb1nY?t=3814)\n",
    "\n",
    "> Different will find other tokens more or less interesting and we want that data dependent. If I/token is a vowel, I am looking for consonants in my past and want to know what consonants were. And I want the information to flow to me (connection). This is the problem that Self Attention solves.\n",
    "\n",
    "<img src=\"./image/self_attention.jpeg\" align=\"left\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff9381-ea71-4a56-8695-dbd0e14ade54",
   "metadata": {},
   "source": [
    "### Purpose of  using $W_V$\n",
    "\n",
    "$v$ looks to be a proxy of $x$ but what transformation or meaning does $W_V$ gives by having transformation from $x$ to $v$? (Note: ```x``` in the diagram above is actually $v$ as $v=x@W{_V}{^T}$).\n",
    "\n",
    "* [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY?t=4258)\n",
    "\n",
    "> $x$ is like a private information to a token. For the purpose of the single attention head, $v$ is what I give for you to communicate with if you find me interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49421e67-b3f2-43a0-b9e0-41ab241817cf",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47923ef-9b0c-4ade-91f6-90a9e1ce09e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inspect.getsource(calculate_attention_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4aa90d-f685-4c1b-b90a-5e493756d6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attensions = calculate_attention_values(similarities=normalized_similarities, values=v)\n",
    "print(attensions.shape)    # (B,H,T,d_v)\n",
    "attensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a332bafc-a932-4a45-9399-12601a71ca39",
   "metadata": {},
   "source": [
    "### Verify ScaledDotProductAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd60950a-64bd-4b2a-a124-aca111e59712",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdpa = ScaledDotProductAttention(do_mask=True, max_time_steps=_T)\n",
    "a = sdpa(q=q,k=k,v=v)\n",
    "print(f\"a.std:{torch.std(a, dim=-1)}\")\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29037d96-27c9-482e-84e1-be8e17e18218",
   "metadata": {},
   "source": [
    "# Multi Attention Head\n",
    "\n",
    "Divide the embedding vector q, k, v into ```h``` number of segmenets and apply self attention to each segment in parallel respectively.\n",
    "\n",
    "* <img src=\"./image/transformer_paper_multi_head_attention.png\" align=\"left\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ddc47c-f539-4bd9-a7e7-27d87bbd9a38",
   "metadata": {},
   "source": [
    "\n",
    "|<img src=\"./image/transformer_multi_head_attentions.png\" align=\"left\" width=200/>   | \n",
    "<img src=\"./image/transformer_multi_head_attention_formula.png\" align=\"left\" width=700/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef7f98-f117-4ae5-8485-25ba4f3f488b",
   "metadata": {},
   "source": [
    "* [Transformers Explained Visually (Part 3): Multi-head Attention, deep dive](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)\n",
    "\n",
    "<img src=\"./image/transformer_multi_head_attention.png\" align=\"left\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d81e2d-efae-4c53-9517-87f430d03197",
   "metadata": {},
   "source": [
    "### Concatenate multiple outputs from Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03238ec0-1b00-4714-a6de-97060a919fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "attensions = attensions.transpose(2,1)  # (B,T,H,d_v)\n",
    "attentions = attensions.reshape(_B,_T,-1)\n",
    "print(attentions.shape)    # (B,T,D)\n",
    "attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e6e80-a5b6-4e2e-b38b-5fd799af1c52",
   "metadata": {},
   "source": [
    "### Linear Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd009f4-8b76-4962-96a8-859446ac0a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = Wo(attentions)\n",
    "print(attentions.shape)    # (B,T,D)\n",
    "attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f06caa-8a74-4974-890a-8186891d2066",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c863d8-2dd4-43e9-8a29-bcd78de2afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inspect.getsource(MultiHeadAttention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ff1bb-3431-4400-97ea-18e140b3e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the MultiHeadAttention output\n",
    "mha = MultiHeadAttention(\n",
    "    num_heads=_H,\n",
    "    d_model=_D,\n",
    "    dtype=TYPE_FLOAT,\n",
    "    do_mask=True,\n",
    "    max_time_steps=_T,\n",
    "    bias=True,\n",
    ")\n",
    "mha.Wq = Wq\n",
    "mha.Wk = Wk\n",
    "mha.Wv = Wv\n",
    "mha.Wo = Wo\n",
    "\n",
    "\n",
    "attentions = mha(q=x, k=x, v=x)\n",
    "attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a48aa65-5906-4e4f-a32c-c5145dd419ca",
   "metadata": {},
   "source": [
    "## Dropout and Residual Connection\n",
    "\n",
    "Dropout is applied to the output of each sub-layer and added to input x to the sub-layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29033c75-8d27-49b4-9708-45b9b9f54ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x + DOa(x)   # DO NOT use x += DOa(x) as it is in-place operation for PyTorch.\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5150f0df-6568-4c82-b5b5-daf101c955bb",
   "metadata": {},
   "source": [
    "### \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea5fac-4b0e-453b-a073-5e49dab46628",
   "metadata": {},
   "source": [
    "# Position-wise Feed Forward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d30f0-4942-44e5-9192-f9fd3676f252",
   "metadata": {},
   "source": [
    "## Pre Layer Normalization\n",
    "\n",
    "Laye Normalization is applied to the input to the sub-layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f62d1f6-a9d0-4421-8bcf-88e67ad7aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LNp(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebdcb72-92dc-4204-8fb3-aa237772b8da",
   "metadata": {},
   "source": [
    "## Feed Forward \n",
    "\n",
    "Apply a wider single hidden layer neural network with ReLU activation. Here the features in the attention vector of each token can be amplified or supressed by weights, and multiple combination of features can form a new feature as the output. Then back to d_model dimensions.\n",
    "\n",
    "<img src=\"./image/transformer_paper_positionwise_feedforward.png\" align=\"left\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba8e8dd-b0ec-4b6d-80ba-4e09385c0aaa",
   "metadata": {},
   "source": [
    "The PwFF transfers the token embedding vectors to a space of diffrent semantic allowing Transformer to acquire token-embedding vector level learning.\n",
    "\n",
    "1. ```Self Attention``` learns relationships among tokens (building graph), to be exact among heads via multi head attention.\n",
    "2. ```PwFF``` usea each token of ```D``` dimension as ```D``` features to generate a new token for the next layer.\n",
    "\n",
    "<img src=\"./image/transformer_positionwise_feedforward.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c303c0-4a54-42b8-acca-c4de7338894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedforward = W2(relu(W1(x)))\n",
    "feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526b697-02a9-4ba7-9d2d-da34570a1183",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5cc56c-2c6b-4286-9a9a-1213b828a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inspect.getsource(PositionwiseFeedForward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6d2641-a097-4eab-9938-97ba8459627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the PositionwiseFeedForward output.\n",
    "pwff = PositionwiseFeedForward(d_model=_D, d_ff=d_ff, dtype=TYPE_FLOAT, bias=True)\n",
    "pwff.W1 = W1\n",
    "pwff.W2 = W2\n",
    "pwff.relu = relu\n",
    "\n",
    "feedforward = pwff(x)\n",
    "print(f\"feedforward.std:{torch.std(feedforward, dim=-1)}\")\n",
    "\n",
    "feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35abab71-f498-42fe-9bf8-27ecaae6edd8",
   "metadata": {},
   "source": [
    "## Dropout and Residual Connection\n",
    "\n",
    "Dropout is applied to the output of each sub-layer and added to input x to the sub-layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1641fb9d-9a19-4cd6-b57b-c743a83e2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = x + DOp(feedforward)\n",
    "print(f\"encoded.std:{torch.std(encoded, dim=-1)}\")\n",
    "\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbc10b4-18fd-45a2-8611-c6067002774a",
   "metadata": {},
   "source": [
    "---\n",
    "# Encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f0277d-bd61-4cd6-805d-a77d5b389588",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inspect.getsource(Encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f8c5da-bfdc-4424-8e4f-486eeb595a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    vocabulary_size=_T,\n",
    "    num_layers=1,\n",
    "    num_heads=_H,\n",
    "    d_model=_D,\n",
    "    dtype=TYPE_FLOAT,\n",
    "    d_ff=d_ff,\n",
    "    do_mask=True,\n",
    "    max_time_steps=_T,\n",
    "    bias=True,\n",
    "    p_drop=DROPOUT_RATIO,\n",
    "    eps=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39be46b-9057-442d-bd23-75476bbdef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = encoder(indices)\n",
    "print(f\"memory.std:{torch.std(memory, dim=-1)}\")\n",
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1035e80d-22e5-4df0-9ffc-e611a0711f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
