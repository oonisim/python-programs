bias=False before Normalization layers.

# Bias=False before Normalization Layer

```bias=False``` before **Normalization Layer** as it will zero-center the data.

* [When should you not use the bias in a layer?](https://ai.stackexchange.com/a/27742/45763)

> The BatchNorm layer will re-center the data, removing the bias and making it a useless trainable parameter.

* [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)

> Note that, since we normalize ```𝑊𝑢+𝑏```, the **bias ```𝑏``` can be ignored** since its effect will be canceled by the subsequent mean subtraction.

* [Why are biases (typically) not used in attention mechanism?](https://ai.stackexchange.com/a/40256/45763)

> The reason for this is that these layers are typically followed by a normalization layer, such as Batch Normalization or Layer Normalization. These normalization layers center the data at mean=0 (and std=1), effectively removing any bias.