
  Issue 7: Embedding Scaling Might Be Suboptimal

  Location: common.py:798

  Problem:
  def forward(self, indices: Tensor):
      x = self.embedding(indices) * math.sqrt(self._D)  # Multiply by sqrt(d_model)

  Why it's an issue with weight tying:
  - Embeddings are scaled by sqrt(d_model) = sqrt(512) â‰ˆ 22.6
  - This means input embeddings have variance â‰ˆ 0.02Â² Ã— 512 = 0.2
  - But the output projection uses the same weights without scaling

  In the projection:
  # common.py:901
  return torch.log_softmax(self.projection(y), dim=-1)
  # projection(y) does: y @ weight.T where weight is the UNSCALED embedding

  Effect:
  - Input: Embedding weights scaled by 22.6
  - Output: Same weights used unscaled
  - This asymmetry is intentional per the paper but worth noting

  Risk: None (this is by design per the paper)

  Note: The paper is ambiguous about whether the output projection should also be scaled. Current implementation matches standard practice (no scaling at output).

  ---
  ðŸ”§ Recommendations Summary
  ---
  âœ… Testing Recommendations

  Create tests to verify weight tying works correctly:

  def test_weight_tying():
      model = Transformer(...)

      # Test 1: Same memory address
      assert model.projection.projection.weight.data_ptr() == \
             model.output_embedding.embedding.weight.data_ptr()

      # Test 2: Gradient sharing
      model.output_embedding.embedding.weight.grad = torch.randn_like(
          model.output_embedding.embedding.weight
      )
      assert model.projection.projection.weight.grad is \
             model.output_embedding.embedding.weight.grad

      # Test 3: Shape compatibility
      assert model.projection.projection.weight.shape == \
             model.output_embedding.embedding.weight.shape
