"""Module of the Decoder stack in the Transformers Model based on the
"Attention is All You Need" paper and YouTube by Andrej Karpathy.
This is not generated by AI :<

Note that there TWO Decoder Blocks each of which has its attention layer.

1. Masked self‑attention (causal)
Q, K, V all come from the decoder's current sequence (x)
This is what enforces autoregression.

2. Encoder‑Decoder (cross attention)
Q comes from the decoder, K/V come from encoder memory
This is what the paper's 3.2.3 quote refers to.

"""
from typing import Optional

import torch
from torch import (
    Tensor,
    nn
)

from .constant import (
    TYPE_FLOAT,
    DIM_MODEL,
    DIM_PWFF_HIDDEN,
    NUM_LAYERS,
    NUM_HEADS,
    MAX_TIME_STEPS,
    DROPOUT_RATIO,
    EPSILON,
)
from .common import (
    MultiHeadAttention,
    PositionwiseFeedForward,
)


class DecodeLayer(nn.Module):
    """Decoder Layer supporting both encoder-decoder and decoder-only modes.

    Supports two modes:
    1. Encoder-Decoder mode (memory provided): Uses causal self-attention AND
       cross-attention. Used by: Original Transformer, T5, BART.
    2. Decoder-only mode (memory=None): Uses only causal self-attention.
       Used by: GPT-2, GPT-3, LLaMA, Mistral.

    Attention blocks:
    1. Decoder Masked Self Attention (always applied)
    2. Decoder to Encoder Cross Attention (skipped when memory=None)

    > We employ a residual connection around each of the two sub-layers, followed
    > by layer normalization. That is, the output of each sub-layer is
    > LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented
    > by the sub-layer itself.

    > Residual Dropout:
    > We apply dropout to the output of each sub-layer, before it is added to the
    > sub-layer input and normalized. In addition, we apply dropout to the sums of
    > the embeddings and the positional encodings in both the encoder and decoder
    > stacks.

    > 3.2.3
    > In "encoder-decoder attention" layers, the queries come from the previous decoder
    > layer, and the memory keys and values come from the output of the encoder.
    > This allows every position in the decoder to attend over all positions in the
    > input sequence. This mimics the typical encoder-decoder attention mechanisms in
    > sequence-to-sequence models.
    >
    > self-attention layers in the decoder allow each position in the decoder to
    > attend to all positions in the decoder up to and including that position.
    > We need to prevent leftward information flow in the decoder to preserve the
    > auto-regressive property
    > [NOTE]:
    > As in Figure 2 of the paper, the masked attention is first attention only.
    """
    def __init__(
            self,
            i_layer: int,
            num_heads: int = NUM_HEADS,
            d_model: int = DIM_MODEL,
            dtype: Tensor.dtype = TYPE_FLOAT,
            d_ff: int = DIM_PWFF_HIDDEN,
            max_time_steps: int = MAX_TIME_STEPS,
            bias: bool = True,
            p_drop: float = DROPOUT_RATIO,
            eps: float = EPSILON
    ):
        """
        Args:
            i_layer: layer index (i-th layer from the bottom)
            num_heads: number of attention heads
            d_model: dimensions of the model embedding vector.
            d_ff: dimensions of the positionwise feed forward hidden layer output vector
            max_time_steps: max sequence length or time steps T.
            bias: Ture if learning the additive bias at the linear layer.
            p_drop: dropout rate.
            eps: epsilon of LayerNorm
        """
        super().__init__()

        # --------------------------------------------------------------------------------
        # Masked Causal Self Attention
        # --------------------------------------------------------------------------------
        self.layer_norm_input: nn.LayerNorm = nn.LayerNorm(     # Normalize decoder input
            normalized_shape=d_model,
            eps=eps,
            dtype=dtype
        )
        # Self attention to historic time sequence, but not future by masking it
        self.causal_self_attention: MultiHeadAttention = MultiHeadAttention(
            i_layer=i_layer,
            num_heads=num_heads,
            d_model=d_model,
            dtype=dtype,
            do_mask=True,
            max_time_steps=max_time_steps,
            bias=bias,
        )
        self.dropout_causal: nn.Module = nn.Dropout(p=p_drop)

        # --------------------------------------------------------------------------------
        # Decoder Q (memory) to Encoder K Cross Attention
        # Layer normalization on both from encoder memory and from decoder causal attention.
        # --------------------------------------------------------------------------------
        self.layer_norm_memory: nn.LayerNorm = nn.LayerNorm(    # Normalize encoder memory
            normalized_shape=d_model,
            eps=eps,
            dtype=dtype
        )
        self.layer_norm_causal: nn.LayerNorm = nn.LayerNorm(    # Normalize causal attention
            normalized_shape=d_model,
            eps=eps,
            dtype=dtype
        )
        self.cross_attention: MultiHeadAttention = MultiHeadAttention(
            i_layer=i_layer,
            num_heads=num_heads,
            d_model=d_model,
            dtype=dtype,
            do_mask=False,
            max_time_steps=max_time_steps,
            bias=bias,
        )
        self.dropout_cross: nn.Module = nn.Dropout(p=p_drop)

        # --------------------------------------------------------------------------------
        # Point-wise Feed Forward
        # --------------------------------------------------------------------------------
        self.layer_norm_cross: nn.LayerNorm = nn.LayerNorm(     # Normalize cross attention
            normalized_shape=d_model,
            eps=eps,
            elementwise_affine=True,
            bias=True,
            dtype=dtype
        )
        self.feedforward: PositionwiseFeedForward = PositionwiseFeedForward(
            i_layer=i_layer,
            d_model=d_model,
            d_ff=d_ff,
            dtype=dtype,
            bias=bias,
        )
        self.dropout_feedforward: nn.Module = nn.Dropout(p=p_drop)

    def forward(
            self,
            x: Tensor,
            memory: Tensor = None,
            source_pad_mask: Optional[Tensor] = None,
            target_pad_mask: Optional[Tensor] = None
    ) -> Tensor:
        """Decode with optional cross-attention.
        Args:
            x: embedding vector from the decoder layer of shape (B,Tq,D)
            memory: encoder embedding vector of shape (B, Tk, D). If None,
                    cross-attention is skipped (decoder-only mode for GPT-style LM).
            source_pad_mask: optional padding mask of shape (B, Tk) for encoder
                sequence, where True indicates padding positions to be masked out
                in cross-attention.
            target_pad_mask: optional padding mask of shape (B, Tq) for decoder
                sequence, where True indicates padding positions to be masked out
                in causal self-attention.
        """
        # > 3.2.3
        # > In "encoder-decoder attention" layers, the queries come from the previous decoder
        # > layer, and the memory keys and values come from the output of the encoder.
        # > This allows every position in the decoder to attend over all positions in the
        # > input sequence. This mimics the typical encoder-decoder attention mechanisms in
        # > sequence-to-sequence models.
        #
        # Token q from the target sequence identifies the relationship strengths
        # with each token v in the source sequence. This mimics the original attention
        # mechanism (cursor) in the sequence to sequence model.

        #--------------------------------------------------------------------------------
        # 1. Masked Self-Attention (causal): Q, K, V come from Decoder Sequence (x).
        # x = x + attention is Skip connection.
        # Apply target padding mask to prevent attention to padded target tokens.
        # The causal mask (upper triangular) prevents future attention, while
        # target_pad_mask prevents attention to padded positions in variable-length batches.
        #--------------------------------------------------------------------------------
        _q = _k = _v = self.layer_norm_input(x)
        x = x + self.dropout_causal(
            self.causal_self_attention(
                q=_q,
                k=_k,
                v=_v,
                padding_mask=target_pad_mask
            )
        )

        #--------------------------------------------------------------------------------
        # 2. Decoder Q to Encoder K Cross Attention (skipped for decoder-only mode).
        # Q = Decoder query to identify the relevance of keys in the Encoder memory.
        # K/V = encoder memory (source sequence context)
        #
        # In AlphaFold, each amino acid is a token and a protein is a token sequence.
        # A query token q finds how strongly it is 'gravitated' to each token in K,
        # with larger weights indicating stronger influence
        #
        # Apply source padding mask to prevent attending to padded encoder positions.
        #--------------------------------------------------------------------------------
        if memory is not None:
            _q = self.layer_norm_causal(x)
            _k = _v = self.layer_norm_memory(memory)
            # x = x + attention is Skip connection.
            x = x + self.dropout_cross(
                self.cross_attention(
                    q=_q,
                    k=_k,
                    v=_v,
                    padding_mask=source_pad_mask
                )
            )
        else:
            # Decoder-only mode (GPT-style LM with no cross attention)
            # ```
            # hidden = self.decoder(y=x, memory=None)  # ← memory=None skips cross-attention
            # ```
            # --------------------------------------------------------------------------------
            # [Frozen Weights]
            # When memory==None, cross-attention is skipped, which means the cross-attention
            # parameters are not part of the computational graph and do not receive gradients
            # during backpropagation.
            #
            # This means 48 cross-attention parameters never receive gradients:
            # - 4 layers × (Wq, Wk, Wv, Wo + biases) = 32 params
            # - Plus associated layer norms = 16 params
            # - Total = 48 "frozen" params
            # They remain at their initial values and do not update during training, that
            # causes Frozen Weights.
            #
            # This is expected because they aren't part of the computational graph.
            # --------------------------------------------------------------------------------
            pass

        #--------------------------------------------------------------------------------
        # Point-Wise Feed Forward Network (FFN)
        # Point-Wise means the linear projection is applied independently to each token
        # without mixing information across tokens. Point = Position = Token.
        # Where as Attention is communication among tokens, FFN is token independent.
        #--------------------------------------------------------------------------------
        # x = x + attention is Skip connection.
        x = x + self.dropout_feedforward(self.feedforward(self.layer_norm_cross(x)))
        assert torch.all(torch.isfinite(x))
        return x


class Decoder(nn.Module):
    """Class to implement Transformer Encoder.
    Citation:
    > In addition, we apply dropout to the sums of the embeddings and
    > the positional encodings in both the encoder and decoder stacks.
    > For the base model, we use a rate p_drop = 0.1.

    Note that the input to the decoder during the training is the target sequence
    shifted one position to the right by inserting the <START> token at the top
    that signals the beginning of the sentence. Hence, there will be T predictions
    for each sequence.
    """
    @property
    def D(self) -> int:     # pylint: disable=invalid-name
        """Dimension of the model embedding vector
        """
        return self._D

    def __init__(
            self,
            vocabulary_size: int,
            num_layers: int = NUM_LAYERS,
            num_heads: int = NUM_HEADS,
            d_model: int = DIM_MODEL,
            dtype: Tensor.dtype = TYPE_FLOAT,
            d_ff: int = DIM_PWFF_HIDDEN,
            max_time_steps: int = MAX_TIME_STEPS,
            bias: bool = True,
            p_drop: float = DROPOUT_RATIO,
            eps: float = EPSILON
    ):
        """
        Args:
            vocabulary_size: size of the entire token vocabulary of tokenizer
            num_layers: number of layers to stack in the Decoder
            num_heads: number of heads in multi head attention
            d_model: dimension D of the signal y: (B, T, D)
            dtype: data type of the signal
            d_ff: dimension of the point-wise feed forward layer signal
            max_time_steps: max sequence size T
            bias: flag to use bias in the linear layer
            p_drop: dropout ratio at Dropout layers
            eps: small number to prevent e.g. divide by zero.
        """
        super().__init__()
        self._D: int = d_model      # pylint: disable=invalid-name
        assert vocabulary_size > 0, f"invalid vocabulary size [{vocabulary_size}]."

        # --------------------------------------------------------------------------------
        # Decoder layers
        # --------------------------------------------------------------------------------
        self.layers: nn.ModuleList = nn.ModuleList([
            DecodeLayer(
                i_layer=_layer,
                num_heads=num_heads, d_model=d_model, dtype=dtype, d_ff=d_ff,
                max_time_steps=max_time_steps, bias=bias,
                p_drop=p_drop, eps=eps
            ) for _layer in range(num_layers)
        ])

    def forward(
            self,
            y: Tensor,
            memory: Tensor = None,
            source_pad_mask: Optional[Tensor] = None,
            target_pad_mask: Optional[Tensor] = None
    ) -> Tensor:
        """Decode the input embeddings.
        The memory (embeddings of the source sequence) is regarded as a prompt or context
        to condition the model output given the historic tokens in the target sequence.

        > https://youtu.be/kCc8FmEb1nY?t=6351
        > The decoding is conditioned not only on the historic tokens in the target sequence,
        > but on the encoded memory of the entire source sequence as a prompt, soft of.

        Args:
            y: embedding vectors of shape (B, T, D).
                Embedding and positional encoding are applied in model.py / lm.py.
            memory: encoder embeddings. If None, decoder-only mode (GPT-style LM).
            source_pad_mask: optional padding mask of shape (B, Tk) for encoder
                sequence, where True indicates padding positions to be masked out
                in cross-attention.
            target_pad_mask: optional padding mask of shape (B, Tq) for decoder
                sequence, where True indicates padding positions to be masked out
                in causal self-attention.

        Returns: Decoder next token predictions of shape (B, T, D)
        """
        assert torch.is_tensor(y) and y.ndim == 3   # shape (B, T, D)
        _B, _T, _D = y.shape        # pylint: disable=invalid-name
        assert _D == self.D, (f"Expected the dimension D: of input y:(B,T,D) as [{self.D}] "
             f"defined at class instantiation, got [{_D}]."
        )

        # --------------------------------------------------------------------------------
        # N x Decode Layers
        # --------------------------------------------------------------------------------
        # How encoder output (memory) flows to decoder layers:
        #
        # 1. Encoder runs once and produces the final encoder output from the top layer.
        #
        # 2. Same memory goes to ALL decoder layers:
        #    Each decoder layer receives the SAME encoder output.
        #
        # 3. Each decoder layer uses it for cross-attention where:
        #    Q comes from the decoder (current layer's causal self-attention output)
        #    K, V come from encoder memory (shared across all decoder layers)
        #
        # Architecture Flow:
        #
        # Encoder Stack:
        #   Layer 1 → Layer 2 → ... → Layer N → [memory output]
        #                                             ↓
        #                                     (shared by all decoder layers)
        #                                             ↓
        # Decoder Stack:                              ↓
        #   Layer 1: Self-Attn → Cross-Attn ← ────────┘
        #               ↓              ↑
        #   Layer 2: Self-Attn → Cross-Attn ← ────────┘
        #               ↓              ↑
        #   Layer N: Self-Attn → Cross-Attn ← ────────┘
        #
        # Why This Is Correct:
        #
        # According to the "Attention is All You Need" paper (Section 3.2.3):
        # "In encoder-decoder attention layers, the queries come from the previous
        # decoder layer, and the memory keys and values come from the output of
        # the encoder."
        #
        # Note: "the output of the encoder" (singular) - meaning the final encoder
        # output, not layer-by-layer correspondence.
        # --------------------------------------------------------------------------------
        for _layer in self.layers:
            y = _layer(
                x=y,
                memory=memory,
                source_pad_mask=source_pad_mask,
                target_pad_mask=target_pad_mask
            )

        assert y.shape == (_B, _T, self.D)
        return y
