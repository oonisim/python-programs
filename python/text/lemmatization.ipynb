{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7720d1c",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "* [Stemming and Lemmatization in Python](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python)\n",
    "\n",
    "> Lemmatization returns an actual word of the language, it is used where it is necessary to get valid words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc0a23",
   "metadata": {},
   "source": [
    "# NLTK \n",
    "\n",
    "* [How to download the NLTK library?](https://www.dezyre.com/recipes/download-nltk-library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19254815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/oonisim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/oonisim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/oonisim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('all')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "817b0128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b6941",
   "metadata": {},
   "source": [
    "You need to provide the context in which you want to lemmatize that is the parts-of-speech (POS). This is done by giving the value for pos parameter in wordnet_lemmatizer.lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29ac0cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hug'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize(\"hugging\", pos=\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ab07bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "he                  he                  \n",
      "was                 be                  \n",
      "playing             play                \n",
      "a                   a                   \n",
      "baseball            baseball            \n",
      "among               among               \n",
      "a                   a                   \n",
      "lot                 lot                 \n",
      "of                  of                  \n",
      "people              people              \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      ".                   .                   \n",
      "he                  he                  \n",
      "has                 have                \n",
      "the                 the                 \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "sun                 sun                 \n",
      ".                   .                   \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was playing a baseball among a lot of people and eating at same time. He has the bad habit of swimming after playing long hours in the Sun.\"\n",
    "tokens = nltk.word_tokenize(sentence.lower())\n",
    "\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in tokens:\n",
    "    print (\"{0:20}{1:20}\".format(word,lemmatizer.lemmatize(word, pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "990492a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    return \" \".join([\n",
    "        lemmatizer.lemmatize(word, pos=\"v\") \n",
    "        for word in nltk.word_tokenize(sentence.lower()) \n",
    "        if word not in stopwords.words('english')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c2f483b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'play baseball among lot people eat time . bad habit swim play long hours sun .'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eb6486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0a6c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1840adcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"\n",
    "He was playing a baseball among a lot of people and eating at same time. He has the bad habit of swimming after playing long hours in the Sun.\n",
    "Zürich has a famous website https://www.zuerich.com/ \n",
    "WHICH ACCEPTS 40,000 € and adding a random string, :\n",
    "abc123def456ghi789zero0 for this demo. !!!&*^% tako.hoge@gmail.com' \n",
    "I Won't !*%$^&*#$#!!! ?? ? ~!@#$%^&*()_=+\\[\\]{}\\\\\\|;:\\-\"\\'<>.,/? pierod.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e26bfdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from cleantext import clean\n",
    "\n",
    "def decontracted(sentences):\n",
    "    \"\"\"Restore the contracted words\"\"\"\n",
    "    # specific\n",
    "    sentences = re.sub(r\"won\\'t\", \"will not\", sentences, flags=re.IGNORECASE)\n",
    "    sentences = re.sub(r\"can\\'t\", \"can not\", sentences, flags=re.IGNORECASE)\n",
    "    # general\n",
    "    sentences = re.sub(r\"n\\'t\", \" not\", sentences, flags=re.IGNORECASE)\n",
    "    sentences = re.sub(r\"\\'re\", \" are\", sentences, flags=re.IGNORECASE)\n",
    "    sentences = re.sub(r\"\\'s\", \" is\", sentences, flags=re.IGNORECASE)\n",
    "    sentences = re.sub(r\"\\'d\", \" would\", sentences, flags=re.IGNORECASE)\n",
    "    sentences = re.sub(r\"\\'ll\", \" will\", sentences, flags=re.IGNORECASE)\n",
    "    sentences = re.sub(r\"\\'t\", \" not\", sentences, flags=re.IGNORECASE)\n",
    "    sentences = re.sub(r\"\\'ve\", \" have\", sentences, flags=re.IGNORECASE)\n",
    "    sentences = re.sub(r\"\\'m\", \" am\", sentences, flags=re.IGNORECASE)\n",
    "    return sentences\n",
    "\n",
    "def remove_noises(sentences):\n",
    "    \"\"\"Clean up noises in the text\n",
    "    \"\"\"\n",
    "    sentences = re.sub(r'[~=+|<>.^]+', \"\", sentences)\n",
    "    sentences = clean(sentences,\n",
    "        fix_unicode=True,               # fix various unicode errors\n",
    "        to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "        lower=True,                     # lowercase text\n",
    "        no_line_breaks=True,            # fully strip line breaks as opposed to only normalizing them\n",
    "        no_urls=True,                   # replace all URLs with a special token\n",
    "        no_emails=True,                 # replace all email addresses with a special token\n",
    "        no_phone_numbers=True,          # replace all phone numbers with a special token\n",
    "        no_numbers=True,                # replace all numbers with a special token\n",
    "        no_digits=True,                 # replace all digits with a special token\n",
    "        no_currency_symbols=True,       # replace all currency symbols with a special token\n",
    "        no_punct=True,                  # remove punctuations\n",
    "        replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_phone_number=\"<PHONE>\",\n",
    "        replace_with_number=\"\",\n",
    "        replace_with_digit=\"\",\n",
    "        replace_with_currency_symbol=\"\",\n",
    "        lang=\"en\"                       # set to 'de' for German special handling\n",
    "    )\n",
    "    return sentences\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(sentence):\n",
    "    return \" \".join([\n",
    "        lemmatizer.lemmatize(word, pos=\"v\") \n",
    "        for word in nltk.word_tokenize(sentence.lower()) \n",
    "        if word not in stopwords.words('english')\n",
    "    ])\n",
    "\n",
    "def clean_comment_text(sentences):\n",
    "    return lemmatize(remove_noises(decontracted(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a9e539f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'play baseball among lot people eat time bad habit swim play long hours sun zurich famous website httpswwwzuerichcom accept add random string abcdefghizero demo takohogegmailcom pierod'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_comment_text(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a198cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
