{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c047513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f6be3",
   "metadata": {},
   "source": [
    "# Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666c06e6",
   "metadata": {},
   "source": [
    "# Terminologies\n",
    "\n",
    "## Pre-tokenized\n",
    "\n",
    "* [Pre-tokenized](https://huggingface.co/transformers/preprocessing.html#pre-tokenized-inputs)\n",
    "\n",
    "It is a ***list of string words*** e.g. ```[\"Hello\", \"I'm\", \"a\", \"single\", \"sentence\"]```. The usage of **tokenized** is misleading in the documentation.\n",
    "\n",
    "> Pre-tokenized does not mean your inputs are already tokenized (you wouldnâ€™t need to pass them through the tokenizer if that was the case) but just split into words.\n",
    "> If you want to use pre-tokenized inputs, you **MUST** set **```is_split_into_words=True```** when passing your inputs to the tokenizer.\n",
    "\n",
    "When passing ```[\"Hello\", \"I'm\", \"a\", \"single\", \"sentence\"]``` instead of a sentence ```\"Hello I'm a single sentence```, the list is called **Pre-tokenized inputs**.\n",
    "```\n",
    "encoded_input = tokenizer([\"Hello\", \"I'm\", \"a\", \"single\", \"sentence\"], is_split_into_words=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9dd1b0",
   "metadata": {},
   "source": [
    "# Config file\n",
    "\n",
    "When instaitiating a model, you need to define the model inisitlization parameters that are defined in the Transformers configuration file. The base class is PretrainedConfig.\n",
    "\n",
    "* [PretrainedConfig](https://huggingface.co/transformers/main_classes/configuration.html#pretrainedconfig)\n",
    "\n",
    "> Base class for all configuration classes. Handles a few parameters common to all modelsâ€™ configurations as well as methods for loading/downloading/saving configurations.\n",
    "\n",
    "Each sub class has its own parameters. For instance, Bert pretrained models have the BertConfig.\n",
    "\n",
    "* [BertConfig](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertConfig)\n",
    "\n",
    "> This is the configuration class to store the configuration of a BertModel or a TFBertModel. It is used to instantiate a BERT model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the BERT bert-base-uncased architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fc50b7",
   "metadata": {},
   "source": [
    "For instance, the ```num_labels``` parameter is from the [PretrainedConfig](https://huggingface.co/transformers/main_classes/configuration.html#transformers.PretrainedConfig)\n",
    "\n",
    "> num_labels (int, optional) â€“ Number of labels to use in the last layer added to the model, typically for a classification task.\n",
    "\n",
    "```\n",
    "TFBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "``` \n",
    "\n",
    "* [Training TFBertForSequenceClassification with custom X and Y data](https://stackoverflow.com/a/63295240/4281353)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1246e46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence, model_name, num_labels):\n",
    "    bert_model = trfs.TFBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    \n",
    "    # This is the input for the tokens themselves(words from the dataset after encoding):\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_sequence,), dtype=tf.int32, name='input_ids')\n",
    "\n",
    "    # attention_mask - is a binary mask which tells BERT which tokens to attend and which not to attend.\n",
    "    # Encoder will add the 0 tokens to the some sequence which smaller than MAX_SEQUENCE_LENGTH, \n",
    "    # and attention_mask, in this case, tells BERT where is the token from the original data and where is 0 pad token:\n",
    "    attention_mask = tf.keras.layers.Input((max_sequence,), dtype=tf.int32, name='attention_mask')\n",
    "    \n",
    "    # Use previous inputs as BERT inputs:\n",
    "    output = bert_model([input_ids, attention_mask])[0]\n",
    "\n",
    "    # We can also add dropout as regularization technique:\n",
    "    #output = tf.keras.layers.Dropout(rate=0.15)(output)\n",
    "\n",
    "    # Provide number of classes to the final layer:\n",
    "    output = tf.keras.layers.Dense(num_labels, activation='softmax')(output)\n",
    "\n",
    "    # Final model:\n",
    "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983cdc6f",
   "metadata": {},
   "source": [
    "The configuration file for the model ```bert-base-uncased``` is published at [Huggingface model - bert-base-uncased - config.json](https://huggingface.co/bert-base-uncased/blob/main/config.json).\n",
    "\n",
    "```\n",
    "{\n",
    "  \"architectures\": [\n",
    "    \"BertForMaskedLM\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"gradient_checkpointing\": false,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-12,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"model_type\": \"bert\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"transformers_version\": \"4.6.0.dev0\",\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 30522\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f36cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max length of encoded string(including special tokens such as [CLS] and [SEP]):\n",
    "MAX_SEQUENCE_LENGTH = 64 \n",
    "\n",
    "# Standard BERT model with lowercase chars only:\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-uncased' \n",
    "\n",
    "# Batch size for fitting:\n",
    "BATCH_SIZE = 16 \n",
    "\n",
    "# Number of epochs:\n",
    "EPOCHS=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2458c0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-86f4162a6e49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRETRAINED_MODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "model = create_model(MAX_SEQUENCE_LENGTH, PRETRAINED_MODEL_NAME, df.target.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f27ef6",
   "metadata": {},
   "source": [
    "\n",
    "# Fine-Tuning (Transfer Learning)\n",
    "\n",
    "* [Training TFBertForSequenceClassification with custom X and Y data](https://stackoverflow.com/a/68171171/4281353)\n",
    "\n",
    "For instance, utilize the [Sequence Classification](https://huggingface.co/transformers/task_summary.html#sequence-classification) capabilty of BERT for the text classification by fine-tuing the pre-trained BERT model upon the data provided. \n",
    "\n",
    "* [Fine-tuning a pretrained model](https://huggingface.co/transformers/training.html)\n",
    "> How to fine-tune a pretrained model from the Transformers library. In TensorFlow, models can be directly trained using Keras and the fit method. \n",
    "\n",
    "* [Fine-tuning with custom datasets](https://huggingface.co/transformers/custom_datasets.html)\n",
    "> This tutorial will take you through several examples of using ðŸ¤— Transformers models with your own datasets.<br>\n",
    "> [Fine-tuning with native PyTorch/TensorFlow](https://huggingface.co/transformers/custom_datasets.html#fine-tuning-with-native-pytorch-tensorflow)\n",
    "> ```\n",
    "> from transformers import TFDistilBertForSequenceClassification\n",
    "> \n",
    "> model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "> \n",
    "> optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "> model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn\n",
    "> model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)\n",
    "> ```\n",
    "\n",
    "* [HuggingFace Text classification examples](https://github.com/huggingface/transformers/tree/master/examples/tensorflow/text-classification)\n",
    "> This folder contains some scripts showing examples of text classification with the hugs Transformers library. \n",
    "\n",
    "[run_text_classification.py](https://github.com/huggingface/transformers/blob/master/examples/tensorflow/text-classification/run_text_classification.py) is the example for text classification fine-tuning for TensorFlow(https://huggingface.co/transformers/custom_datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d478f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
