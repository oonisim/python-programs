{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    TFBertForSequenceClassification, \n",
    "    BertForSequenceClassification, \n",
    "    glue_convert_examples_to_features\n",
    ")\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER (Named Entity Recognition) using BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215bac9c6429442ebd8e272b6eb61db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5addea1c5a264fe2a6bcc8a8aaedd3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing TFBertForTokenClassification: ['dropout_147']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681587be535d4bcf8db22fd48b8f3a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe249347eb92467cbce37c83545737b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"\"\"\n",
    "The order is in stark contrast to a message issued to ADF members earlier this month, encouraging them to support their LGBTI colleagues on International Day Against Homophobia, Biphobia, Interphobia and Transphobia (IDAHOBIT) on May 17.\n",
    "\"\"\"\n",
    "entities = nlp(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'AD', 'score': 0.9978134036064148, 'entity': 'I-ORG', 'index': 12, 'start': 55, 'end': 57}\n",
      "{'word': '##F', 'score': 0.9970282316207886, 'entity': 'I-ORG', 'index': 13, 'start': 57, 'end': 58}\n",
      "{'word': 'LGBT', 'score': 0.9701831340789795, 'entity': 'I-MISC', 'index': 24, 'start': 121, 'end': 125}\n",
      "{'word': '##I', 'score': 0.9137035608291626, 'entity': 'I-MISC', 'index': 25, 'start': 125, 'end': 126}\n",
      "{'word': 'International', 'score': 0.9983857274055481, 'entity': 'I-MISC', 'index': 28, 'start': 141, 'end': 154}\n",
      "{'word': 'Day', 'score': 0.9974353313446045, 'entity': 'I-MISC', 'index': 29, 'start': 155, 'end': 158}\n",
      "{'word': 'Against', 'score': 0.9968128204345703, 'entity': 'I-MISC', 'index': 30, 'start': 159, 'end': 166}\n",
      "{'word': 'Ho', 'score': 0.9952948689460754, 'entity': 'I-MISC', 'index': 31, 'start': 167, 'end': 169}\n",
      "{'word': '##mo', 'score': 0.9818452000617981, 'entity': 'I-MISC', 'index': 32, 'start': 169, 'end': 171}\n",
      "{'word': '##ph', 'score': 0.9324475526809692, 'entity': 'I-MISC', 'index': 33, 'start': 171, 'end': 173}\n",
      "{'word': '##obia', 'score': 0.9893725514411926, 'entity': 'I-MISC', 'index': 34, 'start': 173, 'end': 177}\n",
      "{'word': ',', 'score': 0.894406795501709, 'entity': 'I-MISC', 'index': 35, 'start': 177, 'end': 178}\n",
      "{'word': 'B', 'score': 0.9902522563934326, 'entity': 'I-MISC', 'index': 36, 'start': 179, 'end': 180}\n",
      "{'word': '##ip', 'score': 0.990612804889679, 'entity': 'I-MISC', 'index': 37, 'start': 180, 'end': 182}\n",
      "{'word': '##ho', 'score': 0.9257200360298157, 'entity': 'I-MISC', 'index': 38, 'start': 182, 'end': 184}\n",
      "{'word': '##bia', 'score': 0.9875982999801636, 'entity': 'I-MISC', 'index': 39, 'start': 184, 'end': 187}\n",
      "{'word': ',', 'score': 0.9348754286766052, 'entity': 'I-MISC', 'index': 40, 'start': 187, 'end': 188}\n",
      "{'word': 'Inter', 'score': 0.9936442971229553, 'entity': 'I-MISC', 'index': 41, 'start': 189, 'end': 194}\n",
      "{'word': '##ph', 'score': 0.9365575313568115, 'entity': 'I-MISC', 'index': 42, 'start': 194, 'end': 196}\n",
      "{'word': '##obia', 'score': 0.9869580268859863, 'entity': 'I-MISC', 'index': 43, 'start': 196, 'end': 200}\n",
      "{'word': 'and', 'score': 0.9718170166015625, 'entity': 'I-MISC', 'index': 44, 'start': 201, 'end': 204}\n",
      "{'word': 'Trans', 'score': 0.9875824451446533, 'entity': 'I-MISC', 'index': 45, 'start': 205, 'end': 210}\n",
      "{'word': '##ph', 'score': 0.8972875475883484, 'entity': 'I-MISC', 'index': 46, 'start': 210, 'end': 212}\n",
      "{'word': '##obia', 'score': 0.9874422550201416, 'entity': 'I-MISC', 'index': 47, 'start': 212, 'end': 216}\n",
      "{'word': 'ID', 'score': 0.9940922260284424, 'entity': 'I-MISC', 'index': 49, 'start': 218, 'end': 220}\n",
      "{'word': '##A', 'score': 0.8670316934585571, 'entity': 'I-MISC', 'index': 50, 'start': 220, 'end': 221}\n",
      "{'word': '##H', 'score': 0.8382236361503601, 'entity': 'I-MISC', 'index': 51, 'start': 221, 'end': 222}\n",
      "{'word': '##O', 'score': 0.8045788407325745, 'entity': 'I-MISC', 'index': 52, 'start': 222, 'end': 223}\n",
      "{'word': '##BI', 'score': 0.7906360030174255, 'entity': 'I-MISC', 'index': 53, 'start': 223, 'end': 225}\n",
      "{'word': '##T', 'score': 0.9393799304962158, 'entity': 'I-MISC', 'index': 54, 'start': 225, 'end': 226}\n"
     ]
    }
   ],
   "source": [
    "for entity in entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
