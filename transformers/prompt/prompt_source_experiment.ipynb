{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d208e1",
   "metadata": {},
   "source": [
    "# PromptSource \n",
    "\n",
    "PromptSource provides IDE in HuggingFace to develop/review Prompts from Huggingface Datasets, and toolkit (github) to use the prompts that have been developed.\n",
    "\n",
    "\n",
    "* [PromptSource Github](https://github.com/bigscience-workshop/promptsource)\n",
    "\n",
    "> PromptSource is a toolkit for creating, sharing and using natural language prompts.\n",
    "> \n",
    "> Recent work has shown that large language models exhibit the ability to perform reasonable zero-shot generalization to new tasks. For instance, GPT-3 demonstrated that large language models have strong zero- and few-shot abilities. FLAN and T0 then demonstrated that pre-trained language models fine-tuned in a massively multitask fashion yield even stronger zero-shot performance. A common denominator in these works is the use of prompts which have gathered of interest among NLP researchers and engineers. This emphasizes the need for new tools to create, share and use natural language prompts.\n",
    "> \n",
    "> Prompts are functions that map an example from a dataset to a natural language input and target output PromptSource contains a growing collection of prompts (which we call P3: Public Pool of Prompts). As of January 20, 2022, there are ~2'000 English prompts for 170+ English datasets in P3.\n",
    "\n",
    "* [API_DOCUMENTATION](https://github.com/bigscience-workshop/promptsource/blob/main/API_DOCUMENTATION.md)\n",
    "\n",
    "> PromptSource implements 4 classes to store, manipulate and use prompts and their metadata: ```Template```, ```Metadata```, ```DatasetTemplates``` and ```TemplateCollection```. All of them are implemented in templates.py\n",
    "> ### Class DatasetTemplates\n",
    "> DatasetTemplates is a class that wraps all the prompts (each of them are instances of Template) for a specific dataset/subset and implements all the helper functions necessary to read/write to the YAML file in which the prompts are saved.\n",
    "\n",
    "* [PromptSource - an IDE and repository for natural language prompts](https://www.youtube.com/watch?v=gIthK9J52IM)\n",
    "\n",
    "> The **Public Pool of Prompts\" (P3)** gathered with PromptSource (as of September 2022) includes spans more than 2000 prompts spanning 180 datasets.  \n",
    "> \n",
    "> It's useful to note that creating prompts is quite different to traditional NLP annotation in several ways: prompts are functions (not labels), they apply to datasets (not examples) and variation between prompts is desirable (rather than a nuisance). PromptSource proposes a simple workflow to meet these challenges. It also works well with ðŸ¤— Datasets so it can be applied to a wide range of existing datasets.\n",
    "> \n",
    "> To support flexible prompt editing, the Jinja2 template engine is used. This is a bit more flexible than a rule-based approach but easier to analyse than pure Python code.\n",
    "\n",
    "* [2022-09-promptsource.pdf](./2022-09-promptsource_ide_for_nlp.pdf)\n",
    "\n",
    "<img src=\"./image/prompt_source_usage.png\" align=\"left\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd63affe",
   "metadata": {},
   "source": [
    "# PromptSource UI @ HuggingFace \n",
    "\n",
    "Prompts developed for the Huggingface [AWS product review dataset](https://huggingface.co/datasets/amazon_us_reviews) on wireless category for multi label classification.\n",
    "\n",
    "* [bigscience/promptsource](https://huggingface.co/spaces/bigscience/promptsource)\n",
    "\n",
    "Prompt source provides multiple templates (select in **prompt name** box) for a dataset for different ML tasks e.g. multi-label classification, summarization, etc. \n",
    "\n",
    "<img src=\"./image/huggngface_promptsource.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c563f7",
   "metadata": {},
   "source": [
    "### Example prompt\n",
    "\n",
    "<img src=\"./image/huggingface_promptsource_example.png\" align=\"left\" width=750/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d389dc",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38fe6d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install promptsource --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a06811",
   "metadata": {},
   "source": [
    "# Huggingface Dataset for Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc5ed35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from promptsource.templates import DatasetTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca849fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME: str = \"rotten_tomatoes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c5d9511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset rotten_tomatoes (/Users/oonisim/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    }
   ],
   "source": [
    "train = load_dataset(DATASET_NAME, split=\"train\")\n",
    "example = train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79fcccf",
   "metadata": {},
   "source": [
    "# Prompt Templates for the Huggingface Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a39c485d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Movie Expressed Sentiment',\n",
       " 'Movie Expressed Sentiment 2',\n",
       " 'Reviewer Enjoyment',\n",
       " 'Reviewer Enjoyment Yes No',\n",
       " 'Reviewer Expressed Sentiment',\n",
       " 'Reviewer Opinion bad good choices',\n",
       " 'Reviewer Sentiment Feeling',\n",
       " 'Sentiment with choices ',\n",
       " 'Text Expressed Sentiment',\n",
       " 'Writer Expressed Sentiment']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templates = DatasetTemplates(\n",
    "    # The dataset_name should be known/accepted Huggingface dataset name.\n",
    "    # Then how can we use PromptSource for new dataset?\n",
    "    dataset_name=DATASET_NAME   \n",
    ")  \n",
    "templates.all_template_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2bd6d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sentiment with choices '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = templates['Sentiment with choices ']\n",
    "template.get_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a24c2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "promptsource.templates.Template"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b55106b",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "532643ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = template.apply(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dbbe096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "Is this review positive or negative?\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccee6749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \\nIs this review positive or negative?',\n",
       " 'positive']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c26799e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[\\'the rock is destined to be the 21st century\\\\\\'s new \" conan \" and that he\\\\\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\\', \\'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\\\\\'s expanded vision of j . r . r . tolkien\\\\\\'s middle-earth .\\'] \\nIs this review positive or negative?',\n",
       " '']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.apply(train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb6b823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
