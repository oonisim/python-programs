[objective]
Huggingface fine-tune model.

positive_negative_ratio=5.0,
negative_replication_factor=0.2,

[toxic        ] Threshold 0.50000
[toxic        ] TP 0.091 FP 0.115 TN: 0.789 FN 0.004
[toxic        ] True Positive Rate (Recall)      : 0.955
[toxic        ] Positive Precision               : 0.441
[toxic        ] True Negative Rate (Specificity) : 0.873
[toxic        ] Negative Precision               : 0.995
[toxic        ] Accuracy                         : 0.880
[toxic        ] AUC                              : 0.966

[toxic        ] Threshold 0.99994
[toxic        ] TP 0.042 FP 0.011 TN: 0.894 FN 0.053
[toxic        ] True Positive Rate (Recall)      : 0.442
[toxic        ] Positive Precision               : 0.798
[toxic        ] True Negative Rate (Specificity) : 0.988
[toxic        ] Negative Precision               : 0.944
[toxic        ] Accuracy                         : 0.936
[toxic        ] AUC                              : 0.966

TIMESTAMP = 2021JUL17_2212
CLEANING_FOR_TRAINING = False
MAX_SEQUENCE_LENGTH = 256
FREEZE_BASE_MODEL = False
NUM_LABELS = 2
NUM_EPOCHS = 10
BATCH_SIZE = 32
LEARNING_RATE = 2e-05
L2 = 0.0001
METRIC_NAME = loss
REDUCE_LR_PATIENCE = 1
EARLY_STOP_PATIENCE = 3
RESULT_DIRECTORY = /content/drive/MyDrive/home/repository/mon/kaggle/toxic_comment_classification/toxicity_classification_2021JUL17_2212

Model: "tf_distil_bert_for_sequence_classification_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
distilbert (TFDistilBertMain multiple                  66362880
_________________________________________________________________
pre_classifier (Dense)       multiple                  590592
_________________________________________________________________
classifier (Dense)           multiple                  1538
_________________________________________________________________
dropout_79 (Dropout)         multiple                  0
=================================================================
Total params: 66,955,010
Trainable params: 66,955,010
Non-trainable params: 0
_________________________________________________________________
Epoch 1/10
4329/4329 [==============================] - 2226s 512ms/step - loss: 0.0788 - accuracy: 0.9740 - val_loss: 0.0410 - val_accuracy: 0.9880
Model val_loss improved from [inf] to [0.04104]
Saving to /content/drive/MyDrive/home/repository/mon/kaggle/toxic_comment_classification/toxicity_classification_2021JUL17_2212/model_Ctoxic_B32_L256
Epoch 2/10
4329/4329 [==============================] - 2214s 511ms/step - loss: 0.0269 - accuracy: 0.9929 - val_loss: 0.0492 - val_accuracy: 0.9875

Epoch 00002: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.
Epoch 3/10
4329/4329 [==============================] - 2215s 512ms/step - loss: 0.0087 - accuracy: 0.9981 - val_loss: 0.0397 - val_accuracy: 0.9917
Model val_loss improved from [0.04104] to [0.03966]
Saving to /content/drive/MyDrive/home/repository/mon/kaggle/toxic_comment_classification/toxicity_classification_2021JUL17_2212/model_Ctoxic_B32_L256
Epoch 4/10
4329/4329 [==============================] - 2216s 512ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.0312 - val_accuracy: 0.9936
Model val_loss improved from [0.03966] to [0.03117]
Saving to /content/drive/MyDrive/home/repository/mon/kaggle/toxic_comment_classification/toxicity_classification_2021JUL17_2212/model_Ctoxic_B32_L256
Epoch 5/10
4329/4329 [==============================] - 2214s 512ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0303 - val_accuracy: 0.9943
Model val_loss improved from [0.03117] to [0.03028]
Saving to /content/drive/MyDrive/home/repository/mon/kaggle/toxic_comment_classification/toxicity_classification_2021JUL17_2212/model_Ctoxic_B32_L256
Epoch 6/10
4329/4329 [==============================] - 2214s 511ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.0523 - val_accuracy: 0.9911

Epoch 00006: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.
Epoch 7/10
4329/4329 [==============================] - 2211s 511ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0541 - val_accuracy: 0.9915

Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.
Epoch 8/10
4329/4329 [==============================] - 2212s 511ms/step - loss: 8.1067e-04 - accuracy: 0.9998 - val_loss: 0.0497 - val_accuracy: 0.9924

Epoch 00008: ReduceLROnPlateau reducing learning rate to 3.199999980552093e-08.
Restoring model weights from the end of the best epoch.
Epoch 00008: early stopping