{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2882c430",
   "metadata": {},
   "source": [
    "# Huggingface Trainer\n",
    "\n",
    "* [Processing the data](https://huggingface.co/course/chapter3/2?fw=pt)\n",
    "\n",
    "> The last thing we will need to do is pad all the examples to the length of the longest element when we batch elements together â€” a technique we refer to as dynamic padding.\n",
    "> The function that is responsible for **putting together samples inside a batch** is called a **collate function**. The default being a function that will just **convert your samples to PyTorch tensors and concatenate them (recursively if your elements are lists, tuples, or dictionaries)**.\n",
    "> \n",
    "> Transformers library provides us with such a function via ```DataCollatorWithPadding```. It takes a tokenizer when you instantiate it (to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs) and will do everything you need:\n",
    "> ```\n",
    "> from transformers import DataCollatorWithPadding\n",
    "> \n",
    "> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "> batch = data_collator(samples)\n",
    "> ```\n",
    "> \n",
    "> Dynamic padding means the samples in this batch should all be padded to a length of 67, the maximum length inside the batch.\n",
    "\n",
    "* [Fine-tuning a model with the Trainer API](https://huggingface.co/course/chapter3/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c21ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
