{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* [CS231n: Convolutional Neural Networks for Visual Recognition 2017](http://cs231n.stanford.edu/2017/syllabus)\n",
    "    - [cs231n 2017 assignment #1 kNN, SVM, SoftMax, two-layer network](https://cs231n.github.io/assignments2017/assignment1/)\n",
    "    - [Training a Softmax Linear Classifier](https://cs231n.github.io/neural-networks-case-study)\n",
    "* [ゼロから作る Deep Learning](https://github.com/oreilly-japan/deep-learning-from-scratch)\n",
    "* [Mathematics for Machine Learning](https://mml-book.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network overview\n",
    "\n",
    "Structure of the network and how forward and backward propagations work.\n",
    "\n",
    "<img src=\"image/nn_diagram.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts \n",
    "\n",
    "## Objective function\n",
    "\n",
    "The network trains the layers so as to minimize the objective function ```L``` which calculates the loss. Each layer at ```i``` is a function $f_i$ which takes an input $X_i$ from a previous layer and outputs $Y_i = f(X_i)$. The post layers of the form an objective function $L_i$ for the layer: $L = L_i(Y_i)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/nn_functions.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward path\n",
    "\n",
    "The process where each layer ```i``` calculate its output $Y_i = f(X_i)$ and forward it to the next layer(s) as their input $X_{i+1}$.\n",
    "\n",
    "## Backward path\n",
    "\n",
    "The process of automatic differentication, or *back-propagation* where each layer calculates its gradient $\\frac {\\partial L_i(Y_i)}{\\partial Y_i}$ , that is, the impact $Y_i$ will make on the objective ```L``` when it changes. With the gradient, we can apply the gradient descent $X_i = X_i - \\lambda  \\frac {\\partial L_i(Y_i)}{\\partial Y_i} \\frac {\\partial Y_i }{\\partial X_i}$ to update $X_i$ that would reduce the objective ```L```.\n",
    "\n",
    "## Cycle\n",
    "\n",
    "A round-trip of a forward path and a backward path with a batch data set $(X, T)$. How many cycles to happen with each batch is an implementation decision. \n",
    "\n",
    "## Epoch\n",
    "\n",
    "Total cycles to consume the entire training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminologies\n",
    "\n",
    "## X\n",
    "A batch input to a layer. Matrix shape is ```(N, D)```.\n",
    "\n",
    "* ```N``` : Number of rows in a batch X, or batch size\n",
    "* ```D``` : Number of features in a data in X.\n",
    "\n",
    "\n",
    "## T\n",
    "Labels for X. There are two formats available for the label.\n",
    "\n",
    "#### One Hot Encoding (OHE) labels\n",
    "\n",
    "When a neural network predicts a class out of ```3``` classes for an input ```x``` and the correct class is ```2```, then the label ```t``` is specified as ```t = [0, 1, 0]```.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,M) }{ T_{_{OHE}} } &= ( \n",
    "    \\overset{ (M,) }{ T_{(n=0)} }, \\dots , \\overset{ (M,) }{ T_{(n=N-1)} } \n",
    ") \n",
    "\\\\\n",
    "\\overset{ (M,) }{ T_{ _{OHE} (n)} } &= ( \\overset{ () }{ t_{(n)(m=0)} }, \\; \\dots \\;, \\overset{ () }{ t_{(n)(m=M-1)} })\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "#### Index labels\n",
    "\n",
    "The label ```t``` is specified as ```t = 2```. \n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overset{ (N,) }{ T_{_{IDX}} } &= (\\overset{ () }{ t_{(n=0)} }, \\; \\dots \\;, \\overset{ () }{ t_{(n=N-1)} }) \\end{align*}\n",
    "$\n",
    "\n",
    "## W\n",
    "A set of weight parameters of a node in a Matmul layer. Shape is ```(M, D)```.\n",
    "\n",
    "* ```M``` : Number of nodes in a layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Matrix order\n",
    "\n",
    "Use the row-order matrix. For instance, the weight matrix ```W``` of a Matmul layer has a shape ```(M, D)``` where each row in ```W``` represents a node in the layer. It will be efficient to use the column order matrix of shape ```(D, M)``` for ```W``` so that the matrix multiplication at a Matmul layer can be executed as ```X@W```  which is a ```shape:(N,D) @ shape:(D,M)``` operation without transpose. \n",
    "\n",
    "However, for the purpose of consistency and clarity, use the shape ```W:(M, D)``` although it will cause transposes ```W.T``` at the Matmul operations, and revese transposing ```dL/dW.T``` to ```dL/dW``` when updating ```W``` at the gradient descents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python & Jupyter setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Optional,\n",
    "    Union,\n",
    "    List,\n",
    "    Dict,\n",
    "    Tuple,\n",
    "    Callable\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python path\n",
    "Python path setup to avoid the relative imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from functools import partial\n",
    "import copy\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install line_profile memory_profiler\n",
    "%load_ext memory_profiler\n",
    "%load_ext line_profiler\n",
    "\n",
    "# Logging is enabled by calling logging.basicConfig\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.ERROR)\n",
    "# Logger = logging.getLogger(\"neural_network\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.style as mplstyle\n",
    "mplstyle.use('fast')\n",
    "plt.ion()\n",
    "\n",
    "# Note: with notebook backend from the top, updating the plot line does not work...\n",
    "%matplotlib notebook\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=80) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Classification on Non-linearly separable data\n",
    "\n",
    "Use Matmul and CrossEntropyLogLoss layers to classify M categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import common.weights as weights \n",
    "from common.function import (\n",
    "    transform_X_T,\n",
    "    softmax_cross_entropy_log_loss\n",
    ")\n",
    "from optimizer import (\n",
    "    Optimizer,\n",
    "    SGD\n",
    ")\n",
    "from network.test_040_two_layer_classifier import (\n",
    "    train_two_layer_classifier\n",
    ")\n",
    "from common.function import (\n",
    "    prediction_grid_2d\n",
    ")\n",
    "\n",
    "from drawing import (\n",
    "    plot,\n",
    "    scatter,\n",
    "    COLOR_LABELS,   # labels to classify outside/0/red or inside/1/green.\n",
    "    plot_categorical_predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data X and Label T\n",
    "Training data set that cannot be linearly classified. ```X = ((A not B), (B not C), (C not A), (A and B and C and D))``` for circles A, B, C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from data import (\n",
    "    sets_of_circle_A_not_B\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuber of pots per class\n",
    "__N = 500\n",
    "\n",
    "# Number of classes\n",
    "__M = 3        # Number of circles (A, B, C) and same with (A not B), (B not C), (C not A)\n",
    "M = __M + 1    # Intersection of circies (A and B and C and D)\n",
    "\n",
    "# Traing data (A not B), (B not C), (C not A) to classify\n",
    "radius = 1\n",
    "circles, centres, intersection = sets_of_circle_A_not_B(radius=radius, ratio=1.0, m=__M, n=__N)\n",
    "\n",
    "# Stack all circles and intersect\n",
    "X = np.vstack(\n",
    "    [circles[i] for i in range(M-1)] + \n",
    "    [intersection]\n",
    ")\n",
    "\n",
    "T = np.hstack(\n",
    "    [np.full(circles[i].shape[0], i) for i in range(M-1)] + \n",
    "    [np.full(intersection.shape[0], M-1)]\n",
    ")\n",
    "N = T.shape[0]\n",
    "assert T.shape[0] == X.shape[0]\n",
    "\n",
    "# Shuffle the data\n",
    "indices = np.random.permutation(range(T.shape[0]))\n",
    "X = X[indices]\n",
    "T = T[indices]\n",
    "Y = COLOR_LABELS[T]\n",
    "X, T = transform_X_T(X, T)\n",
    "\n",
    "x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "\n",
    "print(f\"X:{X.shape} T:{T.shape} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,4)) \n",
    "ax.set_xlabel('x label')\n",
    "ax.set_ylabel('y label')\n",
    "ax.axis('equal')\n",
    "ax.grid()\n",
    "r = np.linspace(0, 2*np.pi, 100)\n",
    "\n",
    "# (A not B), (B not C), (C not A)\n",
    "for i in range(__M):\n",
    "    circle = circles[i]\n",
    "    if circle.size > 0:\n",
    "        x = centres[i][0]\n",
    "        y = centres[i][1]\n",
    "        ax.scatter(circle[::, 0], circle[::, 1], color=COLOR_LABELS[i])\n",
    "        ax.plot(\n",
    "            x + radius * np.cos(r), \n",
    "            y + radius * np.sin(r), \n",
    "            linestyle='dashed', \n",
    "            color=COLOR_LABELS[i]\n",
    "        )\n",
    "\n",
    "# (A and B and C and D)\n",
    "ax.scatter(intersection[::, 0], intersection[::, 1], color='gold')\n",
    "plt.draw()\n",
    "plt.show()\n",
    "import time\n",
    "time.sleep(1)\n",
    "time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on non-linear separable data\n",
    "\n",
    "During the training, the loss often does not decrease. \n",
    "\n",
    "> Iteration [19976]: Loss[0.06914290965513335] has not improved from the previous [0.06914225566912098] for 1 times.\n",
    "\n",
    "<ins>If reduce the **learning rate** at those points, the situation gets worse </ins>(continuous non-improvements instead of sporadic) and the training fails (the result model cannot classify). If keep using the same learning rate, the non-improvement continues more frequently but the training itself makes a progress. \n",
    "\n",
    "Need to understand why it happens and why reducing the rate will make the training fail. Possibl approach is visualizing the loss function with contour lines and the track of the gradient descent to see the terrain it went through. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2\n",
    "M1 = 8\n",
    "M2: int = M                 # Nuber of output = Number of classes to classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initialization\n",
    "#### Trick\n",
    "Because the data is almost zero-centered, the bias ```x0``` is not required. Hence set the bias weight ```w0``` to zero to short-cut the training. Without, the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = weights.he(M1, D+1)\n",
    "W2 = weights.he(M2, M1+1)\n",
    "\n",
    "W1_bias_0 = copy.deepcopy(W1)  # np.copy() is sufficient without deepcopy.\n",
    "W2_bias_0 = copy.deepcopy(W2)\n",
    "W1_bias_0[\n",
    "    ::,\n",
    "    0\n",
    "] = 0\n",
    "W2_bias_0[\n",
    "    ::,\n",
    "    0\n",
    "] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(lr=0.1, l2=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TEST_TIMES = 50000\n",
    "\n",
    "\n",
    "W1_result_with_trick, W2_result_with_trick, objective, prediction_with_trick, history_with_trick = \\\n",
    "train_two_layer_classifier(\n",
    "    N=N,\n",
    "    D=D,\n",
    "    X=X,\n",
    "    T=T,\n",
    "    M1=M1,\n",
    "    W1=W1_bias_0,\n",
    "    M2=M2,\n",
    "    W2=W2_bias_0,\n",
    "    log_loss_function=softmax_cross_entropy_log_loss,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=MAX_TEST_TIMES,\n",
    "    test_numerical_gradient=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,5)) \n",
    "x_grid, y_grid, predictions = prediction_grid_2d(x_min, x_max, y_min, y_max, prediction_with_trick)\n",
    "plot_categorical_predictions(ax, [x_grid, y_grid], X, Y, predictions)\n",
    "\n",
    "ax.set_xlabel('x label')\n",
    "ax.set_ylabel('y label')\n",
    "ax.axis('equal')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x = range(len(history_with_trick))\n",
    "_y = history_with_trick\n",
    "xlabel = 'iterations (log scale)'\n",
    "ylabel = 'loss'\n",
    "title = \"training error\"\n",
    "fig, ax = plot(_x, _y, title=title, xlabel=xlabel, ylabel=ylabel,figsize=(5,4))\n",
    "ax.set_ylim(0.0, 1.5)\n",
    "ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with weight without trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1 = np.copyto(W1, W1_backup)  # None will be set. Why not return the reference!?\n",
    "W1_bias_not_0 = copy.deepcopy(W1)\n",
    "W2_bias_not_0 = copy.deepcopy(W2)\n",
    "\n",
    "W1_result_without_trick, W2_result_without_trick, objective, prediction_without_trick, history_without_trick=\\\n",
    "train_two_layer_classifier(\n",
    "    N=N,\n",
    "    D=D,\n",
    "    X=X,\n",
    "    T=T,\n",
    "    M1=M1,\n",
    "    W1=W1_bias_not_0,\n",
    "    M2=M2,\n",
    "    W2=W2_bias_not_0,\n",
    "    log_loss_function=softmax_cross_entropy_log_loss,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=MAX_TEST_TIMES,\n",
    "    test_numerical_gradient=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid, y_grid, predictions = prediction_grid_2d(x_min, x_max, y_min, y_max, prediction_without_trick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,5)) \n",
    "ax.set_xlabel('x label')\n",
    "ax.set_ylabel('y label')\n",
    "ax.axis('equal')\n",
    "ax.grid()\n",
    "plot_categorical_predictions(ax, [x_grid, y_grid], X, Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x = range(len(history_without_trick))\n",
    "_y = history_without_trick\n",
    "xlabel = 'iterations (log scale)'\n",
    "ylabel = 'loss'\n",
    "title = \"training error without trick\"\n",
    "\n",
    "fig, ax = plot(_x, _y, title=title, xlabel=xlabel, ylabel=ylabel,figsize=(5,4))\n",
    "ax.set_ylim(0.0, 1.5)\n",
    "ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization\n",
    "Observe the effect of the batch normalization by inserting the layer in-between activation and matmul layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
