{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=400) \n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import (\n",
    "    DELIMITER,\n",
    "    SPACE,\n",
    "    NIL\n",
    ")\n",
    "\n",
    "STRIDE = 2\n",
    "CONTEXT_SIZE = 1 + (STRIDE * 2)\n",
    "\n",
    "USE_PTB = False\n",
    "USE_NATIVE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "VALIDATION = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = \"The fool doth think he is wise, but the wise man knows himself to be a fool.\"\n",
    "#corpus = \"To be, or not to be, that is the question\"\n",
    "#corpus = \"To to be be, or not not not not not to be, that is that the question that matters\"\n",
    "corpus = \"To be, or not to be, that is the question that matters\"\n",
    "#corpus = \"You say goodbye and I say hello .\"\n",
    "#corpus = \"I know how to build an attention in neural networks. But I don’t understand how attention layers learn the weights that pay attention to some specific embedding. I have this question because I’m tackling a NLP task using attention layer. I believe it should be very easy to learn (the most important part is to learn alignments). However, my neural networks only achieve 50% test set accuracy. And the attention matrix is weird. I don’t know how to improve my networks.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTB (Penn Treebank) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oonisim/dataset\n"
     ]
    }
   ],
   "source": [
    "from data.ptb import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid boundary checking when iterate through the sequenced corpus, pad the source text with '<nil>'.\n",
    "e.g. (when context is of size 5):    \n",
    "From:\n",
    "```\n",
    "|B|X|Y|Z|...|P|Q|R|E|\n",
    "```\n",
    "\n",
    "To:\n",
    "```\n",
    "|<nil>|<nil>|B|X|Y|Z|...|P|Q|R|E|<nil>|<nil>| \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.text import (\n",
    "    PAD_MODE_SQUEEZE,\n",
    "    pad_text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word indexing\n",
    "Assign a numerical id to each word.\n",
    "\n",
    "The row index of co-occurrence matrix is a word index. The number of words in the corpus can be less than the number of word indices because additional meta-word such as OOV, UNK, NIL can be added to the original corpus.\n",
    "\n",
    "Make sure **the co-occurrence matrix row index matches with the word index**, unless explicitly adjust when row-index and word-index do not match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and pad the corpus text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original corpus: \n",
      "[To be, or not to be, that is the question that matters]\n",
      "Padded corpus: \n",
      "[<nil> <nil> To be, or not to be, that is the question that matters <nil> <nil>]\n"
     ]
    }
   ],
   "source": [
    "if USE_PTB:\n",
    "    corpus = pad_text(\n",
    "        corpus=load_text('train'), mode=PAD_MODE_SQUEEZE, delimiter=SPACE, padding=NIL, length=STRIDE\n",
    "    )\n",
    "else:\n",
    "    print(\"Original corpus: \\n[{}]\".format(corpus))\n",
    "    corpus = pad_text(\n",
    "        corpus=corpus, mode=PAD_MODE_SQUEEZE, delimiter=SPACE, padding=NIL, length=STRIDE\n",
    "    )\n",
    "    print(\"Padded corpus: \\n[{}]\".format(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of padding with NIL to get F((w)\n",
    "Be able get the number of times when the word **w** occurred in the sequence from the co occurrence matrix.\n",
    "<img src=\"image/co_occurrence_matrix_counting_with_nil.png\" align=\"left\" width=1000/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native word indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.text import (\n",
    "    text_to_sequence\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = re.sub(r'[.,:;]+', SPACE, corpus.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "{'<nil>': 0, 'to': 1, 'be': 2, 'or': 3, 'not': 4, 'that': 5, 'is': 6, 'the': 7, 'question': 8, 'matters': 9}\n"
     ]
    }
   ],
   "source": [
    "if USE_NATIVE:\n",
    "    (sequence, word_to_id, id_to_word, vocabulary_size) = text_to_sequence(corpus)\n",
    "\n",
    "print(vocabulary_size)\n",
    "if not USE_PTB:\n",
    "    print(word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Tokenizer indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "\n",
    "USE_TENSORFLOW = (not USE_NATIVE)\n",
    "if USE_TENSORFLOW:\n",
    "    # Each text in \"texts\" is a complete document as one string, \n",
    "    # e.g \"To be or not to be, that is the question.\"\n",
    "    texts = [ corpus ]   \n",
    "\n",
    "    # fit_on_texts() processes multiple documents and handles all words in all the documents.\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    word_to_id = tokenizer.word_index\n",
    "\n",
    "    # texts_to_sequences() ruturns sequences, one sequence for each text in \"texts\".\n",
    "    sequences = (tokenizer.texts_to_sequences(texts))\n",
    "    sequence = sequences[0]\n",
    "\n",
    "    print(len(sequences))\n",
    "    print(len(word_to_id))\n",
    "    \n",
    "    # Index of tokenizer.word_index starts at 1, NOT 0.\n",
    "    # e.g. {'<OOV>': 1, 'the': 2, 'fool': 3, 'wise': 4, 'doth': 5, ...}\n",
    "    vocabulary_size = max(word_to_id.values()) + 1\n",
    "    print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to id \n",
      "{'<nil>': 0, 'to': 1, 'be': 2, 'or': 3, 'not': 4, 'that': 5, 'is': 6, 'the': 7, 'question': 8, 'matters': 9}\n",
      "id to word \n",
      "{0: '<nil>', 1: 'to', 2: 'be', 3: 'or', 4: 'not', 5: 'that', 6: 'is', 7: 'the', 8: 'question', 9: 'matters'}\n",
      "\n",
      "corpus is \n",
      "[<nil> <nil> to be  or not to be  that is the question that matters <nil> <nil>]\n",
      "sequence is \n",
      "[0 0 1 2 3 4 1 2 5 6 7 8 5 9 0 0]\n",
      "corpus size is 16 sequence size is 16 expected sum is 48\n",
      "['<nil>' '<nil>' 'to' 'be' 'or' 'not' 'to' 'be' 'that' 'is' 'the' 'question' 'that' 'matters' '<nil>' '<nil>']\n"
     ]
    }
   ],
   "source": [
    "if not USE_PTB:\n",
    "    print(\"word to id \\n{}\".format(word_to_id))\n",
    "    print(\"id to word \\n{}\".format(id_to_word))\n",
    "    print()\n",
    "    print(\"corpus is \\n[{}]\".format(corpus))\n",
    "    print(\"sequence is \\n{}\".format(sequence))\n",
    "    print(\"corpus size is {} sequence size is {} expected sum is {}\".format(\n",
    "        len(re.compile('[\\t\\s]+').split(corpus)), \n",
    "        len(sequence), \n",
    "        (len(sequence) - (2*STRIDE)) * (2*STRIDE)  # Exclude NIL from the sequence\n",
    "    ))\n",
    "    #print([id_to_word[index] for index in sequence])\n",
    "    print(np.array([id_to_word[index] for index in sequence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/cooccurrence_matrix.png\" align=\"left\" width=1000 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from np.analytics import (\n",
    "    create_cooccurrence_matrix,\n",
    "    cooccurrence_words,\n",
    "    word_frequency,\n",
    "    total_frequencies\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [2 0 2 2 1 1 0 0 0 0]\n",
      " [1 2 0 1 2 1 1 0 0 0]\n",
      " [0 2 1 0 1 0 0 0 0 0]\n",
      " [0 1 2 1 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 1 2 1 1]\n",
      " [0 0 1 0 0 1 0 1 1 0]\n",
      " [0 0 0 0 0 2 1 0 1 0]\n",
      " [0 0 0 0 0 1 1 1 0 1]\n",
      " [2 0 0 0 0 1 0 0 1 0]]\n",
      ".sum() 48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 2, 1, 1, 0, 0, 0, 0],\n",
       "       [2, 0, 1, 2, 1, 1, 0, 0, 0],\n",
       "       [2, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 1, 2, 1, 1],\n",
       "       [0, 1, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 2, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 0]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_occurrece_matrix = create_cooccurrence_matrix(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "\n",
    "if VALIDATION:\n",
    "    print(co_occurrece_matrix.shape)\n",
    "\n",
    "    if not USE_PTB:\n",
    "        print(co_occurrece_matrix)\n",
    "        print(\".sum() {}\".format(co_occurrece_matrix.sum()))\n",
    "\n",
    "    assert total_frequencies(co_occurrece_matrix, word_to_id, CONTEXT_SIZE, NIL) == len(sequence) - (CONTEXT_SIZE -1)\n",
    "    assert np.array_equal(co_occurrece_matrix, co_occurrece_matrix)\n",
    "\n",
    "co_occurrece_matrix[1::, 1::]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Reference implementation from DSFS 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsfs2_ppmi(C, verbose=False, eps = 1e-8):\n",
    "    '''PPMI（正の相互情報量）の作成\n",
    "\n",
    "    :param C: 共起行列\n",
    "    :param verbose: 進行状況を出力するかどうか\n",
    "    :return:\n",
    "    '''\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit n=1000\n",
    "M = dsfs2_ppmi(co_occurrece_matrix)[1::, 1::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.1926451  2.         1.         0.19264509 0.         0.         0.         0.        ]\n",
      " [1.1926451  0.         0.77760756 1.7776076  0.         0.77760756 0.         0.         0.        ]\n",
      " [2.         0.77760756 0.         1.5849625  0.         0.         0.         0.         0.        ]\n",
      " [1.         1.7776076  1.5849625  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.19264509 0.         0.         0.         0.         0.77760756 1.7776076  0.77760756 1.7776076 ]\n",
      " [0.         0.77760756 0.         0.         0.77760756 0.         1.5849625  1.5849625  0.        ]\n",
      " [0.         0.         0.         0.         1.7776076  1.5849625  0.         1.5849625  0.        ]\n",
      " [0.         0.         0.         0.         0.77760756 1.5849625  1.5849625  0.         2.5849626 ]\n",
      " [0.         0.         0.         0.         1.7776076  0.         0.         2.5849626  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Own implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMI\n",
    "\n",
    "<img src=\"image/pmi_from_co_occurrence_matrix.png\" width=650 align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from np.analytics import (\n",
    "    pmi,\n",
    "    ppmi\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit n=1000\n",
    "m = ppmi(co_occurrece_matrix)[1::, 1::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.58496199 1.58496147 0.58496147 0.         0.         0.         0.         0.        ]\n",
      " [0.58496199 0.         0.58496147 1.58496147 0.         0.58496147 0.         0.         0.        ]\n",
      " [1.58496147 0.58496147 0.         1.58496043 0.         0.         0.         0.         0.        ]\n",
      " [0.58496147 1.58496147 1.58496043 0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.58496147 1.58496147 0.58496147 0.58496147]\n",
      " [0.         0.58496147 0.         0.         0.58496147 0.         1.58496043 1.58496043 0.        ]\n",
      " [0.         0.         0.         0.         1.58496147 1.58496043 0.         1.58496043 0.        ]\n",
      " [0.         0.         0.         0.         0.58496147 1.58496043 1.58496043 0.         1.58496043]\n",
      " [0.         0.         0.         0.         0.58496147 0.         0.         1.58496043 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(M - m).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from np.analytics import (\n",
    "    create_context_set\n",
    ")\n",
    "\n",
    "contexts, labels = create_context_set(sequence, CONTEXT_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
