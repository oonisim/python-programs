{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(linewidth=80) \n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIL = \"<nil>\"  # Lower letter as lower() will be applied.\n",
    "STRIDE = int(input(\"STRIDE size?\"))\n",
    "CONTEXT_SIZE = 1 + (STRIDE * 2)\n",
    "\n",
    "SPACE = ' '\n",
    "\n",
    "USE_PTB = bool(input(\"USE_PTB? Just enter for False or any string for True\"))\n",
    "USE_NATIVE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "VALIDATION = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = \"The fool doth think he is wise, but the wise man knows himself to be a fool.\"\n",
    "#corpus = \"To be, or not to be, that is the question\"\n",
    "#corpus = \"To to be be, or not not not not not to be, that is that the question that matters\"\n",
    "corpus = \"To be, or not to be, that is the question that matters\"\n",
    "#corpus = \"I know how to build an attention in neural networks. But I don’t understand how attention layers learn the weights that pay attention to some specific embedding. I have this question because I’m tackling a NLP task using attention layer. I believe it should be very easy to learn (the most important part is to learn alignments). However, my neural networks only achieve 50% test set accuracy. And the attention matrix is weird. I don’t know how to improve my networks.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTB (Penn Treebank) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding: utf-8\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "try:\n",
    "    import urllib.request\n",
    "except ImportError:\n",
    "    raise ImportError('Use Python3!')\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "url_base = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/'\n",
    "key_file = {\n",
    "    'train':'ptb.train.txt',\n",
    "    'test':'ptb.test.txt',\n",
    "    'valid':'ptb.valid.txt'\n",
    "}\n",
    "save_file = {\n",
    "    'train':'ptb.train.npy',\n",
    "    'test':'ptb.test.npy',\n",
    "    'valid':'ptb.valid.npy'\n",
    "}\n",
    "vocab_file = 'ptb.vocab.pkl'\n",
    "\n",
    "#dataset_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "dataset_dir = os.path.dirname(os.path.abspath(\"/home/oonisim/dataset/hoge\"))\n",
    "print(dataset_dir)\n",
    "\n",
    "def _download(file_name):\n",
    "    file_path = dataset_dir + '/' + file_name\n",
    "    if os.path.exists(file_path):\n",
    "        return\n",
    "\n",
    "    print('Downloading ' + file_name + ' ... ')\n",
    "\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "    except urllib.error.URLError:\n",
    "        import ssl\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "\n",
    "    print('Done')\n",
    "\n",
    "\n",
    "def load_text(data_type):\n",
    "#    data_type = 'train'\n",
    "    file_name = key_file[data_type]\n",
    "    file_path = dataset_dir + '/' + file_name\n",
    "    vocab_path = dataset_dir + '/' + vocab_file\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        _download(file_name)\n",
    "\n",
    "    text = open(file_path).read().replace('\\n', '<eos>').strip()\n",
    "    return(text)\n",
    "    \n",
    "def load_vocab():\n",
    "    vocab_path = dataset_dir + '/' + vocab_file\n",
    "\n",
    "    if os.path.exists(vocab_path):\n",
    "        with open(vocab_path, 'rb') as f:\n",
    "            word_to_id, id_to_word = pickle.load(f)\n",
    "        return word_to_id, id_to_word\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    data_type = 'train'\n",
    "    file_name = key_file[data_type]\n",
    "    file_path = dataset_dir + '/' + file_name\n",
    "\n",
    "    _download(file_name)\n",
    "\n",
    "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in word_to_id:\n",
    "            tmp_id = len(word_to_id)\n",
    "            word_to_id[word] = tmp_id\n",
    "            id_to_word[tmp_id] = word\n",
    "\n",
    "    with open(vocab_path, 'wb') as f:\n",
    "        pickle.dump((word_to_id, id_to_word), f)\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "\n",
    "def load_data(data_type='train'):\n",
    "    '''\n",
    "        :param data_type: データの種類：'train' or 'test' or 'valid (val)'\n",
    "        :return:\n",
    "    '''\n",
    "    if data_type == 'val': data_type = 'valid'\n",
    "    save_path = dataset_dir + '/' + save_file[data_type]\n",
    "\n",
    "    word_to_id, id_to_word = load_vocab()\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        sequence = np.load(save_path)\n",
    "        return sequence, word_to_id, id_to_word\n",
    "\n",
    "    file_name = key_file[data_type]\n",
    "    file_path = dataset_dir + '/' + file_name\n",
    "    _download(file_name)\n",
    "\n",
    "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
    "    sequence = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    np.save(save_path, sequence)\n",
    "    return sequence, word_to_id, id_to_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid boundary checking when iterate through the sequenced corpus, pad the source text with '<nil>'.\n",
    "e.g. (when context is of size 5):    \n",
    "From:\n",
    "```\n",
    "|B|X|Y|Z|...|P|Q|R|E|\n",
    "```\n",
    "\n",
    "To:\n",
    "```\n",
    "|<nil>|<nil>|B|X|Y|Z|...|P|Q|R|E|<nil>|<nil>| \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELIMITER = \" \"\n",
    "def pad(corpus:str) -> str:\n",
    "    \"\"\"Prepand and appeend STRIDE times of the NIL word to the corpus\"\"\"\n",
    "    assert corpus and len(corpus) > 0 and isinstance(corpus, str)\n",
    "    \n",
    "    padded = DELIMITER.join(\n",
    "        [ NIL ] * STRIDE + [ corpus ] + [ NIL ] * STRIDE\n",
    "    )\n",
    "    \"\"\"\n",
    "    padded = sum(\n",
    "        [ \n",
    "            [ NIL ] * STRIDE, \n",
    "            corpus.split(' '),\n",
    "            [ NIL ] * STRIDE\n",
    "        ],\n",
    "        start=[]\n",
    "    )\n",
    "    \"\"\"\n",
    "    return padded\n",
    "\n",
    "#print(\"[{}]\".format(pad(\"tako ika bin\")))\n",
    "assert pad(\"tako ika bin\") == NIL + DELIMITER + NIL + DELIMITER + \"tako ika bin \" + NIL + DELIMITER + NIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### co-occurrence check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurrence_words(co_occurrence_matrix, word, word_to_id, id_to_word):\n",
    "    \"\"\"Provide the co-occurred words for the word\"\"\"\n",
    "    return [(id_to_word[i], count) for i, count in enumerate(co_occurrence_matrix[word_to_id[word]])]\n",
    "\n",
    "def word_frequency(co_occurrence_matrix, word, word_to_id):\n",
    "    \"\"\"Number of times when the word occurred in the sequene\"\"\"\n",
    "    # Each time the word occurrs in the sequence, it will see (CONTEXT_SIZE -1) words. \n",
    "    co_occurrence_matrix[\n",
    "        word_to_id[word]\n",
    "    ].sum() / (CONTEXT_SIZE -1)\n",
    "    \n",
    "def total_frequencies(co_occurrence_matrix, word_to_id):\n",
    "    \"\"\"Sum of all word occurrence except NIL (same with vocabrary size excluding NIL)\"\"\"\n",
    "    return (co_occurrence_matrix.sum() - co_occurrence_matrix[word_to_id[NIL]].sum()) / (CONTEXT_SIZE -1)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract gapped slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def xslice(x, slices):\n",
    "    \"\"\"Extract multiple slices from an array-like and concatenate them.\n",
    "    Args:\n",
    "        x: array-like\n",
    "        slices: slice or tuple of slice objects\n",
    "    Return:\n",
    "        Combined slices\n",
    "    \"\"\"\n",
    "    if isinstance(slices, tuple):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            return np.concatenate([x[_slice] for _slice in slices])\n",
    "        else:\n",
    "            return sum((x[s] if isinstance(s, slice) else [x[s]] for s in slices), [])        \n",
    "    elif isinstance(slices, slice):\n",
    "        return x[slices]\n",
    "    else:\n",
    "        return [x[slices]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word indexing\n",
    "Assign a numerical id to each word.\n",
    "\n",
    "The row index of co-occurrence matrix is a word index. The number of words in the corpus can be less than the number of word indices because additional meta-word such as OOV, UNK, NIL can be added to the original corpus.\n",
    "\n",
    "Make sure **the co-occurrence matrix row index matches with the word index**, unless explicitly adjust when row-index and word-index do not match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the corpus text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PTB:\n",
    "    corpus = pad(load_text('train'))\n",
    "else:\n",
    "    print(\"Original corpus: \\n[{}]\".format(corpus))\n",
    "    corpus = pad(corpus)\n",
    "    print(\"Padded corpus: \\n[{}]\".format(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native word indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def native_word_indexing(corpus):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        corpus: A string including sentences to process.\n",
    "    Returns:\n",
    "        sequence: \n",
    "            A numpy array of word indices to every word in the originlal corpus as as they appear in it.\n",
    "            The objective of sequence is to preserve the original corpus but as numerical indices.\n",
    "        word_to_id: A dictionary to map a word to a word index\n",
    "        id_to_word: A dictionary to map a word index to a word\n",
    "        vocabulary_size: Number of words identiifed in the corpus\n",
    "    \"\"\"\n",
    "    words = re.compile('[\\s\\t]+').split(corpus)\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    \n",
    "    min_id = len(word_to_id)\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "            \n",
    "    # Word index starts with 0. Total words = max(word index) + 1\n",
    "    vocabulary_size = new_id + 1\n",
    "    assert vocabulary_size == (max(word_to_id.values()) + 1)\n",
    "\n",
    "    sequence = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return sequence, word_to_id, id_to_word, vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = re.sub('[.,:;]+', SPACE, corpus.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_NATIVE:\n",
    "    (sequence, word_to_id, id_to_word, vocabulary_size) = native_word_indexing(corpus)\n",
    "\n",
    "print(vocabulary_size)\n",
    "if not USE_PTB:\n",
    "    print(word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Tokenizer indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "\n",
    "USE_TENSORFLOW = (not USE_NATIVE)\n",
    "if USE_TENSORFLOW:\n",
    "    # Each text in \"texts\" is a complete document as one string, \n",
    "    # e.g \"To be or not to be, that is the question.\"\n",
    "    texts = [ corpus ]   \n",
    "\n",
    "    # fit_on_texts() processes multiple documents and handles all words in all the documents.\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    word_to_id = tokenizer.word_index\n",
    "\n",
    "    # texts_to_sequences() ruturns sequences, one sequence for each text in \"texts\".\n",
    "    sequences = (tokenizer.texts_to_sequences(texts))\n",
    "    sequence = sequences[0]\n",
    "\n",
    "    print(len(sequences))\n",
    "    print(len(word_to_id))\n",
    "    \n",
    "    # Index of tokenizer.word_index starts at 1, NOT 0.\n",
    "    # e.g. {'<OOV>': 1, 'the': 2, 'fool': 3, 'wise': 4, 'doth': 5, ...}\n",
    "    vocabulary_size = max(word_to_id.values()) + 1\n",
    "    print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_PTB:\n",
    "    print(\"word to id \\n{}\".format(word_to_id))\n",
    "    print(\"id to word \\n{}\".format(id_to_word))\n",
    "    print()\n",
    "    print(\"corpus is \\n[{}]\".format(corpus))\n",
    "    print(\"sequence is \\n{}\".format(sequence))\n",
    "    print(\"corpus size is {} sequence size is {} expected sum is {}\".format(\n",
    "        len(re.compile('[\\t\\s]+').split(corpus)), \n",
    "        len(sequence), \n",
    "        (len(sequence) - (2*STRIDE)) * (2*STRIDE)  # Exclude NIL from the sequence\n",
    "    ))\n",
    "    #print([id_to_word[index] for index in sequence])\n",
    "    print(np.array([id_to_word[index] for index in sequence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-occurrence vector(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLFS2 iterative approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlfs2_create_co_matrix(sequence, vocabulary_size, context_size=3):\n",
    "    '''Generate co-occurreance matrix for the sequence.\n",
    "    :param sequence: word index sequence of the sequence\n",
    "    :param vocabulary_size:The number of unique words in the sequence. \n",
    "    :param window_size: \n",
    "        The number of words either left or right of the word to count co-occurreances, which is (context_ize / 2)\n",
    "    :return: co-occurrence matrix\n",
    "    '''\n",
    "    assert (context_size % 2) == 1\n",
    "    \n",
    "    n = sequence_size = len(sequence)\n",
    "    co_matrix = np.zeros((vocabulary_size, vocabulary_size), dtype=np.int32)\n",
    "\n",
    "    window_size = int((context_size -1) / 2)\n",
    "    for idx, word_id in enumerate(sequence):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = sequence[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < sequence_size:\n",
    "                right_word_id = sequence[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "                \n",
    "    # Zero clear the co-occurrence words of NIL because NIL should not see other words.\n",
    "    co_matrix[\n",
    "        word_to_id[NIL.lower()]\n",
    "    ] = 0\n",
    "\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit -n 1 \n",
    "if VALIDATION:\n",
    "    com0 = dlfs2_create_co_matrix(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "    print(com0.shape)\n",
    "    \n",
    "    if not USE_PTB:\n",
    "        print(com0)\n",
    "        print(\"com0.sum() {}\".format(com0.sum()))\n",
    "\n",
    "    # Total sum of all word occurrences except NIL must matches with the original corpus size.\n",
    "    assert total_frequencies(com0, word_to_id) == len(sequence) - (CONTEXT_SIZE -1)\n",
    "    assert com0.sum() == (len(sequence) - (2*STRIDE)) * (2*STRIDE)  # Exclude NIL from the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(id_to_word[4])\n",
    "#cooccurrence_words(com0, id_to_word[4], word_to_id, id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun \\\n",
    "    -T dlfs2_create_co_matrix.log \\\n",
    "    -f dlfs2_create_co_matrix \\\n",
    "    dlfs2_create_co_matrix(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "    \n",
    "print(open('dlfs2_create_co_matrix.log', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved version\n",
    "* With the NIL padded sequence, no need for the boundary checks e.g. left_idx >= 0.\n",
    "* By limiting the position range to ```sequence[stride : ((n-1)-stride) +1]```, no need to zero-clear the co-occurrence words of NIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(sequence, vocabulary_size, context_size=3):\n",
    "    '''Generate co-occurreance matrix for the sequence.\n",
    "    :param sequence: word index sequence of the sequence\n",
    "    :param vocabulary_size:The number of unique words in the sequence. \n",
    "    :param stride: \n",
    "        The number of words either left or right of the word to count co-occurreances, which is (context_ize / 2)\n",
    "    :return: co-occurrence matrix\n",
    "    '''\n",
    "    assert (context_size % 2) == 1\n",
    "    \n",
    "    n = sequence_size = len(sequence)\n",
    "    co_matrix = np.zeros((vocabulary_size, vocabulary_size), dtype=np.int32)\n",
    "\n",
    "    stride = int((context_size -1) / 2)\n",
    "    for position, word_id in enumerate(sequence[stride : ((n-1)-stride) +1], stride):\n",
    "        for i in range(1, stride + 1):\n",
    "            left_idx =position - i\n",
    "            right_idx =position + i\n",
    "\n",
    "            left_word_id = sequence[left_idx]\n",
    "            co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            right_word_id = sequence[right_idx]\n",
    "            co_matrix[word_id, right_word_id] += 1\n",
    "                \n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit -n 1 \n",
    "if VALIDATION:\n",
    "    com1 = create_co_matrix(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "    print(com1.shape)\n",
    "\n",
    "    if not USE_PTB:\n",
    "        print(com1)\n",
    "        print(\"com1.sum() {}\".format(com1.sum()))\n",
    "\n",
    "    # Total sum of all word occurrences except NIL must matches with the original corpus size.\n",
    "    assert total_frequencies(com1, word_to_id) == len(sequence) - (CONTEXT_SIZE -1)\n",
    "    assert np.array_equal(com1, com0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun \\\n",
    "    -T create_co_matrix.log \\\n",
    "    -f create_co_matrix \\\n",
    "    create_co_matrix(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "\n",
    "print(open('create_co_matrix.log', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-impementation of the DLFS2 improved version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_create_co_matrix(sequence, co_occurrence_vector_size, context_size=3):\n",
    "    \"\"\"Implement the same logic with dlfs2 create_co_matrix.\n",
    "    Args: \n",
    "        sequence: word index sequence of the original corpus text\n",
    "        co_occurrence_vector_size: \n",
    "        context_size: context (N-gram size N) within to check co-occurrences.\n",
    "    Returns:\n",
    "        co_occurrence matrix\n",
    "    \"\"\"\n",
    "    assert int(context_size %2) == 1\n",
    "    \n",
    "    n = sequence_size = len(sequence)\n",
    "    co_occurrence_matrix = np.zeros((co_occurrence_vector_size, co_occurrence_vector_size), dtype=np.int32)\n",
    "\n",
    "    stride = int((context_size - 1)/2 )\n",
    "    assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
    "        n, stride\n",
    "    )\n",
    "\n",
    "    for position in range(stride, (n-1) - stride +1):        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Super slow spending approx 75% of execution time 35 secs\n",
    "        # Line #   Hits    Time       Per Hit   % Time \n",
    "        # 36    932231     805098.0      0.9      2.3          word_id = sequence[position]\n",
    "        # 37   2796693    2240226.0      0.8      6.4          for offset in range(1, stride+1):\n",
    "        # 38   5593386   11444886.0      2.0     32.6              co_occurrence_matrix[\n",
    "        # 39   3728924    1921460.0      0.5      5.5                  word_id,\n",
    "        # 40   1864462    1427308.0      0.8      4.1                  sequence[position - offset]\n",
    "        # 41   1864462    1022951.0      0.5      2.9              ] +=1\n",
    "        # 42   5593386   11203813.0      2.0     32.0              co_occurrence_matrix[\n",
    "        # 43   3728924    1972667.0      0.5      5.6                  word_id,\n",
    "        # 44   1864462    1473145.0      0.8      4.2                  sequence[position + offset]\n",
    "        # 45   1864462    1029893.0      0.6      2.9              ] +=1\n",
    "        # --------------------------------------------------------------------------------\n",
    "        word_id = sequence[position]\n",
    "        for offset in range(1, stride+1):\n",
    "            co_occurrence_matrix[\n",
    "                word_id,\n",
    "                sequence[position - offset]\n",
    "            ] +=1\n",
    "            co_occurrence_matrix[\n",
    "                word_id,\n",
    "                sequence[position + offset]\n",
    "            ] +=1\n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "    return co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VALIDATION:\n",
    "    com2 = simulate_create_co_matrix(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "    print(com2.shape)\n",
    "\n",
    "    if not USE_PTB:\n",
    "        print(com2)\n",
    "        print(\"com2.sum() {}\".format(com2.sum()))\n",
    "\n",
    "    # Total sum of all word occurrences except NIL must matches with the original corpus size.\n",
    "    assert total_frequencies(com2, word_to_id) == len(sequence) - (CONTEXT_SIZE -1)\n",
    "    assert np.array_equal(com2, com0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun \\\n",
    "    -T simulate_create_co_matrix.log \\\n",
    "    -f simulate_create_co_matrix \\\n",
    "    simulate_create_co_matrix(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "\n",
    "print(open('simulate_create_co_matrix.log', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(sequence, index, stride, flag=False):\n",
    "    if not flag:\n",
    "        return\n",
    "    \n",
    "    n = len(sequence)\n",
    "    print(\"word is {} and context is {}\".format(\n",
    "        id_to_word[sequence[index]],\n",
    "        [ id_to_word[i] for i in sequence[max(0, (index-stride)) : min((index+stride) +1, n)]]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/cooccurrence_matrix.png\" align=\"left\" width=1000 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of padding with NIL to get F((w)\n",
    "Be able get the number of times when the word **w** occurred in the sequence from the co occurrence matrix.\n",
    "<img src=\"image/co_occurrence_matrix_counting_with_nil.png\" align=\"left\" width=1000/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cooccurrence_matrix(sequence, co_occurrence_vector_size, context_size=3):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        sequence: word index sequence of the original corpus text\n",
    "        co_occurrence_vector_size: \n",
    "        context_size: context (N-gram size N) within to check co-occurrences.\n",
    "    Returns:\n",
    "        co_occurrence matrix\n",
    "    \"\"\"\n",
    "    assert int(context_size %2) == 1\n",
    "    \n",
    "    n = sequence_size = len(sequence)\n",
    "    co_occurrence_matrix = np.zeros((co_occurrence_vector_size, co_occurrence_vector_size), dtype=np.int32)\n",
    "\n",
    "    stride = int((context_size - 1)/2 )\n",
    "    assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
    "        n, stride\n",
    "    )\n",
    "\n",
    "    for position in range(stride, (n-1) - stride +1):        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Consider counting a word multiple time, and the word itself for performance.\n",
    "        # e.g. stride=2\n",
    "        # |W|W|W|W|W| If co-occurrences are all same word W at the position, need +4 for W\n",
    "        # |X|X|W|X|X| If co-occurrences are all same word X, need +4 for X\n",
    "        # |X|X|W|Y|Y| If co-occurrences X x 2, Y x 2, then need +2 for X and Y respectively.\n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Line #   Hits    Time       Per Hit   % Time \n",
    "        # 32   1864462    5358433.0      2.9     61.5          np.add.at(\n",
    "        # 33    932231     446858.0      0.5      5.1             co_occurrence_matrix,\n",
    "        # 34    932231     463579.0      0.5      5.3             (\n",
    "        # 35    932231     609110.0      0.7      7.0                 sequence[position],\n",
    "        # 36    932231     862299.0      0.9      9.9                 sequence[position-stride : (position+stride) +1]    \n",
    "        # 37                                                      ),\n",
    "        # 38    932231     437542.0      0.5      5.0             1\n",
    "        # 39                                                   )\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # word_id = sequence[position]\n",
    "        np.add.at(\n",
    "           co_occurrence_matrix,\n",
    "           (\n",
    "               sequence[position],                               # word_id\n",
    "               sequence[position-stride : (position+stride) +1]  # indices to co-occurrence words \n",
    "           ),\n",
    "           1\n",
    "        )\n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Compensate the +1 self count of a word at each occurrence.\n",
    "    # F(w) (frequency/occurrences of a word in the sequence) has been extra added besides \n",
    "    # the expected (2 * stride) * F(w) times, resulting in (context_size) * F(w).\n",
    "    # --------------------------------------------------------------------------------\n",
    "    np.fill_diagonal(\n",
    "        co_occurrence_matrix,\n",
    "        (co_occurrence_matrix.diagonal() - co_occurrence_matrix.sum(axis=1) / context_size)\n",
    "    )\n",
    "\n",
    "    return co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug version for trouble shooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_cooccurrence_matrix(sequence, co_occurrence_vector_size, context_size=3):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        sequence: word position sequence of the original corpus text\n",
    "        co_occurrence_vector_size: \n",
    "        context_size: context (N-gram size N) within to check co-occurrences.\n",
    "    Returns:\n",
    "        co_occurrence matrix\n",
    "    \"\"\"\n",
    "    assert int(context_size %2) == 1\n",
    "\n",
    "    n = sequence_size = len(sequence)\n",
    "    co_occurrence_matrix = np.zeros((co_occurrence_vector_size, co_occurrence_vector_size), dtype=np.int32)\n",
    "    co_matrix = np.zeros((co_occurrence_vector_size, co_occurrence_vector_size), dtype=np.int32)\n",
    "\n",
    "    stride = int((context_size - 1)/2 )\n",
    "    assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
    "        n, stride\n",
    "    )\n",
    "\n",
    "    print(\"Starting comparison\")\n",
    "    for position in range(stride, (n-1) - stride +1):\n",
    "        print(position)\n",
    "        word_id = sequence[position]\n",
    "\n",
    "        for i in range(1, stride + 1):\n",
    "            left_idx = position - i\n",
    "            right_idx = position + i\n",
    "\n",
    "            left_word_id = sequence[left_idx]\n",
    "            co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            right_word_id = sequence[right_idx]\n",
    "            co_matrix[word_id, right_word_id] += 1\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Consider counting the word itself. \n",
    "        # e.g. stride=2\n",
    "        # |W|W|W|W|W| If co-occurrences are all same word W at the position, need +4 for W\n",
    "        # |X|X|W|X|X| If co-occurrences are all same word X, need +4 for X\n",
    "        # |X|X|W|Y|Y| If co-occurrences X x 2, Y x 2, then need +2 for X and Y respectively.\n",
    "        # --------------------------------------------------------------------------------\n",
    "        np.add.at(\n",
    "            co_occurrence_matrix,\n",
    "            (\n",
    "                word_id,\n",
    "                sequence[position-stride : (position+stride) +1]    # positions to co-occurence words \n",
    "            ),\n",
    "            1\n",
    "        )\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Remove the +1 self count of the word itself.\n",
    "        # To avoid the cost of creating a gapped indices np.r_[sequence, [i:j, j+1:k]].\n",
    "        # --------------------------------------------------------------------------------\n",
    "        co_occurrence_matrix[word_id, word_id] -=1\n",
    "\n",
    "        \n",
    "        if(np.array_equal(co_matrix, co_occurrence_matrix)) is not True:\n",
    "            print(\"sequence position is {}\".format(position))\n",
    "            print(\"Test matrix index \\n{}\\ matrix {}\\n\".format(\n",
    "                [\n",
    "                    word_id,                       # position to the word\n",
    "                    sequence[(position-stride) : (position+stride) +1]  # indices to right co-occurrence words excluding word itself\n",
    "                ],\n",
    "                co_matrix[\n",
    "                    [\n",
    "                        word_id,                       # position to the word\n",
    "                        sequence[(position-stride) : (position+stride) +1]  # indices to right co-occurrence words excluding word itself\n",
    "                    ]\n",
    "                ]\n",
    "            ))\n",
    "            print(\"co_occurrence_matrix index is \\n{}\\nmatrix is \\n{}\\n\".format(\n",
    "                [\n",
    "                    word_id,   # position  to the word \n",
    "                    sequence[position-stride : (position+stride) +1]  # positions to co-occurence words \n",
    "                ],\n",
    "                co_occurrence_matrix[          \n",
    "                    [\n",
    "                        word_id,   # position  to the word \n",
    "                        sequence[position-stride : (position+stride) +1]  # positions to co-occurence words \n",
    "                    ]\n",
    "                ]\n",
    "            ))\n",
    "            debug(sequence, position, stride, True) \n",
    "            print(\"diff \\n{}\".format(co_matrix - co_occurrence_matrix))\n",
    "\n",
    "            assert False\n",
    "        \n",
    "    return co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit -n 1 \n",
    "if DEBUG:\n",
    "    f = debug_cooccurrence_matrix\n",
    "else:\n",
    "    f = create_cooccurrence_matrix\n",
    "\n",
    "if VALIDATION:\n",
    "    com = f(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "    print(com.shape)\n",
    "\n",
    "    if not USE_PTB:\n",
    "        print(com)\n",
    "        print(\"com.sum() {}\".format(com.sum()))\n",
    "\n",
    "    assert total_frequencies(com, word_to_id) == len(sequence) - (CONTEXT_SIZE -1)\n",
    "    assert np.array_equal(com0, com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun \\\n",
    "    -T create_cooccurrence_matrix.log \\\n",
    "    -f create_cooccurrence_matrix \\\n",
    "    create_cooccurrence_matrix(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "\n",
    "print(open('create_cooccurrence_matrix.log', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_create_co_occurrence_matrix(sequence, co_occurrence_vector_size, context_size=3):\n",
    "    \"\"\"Implement the same logic with dlfs2 create_co_matrix.\n",
    "    Args: \n",
    "        sequence: word index sequence of the original corpus text\n",
    "        co_occurrence_vector_size: \n",
    "        context_size: context (N-gram size N) within to check co-occurrences.\n",
    "    Returns:\n",
    "        co_occurrence matrix\n",
    "    \"\"\"\n",
    "    assert int(context_size %2) == 1\n",
    "    \n",
    "    n = sequence_size = len(sequence)\n",
    "    co_occurrence_matrix = np.zeros((co_occurrence_vector_size, co_occurrence_vector_size), dtype=np.int32)\n",
    "\n",
    "    stride = int((context_size - 1)/2 )\n",
    "    assert(n > stride), \"sequence_size {} is less than/equal to stride {}\".format(\n",
    "        n, stride\n",
    "    )\n",
    "\n",
    "    for position in range(stride, (n-1) - stride +1):        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Consider counting the word itself. \n",
    "        # e.g. stride=2\n",
    "        # |W|W|W|W|W| If co-occurrences are all same word W at the position, need +4 for W\n",
    "        # |X|X|W|X|X| If co-occurrences are all same word X, need +4 for X\n",
    "        # |X|X|W|Y|Y| If co-occurrences X x 2, Y x 2, then need +2 for X and Y respectively.\n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Super slow spending approx 75% of execution time\n",
    "        # --------------------------------------------------------------------------------\n",
    "        #co_occurrence_vector = co_occurrence_matrix[sequence[position]]\n",
    "        #for index in range(context_size):\n",
    "        #    co_occurrence_vector[index] += 1\n",
    "        #REQUIRE_REMOVE_SELF_COUNTING = True\n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Super slow spending approx 75% of execution time 35 secs\n",
    "        # Line #   Hits    Time       Per Hit   % Time \n",
    "        # 36    932231     805098.0      0.9      2.3          word_id = sequence[position]\n",
    "        # 37   2796693    2240226.0      0.8      6.4          for offset in range(1, stride+1):\n",
    "        # 38   5593386   11444886.0      2.0     32.6              co_occurrence_matrix[\n",
    "        # 39   3728924    1921460.0      0.5      5.5                  word_id,\n",
    "        # 40   1864462    1427308.0      0.8      4.1                  sequence[position - offset]\n",
    "        # 41   1864462    1022951.0      0.5      2.9              ] +=1\n",
    "        # 42   5593386   11203813.0      2.0     32.0              co_occurrence_matrix[\n",
    "        # 43   3728924    1972667.0      0.5      5.6                  word_id,\n",
    "        # 44   1864462    1473145.0      0.8      4.2                  sequence[position + offset]\n",
    "        # 45   1864462    1029893.0      0.6      2.9              ] +=1\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # word_id = sequence[position]\n",
    "        # for offset in range(1, stride+1):\n",
    "        #     co_occurrence_matrix[\n",
    "        #         word_id,\n",
    "        #         sequence[position - offset]\n",
    "        #     ] +=1\n",
    "        #     co_occurrence_matrix[\n",
    "        #         word_id,\n",
    "        #         sequence[position + offset]\n",
    "        #     ] +=1\n",
    "        # REQUIRE_REMOVE_SELF_COUNTING = False\n",
    "        # --------------------------------------------------------------------------------\n",
    "            \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Consider counting the word itself. \n",
    "        # e.g. stride=2\n",
    "        # |W|W|W|W|W| If co-occurrences are all same word W at the position, need +4 for W\n",
    "        # |X|X|W|X|X| If co-occurrences are all same word X, need +4 for X\n",
    "        # |X|X|W|Y|Y| If co-occurrences X x 2, Y x 2, then need +2 for X and Y respectively.\n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # xslice() to create combined slices costs a lot.\n",
    "        # Total time: 22.0234 s\n",
    "        # Line #   Hits    Time       Per Hit   % Time \n",
    "        # 72    932231     945544.0      1.0      4.3          word_id = sequence[position]\n",
    "        # 73   1864462    6838467.0      3.7     31.1          np.add.at(\n",
    "        # 74    932231     613173.0      0.7      2.8              co_occurrence_matrix,\n",
    "        # 75    932231     671443.0      0.7      3.0              (\n",
    "        # 76    932231     583880.0      0.6      2.7                  word_id,\n",
    "        # 77   1864462    7954610.0      4.3     36.1                  xslice(\n",
    "        # 78    932231     600371.0      0.6      2.7                      sequence,\n",
    "        # 79    932231    1868599.0      2.0      8.5                      np.s_[position-stride: position, position+1 : position+stride +1]\n",
    "        # 80                                                           )\n",
    "        # 81                                                       ),\n",
    "        # 82    932231     615451.0      0.7      2.8              1\n",
    "        # 83                                                   )\n",
    "        # 84    932231     692304.0      0.7      3.1          REQUIRE_REMOVE_SELF_COUNTING = False        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        word_id = sequence[position]\n",
    "        np.add.at(\n",
    "            co_occurrence_matrix,\n",
    "            (\n",
    "                word_id,\n",
    "                xslice(\n",
    "                    sequence,\n",
    "                    np.s_[position-stride: position, position+1 : position+stride +1]\n",
    "                )\n",
    "            ),\n",
    "            1\n",
    "        )\n",
    "        REQUIRE_REMOVE_SELF_COUNTING = False\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Line #   Hits    Time       Per Hit   % Time \n",
    "        # 32   1864462    5358433.0      2.9     61.5          np.add.at(\n",
    "        # 33    932231     446858.0      0.5      5.1             co_occurrence_matrix,\n",
    "        # 34    932231     463579.0      0.5      5.3             (\n",
    "        # 35    932231     609110.0      0.7      7.0                 sequence[position],\n",
    "        # 36    932231     862299.0      0.9      9.9                 sequence[position-stride : (position+stride) +1]    \n",
    "        # 37                                                      ),\n",
    "        # 38    932231     437542.0      0.5      5.0             1\n",
    "        # 39                                                   )\n",
    "        # --------------------------------------------------------------------------------\n",
    "        #np.add.at(\n",
    "        #   co_occurrence_matrix,\n",
    "        #   (\n",
    "        #       sequence[position],\n",
    "        #       sequence[position-stride : (position+stride) +1]    # positions to co-occurence words \n",
    "        #   ),\n",
    "        #   1\n",
    "        #)\n",
    "        #REQUIRE_REMOVE_SELF_COUNTING = True\n",
    "        # --------------------------------------------------------------------------------\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Remove self counting of the word at position itself.\n",
    "        # To avoid the cost of creating a gapped indices np.r_[sequence, [i:j, j+1:k]].\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # Very slow:  \n",
    "        # Line #   Hits    Time       Per Hit   % Time \n",
    "        # ==============================================================\n",
    "        # X        932231  4903853.0  5.3       33.5          \n",
    "        # --------------------------------------------------------------------------------\n",
    "        # co_occurrence_matrix[sequence[position],sequence[position]] -=1\n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Remove self counting of the word at position itself.\n",
    "    # Line #   Hits   Time       Per Hit   % Time \n",
    "    # ==============================================================\n",
    "    # X        1      86069.0    86069.0   0.9 \n",
    "    # --------------------------------------------------------------------------------\n",
    "    if REQUIRE_REMOVE_SELF_COUNTING:\n",
    "        np.fill_diagonal(\n",
    "            co_occurrence_matrix,\n",
    "            (co_occurrence_matrix.diagonal() - co_occurrence_matrix.sum(axis=1) / context_size)\n",
    "        )\n",
    "\n",
    "    return co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VALIDATION:\n",
    "    _matrix = research_create_co_occurrence_matrix(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "    print(_matrix.shape)\n",
    "\n",
    "    if not USE_PTB:\n",
    "        print(_matrix)\n",
    "        print(\"_matrix.sum() {}\".format(_matrix.sum()))\n",
    "\n",
    "    # Total sum of all word occurrences except NIL must matches with the original corpus size.\n",
    "    assert total_frequencies(_matrix, word_to_id) == len(sequence) - (CONTEXT_SIZE -1)\n",
    "    assert np.array_equal(_matrix, com0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun \\\n",
    "    -T research_create_co_occurrence_matrix.log \\\n",
    "    -f research_create_co_occurrence_matrix \\\n",
    "    research_create_co_occurrence_matrix(sequence, vocabulary_size, CONTEXT_SIZE)\n",
    "\n",
    "print(open('research_create_co_occurrence_matrix.log', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
