{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44fe5ce0-e04c-4a87-adae-635b47f6958d",
   "metadata": {},
   "source": [
    "# OpenAI CLIP Demo\n",
    "\n",
    "Based on [Github Minimal user-friendly demo of OpenAI's CLIP](https://github.com/vivien000/clip-demo) which is also available at [Huggingface CLIP demo](https://huggingface.co/spaces/vivien/clip), however heavily modified.\n",
    "\n",
    "NOTE:\n",
    "\n",
    "Huggingface offeres different interfaces/functions from those in the original CLIP. e.g. ```CLIPProcessor``` handles both text and images. Whereas the original CLIP in github handles separately.\n",
    "\n",
    "## References\n",
    "\n",
    "### Open AI\n",
    "\n",
    "* [OpenAI Github CLIP](https://github.com/openai/CLIP)\n",
    "\n",
    "```\n",
    "import torch\n",
    "import clip   # <--- This is effective when cloning the git repository only\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]\n",
    "```\n",
    "\n",
    "### Huggingface\n",
    "\n",
    "* [Huggingface CLIP model](https://huggingface.co/docs/transformers/main/en/model_doc/clip)\n",
    "* [CLIPProcessor](https://huggingface.co/docs/transformers/main/en/model_doc/clip#transformers.CLIPProcessor)\n",
    "\n",
    "> Constructs a CLIP processor which wraps a CLIP image processor and a CLIP tokenizer into a single processor. CLIPProcessor offers all the functionalities of CLIPImageProcessor and CLIPTokenizerFast. See the __call__() and decode() for more information.\n",
    "\n",
    "* [Huggingface Model openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32)\n",
    "\n",
    "### Tutorials\n",
    "\n",
    "* [Quick-fire Guide to Multi-Modal ML With OpenAIâ€™s CLIP](https://towardsdatascience.com/quick-fire-guide-to-multi-modal-ml-with-openais-clip-2dad7e398ac0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b30e8441-5a60-4160-a1f4-c81ea8887ac9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\n",
      "sagemaker 2.145.0 requires importlib-metadata<5.0,>=1.4.0, but you have importlib-metadata 6.3.0 which is incompatible.\n",
      "sagemaker 2.145.0 requires PyYAML==5.4.1, but you have pyyaml 6.0 which is incompatible.\n",
      "docker-compose 1.29.2 requires PyYAML<6,>=3.10, but you have pyyaml 6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers torch tensorflow > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "961ee63e-486e-40da-8578-dfceba0ad925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pathlib\n",
    "import urllib.request\n",
    "import urllib\n",
    "import requests\n",
    "import multiprocessing\n",
    "from multiprocessing.dummy import Pool\n",
    "from typing import (\n",
    "    List,\n",
    "    Dict,\n",
    "    Callable,\n",
    "    Any,\n",
    "    Union,\n",
    "    Optional\n",
    ")\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "from IPython.display import display, Markdown, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    CLIPProcessor, \n",
    "    CLIPTextModel, \n",
    "    CLIPModel, \n",
    "    logging\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a0bc3da-714a-44ef-8937-51d1c3dc0398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.get_verbosity = lambda: logging.NOTSET\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d05fa",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "348bd1f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE_CPU: str = 'cpu'\n",
    "DEVICE_CUDA: str = 'cuda'\n",
    "DEVICE_IS_CUDA: bool = torch.cuda.is_available()\n",
    "DEVICE = torch.device(DEVICE_CUDA if torch.cuda.is_available() else DEVICE_CPU)\n",
    "DEVICE_TYPE: str = DEVICE.type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7068dd",
   "metadata": {},
   "source": [
    "# Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eae7c6c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_CPUS: int = multiprocessing.cpu_count()\n",
    "DATA_DIR = \"./data\"\n",
    "\n",
    "PATH_TO_UNSPLUSH_CSV: str = os.path.join(DATA_DIR, 'unsplush.csv')\n",
    "PATH_TO_MOVIES_CSV: str = os.path.join(DATA_DIR, 'movies.csv')\n",
    "\n",
    "MODEL_NAME: str = \"openai/clip-vit-base-patch32\"\n",
    "BATCH_SIZE: int = 256\n",
    "\n",
    "DO_DOWNLOAD: bool = False\n",
    "DO_EMBEDDING: bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1098abd6-4e00-4b1f-8648-dbbf990332c9",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bad87eb2-9820-430b-8a6f-94c0af8d7956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mkdir(path: str):\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)    \n",
    "\n",
    "\n",
    "def exists_url(url: str) -> bool:\n",
    "    \"\"\"Check if URL exists\"\"\"\n",
    "    response: requests.models.Response = requests.head(url)\n",
    "    if response.status_code not in [200, 404]:\n",
    "        response.raise_for_status()\n",
    "        \n",
    "    return response.status_code == 200\n",
    "\n",
    "\n",
    "def fetch_url(url_filename, data_dir=DATA_DIR):\n",
    "    try:\n",
    "        url, filename = url_filename\n",
    "        path_to_image_file: str = os.path.join(data_dir, filename)\n",
    "        if not is_file(path_to_image_file):\n",
    "            urllib.request.urlretrieve(url, path_to_image_file)\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "    except urllib.error.HTTPError as error:\n",
    "        msg = f\"featch URL:[{url}] filename:[{filename}] failed due to {error}\"\n",
    "        raise RuntimeError(msg)\n",
    "\n",
    "        \n",
    "def get_fetch_url(data_dir):\n",
    "    def fetch(url_filename):\n",
    "        fetch_url(url_filename, data_dir)\n",
    "        \n",
    "    return fetch\n",
    "\n",
    "\n",
    "def load_image(path_to_file, same_height=False):\n",
    "    try:\n",
    "        im = Image.open(path_to_file)\n",
    "        if im.mode != 'RGB':\n",
    "            im = im.convert('RGB')\n",
    "        if same_height:\n",
    "            ratio = 224/im.size[1]\n",
    "            return im.resize((int(im.size[0]*ratio), int(im.size[1]*ratio)))    \n",
    "        else:\n",
    "            ratio = 224/min(im.size)\n",
    "            return im.resize((int(im.size[0]*ratio), int(im.size[1]*ratio)))\n",
    "    except FileNotFoundError as error:\n",
    "        print(f\"path: {os.path.join(data_dir, path)} does not exist.\")\n",
    "        \n",
    "        \n",
    "def is_file(path):\n",
    "    return pathlib.Path(path).is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "724c6ad5-82db-4f68-89c1-672423038eaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_rows_with_non_exist_url(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"start remove_rows_with_non_exist_url\")\n",
    "    missings = []\n",
    "    for index in df.index.values:\n",
    "        url = df.iloc[index]['path']\n",
    "        if not exists_url(url=url):\n",
    "            print(f\"index:[{index}] url:[{url}] does not exist\")\n",
    "            missings.append(index)\n",
    "\n",
    "        if index % 500 == 0:\n",
    "            print(index)\n",
    "            \n",
    "    df.drop(labels=missings, axis=0, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d82ac5-8d04-4493-a7fe-7d93684b6820",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ccd5d3-c766-4e3a-9a63-2bb41a9d07d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mkdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8e95772-e7b3-4ffb-aba3-ebaea96d7158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not is_file(PATH_TO_UNSPLUSH_CSV):\n",
    "    urllib.request.urlretrieve(\n",
    "        'https://drive.google.com/uc?export=download&id=1bt1O-iArKuU9LGkMV1zUPTEHZk8k7L65', \n",
    "        PATH_TO_UNSPLUSH_CSV\n",
    "    )\n",
    "if not is_file(PATH_TO_MOVIES_CSV):\n",
    "    urllib.request.urlretrieve(\n",
    "        'https://drive.google.com/uc?export=download&id=19aVnFBY-Rc0-3VErF_C7PojmWpBsb5wk', \n",
    "        PATH_TO_MOVIES_CSV\n",
    "    )\n",
    "\n",
    "# urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1onKr-pfWb4l6LgL-z8WDod3NMW-nIJxE', 'embeddings.npy')\n",
    "# urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1KbwUkE0T8bpnHraqSzTeGGV4-TZO_CFB', 'embeddings2.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7adbf74-fca1-4156-8feb-abfcadd44414",
   "metadata": {},
   "source": [
    "Remove rows if URL does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69e7ebc1-9cf1-44bd-b222-e689349895dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_DOWNLOAD:\n",
    "    unsplush_df: pd.DataFrame = remove_rows_with_non_exist_url(df=pd.read_csv(os.path.join(DATA_DIR, 'unsplush.csv')))\n",
    "else:\n",
    "    unsplush_df: pd.DataFrame = pd.read_csv(os.path.join(DATA_DIR, 'unsplush.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8b11086-995d-4703-bd3a-e5e14dc3b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_DOWNLOAD:\n",
    "    movies_df: pd.DataFrame = remove_rows_with_non_exist_url(df=pd.read_csv(os.path.join(DATA_DIR, 'movies.csv')))\n",
    "else:\n",
    "    movies_df = pd.read_csv(os.path.join(DATA_DIR, 'movies.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8daeb080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframes: Dict[int, pd.DataFrame] = {\n",
    "    0: unsplush_df,\n",
    "    1: movies_df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8e5a3f1-f3fb-43a2-a26c-f418c4454638",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24994"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataframes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d43ac345-0257-4053-a48b-8354097a863e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DO_DOWNLOAD:\n",
    "    dataframes[0].to_csv(PATH_TO_UNSPLUSH_CSV, index=False, encoding='utf-8')\n",
    "    dataframes[1].to_csv(PATH_TO_MOVIES_CSV, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08680b9b-1228-40e3-8a23-e743d2430969",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>tooltip</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://images.unsplash.com/uploads/1411949294...</td>\n",
       "      <td>\"Woman exploring a forest\" by Michelle Spencer</td>\n",
       "      <td>https://unsplash.com/photos/XMyPniM9LF0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://images.unsplash.com/photo-141633941111...</td>\n",
       "      <td>\"Succulents in a terrarium\" by Jeff Sheldon</td>\n",
       "      <td>https://unsplash.com/photos/rDLBArZUl1c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://images.unsplash.com/photo-142014251503...</td>\n",
       "      <td>\"Rural winter mountainside\" by John Price</td>\n",
       "      <td>https://unsplash.com/photos/cNDGZ2sQ3Bo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  https://images.unsplash.com/uploads/1411949294...   \n",
       "1  https://images.unsplash.com/photo-141633941111...   \n",
       "2  https://images.unsplash.com/photo-142014251503...   \n",
       "\n",
       "                                          tooltip  \\\n",
       "0  \"Woman exploring a forest\" by Michelle Spencer   \n",
       "1     \"Succulents in a terrarium\" by Jeff Sheldon   \n",
       "2       \"Rural winter mountainside\" by John Price   \n",
       "\n",
       "                                      link  \n",
       "0  https://unsplash.com/photos/XMyPniM9LF0  \n",
       "1  https://unsplash.com/photos/rDLBArZUl1c  \n",
       "2  https://unsplash.com/photos/cNDGZ2sQ3Bo  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df: pd.DataFrame = dataframes[0]\n",
    "movies_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c07ea68-6e5b-4767-a94c-5df80bb98166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download(df, data_dir:str, fetch_fn=None):\n",
    "    max_n_parallel = NUM_CPUS * 2\n",
    "    latency = 1  # idle duration to reduce the download rate for the images\n",
    "    divider = 300\n",
    "    length = len(df)\n",
    "    print(f\"total images:[{length}]\")\n",
    "\n",
    "    position: int = 0\n",
    "    while position < length:\n",
    "        n_parallel = min(max_n_parallel, length - position)\n",
    "        url_filename_list = [\n",
    "            (df.iloc[position + increment]['path'], str(position + increment) + '.jpeg') \n",
    "            for increment in range(n_parallel)\n",
    "        ]\n",
    "        _ = Pool(n_parallel).map(fetch_fn, url_filename_list)\n",
    "        position += n_parallel\n",
    "\n",
    "        if position // divider > 0:\n",
    "            print(position)\n",
    "            divider += 300\n",
    "\n",
    "        # time.sleep(latency)\n",
    "    assert position == length, f\"expected position:[{position}] == length:[{length}]\"\n",
    "    print(f\"done [{data_dir}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a53330-1984-4dfa-bd22-655032a3210d",
   "metadata": {},
   "source": [
    "Python multiprocessing can cause \"can't start new thread\". The kernel resource may have been exhuaused, then the instance itself needs to be restarted, not just Python kernel.\n",
    "\n",
    "```\n",
    "----> 5     download(df=dataframes[1], data_dir=data_dir, fetch_fn=get_fetch_url(data_dir))\n",
    "\n",
    "<ipython-input-21-50353c4a8d6f> in download(df, data_dir, fetch_fn)\n",
    "     13             for increment in range(n_parallel)\n",
    "     14         ]\n",
    "---> 15         _ = Pool(n_parallel).map(fetch_fn, url_filename_list)\n",
    "     16         position += n_parallel\n",
    "     17 \n",
    "\n",
    "/opt/conda/lib/python3.7/multiprocessing/dummy/__init__.py in Pool(processes, initializer, initargs)\n",
    "    122 def Pool(processes=None, initializer=None, initargs=()):\n",
    "    123     from ..pool import ThreadPool\n",
    "--> 124     return ThreadPool(processes, initializer, initargs)\n",
    "    125 \n",
    "    126 JoinableQueue = Queue\n",
    "\n",
    "/opt/conda/lib/python3.7/multiprocessing/pool.py in __init__(self, processes, initializer, initargs)\n",
    "    800 \n",
    "    801     def __init__(self, processes=None, initializer=None, initargs=()):\n",
    "--> 802         Pool.__init__(self, processes, initializer, initargs)\n",
    "    803 \n",
    "    804     def _setup_queues(self):\n",
    "\n",
    "/opt/conda/lib/python3.7/multiprocessing/pool.py in __init__(self, processes, initializer, initargs, maxtasksperchild, context)\n",
    "    174         self._processes = processes\n",
    "    175         self._pool = []\n",
    "--> 176         self._repopulate_pool()\n",
    "    177 \n",
    "    178         self._worker_handler = threading.Thread(\n",
    "\n",
    "/opt/conda/lib/python3.7/multiprocessing/pool.py in _repopulate_pool(self)\n",
    "    239             w.name = w.name.replace('Process', 'PoolWorker')\n",
    "    240             w.daemon = True\n",
    "--> 241             w.start()\n",
    "    242             util.debug('added worker')\n",
    "    243 \n",
    "\n",
    "/opt/conda/lib/python3.7/multiprocessing/dummy/__init__.py in start(self)\n",
    "     49         if hasattr(self._parent, '_children'):\n",
    "     50             self._parent._children[self] = None\n",
    "---> 51         threading.Thread.start(self)\n",
    "     52 \n",
    "     53     @property\n",
    "\n",
    "/opt/conda/lib/python3.7/threading.py in start(self)\n",
    "    850             _limbo[self] = self\n",
    "    851         try:\n",
    "--> 852             _start_new_thread(self._bootstrap, ())\n",
    "    853         except Exception:\n",
    "    854             with _active_limbo_lock:\n",
    "\n",
    "RuntimeError: can't start new thread\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c13692c7-2edd-4cae-8a6b-5a70526e7ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total images:[24994]\n",
      "304\n",
      "608\n",
      "912\n",
      "1200\n",
      "1504\n",
      "1808\n",
      "2112\n",
      "2400\n",
      "2704\n",
      "3008\n",
      "3312\n",
      "3600\n",
      "3904\n",
      "4208\n",
      "4512\n",
      "4800\n",
      "5104\n",
      "5408\n",
      "5712\n",
      "6000\n",
      "6304\n",
      "6608\n",
      "6912\n",
      "7200\n",
      "7504\n",
      "7808\n",
      "8112\n",
      "8400\n",
      "8704\n",
      "9008\n",
      "9312\n",
      "9600\n",
      "9904\n",
      "10208\n",
      "10512\n",
      "10800\n",
      "11104\n",
      "11408\n",
      "11712\n",
      "12000\n",
      "12304\n",
      "12608\n",
      "12912\n",
      "13200\n",
      "13504\n",
      "13808\n",
      "14112\n",
      "14400\n",
      "14704\n",
      "15008\n",
      "15312\n",
      "15600\n",
      "15904\n",
      "16208\n",
      "16512\n",
      "16800\n",
      "17104\n",
      "17408\n",
      "17712\n",
      "18000\n",
      "18304\n",
      "18608\n",
      "18912\n",
      "19200\n",
      "19504\n",
      "19808\n",
      "20112\n",
      "20400\n",
      "20704\n",
      "21008\n",
      "21312\n",
      "21600\n",
      "21904\n",
      "22208\n",
      "22512\n",
      "22800\n",
      "23104\n",
      "23408\n",
      "23712\n",
      "24000\n",
      "24304\n",
      "24608\n",
      "24912\n",
      "done [./data/unsplush]\n"
     ]
    }
   ],
   "source": [
    "if DO_DOWNLOAD:\n",
    "    data_dir=data_dir=os.path.join(DATA_DIR, 'unsplush')\n",
    "    mkdir(data_dir)\n",
    "\n",
    "    download(df=dataframes[0], data_dir=data_dir, fetch_fn=get_fetch_url(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08704261-5965-4cb2-93be-e781ed196c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total images:[8170]\n",
      "304\n",
      "608\n",
      "912\n",
      "1200\n",
      "1504\n",
      "1808\n",
      "2112\n",
      "2400\n",
      "2704\n",
      "3008\n",
      "3312\n",
      "3600\n",
      "3904\n",
      "4208\n",
      "4512\n",
      "4800\n",
      "5104\n",
      "5408\n",
      "5712\n",
      "6000\n",
      "6304\n",
      "6608\n",
      "6912\n",
      "7200\n",
      "7504\n",
      "7808\n",
      "8112\n",
      "done [./data/movies]\n"
     ]
    }
   ],
   "source": [
    "if DO_DOWNLOAD:\n",
    "    data_dir=os.path.join(DATA_DIR, 'movies')\n",
    "    mkdir(data_dir)\n",
    "\n",
    "    download(df=dataframes[1], data_dir=data_dir, fetch_fn=get_fetch_url(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9b249-dbd7-4a0b-aee4-7baaf6c33d55",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cf0c4c9-0a17-4a55-af26-466295c03b43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", from_tf=True)\n",
    "model = CLIPModel.from_pretrained(MODEL_NAME)\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4705b4f-d332-41eb-b088-20b530429099",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEVICE_IS_CUDA:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c7cb2a",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "\n",
    "* [compute_CLIP_embeddings.ipynb](https://github.com/vivien000/clip-demo/blob/master/compute_CLIP_embeddings.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93ecab67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def compute_text_embeddings(\n",
    "    queries: List[str], model: torch.nn.Module, device: torch.device\n",
    "):\n",
    "    inputs = processor(text=queries, return_tensors=\"pt\", padding=True)\n",
    "    inputs.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = model.get_text_features(**inputs)\n",
    "        return model.get_text_features(**inputs)\n",
    "\n",
    "    return features / features.norm(dim=-1, keepdim=True)\n",
    "    # return features\n",
    "\n",
    "\n",
    "def compute_image_embeddings(\n",
    "    images: List[np.ndarray], model: torch.nn.Module, device: torch.device\n",
    "):\n",
    "    processed = processor(images=images, return_tensors=\"pt\", padding=True)\n",
    "    processed['pixel_values'] = processed['pixel_values'].to(device)\n",
    "    \n",
    "    # return model.get_image_features(**processor(images=list_of_images, return_tensors=\"pt\", padding=True))    \n",
    "    # return model.get_image_features(**processed)\n",
    "    with torch.no_grad():\n",
    "        features = model.get_image_features(**processed)\n",
    "\n",
    "    return features / features.norm(dim=-1, keepdim=True)\n",
    "    # return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7800cffa-2c46-431e-a502-3c9685c00002",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def path_to_file(position, index) -> str:\n",
    "    return os.path.join(\n",
    "        image_dir,\n",
    "        str(position + index) + '.jpeg'\n",
    "    )\n",
    "\n",
    "def run_image_embeddings(\n",
    "    df: pd.DataFrame,\n",
    "    image_dir:str, \n",
    "    model: torch.nn.Module, \n",
    "    device: torch.device,\n",
    "    path_to_embedding_file:str,\n",
    "    batch_size: int = BATCH_SIZE\n",
    "):\n",
    "    files: List[str] = glob.glob(os.path.join(image_dir, '*.jpeg'))\n",
    "    # num_files: int = len(files)\n",
    "    length: int = len(df)\n",
    "    assert length > 0\n",
    "\n",
    "    image_embeddings: np.ndarray = None\n",
    "    position :int = 0      # Current position \n",
    "    checkpoint_size: int = 1000\n",
    "    \n",
    "    while position < length:\n",
    "        num_images = min(batch_size, length - position)\n",
    "        images: List[np.ndarray] = [\n",
    "            load_image(path_to_file=path_to_file(position=position, index=index)) \n",
    "            for index in range(num_images)\n",
    "            # if is_file(path=path_to_file(position=position, index=index))\n",
    "        ]\n",
    "        assert len(images) > 0 and len(images) == num_images, \\\n",
    "            f\"expected [{num_images}], got [{len(images)}]\"\n",
    "\n",
    "        batch_embeddings = compute_image_embeddings(images=images, model=model, device=device)\n",
    "        \n",
    "        if device.type == DEVICE_CUDA:\n",
    "            batch_embeddings = batch_embeddings.cpu()\n",
    "\n",
    "        batch_embeddings = batch_embeddings.detach().numpy()\n",
    "\n",
    "        if image_embeddings is None:\n",
    "            image_embeddings = batch_embeddings\n",
    "        else:\n",
    "            image_embeddings = np.vstack((image_embeddings, batch_embeddings))\n",
    "\n",
    "        position += num_images\n",
    "        assert position == image_embeddings.shape[0], \\\n",
    "            f\"expected position:[{position}] >= size of embeddings:[{image_embeddings.shape[0]}]\"\n",
    "        \n",
    "        # Save the current embeddings\n",
    "        if position // checkpoint_size > 0:\n",
    "            checkpoint_size += 1000\n",
    "            np.save(path_to_embedding_file, image_embeddings)\n",
    "            print(position)\n",
    "\n",
    "    np.save(path_to_embedding_file, image_embeddings)\n",
    "    print(f\"done: {image_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b807d11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "2048\n",
      "3072\n",
      "4096\n",
      "5120\n",
      "6144\n",
      "7168\n",
      "8192\n",
      "9216\n",
      "10240\n",
      "11008\n",
      "12032\n",
      "13056\n",
      "14080\n",
      "15104\n",
      "16128\n",
      "17152\n",
      "18176\n",
      "19200\n",
      "20224\n",
      "21248\n",
      "22016\n",
      "23040\n",
      "24064\n",
      "done: ./data/unsplush\n",
      "1024\n",
      "2048\n",
      "3072\n",
      "4096\n",
      "5120\n",
      "6144\n",
      "7168\n",
      "8170\n",
      "done: ./data/movies\n"
     ]
    }
   ],
   "source": [
    "if DO_EMBEDDING:\n",
    "    for index, name in enumerate(['unsplush', 'movies']):\n",
    "        df = dataframes[index]\n",
    "\n",
    "        image_dir: str = os.path.join(DATA_DIR, name)\n",
    "        path_to_embedding_file: str = f\"embedding_{name}.npy\"\n",
    "        run_image_embeddings(\n",
    "            df=df,\n",
    "            image_dir=image_dir, \n",
    "            path_to_embedding_file=path_to_embedding_file, \n",
    "            model=model,\n",
    "            device=DEVICE\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63386fd",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00765492-ce49-4a6c-89d6-c61033d64cc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = {\n",
    "    0: np.load(\"embedding_unsplush.npy\"), \n",
    "    1: np.load(\"embedding_movies.npy\")\n",
    "}\n",
    "\n",
    "source = {\n",
    "    0: '\\nSource: Unsplash', \n",
    "    1: '\\nSource: The Movie Database (TMDB)'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79bc0621-3483-4924-9612-a151a03891e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24994, 512)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d745e4a5-367a-4ccd-a609-4f932ef394d2",
   "metadata": {},
   "source": [
    "# Image Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53783acf-b93d-48a3-8109-beb59bc8608f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_html(url_list, height=200):\n",
    "    html = \"<div style='margin-top: 20px; display: flex; flex-wrap: wrap; justify-content: space-evenly'>\"\n",
    "    for url, title, link in url_list:\n",
    "        html2 = f\"<img title='{title}' style='height: {height}px; margin-bottom: 10px' src='{url}'>\"\n",
    "        if len(link) > 0:\n",
    "            html2 = f\"<a href='{link}' target='_blank'>\" + html2 + \"</a>\"\n",
    "        html = html + html2\n",
    "    html += \"</div>\"\n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5e7442-1c45-4bc9-9b32-1588d2317de5",
   "metadata": {},
   "source": [
    "# Image Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3a37d34-76b3-47b7-970d-ae34cc67bb98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24275cd90b74c5aac16f96697a66107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', layout=Layout(width='400px')), Button(description='Search', style=ButtonStyle())â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ee1da95ba04bc5a8203ed30ae80b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = widgets.Text(layout=widgets.Layout(width='400px'))\n",
    "dataset =widgets.Dropdown(\n",
    "    options=['Unsplash', 'Movies'],\n",
    "    value='Unsplash'\n",
    ")\n",
    "button = widgets.Button(description=\"Search\")\n",
    "output = widgets.Output()\n",
    "\n",
    "display(\n",
    "    widgets.HBox(\n",
    "        [query, button, dataset],\n",
    "        layout=widgets.Layout(justify_content='center')\n",
    "    ),\n",
    "    output\n",
    ")\n",
    "\n",
    "def image_search(\n",
    "    query: str, model: torch.nn.Module, device:torch.device, n_results: int = 15\n",
    "):\n",
    "    text_embeddings = compute_text_embeddings(queries=[query], model=model, device=device)\n",
    "    if device.type == DEVICE_CUDA:\n",
    "        text_embeddings = text_embeddings.cpu()\n",
    "        \n",
    "    text_embeddings = text_embeddings.detach().numpy()\n",
    "    \n",
    "    k = 0 if dataset.value == 'Unsplash' else 1\n",
    "    results = np.argsort((embeddings[k] @ text_embeddings.T)[:, 0])[-1:-n_results-1:-1]\n",
    "    return [\n",
    "        (dataframes[k].iloc[i]['path'], dataframes[k].iloc[i]['tooltip'] + source[k], dataframes[k].iloc[i]['link']) \n",
    "        for i in results\n",
    "    ]\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    if len(query.value) > 0:\n",
    "        results = image_search(query=query.value, model=model, device=DEVICE)\n",
    "        output.clear_output()\n",
    "        with output:\n",
    "            display(HTML(get_html(results)))\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "dataset.observe(on_button_clicked, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228268ae-12ea-4f74-9854-31e22ecbf7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.g4dn.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-2:452832661640:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
