{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b18fb3",
   "metadata": {},
   "source": [
    "# Open AI\n",
    "\n",
    "## Start Here\n",
    "\n",
    "\n",
    "* [How to format inputs to ChatGPT models](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb)\n",
    "\n",
    "* [How to work with large language models](https://github.com/openai/openai-cookbook/blob/main/how_to_work_with_large_language_models.md)\n",
    "\n",
    "* [Techniques to improve reliability](https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md)\n",
    "\n",
    "> When GPT-3 fails on a task, what should you do?\n",
    "\n",
    "* [Unit test writing using a multi-step prompt](https://github.com/openai/openai-cookbook/blob/main/examples/Unit_test_writing_using_a_multi-step_prompt.ipynb)\n",
    "\n",
    "> Complex tasks, such as writing unit tests, can benefit from multi-step prompts. In contrast to a single prompt, a multi-step prompt generates text from GPT and then feeds that output text back into subsequent prompts. This can help in cases where you want GPT to reason things out before answering, or brainstorm a plan before executing it.\n",
    "> \n",
    "> In this notebook, we use a 3-step prompt to write unit tests in Python using the following steps:\n",
    "> 1. Explain: Given a Python function, we ask GPT to explain what the function is doing and why.\n",
    "> 2. Plan: We ask GPT to plan a set of unit tests for the function.\n",
    ">    If the plan is too short, we ask GPT to elaborate with more ideas for unit tests.\n",
    "> 3. Execute: Finally, we instruct GPT to write unit tests that cover the planned cases.\n",
    "\n",
    "* [How to stream completions](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb)\n",
    "\n",
    "> By default, when you request a completion from the OpenAI, the entire completion is generated before being sent back in a single response. If you're generating long completions, waiting for the response can take many seconds.  \n",
    "> \n",
    "> To get responses sooner, you can 'stream' the completion as it's being generated. This allows you to start printing or processing the beginning of the completion before the full completion is finished. To stream completions, set ```stream=True``` when calling the chat completions or completions endpoints. This will return an object that streams back the response as data-only server-sent events. Extract chunks from the delta field rather than the message field.\n",
    "\n",
    "## References\n",
    "\n",
    "* [OpenAI Cookbook](https://github.com/openai/openai-cookbook)\n",
    "\n",
    "> The OpenAI Cookbook shares example code for accomplishing common tasks with the OpenAI API.\n",
    "\n",
    "* [Open AI Playground](https://platform.openai.com/playground)\n",
    "\n",
    "<img src=\"./image/openai_playground.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7351e32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
